{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "funky-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "close-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"content.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "helpful-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_to_index = {}\n",
    "for index in range(len(data)):\n",
    "    paper_to_index[data[index][\"id\"]] = index\n",
    "# paper_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "essential-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrice = np.zeros((len(data), len(data)))\n",
    "for i in range(len(data)):\n",
    "    refs = data[i][\"references\"]\n",
    "    for ref in refs:\n",
    "        j = paper_to_index.get(ref, -1)\n",
    "        if j != -1:\n",
    "            adj_matrice[i, j] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "settled-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pageRank(adj_matrice, alpha):\n",
    "    P = adj_matrice.copy()\n",
    "    V = np.zeros(len(P[0]))\n",
    "    V.fill(1 / len(P[0]))\n",
    "    \n",
    "    for i in range(len(P)):\n",
    "        if sum(P[i]) == 0:\n",
    "            P[i] = V\n",
    "        else:\n",
    "            P[i] = (1 - alpha) * P[i] + alpha * V\n",
    "    res = V.reshape((1, len(V))).dot(P)\n",
    "    for i in range(10):\n",
    "        res = res.dot(P)\n",
    "    \n",
    "    return dict([(i, res[0, paper_to_index[i]]) for i in paper_to_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "tropical-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = cal_pageRank(adj_matrice, 0.5)\n",
    "# P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "encouraging-herald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '2981549002',\n",
       "  'title': 'Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes.',\n",
       "  'abstract': 'Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at this http URL.',\n",
       "  'date': '2019',\n",
       "  'authors': ['Greg Yang'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Multilayer perceptron',\n",
       "   'Gaussian process',\n",
       "   'Normalization (statistics)',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Tensor',\n",
       "   'Algorithm',\n",
       "   'Feed forward',\n",
       "   'Computation',\n",
       "   'Pooling',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '27',\n",
       "  'reference_count': '55',\n",
       "  'references': ['2194775991',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2964308564',\n",
       "   '2963446712',\n",
       "   '1677182931',\n",
       "   '2157331557',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '1533861849']},\n",
       " {'id': '3105081694',\n",
       "  'title': 'COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images.',\n",
       "  'abstract': \"The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Linda Wang', 'Zhong Qiu Lin', 'Alexander Wong'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Image processing',\n",
       "   'Benchmark (computing)',\n",
       "   'Machine learning',\n",
       "   'Key (cryptography)',\n",
       "   'Computer science',\n",
       "   'Radiography',\n",
       "   'Artificial intelligence',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Tomography x ray computed',\n",
       "   'X ray image',\n",
       "   'View Less'],\n",
       "  'citation_count': '704',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2194775991',\n",
       "   '3001118548',\n",
       "   '2962835968',\n",
       "   '3008827533',\n",
       "   '2919115771',\n",
       "   '2108598243',\n",
       "   '2963446712',\n",
       "   '3007497549',\n",
       "   '3010604545',\n",
       "   '3008985036']},\n",
       " {'id': '2950893734',\n",
       "  'title': 'Self-Attention Generative Adversarial Networks',\n",
       "  'abstract': 'In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Han Zhang',\n",
       "   'Ian Goodfellow',\n",
       "   'Dimitris Metaxas',\n",
       "   'Augustus Odena'],\n",
       "  'related_topics': ['Boosting (machine learning)',\n",
       "   'Visualization',\n",
       "   'Normalization (statistics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Generative grammar',\n",
       "   'Adversarial system',\n",
       "   'Artificial intelligence',\n",
       "   'Self attention',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,493',\n",
       "  'reference_count': '52',\n",
       "  'references': ['2964121744',\n",
       "   '2963403868',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '2964308564',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2963373786',\n",
       "   '2963470893']},\n",
       " {'id': '3119786062',\n",
       "  'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
       "  'abstract': 'While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.',\n",
       "  'date': '2021',\n",
       "  'authors': ['Alexey Dosovitskiy',\n",
       "   'Lucas Beyer',\n",
       "   'Alexander Kolesnikov',\n",
       "   'Dirk Weissenborn',\n",
       "   'Xiaohua Zhai',\n",
       "   'Thomas Unterthiner',\n",
       "   'Mostafa Dehghani',\n",
       "   'Matthias Minderer',\n",
       "   'Georg Heigold',\n",
       "   'Sylvain Gelly',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Neil Houlsby'],\n",
       "  'related_topics': ['Transformer (machine learning model)',\n",
       "   'Contextual image classification',\n",
       "   'Computer vision',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Scale (chemistry)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Conjunction (grammar)',\n",
       "   'Artificial intelligence',\n",
       "   'Self attention',\n",
       "   'View Less'],\n",
       "  'citation_count': '290',\n",
       "  'reference_count': '49',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2964121744',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2108598243',\n",
       "   '3118608800',\n",
       "   '2963091558',\n",
       "   '3034978746']},\n",
       " {'id': '2145339207',\n",
       "  'title': 'Human-level control through deep reinforcement learning',\n",
       "  'abstract': 'The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Volodymyr Mnih',\n",
       "   'Koray Kavukcuoglu',\n",
       "   'David Silver',\n",
       "   'Andrei A. Rusu',\n",
       "   'Joel Veness',\n",
       "   'Marc G. Bellemare',\n",
       "   'Alex Graves',\n",
       "   'Martin Riedmiller',\n",
       "   'Andreas K. Fidjeland',\n",
       "   'Georg Ostrovski',\n",
       "   'Stig Petersen',\n",
       "   'Charles Beattie',\n",
       "   'Amir Sadik',\n",
       "   'Ioannis Antonoglou',\n",
       "   'Helen King',\n",
       "   'Dharshan Kumaran',\n",
       "   'Daan Wierstra',\n",
       "   'Shane Legg',\n",
       "   'Demis Hassabis'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Temporal difference learning',\n",
       "   'Artificial neural network',\n",
       "   'General video game playing',\n",
       "   'Set (psychology)',\n",
       "   'Sensory processing',\n",
       "   'Reinforcement',\n",
       "   'Artificial intelligence',\n",
       "   'Variety (cybernetics)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,732',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2618530766',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2121863487',\n",
       "   '2952509347',\n",
       "   '1652505363']},\n",
       " {'id': '2153579005',\n",
       "  'title': 'Distributed Representations of Words and Phrases and their Compositionality',\n",
       "  'abstract': 'The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.\\r\\n\\r\\nAn inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Tomas Mikolov',\n",
       "   'Ilya Sutskever',\n",
       "   'Kai Chen',\n",
       "   'Greg S Corrado',\n",
       "   'Jeff Dean'],\n",
       "  'related_topics': ['Word2vec',\n",
       "   'Word embedding',\n",
       "   'Word order',\n",
       "   'Word (computer architecture)',\n",
       "   'Principle of compositionality',\n",
       "   'Softmax function',\n",
       "   'Syntax',\n",
       "   'Distributional semantics',\n",
       "   'Simple (philosophy)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '25,751',\n",
       "  'reference_count': '19',\n",
       "  'references': ['1614298861',\n",
       "   '2141599568',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '1498436455',\n",
       "   '1662133657',\n",
       "   '1889268436',\n",
       "   '2131462252']},\n",
       " {'id': '2194775991',\n",
       "  'title': 'Deep Residual Learning for Image Recognition',\n",
       "  'abstract': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Residual',\n",
       "   'Convolutional neural network',\n",
       "   'Feature learning',\n",
       "   'Vanishing gradient problem',\n",
       "   'Artificial neural network',\n",
       "   'MNIST database',\n",
       "   'Object detection',\n",
       "   'Test set',\n",
       "   'Transfer of learning',\n",
       "   'Softmax function',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Artificial intelligence',\n",
       "   'Residual neural network',\n",
       "   'View Less'],\n",
       "  'citation_count': '77,931',\n",
       "  'reference_count': '52',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1536680647']},\n",
       " {'id': '2963403868',\n",
       "  'title': 'Attention is All You Need',\n",
       "  'abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Ashish Vaswani',\n",
       "   'Noam Shazeer',\n",
       "   'Niki Parmar',\n",
       "   'Jakob Uszkoreit',\n",
       "   'Llion Jones',\n",
       "   'Aidan N. Gomez',\n",
       "   'Lukasz Kaiser',\n",
       "   'Illia Polosukhin'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Encoder',\n",
       "   'BLEU',\n",
       "   'Speech translation',\n",
       "   'Artificial neural network',\n",
       "   'Transduction (machine learning)',\n",
       "   'Byte pair encoding',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)',\n",
       "   'View Less'],\n",
       "  'citation_count': '18,855',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1836465849',\n",
       "  'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "  'abstract': \"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.\",\n",
       "  'date': '2015',\n",
       "  'authors': ['Sergey Ioffe', 'Christian Szegedy'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Initialization',\n",
       "   'Contextual image classification',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Covariate shift',\n",
       "   'Weight decay',\n",
       "   'View Less'],\n",
       "  'citation_count': '25,868',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2097117768',\n",
       "   '2117539524',\n",
       "   '2095705004',\n",
       "   '1677182931',\n",
       "   '2146502635',\n",
       "   '2310919327',\n",
       "   '1665214252',\n",
       "   '2168231600',\n",
       "   '1533861849',\n",
       "   '104184427']},\n",
       " {'id': '2964308564',\n",
       "  'title': 'Neural Machine Translation by Jointly Learning to Align and Translate',\n",
       "  'abstract': 'Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Dzmitry Bahdanau', 'Kyunghyun Cho', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Artificial neural network',\n",
       "   'Phrase',\n",
       "   'Sentence',\n",
       "   'Encoder',\n",
       "   'Artificial intelligence',\n",
       "   'Bottleneck',\n",
       "   'Byte pair encoding',\n",
       "   'Computer science',\n",
       "   'Closed captioning',\n",
       "   'View Less'],\n",
       "  'citation_count': '17,406',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2157331557',\n",
       "   '2064675550',\n",
       "   '6908809',\n",
       "   '2132339004',\n",
       "   '1753482797',\n",
       "   '2294059674',\n",
       "   '1810943226',\n",
       "   '2964199361',\n",
       "   '1815076433',\n",
       "   '2153653739']},\n",
       " {'id': '2963446712',\n",
       "  'title': 'Densely Connected Convolutional Networks',\n",
       "  'abstract': 'Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Gao Huang',\n",
       "   'Zhuang Liu',\n",
       "   'Laurens van der Maaten',\n",
       "   'Kilian Q. Weinberger'],\n",
       "  'related_topics': ['Convolutional code',\n",
       "   'Network architecture',\n",
       "   'Vanishing gradient problem',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Benchmark (computing)',\n",
       "   'Artificial neural network',\n",
       "   'Code (cryptography)',\n",
       "   'Feature (machine learning)',\n",
       "   'Parallel computing',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Residual neural network',\n",
       "   'View Less'],\n",
       "  'citation_count': '16,054',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '1677182931',\n",
       "   '2183341477']},\n",
       " {'id': '1677182931',\n",
       "  'title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "  'abstract': 'Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],\n",
       "  'related_topics': ['Rectifier (neural networks)',\n",
       "   'Initialization',\n",
       "   'Overfitting',\n",
       "   'Artificial neural network',\n",
       "   'Contextual image classification',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,535',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '1536680647']},\n",
       " {'id': '2157331557',\n",
       "  'title': 'Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation',\n",
       "  'abstract': 'In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Kyunghyun Cho',\n",
       "   'Bart van Merrienboer',\n",
       "   'Caglar Gulcehre',\n",
       "   'Dzmitry Bahdanau',\n",
       "   'Fethi Bougares',\n",
       "   'Holger Schwenk',\n",
       "   'Yoshua Bengio',\n",
       "   '',\n",
       "   ''],\n",
       "  'related_topics': ['Encoder',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Machine translation',\n",
       "   'Phrase',\n",
       "   'Feature (machine learning)',\n",
       "   'Conditional probability',\n",
       "   'Sequence',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '13,344',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2618530766',\n",
       "   '2153579005',\n",
       "   '2064675550',\n",
       "   '2147768505',\n",
       "   '6908809',\n",
       "   '2132339004',\n",
       "   '1753482797',\n",
       "   '2294059674',\n",
       "   '2156387975',\n",
       "   '2963504252']},\n",
       " {'id': '2310919327',\n",
       "  'title': 'Gradient-based learning applied to document recognition',\n",
       "  'abstract': 'Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Yann Lecun',\n",
       "   'Leon Bottou',\n",
       "   '',\n",
       "   'Yoshua Bengio',\n",
       "   '',\n",
       "   '',\n",
       "   'Patrick Haffner',\n",
       "   ''],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Convolutional neural network',\n",
       "   'Handwriting recognition',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Language model',\n",
       "   'Graph (abstract data type)',\n",
       "   'Decision boundary',\n",
       "   'Network architecture',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '37,429',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2064675550',\n",
       "  'title': 'Long short-term memory',\n",
       "  'abstract': \"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.\",\n",
       "  'date': '1997',\n",
       "  'authors': ['Sepp Hochreiter', 'Jürgen Schmidhuber'],\n",
       "  'related_topics': ['Vanishing gradient problem',\n",
       "   'Backpropagation through time',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Time complexity',\n",
       "   'Chunking (psychology)',\n",
       "   'Computational complexity theory',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '48,582',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2107878631',\n",
       "   '2128499899',\n",
       "   '2007431958',\n",
       "   '194249466',\n",
       "   '2123716044',\n",
       "   '2143503258',\n",
       "   '2154890045',\n",
       "   '2103452139',\n",
       "   '2048060899',\n",
       "   '1674799117']},\n",
       " {'id': '1533861849',\n",
       "  'title': 'Understanding the difficulty of training deep feedforward neural networks',\n",
       "  'abstract': 'Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).',\n",
       "  'date': '2010',\n",
       "  'authors': ['Xavier Glorot', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Vanishing gradient problem',\n",
       "   'Initialization',\n",
       "   'Artificial neural network',\n",
       "   'Feedforward neural network',\n",
       "   'Activation function',\n",
       "   'Gradient descent',\n",
       "   'Feature (machine learning)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,845',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '1498436455',\n",
       "   '1994197834',\n",
       "   '2131462252',\n",
       "   '2172174689']},\n",
       " {'id': '3001118548',\n",
       "  'title': 'Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China',\n",
       "  'abstract': 'A recent cluster of pneumonia cases in Wuhan, China, was caused by a novel betacoronavirus, the 2019 novel coronavirus (2019-nCoV). We report the epidemiological, clinical, laboratory, and radiological characteristics and treatment and clinical outcomes of these patients. All patients with suspected 2019-nCoV were admitted to a designated hospital in Wuhan. We prospectively collected and analysed data on patients with laboratory-confirmed 2019-nCoV infection by real-time RT-PCR and next-generation sequencing. Data were obtained with standardised data collection forms shared by the International Severe Acute Respiratory and Emerging Infection Consortium from electronic medical records. Researchers also directly communicated with patients or their families to ascertain epidemiological and symptom data. Outcomes were also compared between patients who had been admitted to the intensive care unit (ICU) and those who had not.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Chaolin Huang',\n",
       "   'Yeming Wang',\n",
       "   'Xingwang Li',\n",
       "   'Lili Ren',\n",
       "   'Jianping Zhao',\n",
       "   'Yi Hu',\n",
       "   'Li Zhang',\n",
       "   'Guohui Fan',\n",
       "   'Jiuyang Xu',\n",
       "   'Xiaoying Gu',\n",
       "   'Zhenshun Cheng',\n",
       "   'Ting Yu',\n",
       "   'Jiaan Xia',\n",
       "   'Yuan Wei',\n",
       "   'Wenjuan Wu',\n",
       "   'Xuelei Xie',\n",
       "   'Wen Yin',\n",
       "   'Hui Li',\n",
       "   'Min Liu',\n",
       "   'Yan Xiao',\n",
       "   'Hong Gao',\n",
       "   'Li Guo',\n",
       "   'Jungang Xie',\n",
       "   'Guangfa Wang',\n",
       "   'Rongmeng Jiang',\n",
       "   'Zhancheng Gao',\n",
       "   'Qi Jin',\n",
       "   'Jianwei Wang',\n",
       "   'Bin Cao'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Intensive care unit',\n",
       "   'Viral pneumonia',\n",
       "   'Medical record',\n",
       "   'Epidemiology',\n",
       "   'Pneumonia',\n",
       "   'Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '26,659',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '3000413850',\n",
       "   '2132260239',\n",
       "   '2026274122',\n",
       "   '2104548316',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '3017468735',\n",
       "   '2725497285']},\n",
       " {'id': '2962835968',\n",
       "  'title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "  'abstract': 'Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Karen Simonyan', 'Andrew Zisserman'],\n",
       "  'related_topics': ['Convolution',\n",
       "   'Computer vision',\n",
       "   'Scale (map)',\n",
       "   'Computer science',\n",
       "   'Basis (linear algebra)',\n",
       "   'Architecture',\n",
       "   'Artificial intelligence',\n",
       "   'Crowd counting',\n",
       "   'Region proposal',\n",
       "   'Salient object detection',\n",
       "   'Semantic image segmentation',\n",
       "   'View Less'],\n",
       "  'citation_count': '83,614',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3008827533',\n",
       "  'title': 'Clinical Characteristics of Coronavirus Disease 2019 in China',\n",
       "  'abstract': 'Abstract Background Since December 2019, when coronavirus disease 2019 (Covid-19) emerged in Wuhan city and rapidly spread throughout China, data have been needed on the clinical characteristics of...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Wei-jie Guan',\n",
       "   'Zheng-yi Ni',\n",
       "   'Yu Hu',\n",
       "   'Wenhua Liang',\n",
       "   'Chun-quan Ou',\n",
       "   'Jianxing He',\n",
       "   'Lei Liu',\n",
       "   'Hong Shan',\n",
       "   'Chunliang Lei',\n",
       "   'David S.C. Hui',\n",
       "   'Bin Du',\n",
       "   'Lan-juan Li',\n",
       "   'Guang Zeng',\n",
       "   'Kwok-Yung Yuen',\n",
       "   'Ruchong Chen',\n",
       "   'Chun-Li Tang',\n",
       "   'Tao Wang',\n",
       "   'Ping-yan Chen',\n",
       "   'Jie Xiang',\n",
       "   'Shiyue Li',\n",
       "   'Jinlin Wang',\n",
       "   'Zi Jing Liang',\n",
       "   'Yi-xiang Peng',\n",
       "   'Li Wei',\n",
       "   'Yong Liu',\n",
       "   'Ya-hua Hu',\n",
       "   'Peng Peng',\n",
       "   'Jian-ming Wang',\n",
       "   'Ji-yang Liu',\n",
       "   'Zhong Chen',\n",
       "   'Gang Li',\n",
       "   'Zhi-jian Zheng',\n",
       "   'Shao-qin Qiu',\n",
       "   'Jie Luo',\n",
       "   'Chang-jiang Ye',\n",
       "   'Shao-yong Zhu',\n",
       "   'Nanshan Zhong'],\n",
       "  'related_topics': ['Pandemic',\n",
       "   'Betacoronavirus',\n",
       "   'Viral Epidemiology',\n",
       "   'China',\n",
       "   'Environmental health',\n",
       "   'Medicine',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral therapy',\n",
       "   'View Less'],\n",
       "  'citation_count': '17,855',\n",
       "  'reference_count': '19',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '3004239190',\n",
       "   '3003573988']},\n",
       " {'id': '2919115771',\n",
       "  'title': 'Deep learning',\n",
       "  'abstract': 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Yann LeCun', '', 'Yoshua Bengio', 'Geoffrey Hinton', ''],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Backpropagation',\n",
       "   'Representation (systemics)',\n",
       "   'Computational model',\n",
       "   'Speech recognition',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '39,561',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2145339207',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2064675550',\n",
       "   '2163922914',\n",
       "   '2160815625',\n",
       "   '2022508996',\n",
       "   '2025768430',\n",
       "   '1993882792']},\n",
       " {'id': '2108598243',\n",
       "  'title': 'ImageNet: A large-scale hierarchical image database',\n",
       "  'abstract': 'The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Jia Deng',\n",
       "   'Wei Dong',\n",
       "   'Richard Socher',\n",
       "   'Li-Jia Li',\n",
       "   'Kai Li',\n",
       "   'Li Fei-Fei'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Image retrieval',\n",
       "   'Contextual image classification',\n",
       "   'Ontology (information science)',\n",
       "   'Cluster analysis',\n",
       "   'Information retrieval',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Ontology',\n",
       "   'The Internet',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '28,435',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2151103935',\n",
       "   '2038721957',\n",
       "   '2128017662',\n",
       "   '2110764733',\n",
       "   '1782590233',\n",
       "   '1576445103',\n",
       "   '2145607950',\n",
       "   '2141282920',\n",
       "   '2115733720',\n",
       "   '1528789833']},\n",
       " {'id': '3007497549',\n",
       "  'title': 'Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases.',\n",
       "  'abstract': 'Chest CT had higher sensitivity for diagnosis of COVID-19 as compared with initial reverse-transcription polymerase chain reaction from swab samples in the epidemic area of China.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Tao Ai',\n",
       "   'Zhenlu Yang',\n",
       "   'Hongyan Hou',\n",
       "   'Chenao Zhan',\n",
       "   'Chong Chen',\n",
       "   'Wenzhi Lv',\n",
       "   'Qian Tao',\n",
       "   'Ziyong Sun',\n",
       "   'Liming Xia'],\n",
       "  'related_topics': ['Real-time polymerase chain reaction',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Pneumonia',\n",
       "   'Polymerase chain reaction',\n",
       "   'Radiology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Follow up studies',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,787',\n",
       "  'reference_count': '9',\n",
       "  'references': ['3001118548',\n",
       "   '3008818676',\n",
       "   '3004906315',\n",
       "   '3006643024',\n",
       "   '3006110666',\n",
       "   '3006354146',\n",
       "   '3003901880',\n",
       "   '3005656138',\n",
       "   '3004511262']},\n",
       " {'id': '3010604545',\n",
       "  'title': 'Detection of SARS-CoV-2 in Different Types of Clinical Specimens.',\n",
       "  'abstract': 'This study describes results of PCR and viral RNA testing for SARS-CoV-2 in bronchoalveolar fluid, sputum, feces, blood, and urine specimens from patients with COVID-19 infection in China to identify possible means of non-respiratory transmission.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Wenling Wang',\n",
       "   'Yanli Xu',\n",
       "   'Ruqin Gao',\n",
       "   'Roujian Lu',\n",
       "   'Kai Han',\n",
       "   'Guizhen Wu',\n",
       "   'Wenjie Tan'],\n",
       "  'related_topics': ['Sputum',\n",
       "   'Viral load',\n",
       "   'Feces',\n",
       "   'Betacoronavirus',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Pneumonia (non-human)',\n",
       "   'Polymerase chain reaction',\n",
       "   'Urine',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,735',\n",
       "  'reference_count': '13',\n",
       "  'references': ['3005079553',\n",
       "   '3008962515',\n",
       "   '3008452791',\n",
       "   '3033453353',\n",
       "   '3034408674',\n",
       "   '3035275617',\n",
       "   '3034059415',\n",
       "   '3033952286',\n",
       "   '3036958556',\n",
       "   '3035464429']},\n",
       " {'id': '3008985036',\n",
       "  'title': 'Sensitivity of Chest CT for COVID-19: Comparison to RT-PCR',\n",
       "  'abstract': 'In a series of 51 patients with chest CT and real-time polymerase chain reaction assay (RT-PCR) performed within 3 days, the sensitivity of CT for 2019 novel coronavirus infection was 98% and that ...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yicheng Fang',\n",
       "   'Huangqi Zhang',\n",
       "   'Jicheng Xie',\n",
       "   'Minjie Lin',\n",
       "   'Lingjun Ying',\n",
       "   'Peipei Pang',\n",
       "   'Wenbin Ji'],\n",
       "  'related_topics': ['Real-time polymerase chain reaction',\n",
       "   'Polymerase chain reaction',\n",
       "   'Pneumonia',\n",
       "   'Nuclear medicine',\n",
       "   'Tomography',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,086',\n",
       "  'reference_count': '7',\n",
       "  'references': ['3002108456',\n",
       "   '3006643024',\n",
       "   '3006110666',\n",
       "   '3028749392',\n",
       "   '3032185657',\n",
       "   '3042098369',\n",
       "   '3037255629']},\n",
       " {'id': '2964121744',\n",
       "  'title': 'Adam: A Method for Stochastic Optimization',\n",
       "  'abstract': 'Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Diederik P. Kingma', 'Jimmy Lei Ba'],\n",
       "  'related_topics': ['Stochastic optimization',\n",
       "   'Convex optimization',\n",
       "   'Rate of convergence',\n",
       "   'Invariant (mathematics)',\n",
       "   'Diagonal',\n",
       "   'Uniform norm',\n",
       "   'Mathematical optimization',\n",
       "   'Byte pair encoding',\n",
       "   'Regret',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '68,547',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2117539524',\n",
       "  'title': 'ImageNet Large Scale Visual Recognition Challenge',\n",
       "  'abstract': 'The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Olga Russakovsky',\n",
       "   'Jia Deng',\n",
       "   'Hao Su',\n",
       "   'Jonathan Krause',\n",
       "   'Sanjeev Satheesh',\n",
       "   'Sean Ma',\n",
       "   'Zhiheng Huang',\n",
       "   'Andrej Karpathy',\n",
       "   'Aditya Khosla',\n",
       "   'Michael Bernstein',\n",
       "   'Alexander C. Berg',\n",
       "   'Li Fei-Fei'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Contextual image classification',\n",
       "   'Object (computer science)',\n",
       "   'Field (computer science)',\n",
       "   'Benchmark (computing)',\n",
       "   'Machine learning',\n",
       "   'Categorical variable',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '23,365',\n",
       "  'reference_count': '97',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '1614298861',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2168356304',\n",
       "   '1849277567']},\n",
       " {'id': '2099471712',\n",
       "  'title': 'Generative Adversarial Nets',\n",
       "  'abstract': 'We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Ian Goodfellow',\n",
       "   'Jean Pouget-Abadie',\n",
       "   'Mehdi Mirza',\n",
       "   'Bing Xu',\n",
       "   'David Warde-Farley',\n",
       "   'Sherjil Ozair',\n",
       "   'Aaron Courville',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Discriminative model',\n",
       "   'Approximate inference',\n",
       "   'Markov chain',\n",
       "   'Minimax',\n",
       "   'Perceptron',\n",
       "   'Backpropagation',\n",
       "   'Sample (statistics)',\n",
       "   'Theoretical computer science',\n",
       "   'Machine learning',\n",
       "   'Image translation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '27,329',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2964153729',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2025768430']},\n",
       " {'id': '2963073614',\n",
       "  'title': 'Image-to-Image Translation with Conditional Adversarial Networks',\n",
       "  'abstract': 'We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Phillip Isola',\n",
       "   'Jun-Yan Zhu',\n",
       "   'Tinghui Zhou',\n",
       "   'Alexei A. Efros'],\n",
       "  'related_topics': ['Image translation',\n",
       "   'Image (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'User interface',\n",
       "   'Translation (geometry)',\n",
       "   'Function (engineering)',\n",
       "   'Image resolution',\n",
       "   'Iterative reconstruction',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,065',\n",
       "  'reference_count': '52',\n",
       "  'references': ['2964121744',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '2133665775',\n",
       "   '2963684088',\n",
       "   '2100495367',\n",
       "   '2340897893']},\n",
       " {'id': '2962793481',\n",
       "  'title': 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks',\n",
       "  'abstract': 'Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Jun-Yan Zhu',\n",
       "   'Taesung Park',\n",
       "   'Phillip Isola',\n",
       "   'Alexei A. Efros'],\n",
       "  'related_topics': ['Image translation',\n",
       "   'Image (category theory)',\n",
       "   'Translation (geometry)',\n",
       "   'Graphics',\n",
       "   'Object (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Domain (software engineering)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,237',\n",
       "  'reference_count': '54',\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '1959608418',\n",
       "   '2963684088',\n",
       "   '2100495367',\n",
       "   '2340897893',\n",
       "   '2963373786']},\n",
       " {'id': '2963684088',\n",
       "  'title': 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks',\n",
       "  'abstract': 'Abstract: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Alec Radford', 'Luke Metz', 'Soumith Chintala'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'Feature learning',\n",
       "   'Artificial intelligence',\n",
       "   'Class (computer programming)',\n",
       "   'Hierarchy',\n",
       "   'Computer science',\n",
       "   'Object (computer science)',\n",
       "   'Generative grammar',\n",
       "   'Generator (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,789',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2963373786',\n",
       "  'title': 'Improved techniques for training GANs',\n",
       "  'abstract': 'We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Tim Salimans',\n",
       "   'Ian Goodfellow',\n",
       "   'Wojciech Zaremba',\n",
       "   'Vicki',\n",
       "   'Alec Radford',\n",
       "   'Xi Chen'],\n",
       "  'related_topics': ['MNIST database',\n",
       "   'Pattern recognition',\n",
       "   'Turing test',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,633',\n",
       "  'reference_count': '20',\n",
       "  'references': ['1836465849',\n",
       "   '2183341477',\n",
       "   '2963684088',\n",
       "   '2964153729',\n",
       "   '2271840356',\n",
       "   '648143168',\n",
       "   '2963685250',\n",
       "   '2949416428',\n",
       "   '830076066',\n",
       "   '1487641199']},\n",
       " {'id': '2963470893',\n",
       "  'title': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network',\n",
       "  'abstract': 'Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Christian Ledig',\n",
       "   'Lucas Theis',\n",
       "   'Ferenc Huszar',\n",
       "   'Jose Caballero',\n",
       "   'Andrew',\n",
       "   'Alejandro Acosta',\n",
       "   'Andrew Aitken',\n",
       "   'Alykhan Tejani',\n",
       "   'Johannes Totz',\n",
       "   'Zehan Wang',\n",
       "   'Wenzhe Shi'],\n",
       "  'related_topics': ['Image texture',\n",
       "   'Image resolution',\n",
       "   'Convolutional neural network',\n",
       "   'Image translation',\n",
       "   'Pixel',\n",
       "   'Iterative reconstruction',\n",
       "   'Network architecture',\n",
       "   'Similarity (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,564',\n",
       "  'reference_count': '68',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '1677182931',\n",
       "   '2133665775']},\n",
       " {'id': '2618530766',\n",
       "  'title': 'ImageNet classification with deep convolutional neural networks',\n",
       "  'abstract': 'We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Alex Krizhevsky', 'Ilya Sutskever', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Dropout (neural networks)',\n",
       "   'Overfitting',\n",
       "   'Artificial neural network',\n",
       "   'Softmax function',\n",
       "   'Test data',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Regularization (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '165,784',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2194775991',\n",
       "   '2097117768',\n",
       "   '2108598243',\n",
       "   '2911964244',\n",
       "   '3118608800',\n",
       "   '1904365287',\n",
       "   '1665214252',\n",
       "   '2546302380',\n",
       "   '2110764733',\n",
       "   '2130325614']},\n",
       " {'id': '2963341956',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'date': '2018',\n",
       "  'authors': ['Jacob Devlin',\n",
       "   'Ming-Wei Chang',\n",
       "   'Kenton Lee',\n",
       "   'Kristina N. Toutanova'],\n",
       "  'related_topics': ['Question answering',\n",
       "   'Language model',\n",
       "   'Natural language understanding',\n",
       "   'Named-entity recognition',\n",
       "   'SemEval',\n",
       "   'Inference',\n",
       "   'Winograd Schema Challenge',\n",
       "   'Sequence labeling',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)',\n",
       "   'View Less'],\n",
       "  'citation_count': '26,395',\n",
       "  'reference_count': '52',\n",
       "  'references': ['2963403868',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2108598243',\n",
       "   '2962739339',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '2117130368',\n",
       "   '2025768430']},\n",
       " {'id': '3118608800',\n",
       "  'title': 'Learning Multiple Layers of Features from Tiny Images',\n",
       "  'abstract': 'In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Alex Krizhevsky'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Generative model',\n",
       "   'Boltzmann machine',\n",
       "   'Classifier (UML)',\n",
       "   'MNIST database',\n",
       "   'Pattern recognition',\n",
       "   'Pixel',\n",
       "   'Focus (optics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Federated learning',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,848',\n",
       "  'reference_count': '3',\n",
       "  'references': ['2096192494', '2081580037', '2165225968']},\n",
       " {'id': '2963091558',\n",
       "  'title': 'Non-local Neural Networks',\n",
       "  'abstract': 'Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Xiaolong Wang', 'Ross Girshick', 'Abhinav Gupta', 'Kaiming He'],\n",
       "  'related_topics': ['Pose',\n",
       "   'Object detection',\n",
       "   'Image segmentation',\n",
       "   'Contextual image classification',\n",
       "   'Block (programming)',\n",
       "   'Artificial neural network',\n",
       "   'Feature extraction',\n",
       "   'Process (computing)',\n",
       "   'Segmentation',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,840',\n",
       "  'reference_count': '53',\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1677182931',\n",
       "   '2806070179',\n",
       "   '1861492603',\n",
       "   '2565639579']},\n",
       " {'id': '3034978746',\n",
       "  'title': 'A Simple Framework for Contrastive Learning of Visual Representations',\n",
       "  'abstract': 'This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ting Chen',\n",
       "   'Simon Kornblith',\n",
       "   'Mohammad Norouzi',\n",
       "   'Geoffrey Hinton'],\n",
       "  'related_topics': ['Supervised learning',\n",
       "   'Linear classifier',\n",
       "   'Machine learning',\n",
       "   'Memory bank',\n",
       "   'Representation (mathematics)',\n",
       "   'Matching (statistics)',\n",
       "   'Computer science',\n",
       "   'Quality (business)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear transformation',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,354',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2100495367',\n",
       "  'title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "  'abstract': 'High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.',\n",
       "  'date': '2006',\n",
       "  'authors': ['G. E. Hinton', 'R. R. Salakhutdinov'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Dimensionality reduction',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Artificial neural network',\n",
       "   'Deep belief network',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Gradient descent',\n",
       "   'Curse of dimensionality',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,298',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2136922672',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2293063825',\n",
       "   '2121122425',\n",
       "   '2032647857',\n",
       "   '2021774695']},\n",
       " {'id': '2187089797',\n",
       "  'title': 'Visualizing Data using t-SNE',\n",
       "  'abstract': 'We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Laurens van der', 'Geoffrey'],\n",
       "  'related_topics': ['Sammon mapping',\n",
       "   't-distributed stochastic neighbor embedding',\n",
       "   'Isomap',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Dimensionality reduction',\n",
       "   'Visualization',\n",
       "   'Embedding',\n",
       "   'Multidimensional scaling',\n",
       "   'Data mining',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '20,095',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2100495367',\n",
       "   '2072128103',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2156718197',\n",
       "   '2125637308',\n",
       "   '2139823104',\n",
       "   '2137570937',\n",
       "   '2157444450',\n",
       "   '1742512077']},\n",
       " {'id': '1665214252',\n",
       "  'title': 'Rectified Linear Units Improve Restricted Boltzmann Machines',\n",
       "  'abstract': 'Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Vinod Nair', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Binary number',\n",
       "   'Sigmoid function',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Rule of inference',\n",
       "   'Unit (ring theory)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Face verification',\n",
       "   'Infinite number',\n",
       "   'View Less'],\n",
       "  'citation_count': '13,478',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2546302380',\n",
       "   '2116064496',\n",
       "   '1782590233',\n",
       "   '2134557905',\n",
       "   '2099866409',\n",
       "   '1994197834',\n",
       "   '2536626143',\n",
       "   '2157364932']},\n",
       " {'id': '2072128103',\n",
       "  'title': 'Learning Deep Architectures for AI',\n",
       "  'abstract': 'Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Yoshua Bengio'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Online machine learning',\n",
       "   'Feature learning',\n",
       "   'Robot learning',\n",
       "   'Computational learning theory',\n",
       "   'Learning classifier system',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,705',\n",
       "  'reference_count': '227',\n",
       "  'references': ['2156909104',\n",
       "   '2911964244',\n",
       "   '2136922672',\n",
       "   '2296616510',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2187089797',\n",
       "   '2129131372',\n",
       "   '2119821739',\n",
       "   '2053186076']},\n",
       " {'id': '2546302380',\n",
       "  'title': 'What is the best multi-stage architecture for object recognition?',\n",
       "  'abstract': 'In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (≫ 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).',\n",
       "  'date': '2009',\n",
       "  'authors': ['Kevin Jarrett',\n",
       "   'Koray Kavukcuoglu',\n",
       "   \"Marc'Aurelio Ranzato\",\n",
       "   'Yann LeCun'],\n",
       "  'related_topics': ['Feature extraction',\n",
       "   'Feature (machine learning)',\n",
       "   'Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'Filter bank',\n",
       "   'MNIST database',\n",
       "   'Filter (signal processing)',\n",
       "   'Word error rate',\n",
       "   'Normalization (statistics)',\n",
       "   'Histogram',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,425',\n",
       "  'reference_count': '54',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2110798204',\n",
       "   '2130325614',\n",
       "   '2097018403',\n",
       "   '2166049352',\n",
       "   '2134557905']},\n",
       " {'id': '2121863487',\n",
       "  'title': 'Reinforcement Learning: An Introduction',\n",
       "  'abstract': \"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.\",\n",
       "  'date': '1988',\n",
       "  'authors': ['R.S. Sutton', 'A.G.'],\n",
       "  'related_topics': ['Learning classifier system',\n",
       "   'Reinforcement learning',\n",
       "   'Apprenticeship learning',\n",
       "   'Unsupervised learning',\n",
       "   'Temporal difference learning',\n",
       "   'Robot learning',\n",
       "   'Computational learning theory',\n",
       "   'Q-learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'View Less'],\n",
       "  'citation_count': '44,042',\n",
       "  'reference_count': '84',\n",
       "  'references': ['1639032689',\n",
       "   '2154642048',\n",
       "   '3017143921',\n",
       "   '2100677568',\n",
       "   '1535810436',\n",
       "   '1603765807',\n",
       "   '3011120880',\n",
       "   '1569320505',\n",
       "   '94523489',\n",
       "   '2178806388']},\n",
       " {'id': '2952509347',\n",
       "  'title': 'The arcade learning environment: an evaluation platform for general agents',\n",
       "  'abstract': 'In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Marc G. Bellemare',\n",
       "   'Yavar Naddaf',\n",
       "   'Joel Veness',\n",
       "   'Michael Bowling'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Learning environment',\n",
       "   'Transfer of learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Interface (Java)',\n",
       "   'Human–computer interaction',\n",
       "   'Computer science',\n",
       "   'Software',\n",
       "   'Set (psychology)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,837',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2145339207',\n",
       "   '2952509347',\n",
       "   '2126316555',\n",
       "   '1515851193',\n",
       "   '1625390266',\n",
       "   '1502916507',\n",
       "   '2099587183',\n",
       "   '2013391942',\n",
       "   '2101355568',\n",
       "   '2132622533']},\n",
       " {'id': '1652505363',\n",
       "  'title': 'Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations',\n",
       "  'abstract': 'The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.',\n",
       "  'date': '1986',\n",
       "  'authors': ['David E. Rumelhart', 'James L. McClelland'],\n",
       "  'related_topics': ['Competitive learning',\n",
       "   'Connectionism',\n",
       "   'Parallel processing (DSP implementation)',\n",
       "   'Catastrophic interference',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Linear algebra',\n",
       "   'Resource (project management)',\n",
       "   'Data processing',\n",
       "   'Cognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '24,210',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1614298861',\n",
       "  'title': 'Efficient Estimation of Word Representations in Vector Space',\n",
       "  'abstract': 'We propose two novel model architectures for computing continuous vector\\nrepresentations of words from very large data sets. The quality of these\\nrepresentations is measured in a word similarity task, and the results are\\ncompared to the previously best performing techniques based on different types\\nof neural networks. We observe large improvements in accuracy at much lower\\ncomputational cost, i.e. it takes less than a day to learn high quality word\\nvectors from a 1.6 billion words data set. Furthermore, we show that these\\nvectors provide state-of-the-art performance on our test set for measuring\\nsyntactic and semantic word similarities.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Tomas Mikolov', 'Kai Chen', 'Greg S. Corrado', 'Jeffrey Dean'],\n",
       "  'related_topics': ['Word2vec',\n",
       "   'Word embedding',\n",
       "   'Word (computer architecture)',\n",
       "   'Test set',\n",
       "   'Similarity (psychology)',\n",
       "   'Artificial neural network',\n",
       "   'Distributional semantics',\n",
       "   'Data set',\n",
       "   'Pattern recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,670',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2141599568',\n",
       "  'title': 'Linguistic Regularities in Continuous Space Word Representations',\n",
       "  'abstract': 'Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Tomas Mikolov', 'Wen-tau Yih', 'Geoffrey Zweig'],\n",
       "  'related_topics': ['Syntax',\n",
       "   'Language model',\n",
       "   'Analogy',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,564',\n",
       "  'reference_count': '22',\n",
       "  'references': ['1614298861',\n",
       "   '2100495367',\n",
       "   '2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '2147152072',\n",
       "   '1632114991',\n",
       "   '2131462252',\n",
       "   '1970689298']},\n",
       " {'id': '2117130368',\n",
       "  'title': 'A unified architecture for natural language processing: deep neural networks with multitask learning',\n",
       "  'abstract': 'We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Ronan Collobert', 'Jason Weston'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Language identification',\n",
       "   'Language model',\n",
       "   'Semantic role labeling',\n",
       "   'Sentence',\n",
       "   'Convolutional neural network',\n",
       "   'Host (network)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,697',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2310919327',\n",
       "   '2132339004',\n",
       "   '2130903752',\n",
       "   '2158847908',\n",
       "   '2107008379',\n",
       "   '2914746235',\n",
       "   '2173629880',\n",
       "   '2885050925',\n",
       "   '2158823144',\n",
       "   '2163568299']},\n",
       " {'id': '2132339004',\n",
       "  'title': 'A neural probabilistic language model',\n",
       "  'abstract': 'A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Réjean Ducharme',\n",
       "   'Pascal Vincent',\n",
       "   'Christian Janvin'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Cache language model',\n",
       "   'Word embedding',\n",
       "   'Sentence',\n",
       "   'Joint probability distribution',\n",
       "   'Word (computer architecture)',\n",
       "   'Generalization',\n",
       "   'Probabilistic logic',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,567',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2038721957',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '1575350781',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2914484425']},\n",
       " {'id': '2158139315',\n",
       "  'title': 'Word Representations: A Simple and General Method for Semi-Supervised Learning',\n",
       "  'abstract': 'If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/',\n",
       "  'date': '2010',\n",
       "  'authors': ['Joseph Turian', 'Lev-Arie Ratinov', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Word (computer architecture)',\n",
       "   'Semi-supervised learning',\n",
       "   'Chunking (psychology)',\n",
       "   'Natural language processing',\n",
       "   'Code (cryptography)',\n",
       "   'Computer science',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'General method',\n",
       "   'Word representation',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,492',\n",
       "  'reference_count': '47',\n",
       "  'references': ['1880262756',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '1662133657',\n",
       "   '168564468',\n",
       "   '2131462252',\n",
       "   '2296073425',\n",
       "   '2158997610',\n",
       "   '2004763266',\n",
       "   '2156515921']},\n",
       " {'id': '1423339008',\n",
       "  'title': 'Parsing Natural Scenes and Natural Language with Recursive Neural Networks',\n",
       "  'abstract': 'Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Richard Socher',\n",
       "   'Cliff C. Lin',\n",
       "   'Chris Manning',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Parse tree',\n",
       "   'Treebank',\n",
       "   'Natural language',\n",
       "   'Sentence',\n",
       "   'Syntax',\n",
       "   'Artificial neural network',\n",
       "   'Segmentation',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Annotation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,478',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2100495367',\n",
       "   '2162915993',\n",
       "   '2067191022',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2130325614',\n",
       "   '1574901103',\n",
       "   '1566135517',\n",
       "   '1528789833',\n",
       "   '2536208356']},\n",
       " {'id': '1498436455',\n",
       "  'title': 'Learning representations by back-propagating errors',\n",
       "  'abstract': 'We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.',\n",
       "  'date': '1988',\n",
       "  'authors': ['David E. Rumelhart',\n",
       "   'Geoffrey E. Hinton',\n",
       "   'Ronald J. Williams'],\n",
       "  'related_topics': ['Domain (software engineering)',\n",
       "   'Task (project management)',\n",
       "   'Measure (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Neurophysiology',\n",
       "   'The Internet',\n",
       "   'View Less'],\n",
       "  'citation_count': '23,793',\n",
       "  'reference_count': '2',\n",
       "  'references': ['1652505363', '2322002063']},\n",
       " {'id': '1662133657',\n",
       "  'title': 'From frequency to meaning: vector space models of semantics',\n",
       "  'abstract': 'Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Peter D. Turney', 'Patrick Pantel'],\n",
       "  'related_topics': ['Statistical semantics',\n",
       "   'Distributional semantics',\n",
       "   'Semantics',\n",
       "   'Random indexing',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Meaning (linguistics)',\n",
       "   'Natural language processing',\n",
       "   'Field (computer science)',\n",
       "   'Process (engineering)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,199',\n",
       "  'reference_count': '178',\n",
       "  'references': ['2173213060',\n",
       "   '1880262756',\n",
       "   '1532325895',\n",
       "   '3013264884',\n",
       "   '2038721957',\n",
       "   '2117130368',\n",
       "   '2024165284',\n",
       "   '1660390307',\n",
       "   '2166706824',\n",
       "   '1992419399']},\n",
       " {'id': '1889268436',\n",
       "  'title': 'Semantic Compositionality through Recursive Matrix-Vector Spaces',\n",
       "  'abstract': 'Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Richard Socher',\n",
       "   'Brody Huval',\n",
       "   'Christopher D. Manning',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Parse tree',\n",
       "   'Principle of compositionality',\n",
       "   'Natural language',\n",
       "   'Syntax',\n",
       "   'Recurrent neural network',\n",
       "   'Propositional calculus',\n",
       "   'Vector space',\n",
       "   'Noun',\n",
       "   'Meaning (non-linguistic)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,444',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2251939518',\n",
       "   '2117130368',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '1662133657',\n",
       "   '2097606805',\n",
       "   '2103305545',\n",
       "   '2163455955',\n",
       "   '1984052055',\n",
       "   '2151048449']},\n",
       " {'id': '2131462252',\n",
       "  'title': 'A Scalable Hierarchical Distributed Language Model',\n",
       "  'abstract': 'Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Andriy Mnih', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Binary tree',\n",
       "   'Tree (data structure)',\n",
       "   'Word (computer architecture)',\n",
       "   'Probabilistic logic',\n",
       "   'Scalability',\n",
       "   'Feature (machine learning)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,130',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2038721957',\n",
       "   '2132339004',\n",
       "   '36903255',\n",
       "   '2091812280',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2127314673',\n",
       "   '2056590938',\n",
       "   '2111305191',\n",
       "   '1558797106']},\n",
       " {'id': '2097117768',\n",
       "  'title': 'Going deeper with convolutions',\n",
       "  'abstract': 'We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Christian Szegedy',\n",
       "   'Wei Liu',\n",
       "   'Yangqing Jia',\n",
       "   'Pierre Sermanet',\n",
       "   'Scott Reed',\n",
       "   'Dragomir Anguelov',\n",
       "   'Dumitru Erhan',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Andrew Rabinovich'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Artificial neural network',\n",
       "   'Contextual image classification',\n",
       "   'Hebbian theory',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Residual neural network',\n",
       "   'View Less'],\n",
       "  'citation_count': '30,642',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2963911037',\n",
       "   '2168231600',\n",
       "   '2068730032',\n",
       "   '104184427']},\n",
       " {'id': '639708223',\n",
       "  'title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "  'abstract': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet  [1]  and Fast R-CNN  [2]  have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a  Region Proposal Network  (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model  [3]  , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Shaoqing Ren', 'Kaiming He', 'Ross Girshick', 'Jian Sun'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Convolutional neural network',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Feature extraction',\n",
       "   'Frame rate',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '27,093',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1536680647',\n",
       "   '2168356304']},\n",
       " {'id': '2102605133',\n",
       "  'title': 'Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation',\n",
       "  'abstract': 'Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Ross Girshick',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell',\n",
       "   'Jitendra Malik'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Feature (computer vision)',\n",
       "   'Convolutional neural network',\n",
       "   'Feature extraction',\n",
       "   'Support vector machine',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Minimum bounding box',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,440',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2031489346',\n",
       "   '2088049833',\n",
       "   '2155541015']},\n",
       " {'id': '1903029394',\n",
       "  'title': 'Fully convolutional networks for semantic segmentation',\n",
       "  'abstract': 'Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Jonathan Long', 'Evan Shelhamer', 'Trevor Darrell'],\n",
       "  'related_topics': ['Scale-space segmentation',\n",
       "   'Segmentation',\n",
       "   'Inference',\n",
       "   'Image translation',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Pascal (programming language)',\n",
       "   'Image (mathematics)',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'View Less'],\n",
       "  'citation_count': '23,063',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2155893237',\n",
       "   '1663973292',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2109255472',\n",
       "   '2155541015']},\n",
       " {'id': '2155893237',\n",
       "  'title': 'Caffe: Convolutional Architecture for Fast Feature Embedding',\n",
       "  'abstract': 'Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Yangqing Jia',\n",
       "   'Evan Shelhamer',\n",
       "   'Jeff Donahue',\n",
       "   'Sergey Karayev',\n",
       "   'Jonathan Long',\n",
       "   'Ross Girshick',\n",
       "   'Sergio Guadarrama',\n",
       "   'Trevor Darrell'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Theano',\n",
       "   'CUDA',\n",
       "   'Python (programming language)',\n",
       "   'Convolutional neural network',\n",
       "   'Cloud computing',\n",
       "   'Caffè',\n",
       "   'Artificial neural network',\n",
       "   'Computer architecture',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,849',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2963542991',\n",
       "   '2088049833',\n",
       "   '2155541015',\n",
       "   '753012316',\n",
       "   '1825604117',\n",
       "   '2147414309',\n",
       "   '1872489089',\n",
       "   '2962883796']},\n",
       " {'id': '1536680647',\n",
       "  'title': 'Fast R-CNN',\n",
       "  'abstract': 'This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Ross Girshick'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Python (programming language)',\n",
       "   'Pascal (programming language)',\n",
       "   'Feature extraction',\n",
       "   'Computational science',\n",
       "   'Real-time computing',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Open source software',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,299',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2168356304',\n",
       "   '1861492603',\n",
       "   '2963542991',\n",
       "   '2109255472',\n",
       "   '2164598857']},\n",
       " {'id': '2095705004',\n",
       "  'title': 'Dropout: a simple way to prevent neural networks from overfitting',\n",
       "  'abstract': 'Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Nitish Srivastava',\n",
       "   'Geoffrey Hinton',\n",
       "   'Alex Krizhevsky',\n",
       "   'Ilya Sutskever',\n",
       "   'Ruslan Salakhutdinov'],\n",
       "  'related_topics': ['Overfitting',\n",
       "   'Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Supervised learning',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial neural network',\n",
       "   'Regularization (mathematics)',\n",
       "   'Document classification',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '28,363',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2135046866',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '2145094598',\n",
       "   '2131241448',\n",
       "   '2335728318']},\n",
       " {'id': '2146502635',\n",
       "  'title': 'Adaptive Subgradient Methods for Online Learning and Stochastic Optimization',\n",
       "  'abstract': 'We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.',\n",
       "  'date': '2011',\n",
       "  'authors': ['John Duchi', 'Elad Hazan', 'Yoram Singer'],\n",
       "  'related_topics': ['Subgradient method',\n",
       "   'Online machine learning',\n",
       "   'Empirical risk minimization',\n",
       "   'Stochastic optimization',\n",
       "   'Regularization (mathematics)',\n",
       "   'Mathematical optimization',\n",
       "   'Regret',\n",
       "   'Function (mathematics)',\n",
       "   'Domain (software engineering)',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,784',\n",
       "  'reference_count': '48',\n",
       "  'references': ['2296319761',\n",
       "   '2108598243',\n",
       "   '3120740533',\n",
       "   '2798766386',\n",
       "   '2610857016',\n",
       "   '2150102617',\n",
       "   '2167732364',\n",
       "   '1992208280',\n",
       "   '2160218441',\n",
       "   '1978394996']},\n",
       " {'id': '2168231600',\n",
       "  'title': 'Large Scale Distributed Deep Networks',\n",
       "  'abstract': 'Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Jeffrey Dean',\n",
       "   'Greg Corrado',\n",
       "   'Rajat Monga',\n",
       "   'Kai Chen',\n",
       "   'Matthieu Devin',\n",
       "   'Mark Mao',\n",
       "   \"Marc'aurelio Ranzato\",\n",
       "   'Andrew Senior',\n",
       "   'Paul Tucker',\n",
       "   'Ke Yang',\n",
       "   'Quoc V. Le',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Feature learning',\n",
       "   'Stochastic gradient descent',\n",
       "   'Asynchronous communication',\n",
       "   'Machine learning',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,316',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2108598243',\n",
       "   '2173213060',\n",
       "   '3118608800',\n",
       "   '2146502635',\n",
       "   '2117130368',\n",
       "   '2147768505',\n",
       "   '2132339004',\n",
       "   '2141125852',\n",
       "   '2184045248',\n",
       "   '2118858186']},\n",
       " {'id': '104184427',\n",
       "  'title': 'On the importance of initialization and momentum in deep learning',\n",
       "  'abstract': 'Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.\\r\\n\\r\\nOur success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Ilya Sutskever',\n",
       "   'James Martens',\n",
       "   'George Dahl',\n",
       "   'Geoffrey Hinton'],\n",
       "  'related_topics': ['Initialization',\n",
       "   'Stochastic gradient descent',\n",
       "   'Recurrent neural network',\n",
       "   'Deep learning',\n",
       "   'Momentum (technical analysis)',\n",
       "   'Schedule',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Curvature',\n",
       "   'Training (meteorology)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,585',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2064675550',\n",
       "   '1533861849',\n",
       "   '2147768505',\n",
       "   '2110798204',\n",
       "   '1993882792',\n",
       "   '3141595720',\n",
       "   '2184045248']},\n",
       " {'id': '6908809',\n",
       "  'title': 'ADADELTA: An Adaptive Learning Rate Method',\n",
       "  'abstract': 'We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Matthew D.'],\n",
       "  'related_topics': ['Stochastic gradient descent',\n",
       "   'Gradient descent',\n",
       "   'Online machine learning',\n",
       "   'MNIST database',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Hyperparameter',\n",
       "   'Selection (genetic algorithm)',\n",
       "   'Computer science',\n",
       "   'Scale (ratio)',\n",
       "   'Task (project management)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,485',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2168231600',\n",
       "   '2147768505',\n",
       "   '1498436455',\n",
       "   '2120420045',\n",
       "   '19621276',\n",
       "   '1994616650']},\n",
       " {'id': '1753482797',\n",
       "  'title': 'Recurrent Continuous Translation Models',\n",
       "  'abstract': 'We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Nal Kalchbrenner', 'Phil Blunsom'],\n",
       "  'related_topics': ['Rule-based machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Dynamic and formal equivalence',\n",
       "   'Sentence',\n",
       "   'Language model',\n",
       "   'Perplexity',\n",
       "   'Syntax',\n",
       "   'Translation (geometry)',\n",
       "   'Word order',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,269',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2146502635',\n",
       "   '2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '1889268436',\n",
       "   '2171928131',\n",
       "   '2251222643',\n",
       "   '2103305545',\n",
       "   '2006969979',\n",
       "   '196214544']},\n",
       " {'id': '2294059674',\n",
       "  'title': 'Maxout Networks',\n",
       "  'abstract': \"We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.\",\n",
       "  'date': '2013',\n",
       "  'authors': ['Ian Goodfellow',\n",
       "   'David Warde-Farley',\n",
       "   'Mehdi Mirza',\n",
       "   'Aaron Courville',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['MNIST database',\n",
       "   'Leverage (statistics)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,215',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2618530766',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2546302380',\n",
       "   '2912934387',\n",
       "   '2131241448',\n",
       "   '2335728318',\n",
       "   '2156387975',\n",
       "   '189596042']},\n",
       " {'id': '1810943226',\n",
       "  'title': 'Generating Sequences With Recurrent Neural Networks',\n",
       "  'abstract': 'This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Alex Graves'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Handwriting',\n",
       "   'Sequence',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Point (typography)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,285',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2064675550',\n",
       "   '1554663460',\n",
       "   '2143612262',\n",
       "   '44815768',\n",
       "   '1632114991',\n",
       "   '2131462252',\n",
       "   '196214544',\n",
       "   '2108677974',\n",
       "   '2120861206',\n",
       "   '3023071679']},\n",
       " {'id': '2964199361',\n",
       "  'title': 'On the Properties of Neural Machine Translation: Encoder--Decoder Approaches',\n",
       "  'abstract': 'Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder‐Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Kyunghyun Cho',\n",
       "   'Bart van Merrienboer',\n",
       "   'Dzmitry Bahdanau',\n",
       "   'Yoshua Bengio',\n",
       "   '',\n",
       "   ''],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Convolutional neural network',\n",
       "   'Artificial neural network',\n",
       "   'Encoder',\n",
       "   'Sentence',\n",
       "   'Translation (geometry)',\n",
       "   'Speech recognition',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Focus (optics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,605',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2130942839',\n",
       "   '2157331557',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '1810943226',\n",
       "   '2153653739',\n",
       "   '2395935897',\n",
       "   '1828163288',\n",
       "   '1905522558',\n",
       "   '2341457423']},\n",
       " {'id': '1815076433',\n",
       "  'title': 'On the difficulty of training recurrent neural networks',\n",
       "  'abstract': 'There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Razvan Pascanu', 'Tomas Mikolov', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Dynamical systems theory',\n",
       "   'Artificial intelligence',\n",
       "   'Clipping (computer graphics)',\n",
       "   'Mathematics',\n",
       "   'Effective solution',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,975',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2146502635',\n",
       "   '2064675550',\n",
       "   '1498436455',\n",
       "   '1606347560',\n",
       "   '196214544',\n",
       "   '2110485445',\n",
       "   '2171865010',\n",
       "   '2122585011',\n",
       "   '2118706537',\n",
       "   '2107878631']},\n",
       " {'id': '2153653739',\n",
       "  'title': 'Statistical phrase-based translation',\n",
       "  'abstract': 'We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu'],\n",
       "  'related_topics': ['Noun phrase',\n",
       "   'Phrase',\n",
       "   'Determiner phrase',\n",
       "   'Phrase search',\n",
       "   'Dependency grammar',\n",
       "   'Phrase structure rules',\n",
       "   'Generalized phrase structure grammar',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Hybrid machine translation',\n",
       "   'Example-based machine translation',\n",
       "   'Computer-assisted translation',\n",
       "   'Interactive machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Machine translation software usability',\n",
       "   'Pivot language',\n",
       "   'Evaluation of machine translation',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'BLEU',\n",
       "   'Artificial intelligence',\n",
       "   'Lexicography',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,415',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2101105183',\n",
       "   '2006969979',\n",
       "   '1508165687',\n",
       "   '1973923101',\n",
       "   '1986543644',\n",
       "   '2116316001',\n",
       "   '1517947178',\n",
       "   '2161792612',\n",
       "   '1549285799',\n",
       "   '2158388102']},\n",
       " {'id': '2183341477',\n",
       "  'title': 'Rethinking the Inception Architecture for Computer Vision',\n",
       "  'abstract': 'Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Christian Szegedy',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Sergey Ioffe',\n",
       "   'Jon Shlens',\n",
       "   'Zbigniew Wojna'],\n",
       "  'related_topics': ['Test set',\n",
       "   'Set (abstract data type)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Inference',\n",
       "   'Artificial intelligence',\n",
       "   'State (computer science)',\n",
       "   'Computation',\n",
       "   'Computer vision',\n",
       "   'Variety (cybernetics)',\n",
       "   'Regularization (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '13,450',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '1677182931',\n",
       "   '2096733369',\n",
       "   '2016053056']},\n",
       " {'id': '2147768505',\n",
       "  'title': 'Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition',\n",
       "  'abstract': 'We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.',\n",
       "  'date': '2012',\n",
       "  'authors': ['G. E. Dahl', 'Dong Yu', 'Li Deng', 'A. Acero'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Hidden Markov model',\n",
       "   'Word error rate',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Triphone',\n",
       "   'Context model',\n",
       "   'Context (language use)',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,161',\n",
       "  'reference_count': '84',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2116064496',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '1993882792',\n",
       "   '1498436455']},\n",
       " {'id': '2156387975',\n",
       "  'title': 'Deep Sparse Rectifier Neural Networks',\n",
       "  'abstract': 'While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity',\n",
       "  'date': '2011',\n",
       "  'authors': ['Xavier Glorot', 'Antoine Bordes', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Rectifier (neural networks)',\n",
       "   'Winner-take-all',\n",
       "   'Artificial neural network',\n",
       "   'Logistic function',\n",
       "   'Hyperbolic function',\n",
       "   'Topology',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,910',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1665214252',\n",
       "   '1533861849',\n",
       "   '2072128103',\n",
       "   '2129131372',\n",
       "   '2546302380',\n",
       "   '2097726431',\n",
       "   '2025768430']},\n",
       " {'id': '2963504252',\n",
       "  'title': 'Exact solutions to the nonlinear dynamics of learning in deep linear neural networks',\n",
       "  'abstract': 'Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Andrew M. Saxe', 'James L. McClelland', 'Surya Ganguli'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Deep learning',\n",
       "   'Nonlinear system',\n",
       "   'Gradient descent',\n",
       "   'Convergence (routing)',\n",
       "   'Edge of chaos',\n",
       "   'Gaussian',\n",
       "   'Linearity',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '984',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2618530766',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '104184427',\n",
       "   '2110798204',\n",
       "   '2141125852',\n",
       "   '1993882792',\n",
       "   '1815076433']},\n",
       " {'id': '2107878631',\n",
       "  'title': 'Learning long-term dependencies with gradient descent is difficult',\n",
       "  'abstract': 'Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. >',\n",
       "  'date': '1994',\n",
       "  'authors': ['Y. Bengio', 'P. Simard', 'P. Frasconi'],\n",
       "  'related_topics': ['Vanishing gradient problem',\n",
       "   'Gradient descent',\n",
       "   'Recurrent neural network',\n",
       "   'Gradient method',\n",
       "   'Artificial neural network',\n",
       "   'Term (time)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Machine learning',\n",
       "   'Face (geometry)',\n",
       "   'Dynamical systems theory',\n",
       "   'Computer science',\n",
       "   'Numerical analysis',\n",
       "   'Artificial intelligence',\n",
       "   'Recurrent neural nets',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,843',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2016589492',\n",
       "   '19621276',\n",
       "   '2128499899',\n",
       "   '2088978850',\n",
       "   '2148099973',\n",
       "   '1996741810',\n",
       "   '2125329357',\n",
       "   '1527772862']},\n",
       " {'id': '2128499899',\n",
       "  'title': 'Induction of Multiscale Temporal Structure',\n",
       "  'abstract': 'Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Michael C Mozer'],\n",
       "  'related_topics': ['Computational problem',\n",
       "   'Sequence',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Backpropagation',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Musical composition',\n",
       "   'Fraction (mathematics)',\n",
       "   'Musical form',\n",
       "   'View Less'],\n",
       "  'citation_count': '157',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2154642048',\n",
       "   '2016589492',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '1959983357',\n",
       "   '2028629011',\n",
       "   '2053127376',\n",
       "   '2167607759']},\n",
       " {'id': '2007431958',\n",
       "  'title': 'Generalization of back-propagation to recurrent neural networks.',\n",
       "  'abstract': 'An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\\\ensuremath{\\\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.',\n",
       "  'date': '1987',\n",
       "  'authors': ['Fernando J. Pineda'],\n",
       "  'related_topics': ['Hopfield network',\n",
       "   'Recurrent neural network',\n",
       "   'Time delay neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Feedforward neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Physical neural network',\n",
       "   'Stochastic neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,210',\n",
       "  'reference_count': '3',\n",
       "  'references': ['1652505363', '2177721432', '2075510082']},\n",
       " {'id': '194249466',\n",
       "  'title': 'Untersuchungen zu dynamischen neuronalen Netzen',\n",
       "  'abstract': '',\n",
       "  'date': '1991',\n",
       "  'authors': ['Sepp'],\n",
       "  'related_topics': ['Computer science'],\n",
       "  'citation_count': '855',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2123716044',\n",
       "  'title': 'Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks',\n",
       "  'abstract': 'Although the potential of the powerful mapping and representational capabilities of recurrent network architectures is generally recognized by the neural network research community, recurrent neural networks have not been widely used for the control of nonlinear dynamical systems, possibly due to the relative ineffectiveness of simple gradient descent training algorithms. Developments in the use of parameter-based extended Kalman filter algorithms for training recurrent networks may provide a mechanism by which these architectures will prove to be of practical value. This paper presents a decoupled extended Kalman filter (DEKF) algorithm for training of recurrent networks with special emphasis on application to control problems. We demonstrate in simulation the application of the DEKF algorithm to a series of example control problems ranging from the well-known cart-pole and bioreactor benchmark problems to an automotive subsystem, engine idle speed control. These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise. >',\n",
       "  'date': '1994',\n",
       "  'authors': ['G.V. Puskorius', 'L.A. Feldkamp'],\n",
       "  'related_topics': ['Extended Kalman filter',\n",
       "   'Recurrent neural network',\n",
       "   'Kalman filter',\n",
       "   'Control theory',\n",
       "   'Artificial neural network',\n",
       "   'Dynamical systems theory',\n",
       "   'Gradient descent',\n",
       "   'Control system',\n",
       "   'Feed forward',\n",
       "   'Control theory',\n",
       "   'Computer science',\n",
       "   'Moving horizon estimation',\n",
       "   'View Less'],\n",
       "  'citation_count': '759',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2138484437',\n",
       "   '2016589492',\n",
       "   '2150355110',\n",
       "   '1583833196',\n",
       "   '2143787696',\n",
       "   '2057653135',\n",
       "   '2132152975',\n",
       "   '2112462566',\n",
       "   '2090248140',\n",
       "   '1529008516']},\n",
       " {'id': '2143503258',\n",
       "  'title': 'Learning state space trajectories in recurrent neural networks',\n",
       "  'abstract': 'Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Barak A. Pearlmutter'],\n",
       "  'related_topics': ['Echo state network',\n",
       "   'Recurrent neural network',\n",
       "   'Time delay neural network',\n",
       "   'Feedforward neural network',\n",
       "   'Deep learning',\n",
       "   'Random neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Gradient descent',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '987',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '1597286183',\n",
       "   '2173629880',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '2007431958',\n",
       "   '1971129545',\n",
       "   '1959983357',\n",
       "   '3121926921']},\n",
       " {'id': '2154890045',\n",
       "  'title': 'Gradient calculations for dynamic recurrent neural networks: a survey',\n",
       "  'abstract': 'Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman\\'s history cutoff, and Jordan\\'s output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed. >',\n",
       "  'date': '1995',\n",
       "  'authors': ['B.A. Pearlmutter'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Deep learning',\n",
       "   'Recurrent neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Artificial intelligence',\n",
       "   'Fixed point',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Boltzmann constant',\n",
       "   'Recurrent neural nets',\n",
       "   'View Less'],\n",
       "  'citation_count': '712',\n",
       "  'reference_count': '117',\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2138484437',\n",
       "   '2110485445',\n",
       "   '2147800946',\n",
       "   '2895674046',\n",
       "   '1597286183',\n",
       "   '2173629880',\n",
       "   '1535810436',\n",
       "   '2016589492']},\n",
       " {'id': '2103452139',\n",
       "  'title': 'Learning long-term dependencies in NARX recurrent neural networks',\n",
       "  'abstract': 'It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Tsungnan Lin', 'B.G. Horne', 'P. Tino', 'C.L. Giles'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Nonlinear system identification',\n",
       "   'Nonlinear autoregressive exogenous model',\n",
       "   'Gradient descent',\n",
       "   'Grammar induction',\n",
       "   'Gradient method',\n",
       "   'Artificial intelligence',\n",
       "   'Autoregressive model',\n",
       "   'Machine learning',\n",
       "   'Nonlinear system',\n",
       "   'Computer science',\n",
       "   'Recurrent neural nets',\n",
       "   'View Less'],\n",
       "  'citation_count': '699',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2064675550',\n",
       "   '2154642048',\n",
       "   '2138484437',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '2798813531',\n",
       "   '2128499899',\n",
       "   '2123716044',\n",
       "   '1674799117',\n",
       "   '2098398123']},\n",
       " {'id': '2048060899',\n",
       "  'title': 'A time-delay neural network architecture for isolated word recognition',\n",
       "  'abstract': \"Abstract   A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.\",\n",
       "  'date': '1990',\n",
       "  'authors': ['Kevin J. Lang', 'Alex H. Waibel', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Word recognition',\n",
       "   'Hidden Markov model',\n",
       "   'Network architecture',\n",
       "   'Artificial neural network',\n",
       "   'Vocabulary',\n",
       "   'Speech recognition',\n",
       "   'Segmentation',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '808',\n",
       "  'reference_count': '18',\n",
       "  'references': ['1498436455',\n",
       "   '3017143921',\n",
       "   '2173629880',\n",
       "   '2176028050',\n",
       "   '1966812932',\n",
       "   '2101926813',\n",
       "   '1991133427',\n",
       "   '1959983357',\n",
       "   '3121926921',\n",
       "   '2048330959']},\n",
       " {'id': '1674799117',\n",
       "  'title': 'Gradient-based learning algorithms for recurrent networks and their computational complexity',\n",
       "  'abstract': '',\n",
       "  'date': '1995',\n",
       "  'authors': ['Ronald J. Williams', 'David'],\n",
       "  'related_topics': ['Computational resource',\n",
       "   'Computational learning theory',\n",
       "   'Deep learning',\n",
       "   'Probabilistic analysis of algorithms',\n",
       "   'Artificial neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Stability (learning theory)',\n",
       "   'Types of artificial neural networks',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Theoretical computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '545',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2136922672',\n",
       "  'title': 'A fast learning algorithm for deep belief nets',\n",
       "  'abstract': 'We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Geoffrey E. Hinton', 'Simon Osindero', 'Yee-Whye Teh'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Generative model',\n",
       "   'Content-addressable memory',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Greedy algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Boltzmann machine',\n",
       "   'Artificial intelligence',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,667',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2310919327',\n",
       "   '2116064496',\n",
       "   '2057175746',\n",
       "   '2159080219',\n",
       "   '2156163116',\n",
       "   '2131686571',\n",
       "   '2158778629',\n",
       "   '2567948266',\n",
       "   '2159737176',\n",
       "   '2124914669']},\n",
       " {'id': '2025768430',\n",
       "  'title': 'Extracting and composing robust features with denoising autoencoders',\n",
       "  'abstract': 'Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Pascal Vincent',\n",
       "   'Hugo Larochelle',\n",
       "   'Yoshua Bengio',\n",
       "   'Pierre-Antoine Manzagol'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Unsupervised learning',\n",
       "   'Generative model',\n",
       "   'Discriminative model',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Perspective (graphical)',\n",
       "   'Representation (mathematics)',\n",
       "   'Benchmark (computing)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,631',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2110798204',\n",
       "   '2153663612',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '1994197834',\n",
       "   '2293063825',\n",
       "   '2172174689']},\n",
       " {'id': '2110798204',\n",
       "  'title': 'Greedy Layer-Wise Training of Deep Networks',\n",
       "  'abstract': 'Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Pascal Lamblin',\n",
       "   'Dan Popovici',\n",
       "   'Hugo Larochelle'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Optimization problem',\n",
       "   'Generative model',\n",
       "   'Initialization',\n",
       "   'Context (language use)',\n",
       "   'Artificial intelligence',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,516',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2613634265',\n",
       "   '2124914669',\n",
       "   '1993845689',\n",
       "   '2109779438',\n",
       "   '2103626435',\n",
       "   '2125569215',\n",
       "   '2167967601']},\n",
       " {'id': '1994197834',\n",
       "  'title': 'An empirical evaluation of deep architectures on problems with many factors of variation',\n",
       "  'abstract': 'Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Hugo Larochelle',\n",
       "   'Dumitru Erhan',\n",
       "   'Aaron Courville',\n",
       "   'James Bergstra',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Online machine learning',\n",
       "   'Instance-based learning',\n",
       "   'Computational learning theory',\n",
       "   'Stability (learning theory)',\n",
       "   'Deep learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Ensemble learning',\n",
       "   'Unsupervised learning',\n",
       "   'Competitive learning',\n",
       "   'Artificial neural network',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Algorithmic learning theory',\n",
       "   'Support vector machine',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,060',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2153635508',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2134557905',\n",
       "   '2147800946',\n",
       "   '2613634265',\n",
       "   '2159737176',\n",
       "   '2124914669']},\n",
       " {'id': '2172174689',\n",
       "  'title': 'Efficient Learning of Sparse Representations with an Energy-Based Model',\n",
       "  'abstract': 'We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.',\n",
       "  'date': '2006',\n",
       "  'authors': [\"Marc'aurelio Ranzato\",\n",
       "   'Christopher Poultney',\n",
       "   'Sumit Chopra',\n",
       "   'Yann L. Cun'],\n",
       "  'related_topics': ['Encoder',\n",
       "   'Sparse approximation',\n",
       "   'MNIST database',\n",
       "   'Code (cryptography)',\n",
       "   'Filter (signal processing)',\n",
       "   'Word error rate',\n",
       "   'Energy (signal processing)',\n",
       "   'Pattern recognition',\n",
       "   'Detector',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,541',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2116064496',\n",
       "   '1902027874',\n",
       "   '2156163116',\n",
       "   '2105464873',\n",
       "   '1802356529',\n",
       "   '2075187489',\n",
       "   '2102409316',\n",
       "   '11828546']},\n",
       " {'id': '2903899730',\n",
       "  'title': 'Origin and evolution of pathogenic coronaviruses',\n",
       "  'abstract': 'Severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) are two highly transmissible and pathogenic viruses that emerged in humans at the beginning of the 21st century. Both viruses likely originated in bats, and genetically diverse coronaviruses that are related to SARS-CoV and MERS-CoV were discovered in bats worldwide. In this Review, we summarize the current knowledge on the origin and evolution of these two pathogenic coronaviruses and discuss their receptor usage; we also highlight the diversity and potential of spillover of bat-borne coronaviruses, as evidenced by the recent spillover of swine acute diarrhoea syndrome coronavirus (SADS-CoV) to pigs. Coronaviruses have a broad host range and distribution, and some highly pathogenic lineages have spilled over to humans and animals. Here, Cui, Li and Shi explore the viral factors that enabled the emergence of diseases such as severe acute respiratory syndrome and Middle East respiratory syndrome.',\n",
       "  'date': '2019',\n",
       "  'authors': ['Jie Cui', 'Fang Li', 'Zheng Li Shi'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Viral evolution',\n",
       "   'Viral pathogenesis',\n",
       "   'Virology',\n",
       "   'Phylogenetics',\n",
       "   'Biology',\n",
       "   'Acute diarrhoea',\n",
       "   'Highly pathogenic',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,063',\n",
       "  'reference_count': '145',\n",
       "  'references': ['2166867592',\n",
       "   '2144081223',\n",
       "   '2111211467',\n",
       "   '2470646526',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2306794997',\n",
       "   '1993577573',\n",
       "   '2775086803',\n",
       "   '2119111857']},\n",
       " {'id': '2166867592',\n",
       "  'title': 'Isolation of a Novel Coronavirus from a Man with Pneumonia in Saudi Arabia',\n",
       "  'abstract': 'A previously unknown coronavirus was isolated from the sputum of a 60-year-old man who presented with acute pneumonia and subsequent renal failure with a fatal outcome in Saudi Arabia. The virus (called HCoV-EMC) replicated readily in cell culture, producing cytopathic effects of rounding, detachment, and syncytium formation. The virus represents a novel betacoronavirus species. The closest known relatives are bat coronaviruses HKU4 and HKU5. Here, the clinical data, virus isolation, and molecular identification are presented. The clinical picture was remarkably similar to that of the severe acute respiratory syndrome (SARS) outbreak in 2003 and reminds us that animal coronaviruses can cause severe disease in humans.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Ali Moh Zaki',\n",
       "   'Sander Van Boheemen',\n",
       "   'Theo M. Bestebroer',\n",
       "   'Albert D.M.E.',\n",
       "   'Ron A.M.'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Pneumonia',\n",
       "   'Alphacoronavirus',\n",
       "   'Virus',\n",
       "   'Outbreak',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,493',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2132260239',\n",
       "   '2025170735',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '2116586125',\n",
       "   '1987783718',\n",
       "   '2111412754',\n",
       "   '1963953102',\n",
       "   '2170933940',\n",
       "   '2126707939']},\n",
       " {'id': '3000413850',\n",
       "  'title': 'Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV.',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus (MERS-CoV) is the causative agent of a severe respiratory disease associated with more than 2468 human infections and over 851 deaths in 27 countries since 2012. There are no approved treatments for MERS-CoV infection although a combination of lopinavir, ritonavir and interferon beta (LPV/RTV-IFNb) is currently being evaluated in humans in the Kingdom of Saudi Arabia. Here, we show that remdesivir (RDV) and IFNb have superior antiviral activity to LPV and RTV in vitro. In mice, both prophylactic and therapeutic RDV improve pulmonary function and reduce lung viral loads and severe lung pathology. In contrast, prophylactic LPV/RTV-IFNb slightly reduces viral loads without impacting other disease parameters. Therapeutic LPV/RTV-IFNb improves pulmonary function but does not reduce virus replication or severe lung pathology. Thus, we provide in vivo evidence of the potential for RDV to treat MERS-CoV infections.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Timothy P. Sheahan',\n",
       "   'Amy C. Sims',\n",
       "   'Sarah R. Leist',\n",
       "   'Alexandra Schäfer',\n",
       "   'John Won',\n",
       "   'Ariane J. Brown',\n",
       "   'Stephanie A. Montgomery',\n",
       "   'Alison Hogg',\n",
       "   'Darius Babusis',\n",
       "   'Michael O. Clarke',\n",
       "   'Jamie E. Spahn',\n",
       "   'Laura Bauer',\n",
       "   'Scott Sellers',\n",
       "   'Danielle Porter',\n",
       "   'Joy Y. Feng',\n",
       "   'Tomas Cihlar',\n",
       "   'Robert Jordan',\n",
       "   'Mark R. Denison',\n",
       "   'Ralph S. Baric'],\n",
       "  'related_topics': ['Lopinavir/ritonavir',\n",
       "   'Lopinavir',\n",
       "   'Lung injury',\n",
       "   'Viral load',\n",
       "   'Ritonavir',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Respiratory disease',\n",
       "   'Pulmonary function testing',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,327',\n",
       "  'reference_count': '53',\n",
       "  'references': ['2107277218',\n",
       "   '2470646526',\n",
       "   '2725497285',\n",
       "   '1993577573',\n",
       "   '2565805236',\n",
       "   '2791599184',\n",
       "   '2255243349',\n",
       "   '2292021561',\n",
       "   '2290466312',\n",
       "   '2793022939']},\n",
       " {'id': '2132260239',\n",
       "  'title': 'Identification of a novel coronavirus in patients with severe acute respiratory syndrome.',\n",
       "  'abstract': 'BACKGROUND: The severe acute respiratory syndrome (SARS) has recently been identified as a new clinical entity. SARS is thought to be caused by an unknown infectious agent. METHODS: Clinical specimens from patients with SARS were searched for unknown viruses with the use of cell cultures and molecular techniques. RESULTS: A novel coronavirus was identified in patients with SARS. The virus was isolated in cell culture, and a sequence 300 nucleotides in length was obtained by a polymerase-chain-reaction (PCR)-based random-amplification procedure. Genetic characterization indicated that the virus is only distantly related to known coronaviruses (identical in 50 to 60 percent of the nucleotide sequence). On the basis of the obtained sequence, conventional and real-time PCR assays for specific and sensitive detection of the novel virus were established. Virus was detected in a variety of clinical specimens from patients with SARS but not in controls. High concentrations of viral RNA of up to 100 million molecules per milliliter were found in sputum. Viral RNA was also detected at extremely low concentrations in plasma during the acute phase and in feces during the late convalescent phase. Infected patients showed seroconversion on the Vero cells in which the virus was isolated. CONCLUSIONS: The novel coronavirus might have a role in causing SARS.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Christian Drosten',\n",
       "   'Stephan Günther',\n",
       "   'Wolfgang Preiser',\n",
       "   'Sylvie van der Werf',\n",
       "   'Hans-Reinhard Brodt',\n",
       "   'Stephan Becker',\n",
       "   'Holger Rabenau',\n",
       "   'Marcus Panning',\n",
       "   'Larissa Kolesnikova',\n",
       "   'Ron A.M. Fouchier',\n",
       "   'Annemarie Berger',\n",
       "   'Ana-Maria Burguière',\n",
       "   'Jindrich Cinatl',\n",
       "   'Markus Eickmann',\n",
       "   'Nicolas Escriou',\n",
       "   'Klaus Grywna',\n",
       "   'Stefanie Kramme',\n",
       "   'Jean-Claude Manuguerra',\n",
       "   'Stefanie Müller',\n",
       "   'Volker Rickerts',\n",
       "   'Martin Stürmer',\n",
       "   'Simon Vieth',\n",
       "   'Hans-Dieter Klenk',\n",
       "   'Albert D.M.E. Osterhaus',\n",
       "   'Herbert Schmitz',\n",
       "   'Hans Wilhelm Doerr'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Human coronavirus OC43',\n",
       "   'Human coronavirus NL63',\n",
       "   'Novel virus',\n",
       "   'Virus',\n",
       "   'Nidovirales',\n",
       "   'Coronaviridae',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,038',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2100820722',\n",
       "   '2125251240',\n",
       "   '2107922358',\n",
       "   '2127062009',\n",
       "   '2084994773',\n",
       "   '2149579937',\n",
       "   '2090060897',\n",
       "   '2004869546',\n",
       "   '2030133843']},\n",
       " {'id': '2026274122',\n",
       "  'title': 'KDIGO clinical practice guidelines for acute kidney injury.',\n",
       "  'abstract': 'tion’, implying that most patients ‘should’ receive a particular action. In contrast, level 2 guidelines are essentially ‘suggestions’ and are deemed to be ‘weak’ or discretionary, recognising that management decisions may vary in different clinical contexts. Each recommendation was further graded from A to D by the quality of evidence underpinning them, with grade A referring to a high quality of evidence whilst grade D recognised a ‘very low’ evidence base. The overall strength and quality of the supporting evidence is summarised in table 1 . The guidelines focused on 4 key domains: (1) AKI definition, (2) prevention and treatment of AKI, (3) contrastinduced AKI (CI-AKI) and (4) dialysis interventions for the treatment of AKI. The full summary of clinical practice statements is available at www.kdigo.org, but a few key recommendation statements will be highlighted here.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Arif Khwaja'],\n",
       "  'related_topics': ['Renal angina',\n",
       "   'Psychological intervention',\n",
       "   'Intensive care medicine',\n",
       "   'Health care',\n",
       "   'MEDLINE',\n",
       "   'Dialysis',\n",
       "   'Quality (business)',\n",
       "   'Acute kidney injury',\n",
       "   'Medicine',\n",
       "   'Clinical Practice',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,067',\n",
       "  'reference_count': '23',\n",
       "  'references': ['1967300023',\n",
       "   '2131419242',\n",
       "   '2143432233',\n",
       "   '2117958746',\n",
       "   '1531106656',\n",
       "   '2157775267',\n",
       "   '2028701043',\n",
       "   '2111704803',\n",
       "   '2135163018',\n",
       "   '2042074736']},\n",
       " {'id': '2104548316',\n",
       "  'title': 'A novel coronavirus associated with severe acute respiratory syndrome.',\n",
       "  'abstract': 'background A worldwide outbreak of severe acute respiratory syndrome (SARS) has been associated with exposures originating from a single ill health care worker from Guangdong Province, China. We conducted studies to identify the etiologic agent of this outbreak. methods We received clinical specimens from patients in six countries and tested them, using virus isolation techniques, electron-microscopical and histologic studies, and molecular and serologic assays, in an attempt to identify a wide range of potential pathogens. results No classic respiratory or bacterial respiratory pathogen was consistently identified. However, a novel coronavirus was isolated from patients who met the case definition of SARS. Cytopathological features were noted microscopically in Vero E6 cells inoculated with a throat-swab specimen. Electron-microscopical examination of cultures revealed ultrastructural features characteristic of coronaviruses. Immunohistochemical and immunofluorescence staining revealed reactivity with group I coronavirus polyclonal antibodies. Consensus coronavirus primers designed to amplify a fragment of the polymerase gene by reverse transcription–polymerase chain reaction (RT-PCR) were used to obtain a sequence that clearly identified the isolate as a unique coronavirus only distantly related to previously sequenced coronaviruses. With specific diagnostic RT-PCR primers we identified several identical nucleotide sequences in 12 patients from several locations, a finding consistent with a point source outbreak. Indirect fluorescent antibody tests and enzyme-linked immunosorbent assays made with the new coronavirus isolate have been used to demonstrate a virus-specific serologic response. Preliminary studies suggest that this virus may never before have infected the U.S. population. conclusions A novel coronavirus is associated with this outbreak, and the evidence indicates that this virus has an etiologic role in SARS. The name Urbani SARS-associated coronavirus is proposed for the virus.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Ksiazek Tg',\n",
       "   'Erdman D',\n",
       "   'Goldsmith Cs',\n",
       "   'Zaki',\n",
       "   'Peret T',\n",
       "   'Emery S',\n",
       "   'Tong S',\n",
       "   'Urbani C',\n",
       "   'Comer Ja',\n",
       "   'Lim W',\n",
       "   'Rollin Pe',\n",
       "   'Dowell Sf',\n",
       "   'Ling Ae',\n",
       "   'Humphrey Cd',\n",
       "   'Shieh Wj',\n",
       "   'Guarner J',\n",
       "   'Paddock Cd',\n",
       "   'Rota P',\n",
       "   'Fields B',\n",
       "   'DeRisi J',\n",
       "   'Yang Jy',\n",
       "   'Cox N',\n",
       "   'Hughes Jm',\n",
       "   'LeDuc Jw',\n",
       "   'Bellini Wj',\n",
       "   'Anderson Lj'],\n",
       "  'related_topics': ['Human coronavirus OC43',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus 229E',\n",
       "   'Human coronavirus NL63',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Population',\n",
       "   'Nidovirales',\n",
       "   'Virus',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,166',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2106882534',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2463755683',\n",
       "   '2398786667',\n",
       "   '2127949919',\n",
       "   '1576737979',\n",
       "   '2128788856',\n",
       "   '2076620790']},\n",
       " {'id': '2131262274',\n",
       "  'title': 'A Major Outbreak of Severe Acute Respiratory Syndrome in Hong Kong',\n",
       "  'abstract': 'background There has been an outbreak of the severe acute respiratory syndrome (SARS) worldwide. We report the clinical, laboratory, and radiologic features of 138 cases of suspected SARS during a hospital outbreak in Hong Kong. methods From March 11 to 25, 2003, all patients with suspected SARS after exposure to an index patient or ward were admitted to the isolation wards of the Prince of Wales Hospital. Their demographic, clinical, laboratory, and radiologic characteristics were analyzed. Clinical end points included the need for intensive care and death. Univariate and multivariate analyses were performed. results There were 66 male patients and 72 female patients in this cohort, 69 of whom were health care workers. The most common symptoms included fever (in 100 percent of the patients); chills, rigors, or both (73.2 percent); and myalgia (60.9 percent). Cough and headache were also reported in more than 50 percent of the patients. Other common findings were lymphopenia (in 69.6 percent), thrombocytopenia (44.8 percent), and elevated lactate dehydrogenase and creatine kinase levels (71.0 percent and 32.1 percent, respectively). Peripheral air-space consolidation was commonly observed on thoracic computed tomographic scanning. A total of 32 patients (23.2 percent) were admitted to the intensive care unit; 5 patients died, all of whom had coexisting conditions. In a multivariate analysis, the independent predictors of an adverse outcome were advanced age (odds ratio per decade of life, 1.80; 95 percent confidence interval, 1.16 to 2.81; P=0.009), a high peak lactate dehydrogenase level (odds ratio per 100 U per liter, 2.09; 95 percent confidence interval, 1.28 to 3.42; P=0.003), and an absolute neutrophil count that exceeded the upper limit of the normal range on presentation (odds ratio, 1.60; 95 percent confidence interval, 1.03 to 2.50; P=0.04). conclusions SARS is a serious respiratory illness that led to significant morbidity and mortality in our cohort.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Nelson Lee',\n",
       "   'David Hui',\n",
       "   'Alan Wu',\n",
       "   'Paul Chan',\n",
       "   'Peter Cameron',\n",
       "   'Gavin M Joynt',\n",
       "   'Anil Ahuja',\n",
       "   'Man Yee Yung',\n",
       "   'C B Leung',\n",
       "   'K F To',\n",
       "   'S F Lui',\n",
       "   'C C Szeto',\n",
       "   'Sydney Chung',\n",
       "   'Joseph J Y Sung'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Odds ratio',\n",
       "   'Intensive care unit',\n",
       "   'Chills',\n",
       "   'Confidence interval',\n",
       "   'Cohort',\n",
       "   'myalgia',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Surgery',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,960',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2123324969',\n",
       "   '2130141864',\n",
       "   '2463755683',\n",
       "   '1991467275',\n",
       "   '1982444609']},\n",
       " {'id': '2006434809',\n",
       "  'title': 'Epidemiological, demographic, and clinical characteristics of 47 cases of Middle East respiratory syndrome coronavirus disease from Saudi Arabia: a descriptive study',\n",
       "  'abstract': 'Summary  Background  Middle East respiratory syndrome (MERS) is a new human disease caused by a novel coronavirus (CoV). Clinical data on MERS-CoV infections are scarce. We report epidemiological, demographic, clinical, and laboratory characteristics of 47 cases of MERS-CoV infections, identify knowledge gaps, and define research priorities.  Methods  We abstracted and analysed epidemiological, demographic, clinical, and laboratory data from confirmed cases of sporadic, household, community, and health-care-associated MERS-CoV infections reported from Saudi Arabia between Sept 1, 2012, and June 15, 2013. Cases were confirmed as having MERS-CoV by real-time RT-PCR.  Findings  47 individuals (46 adults, one child) with laboratory-confirmed MERS-CoV disease were identified; 36 (77%) were male (male:female ratio 3·3:1). 28 patients died, a 60% case-fatality rate. The case-fatality rate rose with increasing age. Only two of the 47 cases were previously healthy; most patients (45 [96%]) had underlying comorbid medical disorders, including diabetes (32 [68%]), hypertension (16 [34%]), chronic cardiac disease (13 [28%]), and chronic renal disease (23 [49%]). Common symptoms at presentation were fever (46 [98%]), fever with chills or rigors (41 [87%]), cough (39 [83%]), shortness of breath (34 [72%]), and myalgia (15 [32%]). Gastrointestinal symptoms were also frequent, including diarrhoea (12 [26%]), vomiting (ten [21%]), and abdominal pain (eight [17%]). All patients had abnormal findings on chest radiography, ranging from subtle to extensive unilateral and bilateral abnormalities. Laboratory analyses showed raised concentrations of lactate dehydrogenase (23 [49%]) and aspartate aminotransferase (seven [15%]) and thrombocytopenia (17 [36%]) and lymphopenia (16 [34%]).  Interpretation  Disease caused by MERS-CoV presents with a wide range of clinical manifestations and is associated with substantial mortality in admitted patients who have medical comorbidities. Major gaps in our knowledge of the epidemiology, community prevalence, and clinical spectrum of infection and disease need urgent definition.  Funding  None.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Abdullah Assiri',\n",
       "   'Jaffar A Al-Tawfiq',\n",
       "   'Abdullah A Al-Rabeeah',\n",
       "   'Fahad A Al-Rabiah',\n",
       "   'Sami Al-Hajjar',\n",
       "   'Ali Al-Barrak',\n",
       "   'Hesham Flemban',\n",
       "   'Wafa N Al-Nassir',\n",
       "   'Hanan H Balkhy',\n",
       "   'Rafat F Al-Hakeem',\n",
       "   'Hatem Q Makhdoom',\n",
       "   'Alimuddin I Zumla',\n",
       "   '',\n",
       "   'Ziad A Memish'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Epidemiology',\n",
       "   'myalgia',\n",
       "   'Chills',\n",
       "   'Abdominal pain',\n",
       "   'Young adult',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Odds ratio',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,309',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2131262274',\n",
       "   '1703839189',\n",
       "   '2112147913',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2163627712',\n",
       "   '2140143765',\n",
       "   '2119775949']},\n",
       " {'id': '3017468735',\n",
       "  'title': 'A Novel Coronavirus Genome Identified in a Cluster of Pneumonia Cases — Wuhan, China 2019−2020',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Wenjie',\n",
       "   'Xiang',\n",
       "   'Xuejun',\n",
       "   'Wenling',\n",
       "   'Peihua',\n",
       "   'Wenbo',\n",
       "   'George F.',\n",
       "   'Guizhen'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Disease cluster',\n",
       "   'Genome',\n",
       "   'Medicine',\n",
       "   'Virology',\n",
       "   'View Less'],\n",
       "  'citation_count': '368',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2725497285',\n",
       "  'title': 'Broad-spectrum antiviral GS-5734 inhibits both epidemic and zoonotic coronaviruses.',\n",
       "  'abstract': 'Emerging viral infections are difficult to control because heterogeneous members periodically cycle in and out of humans and zoonotic hosts, complicating the development of specific antiviral therapies and vaccines. Coronaviruses (CoVs) have a proclivity to spread rapidly into new host species causing severe disease. Severe acute respiratory syndrome CoV (SARS-CoV) and Middle East respiratory syndrome CoV (MERS-CoV) successively emerged, causing severe epidemic respiratory disease in immunologically naive human populations throughout the globe. Broad-spectrum therapies capable of inhibiting CoV infections would address an immediate unmet medical need and could be invaluable in the treatment of emerging and endemic CoV infections. We show that a nucleotide prodrug, GS-5734, currently in clinical development for treatment of Ebola virus disease, can inhibit SARS-CoV and MERS-CoV replication in multiple in vitro systems, including primary human airway epithelial cell cultures with submicromolar IC50 values. GS-5734 was also effective against bat CoVs, prepandemic bat CoVs, and circulating contemporary human CoV in primary human lung cells, thus demonstrating broad-spectrum anti-CoV activity. In a mouse model of SARS-CoV pathogenesis, prophylactic and early therapeutic administration of GS-5734 significantly reduced lung viral load and improved clinical signs of disease as well as respiratory function. These data provide substantive evidence that GS-5734 may prove effective against endemic MERS-CoV in the Middle East, circulating human CoV, and, possibly most importantly, emerging CoV of the future.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Timothy P. Sheahan',\n",
       "   'Amy C. Sims',\n",
       "   'Rachel L. Graham',\n",
       "   'Vineet D. Menachery',\n",
       "   'Lisa E. Gralinski',\n",
       "   'James B. Case',\n",
       "   'Sarah R. Leist',\n",
       "   'Krzysztof Pyrc',\n",
       "   'Joy Y. Feng',\n",
       "   'Iva Trantcheva',\n",
       "   'Roy Bannister',\n",
       "   'Yeojin Park',\n",
       "   'Darius Babusis',\n",
       "   'Michael O. Clarke',\n",
       "   'Richard L. Mackman',\n",
       "   'Jamie E. Spahn',\n",
       "   'Christopher A. Palmiotti',\n",
       "   'Dustin Siegel',\n",
       "   'Adrian S. Ray',\n",
       "   'Tomas Cihlar',\n",
       "   'Robert Jordan',\n",
       "   'Mark R. Denison',\n",
       "   'Ralph S. Baric'],\n",
       "  'related_topics': ['Respiratory function',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Ebola virus',\n",
       "   'Viral load',\n",
       "   'Disease',\n",
       "   'Viral replication',\n",
       "   'Respiratory disease',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,073',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2470646526',\n",
       "   '2129542667',\n",
       "   '2195009776',\n",
       "   '2255243349',\n",
       "   '2292021561',\n",
       "   '2115555188',\n",
       "   '2298153446',\n",
       "   '2525468044',\n",
       "   '1945961678',\n",
       "   '2099941783']},\n",
       " {'id': '3001897055',\n",
       "  'title': 'A Novel Coronavirus from Patients with Pneumonia in China, 2019.',\n",
       "  'abstract': 'In December 2019, a cluster of patients with pneumonia of unknown cause was linked to a seafood wholesale market in Wuhan, China. A previously unknown betacoronavirus was discovered through the use of unbiased sequencing in samples from patients with pneumonia. Human airway epithelial cells were used to isolate a novel coronavirus, named 2019-nCoV, which formed a clade within the subgenus sarbecovirus, Orthocoronavirinae subfamily. Different from both MERS-CoV and SARS-CoV, 2019-nCoV is the seventh member of the family of coronaviruses that infect humans. Enhanced surveillance and further investigation are ongoing. (Funded by the National Key Research and Development Program of China and the National Major Project for Control and Prevention of Infectious Disease in China.).',\n",
       "  'date': '2020',\n",
       "  'authors': ['Na Zhu',\n",
       "   'Dingyu Zhang',\n",
       "   'Wenling Wang',\n",
       "   'Xingwang Li',\n",
       "   'Bo Yang',\n",
       "   'Jingdong Song',\n",
       "   'Xiang Zhao',\n",
       "   'Baoying Huang',\n",
       "   'Weifeng Shi',\n",
       "   'Roujian Lu',\n",
       "   'Peihua Niu',\n",
       "   'Faxian Zhan',\n",
       "   'Xuejun Ma',\n",
       "   '',\n",
       "   'Dayan Wang',\n",
       "   'Wenbo Xu',\n",
       "   'Guizhen Wu',\n",
       "   'George F. Gao',\n",
       "   'Wenjie Tan'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Clade',\n",
       "   'Viral Epidemiology',\n",
       "   'Subfamily',\n",
       "   'Virology',\n",
       "   'Subgenus',\n",
       "   'Emergency Use Authorization',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,031',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2306794997',\n",
       "   '1909499787',\n",
       "   '3027518954',\n",
       "   '2792024998',\n",
       "   '2955025503',\n",
       "   '2257005270']},\n",
       " {'id': '3005079553',\n",
       "  'title': 'Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China.',\n",
       "  'abstract': 'Importance  In December 2019, novel coronavirus (2019-nCoV)–infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited.  Objective  To describe the epidemiological and clinical characteristics of NCIP.  Design, Setting, and Participants  Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020.  Exposures  Documented NCIP.  Main Outcomes and Measures  Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked.  Results  Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8\\u2009×\\u2009109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n\\u2009=\\u200936), compared with patients not treated in the ICU (n\\u2009=\\u2009102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n\\u2009=\\u200947), the median hospital stay was 10 days (IQR, 7.0-14.0).  Conclusions and Relevance  In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Dawei Wang',\n",
       "   'Bo Hu',\n",
       "   'Chang Hu',\n",
       "   'Fangfang Zhu',\n",
       "   'Xing Liu',\n",
       "   'Jing Zhang',\n",
       "   'Binbin Wang',\n",
       "   'Hui Xiang',\n",
       "   'Zhenshun Cheng',\n",
       "   'Yong Xiong',\n",
       "   'Yan Zhao',\n",
       "   'Yirong Li',\n",
       "   'Xinghuan Wang',\n",
       "   'Zhiyong Peng'],\n",
       "  'related_topics': ['Interquartile range',\n",
       "   'Intensive care unit',\n",
       "   'Pneumonia',\n",
       "   'Epidemiology',\n",
       "   'Retrospective cohort study',\n",
       "   'ARDS',\n",
       "   'Oxygen therapy',\n",
       "   'Extracorporeal membrane oxygenation',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,854',\n",
       "  'reference_count': '25',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3000834295',\n",
       "   '3003951199',\n",
       "   '2999409984',\n",
       "   '2999318660',\n",
       "   '1803784511']},\n",
       " {'id': '3002108456',\n",
       "  'title': 'Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study',\n",
       "  'abstract': 'In December, 2019, a pneumonia associated with the 2019 novel coronavirus (2019-nCoV) emerged in Wuhan, China. We aimed to further clarify the epidemiological and clinical characteristics of 2019-nCoV pneumonia. In this retrospective, single-centre study, we included all confirmed cases of 2019-nCoV in Wuhan Jinyintan Hospital from Jan 1 to Jan 20, 2020. Cases were confirmed by real-time RT-PCR and were analysed for epidemiological, demographic, clinical, and radiological features and laboratory data. Outcomes were followed up until Jan 25, 2020.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Nanshan Chen',\n",
       "   'Min Zhou',\n",
       "   'Xuan Dong',\n",
       "   'Jieming Qu',\n",
       "   'Fengyun Gong',\n",
       "   'Yang Han',\n",
       "   'Yang Qiu',\n",
       "   'Jingli Wang',\n",
       "   'Ying Liu',\n",
       "   'Yuan Wei',\n",
       "   \"Jia'an Xia\",\n",
       "   'Ting Yu',\n",
       "   'Xinxin Zhang',\n",
       "   'Li Zhang'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Epidemiology',\n",
       "   'Coronavirus',\n",
       "   'Retrospective cohort study',\n",
       "   'Comorbidity',\n",
       "   'Public health',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,942',\n",
       "  'reference_count': '28',\n",
       "  'references': ['3001118548',\n",
       "   '2903899730',\n",
       "   '2166867592',\n",
       "   '2999409984',\n",
       "   '2999318660',\n",
       "   '2132260239',\n",
       "   '2999364275',\n",
       "   '2909194930',\n",
       "   '2991899552',\n",
       "   '2775086803']},\n",
       " {'id': '3003668884',\n",
       "  'title': 'Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.',\n",
       "  'abstract': 'Abstract Background The initial cases of novel coronavirus (2019-nCoV)–infected pneumonia (NCIP) occurred in Wuhan, Hubei Province, China, in December 2019 and January 2020. We analyzed data on the...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Qun Li',\n",
       "   'Xuhua Guan',\n",
       "   'Peng Wu',\n",
       "   'Xiaoye Wang',\n",
       "   'Lei Zhou',\n",
       "   'Yeqing Tong',\n",
       "   'Ruiqi Ren',\n",
       "   'Kathy S.M. Leung',\n",
       "   'Eric H.Y. Lau',\n",
       "   'Jessica Y. Wong',\n",
       "   'Xuesen Xing',\n",
       "   'Nijuan Xiang',\n",
       "   'Yang Wu',\n",
       "   'Chao Li',\n",
       "   'Qi Chen',\n",
       "   'Dan Li',\n",
       "   'Tian Liu',\n",
       "   'Jing Zhao',\n",
       "   'Man Liu',\n",
       "   'Wenxiao Tu',\n",
       "   'Chuding Chen',\n",
       "   'Lianmei Jin',\n",
       "   'Rui Yang',\n",
       "   'Qi Wang',\n",
       "   'Suhua Zhou',\n",
       "   'Rui Wang',\n",
       "   'Hui Liu',\n",
       "   'Yingbo Luo',\n",
       "   'Yuan Liu',\n",
       "   'Ge Shao',\n",
       "   'Huan Li',\n",
       "   'Zhongfa Tao',\n",
       "   'Yang Yang',\n",
       "   'Zhiqiang Deng',\n",
       "   'Boxi Liu',\n",
       "   'Zhitao Ma',\n",
       "   'Yanping Zhang',\n",
       "   'Guoqing Shi',\n",
       "   'Tommy T.Y. Lam',\n",
       "   'Joseph T. Wu',\n",
       "   'George F. Gao',\n",
       "   'Benjamin J. Cowling',\n",
       "   'Bo Yang',\n",
       "   'Gabriel M. Leung',\n",
       "   'Zijian Feng'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Betacoronavirus',\n",
       "   'Outbreak',\n",
       "   'Transmission (mechanics)',\n",
       "   'Virology',\n",
       "   'China',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,678',\n",
       "  'reference_count': '18',\n",
       "  'references': ['3001897055',\n",
       "   '3002539152',\n",
       "   '3000834295',\n",
       "   '3002533507',\n",
       "   '2470646526',\n",
       "   '3002715510',\n",
       "   '1909499787',\n",
       "   '3001971765',\n",
       "   '2147166346',\n",
       "   '2149508011']},\n",
       " {'id': '3002539152',\n",
       "  'title': 'A familial cluster of pneumonia associated with the 2019 novel coronavirus indicating person-to-person transmission: a study of a family cluster.',\n",
       "  'abstract': \"Summary  Background  An ongoing outbreak of pneumonia associated with a novel coronavirus was reported in Wuhan city, Hubei province, China. Affected patients were geographically linked with a local wet market as a potential source. No data on person-to-person or nosocomial transmission have been published to date.  Methods  In this study, we report the epidemiological, clinical, laboratory, radiological, and microbiological findings of five patients in a family cluster who presented with unexplained pneumonia after returning to Shenzhen, Guangdong province, China, after a visit to Wuhan, and an additional family member who did not travel to Wuhan. Phylogenetic analysis of genetic sequences from these patients were done.  Findings  From Jan 10, 2020, we enrolled a family of six patients who travelled to Wuhan from Shenzhen between Dec 29, 2019 and Jan 4, 2020. Of six family members who travelled to Wuhan, five were identified as infected with the novel coronavirus. Additionally, one family member, who did not travel to Wuhan, became infected with the virus after several days of contact with four of the family members. None of the family members had contacts with Wuhan markets or animals, although two had visited a Wuhan hospital. Five family members (aged 36–66 years) presented with fever, upper or lower respiratory tract symptoms, or diarrhoea, or a combination of these 3–6 days after exposure. They presented to our hospital (The University of Hong Kong-Shenzhen Hospital, Shenzhen) 6–10 days after symptom onset. They and one asymptomatic child (aged 10 years) had radiological ground-glass lung opacities. Older patients (aged >60 years) had more systemic symptoms, extensive radiological ground-glass lung changes, lymphopenia, thrombocytopenia, and increased C-reactive protein and lactate dehydrogenase levels. The nasopharyngeal or throat swabs of these six patients were negative for known respiratory microbes by point-of-care multiplex RT-PCR, but five patients (four adults and the child) were RT-PCR positive for genes encoding the internal RNA-dependent RNA polymerase and surface Spike protein of this novel coronavirus, which were confirmed by Sanger sequencing. Phylogenetic analysis of these five patients' RT-PCR amplicons and two full genomes by next-generation sequencing showed that this is a novel coronavirus, which is closest to the bat severe acute respiatory syndrome (SARS)-related coronaviruses found in Chinese horseshoe bats.  Interpretation  Our findings are consistent with person-to-person transmission of this novel coronavirus in hospital and family settings, and the reports of infected travellers in other geographical regions.  Funding  The Shaw Foundation Hong Kong, Michael Seak-Kan Tong, Respiratory Viral Research Foundation Limited, Hui Ming, Hui Hoy and Chow Sin Lan Charity Fund Limited, Marina Man-Wai Lee, the Hong Kong Hainan Commercial Association South China Microbiology Research Fund, Sanming Project of Medicine (Shenzhen), and High Level-Hospital Program (Guangdong Health Commission).\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Jasper Fuk Woo Chan',\n",
       "   'Shuofeng Yuan',\n",
       "   'Kin Hang Kok',\n",
       "   'Kelvin Kai Wang To',\n",
       "   '',\n",
       "   'Hin Chu',\n",
       "   'Jin Yang',\n",
       "   'Fanfan Xing',\n",
       "   'Jieling Liu',\n",
       "   'Cyril Chik Yan Yip',\n",
       "   'Rosana Wing Shan Poon',\n",
       "   'Hoi Wah Tsoi',\n",
       "   'Simon Kam Fai Lo',\n",
       "   'Kwok Hung Chan',\n",
       "   'Vincent Kwok Man Poon',\n",
       "   'Wan Mui Chan',\n",
       "   'Jonathan Daniel Ip',\n",
       "   'Jian Piao Cai',\n",
       "   'Vincent Chi Chung Cheng',\n",
       "   'Honglin Chen',\n",
       "   '',\n",
       "   'Christopher Kim Ming Hui',\n",
       "   'Kwok Yung Yuen'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Epidemiology',\n",
       "   'Disease cluster',\n",
       "   'Pneumonia',\n",
       "   'Outbreak',\n",
       "   'Asymptomatic',\n",
       "   'Internal medicine',\n",
       "   'China',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,856',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2025170735',\n",
       "   '2129542667',\n",
       "   '2103503670',\n",
       "   '2115555188',\n",
       "   '2807736175',\n",
       "   '2105637133',\n",
       "   '2889758689',\n",
       "   '2769543984',\n",
       "   '2140338292',\n",
       "   '2170933940']},\n",
       " {'id': '3004318991',\n",
       "  'title': 'Genomic characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding.',\n",
       "  'abstract': 'Summary  Background  In late December, 2019, patients presenting with viral pneumonia due to an unidentified microbial agent were reported in Wuhan, China. A novel coronavirus was subsequently identified as the causative pathogen, provisionally named 2019 novel coronavirus (2019-nCoV). As of Jan 26, 2020, more than 2000 cases of 2019-nCoV infection have been confirmed, most of which involved people living in or visiting Wuhan, and human-to-human transmission has been confirmed.  Methods  We did next-generation sequencing of samples from bronchoalveolar lavage fluid and cultured isolates from nine inpatients, eight of whom had visited the Huanan seafood market in Wuhan. Complete and partial 2019-nCoV genome sequences were obtained from these individuals. Viral contigs were connected using Sanger sequencing to obtain the full-length genomes, with the terminal regions determined by rapid amplification of cDNA ends. Phylogenetic analysis of these 2019-nCoV genomes and those of other coronaviruses was used to determine the evolutionary history of the virus and help infer its likely origin. Homology modelling was done to explore the likely receptor-binding properties of the virus.  Findings  The ten genome sequences of 2019-nCoV obtained from the nine patients were extremely similar, exhibiting more than 99·98% sequence identity. Notably, 2019-nCoV was closely related (with 88% identity) to two bat-derived severe acute respiratory syndrome (SARS)-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21, collected in 2018 in Zhoushan, eastern China, but were more distant from SARS-CoV (about 79%) and MERS-CoV (about 50%). Phylogenetic analysis revealed that 2019-nCoV fell within the subgenus Sarbecovirus of the genus Betacoronavirus, with a relatively long branch length to its closest relatives bat-SL-CoVZC45 and bat-SL-CoVZXC21, and was genetically distinct from SARS-CoV. Notably, homology modelling revealed that 2019-nCoV had a similar receptor-binding domain structure to that of SARS-CoV, despite amino acid variation at some key residues.  Interpretation  2019-nCoV is sufficiently divergent from SARS-CoV to be considered a new human-infecting betacoronavirus. Although our phylogenetic analysis suggests that bats might be the original host of this virus, an animal sold at the seafood market in Wuhan might represent an intermediate host facilitating the emergence of the virus in humans. Importantly, structural analysis suggests that 2019-nCoV might be able to bind to the angiotensin-converting enzyme 2 receptor in humans. The future evolution, adaptation, and spread of this virus warrant urgent investigation.  Funding  National Key Research and Development Program of China, National Major Project for Control and Prevention of Infectious Disease in China, Chinese Academy of Sciences, Shandong First Medical University.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Roujian Lu',\n",
       "   'Xiang Zhao',\n",
       "   'Juan Li',\n",
       "   'Peihua Niu',\n",
       "   'Bo Yang',\n",
       "   'Honglong Wu',\n",
       "   'Wenling Wang',\n",
       "   'Hao Song',\n",
       "   'Baoying Huang',\n",
       "   'Na Zhu',\n",
       "   'Yuhai Bi',\n",
       "   'Xuejun Ma',\n",
       "   'Faxian Zhan',\n",
       "   'Liang Wang',\n",
       "   'Tao Hu',\n",
       "   'Hong Zhou',\n",
       "   'Zhenhong Hu',\n",
       "   'Weimin Zhou',\n",
       "   'Li Zhao',\n",
       "   'Jing Chen',\n",
       "   'Yao Meng',\n",
       "   'Ji Wang',\n",
       "   'Yang Lin',\n",
       "   'Jianying Yuan',\n",
       "   'Zhihao Xie',\n",
       "   'Jinmin Ma',\n",
       "   'William J Liu',\n",
       "   'Dayan Wang',\n",
       "   'Wenbo Xu',\n",
       "   'Edward C Holmes',\n",
       "   'George F Gao',\n",
       "   '',\n",
       "   'Guizhen Wu',\n",
       "   'Weijun Chen',\n",
       "   'Weifeng Shi',\n",
       "   'Wenjie Tan',\n",
       "   ''],\n",
       "  'related_topics': ['Betacoronavirus',\n",
       "   'Coronavirus',\n",
       "   'Phylogenetics',\n",
       "   'Sanger sequencing',\n",
       "   'Genome',\n",
       "   'Phylogenetic tree',\n",
       "   'Virus',\n",
       "   'Viral pneumonia',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,668',\n",
       "  'reference_count': '36',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '2103441770',\n",
       "   '2141052558',\n",
       "   '2166867592',\n",
       "   '2306794997',\n",
       "   '3017468735',\n",
       "   '2804822363']},\n",
       " {'id': '3003465021',\n",
       "  'title': 'First Case of 2019 Novel Coronavirus in the United States.',\n",
       "  'abstract': \"An outbreak of novel coronavirus (2019-nCoV) that began in Wuhan, China, has spread rapidly, with cases now confirmed in multiple countries. We report the first case of 2019-nCoV infection confirmed in the United States and describe the identification, diagnosis, clinical course, and management of the case, including the patient's initial mild symptoms at presentation with progression to pneumonia on day 9 of illness. This case highlights the importance of close coordination between clinicians and public health authorities at the local, state, and federal levels, as well as the need for rapid dissemination of clinical information related to the care of patients with this emerging infection.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Michelle L Holshue',\n",
       "   'Chas DeBolt',\n",
       "   'Scott',\n",
       "   'Kathy H',\n",
       "   'John',\n",
       "   'Hollianne',\n",
       "   'Christopher',\n",
       "   'Keith',\n",
       "   'Sara',\n",
       "   'Ahmet',\n",
       "   'George',\n",
       "   'Amanda',\n",
       "   'LeAnne',\n",
       "   'Anita',\n",
       "   'Susan I',\n",
       "   'Lindsay',\n",
       "   'Suxiang',\n",
       "   'Xiaoyan',\n",
       "   'Steve',\n",
       "   'Mark A',\n",
       "   'William C',\n",
       "   'Holly M',\n",
       "   'Timothy M',\n",
       "   'Satish K'],\n",
       "  'related_topics': ['Public health',\n",
       "   'Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Outbreak',\n",
       "   'Intensive care medicine',\n",
       "   'MEDLINE',\n",
       "   'Medicine',\n",
       "   'Clinical course',\n",
       "   'Clinical information',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,627',\n",
       "  'reference_count': '7',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002539152',\n",
       "   '3003951199',\n",
       "   '3000413850',\n",
       "   '2991491848',\n",
       "   '2605343262']},\n",
       " {'id': '3004239190',\n",
       "  'title': 'Transmission of 2019-nCoV Infection from an Asymptomatic Contact in Germany.',\n",
       "  'abstract': '2019-nCoV Transmission from Asymptomatic Patient In this report, investigators in Germany detected the spread of the novel coronavirus (2019-nCoV) from a person who had recently traveled from China...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Camilla Rothe',\n",
       "   'Mirjam Schunk',\n",
       "   'Peter Sothmann',\n",
       "   'Gisela Bretzel',\n",
       "   'Guenter Froeschl',\n",
       "   'Claudia Wallrauch',\n",
       "   'Thorbjörn Zimmer',\n",
       "   'Verena Thiel',\n",
       "   'Christian Janke',\n",
       "   'Wolfgang Guggemos',\n",
       "   'Michael Seilmaier',\n",
       "   'Christian Drosten',\n",
       "   'Patrick Vollmar',\n",
       "   'Katrin Zwirglmaier',\n",
       "   'Sabine Zange',\n",
       "   'Roman Wölfel',\n",
       "   'Michael Hoelscher'],\n",
       "  'related_topics': ['Asymptomatic Diseases',\n",
       "   'Asymptomatic',\n",
       "   'Transmission (mechanics)',\n",
       "   'Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Pneumonia',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,469',\n",
       "  'reference_count': '2',\n",
       "  'references': ['3001897055', '3001388158']},\n",
       " {'id': '3003573988',\n",
       "  'title': 'Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study.',\n",
       "  'abstract': 'Summary  Background  Since Dec 31, 2019, the Chinese city of Wuhan has reported an outbreak of atypical pneumonia caused by the 2019 novel coronavirus (2019-nCoV). Cases have been exported to other Chinese cities, as well as internationally, threatening to trigger a global outbreak. Here, we provide an estimate of the size of the epidemic in Wuhan on the basis of the number of cases exported from Wuhan to cities outside mainland China and forecast the extent of the domestic and global public health risks of epidemics, accounting for social and non-pharmaceutical prevention interventions.  Methods  We used data from Dec 31, 2019, to Jan 28, 2020, on the number of cases exported from Wuhan internationally (known days of symptom onset from Dec 25, 2019, to Jan 19, 2020) to infer the number of infections in Wuhan from Dec 1, 2019, to Jan 25, 2020. Cases exported domestically were then estimated. We forecasted the national and global spread of 2019-nCoV, accounting for the effect of the metropolitan-wide quarantine of Wuhan and surrounding cities, which began Jan 23–24, 2020. We used data on monthly flight bookings from the Official Aviation Guide and data on human mobility across more than 300 prefecture-level cities in mainland China from the Tencent database. Data on confirmed cases were obtained from the reports published by the Chinese Center for Disease Control and Prevention. Serial interval estimates were based on previous studies of severe acute respiratory syndrome coronavirus (SARS-CoV). A susceptible-exposed-infectious-recovered metapopulation model was used to simulate the epidemics across all major cities in China. The basic reproductive number was estimated using Markov Chain Monte Carlo methods and presented using the resulting posterior mean and 95% credibile interval (CrI).  Findings  In our baseline scenario, we estimated that the basic reproductive number for 2019-nCoV was 2·68 (95% CrI 2·47–2·86) and that 75\\u2008815 individuals (95% CrI 37\\u2008304–130\\u2008330) have been infected in Wuhan as of Jan 25, 2020. The epidemic doubling time was 6·4 days (95% CrI 5·8–7·1). We estimated that in the baseline scenario, Chongqing, Beijing, Shanghai, Guangzhou, and Shenzhen had imported 461 (95% CrI 227–805), 113 (57–193), 98 (49–168), 111 (56–191), and 80 (40–139) infections from Wuhan, respectively. If the transmissibility of 2019-nCoV were similar everywhere domestically and over time, we inferred that epidemics are already growing exponentially in multiple major cities of China with a lag time behind the Wuhan outbreak of about 1–2 weeks.  Interpretation  Given that 2019-nCoV is no longer contained within Wuhan, other major Chinese cities are probably sustaining localised outbreaks. Large cities overseas with close transport links to China could also become outbreak epicentres, unless substantial public health interventions at both the population and personal levels are implemented immediately. Independent self-sustaining outbreaks in major cities globally could become inevitable because of substantial exportation of presymptomatic cases and in the absence of large-scale public health interventions. Preparedness plans and mitigation interventions should be readied for quick deployment globally.  Funding  Health and Medical Research Fund (Hong Kong, China).',\n",
       "  'date': '2020',\n",
       "  'authors': ['Joseph T Wu', 'Kathy Leung', 'Gabriel M Leung'],\n",
       "  'related_topics': ['Population',\n",
       "   'Mainland China',\n",
       "   'China',\n",
       "   'Beijing',\n",
       "   'Outbreak',\n",
       "   'Serial interval',\n",
       "   'Public health',\n",
       "   'Preparedness',\n",
       "   'Socioeconomics',\n",
       "   'Geography',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,227',\n",
       "  'reference_count': '44',\n",
       "  'references': ['3003668884',\n",
       "   '3004397688',\n",
       "   '3002764620',\n",
       "   '2147166346',\n",
       "   '3002533591',\n",
       "   '1815575713',\n",
       "   '2069251911',\n",
       "   '2096145431',\n",
       "   '2104595316',\n",
       "   '1968393246']},\n",
       " {'id': '2163922914',\n",
       "  'title': 'Representation Learning: A Review and New Perspectives',\n",
       "  'abstract': 'The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Y. Bengio', 'A. Courville', 'P. Vincent'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Instance-based learning',\n",
       "   'Feature learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Computational learning theory',\n",
       "   'Competitive learning',\n",
       "   'Multi-task learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Inductive transfer',\n",
       "   'Sequence learning',\n",
       "   'Artificial neural network',\n",
       "   'Deep learning',\n",
       "   'Autoencoder',\n",
       "   'Boltzmann machine',\n",
       "   'Feature extraction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Inference',\n",
       "   'Probabilistic logic',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Manifold',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,032',\n",
       "  'reference_count': '237',\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2158899491',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2162915993',\n",
       "   '2160815625']},\n",
       " {'id': '2160815625',\n",
       "  'title': 'Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups',\n",
       "  'abstract': 'Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.',\n",
       "  'date': '2012',\n",
       "  'authors': ['G. Hinton',\n",
       "   'Li Deng',\n",
       "   'Dong Yu',\n",
       "   'G. E. Dahl',\n",
       "   'A. Mohamed',\n",
       "   'N. Jaitly',\n",
       "   'Andrew Senior',\n",
       "   'V. Vanhoucke',\n",
       "   'P. Nguyen',\n",
       "   'T. N. Sainath',\n",
       "   'B. Kingsbury'],\n",
       "  'related_topics': ['Acoustic model',\n",
       "   'Time delay neural network',\n",
       "   'FMLLR',\n",
       "   'Hidden Markov model',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Speech recognition',\n",
       "   'Margin (machine learning)',\n",
       "   'Frame (networking)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,951',\n",
       "  'reference_count': '63',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2116064496',\n",
       "   '2145094598',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '44815768',\n",
       "   '1498436455',\n",
       "   '1994197834']},\n",
       " {'id': '2022508996',\n",
       "  'title': 'Learning Hierarchical Features for Scene Labeling',\n",
       "  'abstract': 'Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.',\n",
       "  'date': '2013',\n",
       "  'authors': ['C. Farabet', 'C. Couprie', 'L. Najman', 'Y. LeCun'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Image texture',\n",
       "   'Feature vector',\n",
       "   'Feature extraction',\n",
       "   'Contextual image classification',\n",
       "   'Pixel',\n",
       "   'Segmentation',\n",
       "   'Deep learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,757',\n",
       "  'reference_count': '48',\n",
       "  'references': ['2310919327',\n",
       "   '2110158442',\n",
       "   '2546302380',\n",
       "   '2130325614',\n",
       "   '1999478155',\n",
       "   '2143516773',\n",
       "   '1423339008',\n",
       "   '2156163116',\n",
       "   '2169551590',\n",
       "   '2031342017']},\n",
       " {'id': '1993882792',\n",
       "  'title': 'Acoustic Modeling Using Deep Belief Networks',\n",
       "  'abstract': 'Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.',\n",
       "  'date': '2012',\n",
       "  'authors': ['A. Mohamed', 'G. E. Dahl', 'G. Hinton'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Hidden Markov model',\n",
       "   'Deep belief network',\n",
       "   'Discriminative model',\n",
       "   'Mixture model',\n",
       "   'Artificial neural network',\n",
       "   'Feature vector',\n",
       "   'TIMIT',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,894',\n",
       "  'reference_count': '41',\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147768505',\n",
       "   '2159080219',\n",
       "   '44815768',\n",
       "   '1994197834',\n",
       "   '2913932916',\n",
       "   '2103359087']},\n",
       " {'id': '2151103935',\n",
       "  'title': 'Distinctive Image Features from Scale-Invariant Keypoints',\n",
       "  'abstract': 'This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.',\n",
       "  'date': '2004',\n",
       "  'authors': ['David G. Lowe'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Haar-like features',\n",
       "   'Feature (computer vision)',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Maximally stable extremal regions',\n",
       "   'Scale space',\n",
       "   'Point set registration',\n",
       "   'Hough transform',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'View Less'],\n",
       "  'citation_count': '63,331',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '2154422044',\n",
       "   '2012778485',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2165497495',\n",
       "   '1949116567']},\n",
       " {'id': '2038721957',\n",
       "  'title': 'WordNet : an electronic lexical database',\n",
       "  'abstract': 'Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Christiane'],\n",
       "  'related_topics': ['eXtended WordNet',\n",
       "   'EuroWordNet',\n",
       "   'WordNet',\n",
       "   'Normalized Google distance',\n",
       "   'IndoWordNet',\n",
       "   'Lexical database',\n",
       "   'GermaNet',\n",
       "   'Semantic network',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '21,240',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2128017662',\n",
       "  'title': 'Scalable Recognition with a Vocabulary Tree',\n",
       "  'abstract': 'A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\\x92s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.',\n",
       "  'date': '2006',\n",
       "  'authors': ['D. Nister', 'H. Stewenius'],\n",
       "  'related_topics': ['Vocabulary',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Search engine indexing',\n",
       "   'Robustness (computer science)',\n",
       "   'Scalability',\n",
       "   'Pattern recognition',\n",
       "   'Visualization',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,708',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '2131846894',\n",
       "   '1980911747',\n",
       "   '2104978738',\n",
       "   '2172188317',\n",
       "   '2124404372',\n",
       "   '2147717514',\n",
       "   '2162006472',\n",
       "   '2165497495']},\n",
       " {'id': '2110764733',\n",
       "  'title': 'LabelMe: A Database and Web-Based Tool for Image Annotation',\n",
       "  'abstract': 'We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Bryan C. Russell',\n",
       "   'Antonio Torralba',\n",
       "   'Kevin P. Murphy',\n",
       "   'William T. Freeman'],\n",
       "  'related_topics': ['Image retrieval',\n",
       "   'LabelMe',\n",
       "   'Automatic image annotation',\n",
       "   'Object detection',\n",
       "   'Object (computer science)',\n",
       "   'Supervised learning',\n",
       "   'WordNet',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Information retrieval',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,307',\n",
       "  'reference_count': '44',\n",
       "  'references': ['2156909104',\n",
       "   '2164598857',\n",
       "   '2038721957',\n",
       "   '2138451337',\n",
       "   '2154422044',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2134557905',\n",
       "   '2156598602']},\n",
       " {'id': '1782590233',\n",
       "  'title': 'Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments',\n",
       "  'abstract': 'Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Gary B. Huang',\n",
       "   'Marwan Mattar',\n",
       "   'Tamara Berg',\n",
       "   'Eric Learned-Miller'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Face detection',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Database',\n",
       "   'Quality (business)',\n",
       "   'Range (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Face synthesis',\n",
       "   'Image acquisition',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,170',\n",
       "  'reference_count': '40',\n",
       "  'references': ['3097096317',\n",
       "   '2121647436',\n",
       "   '1999478155',\n",
       "   '2033419168',\n",
       "   '2123921160',\n",
       "   '2137659841',\n",
       "   '2098693229',\n",
       "   '2125310925',\n",
       "   '2994340921',\n",
       "   '2006793117']},\n",
       " {'id': '1576445103',\n",
       "  'title': 'Caltech-256 Object Category Dataset',\n",
       "  'abstract': 'We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Gregory', 'Alex', 'Pietro'],\n",
       "  'related_topics': ['Caltech 101',\n",
       "   'Pyramid (image processing)',\n",
       "   'Set (abstract data type)',\n",
       "   'Clutter',\n",
       "   'Object (computer science)',\n",
       "   'Matching (graph theory)',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer vision',\n",
       "   'Measure (mathematics)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,342',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2145607950',\n",
       "  'title': '80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition',\n",
       "  'abstract': 'With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.',\n",
       "  'date': '2008',\n",
       "  'authors': ['A. Torralba', 'R. Fergus', 'W.T. Freeman'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Object detection',\n",
       "   'Image processing',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Lexical database',\n",
       "   'Human visual system model',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,893',\n",
       "  'reference_count': '54',\n",
       "  'references': ['2164598857',\n",
       "   '2162915993',\n",
       "   '2124386111',\n",
       "   '2038721957',\n",
       "   '2128017662',\n",
       "   '2110764733',\n",
       "   '1576445103',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2111993661']},\n",
       " {'id': '2141282920',\n",
       "  'title': 'Labeling images with a computer game',\n",
       "  'abstract': \"We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.\",\n",
       "  'date': '2004',\n",
       "  'authors': ['Luis von Ahn', 'Laura Dabbish'],\n",
       "  'related_topics': ['Computer game',\n",
       "   'Game design',\n",
       "   'Game development tool',\n",
       "   'Game design document',\n",
       "   'Game art design',\n",
       "   'Multimedia',\n",
       "   'Computer science',\n",
       "   'Block (data storage)',\n",
       "   'Image (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,874',\n",
       "  'reference_count': '10',\n",
       "  'references': ['1666447063',\n",
       "   '1934863104',\n",
       "   '2166770390',\n",
       "   '1587328194',\n",
       "   '2293605478',\n",
       "   '2970081408',\n",
       "   '2055225264',\n",
       "   '2050457084',\n",
       "   '181417509',\n",
       "   '2612148268']},\n",
       " {'id': '2115733720',\n",
       "  'title': 'One-shot learning of object categories',\n",
       "  'abstract': 'Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Li Fei-Fei', 'R. Fergus', 'P. Perona'],\n",
       "  'related_topics': ['One-shot learning',\n",
       "   'Supervised learning',\n",
       "   'Maximum a posteriori estimation',\n",
       "   'Caltech 101',\n",
       "   'Object (computer science)',\n",
       "   'Bayesian probability',\n",
       "   'Prior probability',\n",
       "   \"Bayes' theorem\",\n",
       "   'Probabilistic logic',\n",
       "   'Inference',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,025',\n",
       "  'reference_count': '44',\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2124386111',\n",
       "   '2217896605',\n",
       "   '2154422044',\n",
       "   '2045656233',\n",
       "   '2166049352',\n",
       "   '2134557905',\n",
       "   '2130416410',\n",
       "   '2030536784']},\n",
       " {'id': '1528789833',\n",
       "  'title': 'TextonBoost : joint appearance, shape and context modeling for multi-class object recognition and segmentation',\n",
       "  'abstract': 'This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods.\\r\\n\\r\\nHigh classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).',\n",
       "  'date': '2006',\n",
       "  'authors': ['Jamie Shotton',\n",
       "   'John Winn',\n",
       "   'Carsten Rother',\n",
       "   'Antonio Criminisi'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'One-class classification',\n",
       "   'Conditional random field',\n",
       "   'Discriminative model',\n",
       "   'Context model',\n",
       "   'Segmentation',\n",
       "   'Feature selection',\n",
       "   'Boosting (machine learning)',\n",
       "   'Supervised learning',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Linear discriminant analysis',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,510',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2164598857',\n",
       "   '2147880316',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2124351162',\n",
       "   '2169551590',\n",
       "   '2024046085',\n",
       "   '1666447063',\n",
       "   '2168002178',\n",
       "   '1484228140']},\n",
       " {'id': '3008818676',\n",
       "  'title': 'The epidemiological characteristics of an outbreak of 2019 novel coronavirus diseases (COVID-19) in China',\n",
       "  'abstract': 'Objective\\r\\nAn outbreak of 2019 novel coronavirus diseases (COVID-19) in Wuhan, China has spread quickly nationwide. Here, we report results of a descriptive, exploratory analysis of all cases diagnosed as of February 11, 2020.\\r\\n\\r\\n\\r\\nMethods\\r\\nAll COVID-19 cases reported through February 11, 2020 were extracted from China’s Infectious Disease Information System. Analyses included: 1) summary of patient characteristics; 2) examination of age distributions and sex ratios; 3) calculation of case fatality and mortality rates; 4) geo-temporal analysis of viral spread; 5) epidemiological curve construction; and 6) subgroup analysis.\\r\\n\\r\\n\\r\\nResults\\r\\nA total of 72 314 patient records-44 672 (61.8%) confirmed cases, 16 186 (22.4%) suspected cases, 10567 (14.6%) clinical diagnosed cases (Hubei only), and 889 asymptomatic cases (1.2%)-contributed data for the analysis. Among confirmed cases, most were aged 30-79 years (86.6%), diagnosed in Hubei (74.7%), and considered mild/mild pneumonia (80.9%). A total of 1 023 deaths occurred among confirmed cases for an overall case-fatality rate of 2.3%. The COVID-19 spread outward from Hubei sometime after December 2019 and by February 11, 2020, 1 386 counties across all 31 provinces were affected. The epidemic curve of onset of symptoms peaked in January 23-26, then began to decline leading up to February 11. A total of 1 716 health workers have become infected and 5 have died (0.3%).\\r\\n\\r\\n\\r\\nConclusions\\r\\nThe COVID-19 epidemic has spread very quickly. It only took 30 days to expand from Hubei to the rest of Mainland China. With many people returning from a long holiday, China needs to prepare for the possible rebound of the epidemic.\\r\\n\\r\\n\\r\\nKey words: \\r\\n2019 Novel Coronavirus;\\xa0Outbreak;\\xa0Epidemiological characteristics',\n",
       "  'date': '2020',\n",
       "  'authors': ['Novel Coronavirus Pneumonia Emergency Response Epidemiology Team'],\n",
       "  'related_topics': ['Case fatality rate',\n",
       "   'Outbreak',\n",
       "   'Mortality rate',\n",
       "   'Epidemiology',\n",
       "   'Asymptomatic',\n",
       "   'Subgroup analysis',\n",
       "   'Mainland China',\n",
       "   'Pediatrics',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,789',\n",
       "  'reference_count': '9',\n",
       "  'references': ['3033453353',\n",
       "   '3035018050',\n",
       "   '3037451072',\n",
       "   '3034593359',\n",
       "   '3033301213',\n",
       "   '3021916232',\n",
       "   '3031029566',\n",
       "   '3037552531',\n",
       "   '3037851904']},\n",
       " {'id': '3004906315',\n",
       "  'title': 'CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV).',\n",
       "  'abstract': 'In this retrospective case series, chest CT scans of 21 symptomatic patients from China infected with the 2019 novel coronavirus (2019-nCoV) were reviewed, with emphasis on identifying and characterizing the most common findings. Typical CT findings included bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities, sometimes with a rounded morphology and a peripheral lung distribution. Notably, lung cavitation, discrete pulmonary nodules, pleural effusions, and lymphadenopathy were absent. Follow-up imaging in a subset of patients during the study time window often demonstrated mild or moderate progression of disease, as manifested by increasing extent and density of lung opacities.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Michael Chung',\n",
       "   'Adam Bernheim',\n",
       "   'Xueyan Mei',\n",
       "   'Ning Zhang',\n",
       "   'Mingqian Huang',\n",
       "   'Xianjun Zeng',\n",
       "   'Jiufa Cui',\n",
       "   'Wenjian Xu',\n",
       "   'Yang Yang',\n",
       "   'Zahi A. Fayad',\n",
       "   'Adam Jacobi',\n",
       "   'Kunwei Li',\n",
       "   'Shaolin Li',\n",
       "   'Hong Shan'],\n",
       "  'related_topics': ['Lung',\n",
       "   'Pneumonia',\n",
       "   'Radiology',\n",
       "   'Retrospective cohort study',\n",
       "   'Tomography',\n",
       "   'Parenchyma',\n",
       "   'Medicine',\n",
       "   'Ct imaging',\n",
       "   'Lung cavitation',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,842',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2102634410',\n",
       "   '2800783955',\n",
       "   '2112136274',\n",
       "   '2056155046',\n",
       "   '2279340859',\n",
       "   '2080286891',\n",
       "   '2162899218']},\n",
       " {'id': '3006643024',\n",
       "  'title': 'Time Course of Lung Changes at Chest CT during Recovery from Coronavirus Disease 2019 (COVID-19).',\n",
       "  'abstract': 'Background Chest CT is used to assess the severity of lung involvement in coronavirus disease 2019 (COVID-19). Purpose To determine the changes in chest CT findings associated with COVID-19 from initial diagnosis until patient recovery. Materials and Methods This retrospective review included patients with real-time polymerase chain reaction-confirmed COVID-19 who presented between January 12, 2020, and February 6, 2020. Patients with severe respiratory distress and/or oxygen requirement at any time during the disease course were excluded. Repeat chest CT was performed at approximately 4-day intervals. Each of the five lung lobes was visually scored on a scale of 0 to 5, with 0 indicating no involvement and 5 indicating more than 75% involvement. The total CT score was determined as the sum of lung involvement, ranging from 0 (no involvement) to 25 (maximum involvement). Results Twenty-one patients (six men and 15 women aged 25-63 years) with confirmed COVID-19 were evaluated. A total of 82 chest CT scans were obtained in these patients, with a mean interval (±standard deviation) of 4 days ± 1 (range, 1-8 days). All patients were discharged after a mean hospitalization period of 17 days ± 4 (range, 11-26 days). Maximum lung involved peaked at approximately 10 days (with a calculated total CT score of 6) from the onset of initial symptoms (R2 = 0.25, P < .001). Based on quartiles of chest CT scans from day 0 to day 26 involvement, four stages of lung CT findings were defined. CT scans obtained in stage 1 (0-4 days) showed ground-glass opacities (18 of 24 scans [75%]), with a mean total CT score of 2 ± 2; scans obtained in stage 2 (5-8 days) showed an increase in both the crazy-paving pattern (nine of 17 scans [53%]) and total CT score (mean, 6 ± 4; P = .002); scans obtained in stage 3 (9-13 days) showed consolidation (19 of 21 scans [91%]) and a peak in the total CT score (mean, 7 ± 4); and scans obtained in stage 4 (≥14 days) showed gradual resolution of consolidation (15 of 20 scans [75%]) and a decrease in the total CT score (mean, 6 ± 4) without crazy-paving pattern. Conclusion In patients recovering from coronavirus disease 2019 (without severe respiratory distress during the disease course), lung abnormalities on chest CT scans showed greatest severity approximately 10 days after initial onset of symptoms. © RSNA, 2020.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Feng',\n",
       "   'Tianhe',\n",
       "   'Peng',\n",
       "   'Shan',\n",
       "   'Bo',\n",
       "   'Lingli',\n",
       "   'Dandan',\n",
       "   'Jiazheng',\n",
       "   'Richard L',\n",
       "   'Lian',\n",
       "   'Chuansheng'],\n",
       "  'related_topics': ['Lung',\n",
       "   'Respiratory distress',\n",
       "   'Pneumonia',\n",
       "   'Nuclear medicine',\n",
       "   'Stage (cooking)',\n",
       "   'Radiography',\n",
       "   'Retrospective cohort study',\n",
       "   'Quartile',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,289',\n",
       "  'reference_count': '12',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3004906315',\n",
       "   '2102634410',\n",
       "   '2800783955',\n",
       "   '3004802901',\n",
       "   '2092969802',\n",
       "   '2056155046']},\n",
       " {'id': '3006110666',\n",
       "  'title': 'Chest CT for Typical Coronavirus Disease 2019 (COVID-19) Pneumonia: Relationship to Negative RT-PCR Testing.',\n",
       "  'abstract': 'Some patients with positive chest CT findings may present with negative results of real-time reverse-transcription polymerase chain reaction (RT-PCR) tests for coronavirus disease 2019 (COVID-19). In this study, the authors present chest CT findings from five patients with COVID-19 infection who had initial negative RT-PCR results. All five patients had typical imaging findings, including ground-glass opacity (five patients) and/or mixed ground-glass opacity and mixed consolidation (two patients). After isolation for presumed COVID-19 pneumonia, all patients were eventually confirmed to have COVID-19 infection by means of repeated swab tests. A combination of repeated swab tests and CT scanning may be helpful for individuals with a high clinical suspicion of COVID-19 infection but negative findings at RT-PCR screening.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Xingzhi Xie',\n",
       "   'Zheng Zhong',\n",
       "   'Wei Zhao',\n",
       "   'Chao Zheng',\n",
       "   'Fei Wang',\n",
       "   'Jun Liu'],\n",
       "  'related_topics': ['False Negative Reactions',\n",
       "   'Radiology',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Radiographic image interpretation',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,468',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3001897055',\n",
       "   '3004906315',\n",
       "   '3005272159',\n",
       "   '2112136274',\n",
       "   '2056155046']},\n",
       " {'id': '3006354146',\n",
       "  'title': 'Initial CT findings and temporal changes in patients with the novel coronavirus pneumonia (2019-nCoV): a study of 63 patients in Wuhan, China.',\n",
       "  'abstract': \"The purpose of this study was to observe the imaging characteristics of the novel coronavirus pneumonia. Sixty-three confirmed patients were enrolled from December 30, 2019 to January 31, 2020. High-resolution CT (HRCT) of the chest was performed. The number of affected lobes, ground glass nodules (GGO), patchy/punctate ground glass opacities, patchy consolidation, fibrous stripes and irregular solid nodules in each patient's chest CT image were recorded. Additionally, we performed imaging follow-up of these patients. CT images of 63 confirmed patients were collected. M/F ratio: 33/30. The mean age was 44.9 ± 15.2 years. The mean number of affected lobes was 3.3 ± 1.8. Nineteen (30.2%) patients had one affected lobe, five (7.9%) patients had two affected lobes, four (6.3%) patients had three affected lobes, seven (11.1%) patients had four affected lobes while 28 (44.4%) patients had 5 affected lobes. Fifty-four (85.7%) patients had patchy/punctate ground glass opacities, 14 (22.2%) patients had GGO, 12 (19.0%) patients had patchy consolidation, 11 (17.5%) patients had fibrous stripes and 8 (12.7%) patients had irregular solid nodules. Fifty-four (85.7%) patients progressed, including single GGO increased, enlarged and consolidated; fibrous stripe enlarged, while solid nodules increased and enlarged. Imaging changes in novel viral pneumonia are rapid. The manifestations of the novel coronavirus pneumonia are diverse. Imaging changes of typical viral pneumonia and some specific imaging features were observed. Therefore, we need to strengthen the recognition of image changes to help clinicians to diagnose quickly and accurately. • High-resolution CT (HRCT) of the chest is critical for early detection, evaluation of disease severity and follow-up of patients with the novel coronavirus pneumonia. • The manifestations of the novel coronavirus pneumonia are diverse and change rapidly. • Radiologists should be aware of the various features of the disease and temporal changes.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Yueying Pan',\n",
       "   'Hanxiong Guan',\n",
       "   'Shuchang Zhou',\n",
       "   'Yujin Wang',\n",
       "   'Qian Li',\n",
       "   'Tingting Zhu',\n",
       "   'Qiongjie Hu',\n",
       "   'Liming Xia'],\n",
       "  'related_topics': ['Viral pneumonia',\n",
       "   'Neuroradiology',\n",
       "   'Radiology',\n",
       "   'Interventional radiology',\n",
       "   'Ultrasound',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Ct findings',\n",
       "   'In patient',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '692',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '3001465255',\n",
       "   '3017468735',\n",
       "   '3001456238']},\n",
       " {'id': '3003901880',\n",
       "  'title': 'CT Imaging of the 2019 Novel Coronavirus (2019-nCoV) Pneumonia.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Junqiang Lei', 'Junfeng Li', 'Xun Li', 'Xiaolong Qi'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Respiratory sounds',\n",
       "   'Tomography',\n",
       "   'Radiology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Ct imaging',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed',\n",
       "   'X ray computed',\n",
       "   'View Less'],\n",
       "  'citation_count': '475',\n",
       "  'reference_count': '1',\n",
       "  'references': ['3002533507']},\n",
       " {'id': '3005656138',\n",
       "  'title': 'Use of Chest CT in Combination with Negative RT-PCR Assay for the 2019 Novel Coronavirus but High Clinical Suspicion.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Peikai Huang',\n",
       "   'Tianzhu Liu',\n",
       "   'Lesheng Huang',\n",
       "   'Hailong Liu',\n",
       "   'Ming Lei',\n",
       "   'Wangdong Xu',\n",
       "   'Xiaolu Hu',\n",
       "   'Jun Chen',\n",
       "   'Bo Liu'],\n",
       "  'related_topics': ['Real-time polymerase chain reaction',\n",
       "   'Pneumonia',\n",
       "   'False Negative Reactions',\n",
       "   'Medicine',\n",
       "   'Pathology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Disease progression',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'X ray computed',\n",
       "   'View Less'],\n",
       "  'citation_count': '350',\n",
       "  'reference_count': '2',\n",
       "  'references': ['3001897055', '3004668429']},\n",
       " {'id': '3004511262',\n",
       "  'title': 'Evolution of CT Manifestations in a Patient Recovered from 2019 Novel Coronavirus (2019-nCoV) Pneumonia in Wuhan, China.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Heshui Shi', 'Xiaoyu Han', 'Chuansheng'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Medicine',\n",
       "   'Virology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Lung pathology',\n",
       "   'Patient discharge',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed',\n",
       "   'View Less'],\n",
       "  'citation_count': '189',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3008962515',\n",
       "  'title': 'Molecular and serological investigation of 2019-nCoV infected patients: implication of multiple shedding routes.',\n",
       "  'abstract': 'In December 2019, a novel coronavirus (2019-nCoV) caused an outbreak in Wuhan, China, and soon spread to other parts of the world. It was believed that 2019-nCoV was transmitted through respiratory tract and then induced pneumonia, thus molecular diagnosis based on oral swabs was used for confirmation of this disease. Likewise, patient will be released upon two times of negative detection from oral swabs. However, many coronaviruses can also be transmitted through oral-fecal route by infecting intestines. Whether 2019-nCoV infected patients also carry virus in other organs like intestine need to be tested. We conducted investigation on patients in a local hospital who were infected with this virus. We found the presence of 2019-nCoV in anal swabs and blood as well, and more anal swab positives than oral swab positives in a later stage of infection, suggesting shedding and thereby transmitted through oral-fecal route. We also showed serology test can improve detection positive rate thus should be used in future epidemiology. Our report provides a cautionary warning that 2019-nCoV may be shed through multiple routes.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Wei Zhang',\n",
       "   'Rong-Hui Du',\n",
       "   'Bei Li',\n",
       "   'Xiao-Shuang Zheng',\n",
       "   'Xing-Lou Yang',\n",
       "   'Ben Hu',\n",
       "   'Yan-Yi Wang',\n",
       "   'Geng-Fu Xiao',\n",
       "   'Bing Yan',\n",
       "   'Zheng-Li Shi',\n",
       "   'Peng Zhou'],\n",
       "  'related_topics': ['Viral shedding',\n",
       "   'Coronavirus',\n",
       "   'Serology',\n",
       "   'Outbreak',\n",
       "   'Pneumonia (non-human)',\n",
       "   'Virus',\n",
       "   'Disease',\n",
       "   'Epidemiology',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,372',\n",
       "  'reference_count': '13',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3003668884',\n",
       "   '3004280078',\n",
       "   '2786098272',\n",
       "   '2769543984',\n",
       "   '2021442163',\n",
       "   '3025232310',\n",
       "   '3028321619',\n",
       "   '3027541845']},\n",
       " {'id': '3008452791',\n",
       "  'title': 'Viral load of SARS-CoV-2 in clinical samples.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yang Pan',\n",
       "   '',\n",
       "   'Daitao Zhang',\n",
       "   'Peng Yang',\n",
       "   '',\n",
       "   'Leo L M Poon',\n",
       "   'Quanyi Wang'],\n",
       "  'related_topics': ['Feces analysis',\n",
       "   'Viral load',\n",
       "   'Pneumonia',\n",
       "   'Pandemic',\n",
       "   'Virology',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '850',\n",
       "  'reference_count': '3',\n",
       "  'references': ['3004318991', '3003637715', '2129542667']},\n",
       " {'id': '3033453353',\n",
       "  'title': 'Recent Understandings Toward Coronavirus Disease 2019 (COVID-19): From Bench to Bedside',\n",
       "  'abstract': 'In late December 2019, an unprecedented outbreak of coronavirus disease 2019 (COVID-19) caused by SARS coronavirus 2 (SARS-CoV-2) (previously named 2019-nCoV) in Wuhan became the most challenging health emergency. Since its rapid spread in China and many other countries, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern (PHEIC) on 30th January 2020 and a pandemic on 11th March 2020. Thousands of people have died, and there are currently no vaccines or specific antiviral drugs for COVID-19. Therefore, it is critical to have a comprehensive understanding of the virus. In this review, we highlight the etiology, epidemiology, pathogenesis and pathology, clinical characteristics, diagnosis, clinical management, prognosis, infection control and prevention of COVID-19 based on recent studies.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Jie Yu', 'Peiwei Chai', 'Shengfang Ge', 'Xianqun Fan'],\n",
       "  'related_topics': ['Pandemic',\n",
       "   'Public health',\n",
       "   'Epidemiology',\n",
       "   'Outbreak',\n",
       "   'Infection control',\n",
       "   'Etiology',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '13',\n",
       "  'reference_count': '116',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3004280078',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '3004239190']},\n",
       " {'id': '3034408674',\n",
       "  'title': 'Risk assessment of mixed and displacement ventilation (LAF) during orthopedic and trauma surgery on COVID-19 patients with increased release of infectious aerosols',\n",
       "  'abstract': 'No abstract available\\r\\nKeywords: Displacement ventilation; SARS-CoV-2 spread; laminar air flow; mixed ventilation; negative pressure; no ventilation.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Axel Kramer',\n",
       "   'Rüdiger Külpmann',\n",
       "   'Arnold Brunner',\n",
       "   'Michael Müller',\n",
       "   'Georgi Wassilew'],\n",
       "  'related_topics': ['Ventilation (architecture)',\n",
       "   'Displacement ventilation',\n",
       "   'Trauma surgery',\n",
       "   'Orthopedic surgery',\n",
       "   'Anesthesia',\n",
       "   'Risk assessment',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '4',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3035275617',\n",
       "  'title': 'Recreational waters - A potential transmission route for SARS-CoV-2 to humans?',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19), the respiratory illness caused by the novel virus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which has lead to high morbidity and mortality rates worldwide, has been causing major public health concerns since first detected in late 2019. Following identification of novel pathogens, questions in relation to dissemination of the pathogen and transmission routes begin to emerge. This rapidly spreading SARS-CoV-2 virus has been detected in both faecal and wastewater samples across the globe, highlighting the potential for faecal-oral transmission of the virus. As a result, concerns regarding the transmission of the virus in the environment and the risk associated with contracting the virus in recreational waters, particularly where inadequately treated wastewater is discharged, have been emerging in recent weeks. This paper highlights the need for further research to be carried out to investigate the presence, infectivity and viability of this newly identified SARS-CoV-2 virus in wastewater effluent and receiving recreational waters.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Niamh Cahill', 'Dearbháile Morris'],\n",
       "  'related_topics': ['Novel virus',\n",
       "   'Coronavirus',\n",
       "   'Transmission (medicine)',\n",
       "   'Virus',\n",
       "   'Pandemic',\n",
       "   'Infectivity',\n",
       "   'Betacoronavirus',\n",
       "   'Pathogen',\n",
       "   'Environmental health',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '22',\n",
       "  'reference_count': '34',\n",
       "  'references': ['3004280078',\n",
       "   '3003465021',\n",
       "   '3013893137',\n",
       "   '3010604545',\n",
       "   '3004824173',\n",
       "   '3003464757',\n",
       "   '3009834387',\n",
       "   '3011863580',\n",
       "   '3010096538',\n",
       "   '3006846061']},\n",
       " {'id': '3034059415',\n",
       "  'title': 'Impact of Lockdown on the Epidemic Dynamics of COVID-19 in France',\n",
       "  'abstract': 'The COVID-19 epidemic was reported in the Hubei province in China in December 2019 and then spread around the world reaching the pandemic stage at the beginning of March 2020. Since then, several countries went into lockdown. Using a mechanistic-statistical formalism, we estimate the effect of the lockdown in France on the contact rate and the effective reproduction number Re of the COVID-19. We obtain a reduction by a factor 7 (Re=0.47, 95%-CI: 0.45-0.50), compared to the estimates carried out in France at the early stage of the epidemic. We also estimate the fraction of the population that would be infected by the beginning of May, at the official date at which the lockdown should be relaxed. We find a fraction of 3.7% (95%-CI: 3.0-4.8%) of the total French population, without taking into account the number of recovered individuals before April 1st, which is not known. This proportion is seemingly too low to reach herd immunity. Thus, even if the lockdown strongly mitigated the first epidemic wave, keeping a low value of Re is crucial to avoid an uncontrolled second wave (initiated with much more infectious cases than the first wave) and to hence avoid the saturation of hospital facilities.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Lionel Roques',\n",
       "   'Etienne K. Klein',\n",
       "   'Julien Papaïx',\n",
       "   'Antoine Sar',\n",
       "   'Samuel Soubeyrand'],\n",
       "  'related_topics': ['Population',\n",
       "   'Epidemic model',\n",
       "   'Herd immunity',\n",
       "   'Pandemic',\n",
       "   'Demography',\n",
       "   'Geography',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Epidemic dynamics',\n",
       "   'Formalism (philosophy of mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '37',\n",
       "  'reference_count': '27',\n",
       "  'references': ['3003668884',\n",
       "   '3009885589',\n",
       "   '3008443627',\n",
       "   '3010604545',\n",
       "   '3013967887',\n",
       "   '3015571324',\n",
       "   '3006642361',\n",
       "   '3004397688',\n",
       "   '3013594674',\n",
       "   '3012789146']},\n",
       " {'id': '3033952286',\n",
       "  'title': 'Real-time reverse transcription loop-mediated isothermal amplification for rapid detection of SARS-CoV-2',\n",
       "  'abstract': 'Background  Highly sensitive real-time reverse transcription polymerase chain reaction (RT-qPCR) methods have been developed for the detection of SARS-CoV-2. However, they are costly. Loop-mediated isothermal amplification (LAMP) assay has emerged as a novel alternative isothermal amplification method for the detection of nucleic acid.  Methods  A rapid, sensitive and specific real-time reverse transcription LAMP (RT-LAMP) assay was developed for SARS-CoV-2 detection.  Results  This assay detected one copy/reaction of SARS-CoV-2 RNA in 30 min. Both the clinical sensitivity and specificity of this assay were 100%. The RT-LAMP showed comparable performance with RT-qPCR. Combining simplicity and cost-effectiveness, this assay is therefore recommended for use in resource resource-limited settings.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yee Ling Lau',\n",
       "   'Ilyiana Ismail',\n",
       "   'Nur Izati Mustapa',\n",
       "   'Meng Yee Lai',\n",
       "   'Tuan Suhaila Tuan Soh',\n",
       "   'Afifah Hassan',\n",
       "   'Kalaiarasu M Peariasamy',\n",
       "   'Yee Leng Lee',\n",
       "   'Yoong Min Chong',\n",
       "   'I-Ching Sam',\n",
       "   'Pik Pin Goh'],\n",
       "  'related_topics': ['Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Reverse transcriptase',\n",
       "   'Nucleic acid',\n",
       "   'RNA',\n",
       "   'Molecular biology',\n",
       "   'Chemistry',\n",
       "   'Rapid detection',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '9',\n",
       "  'reference_count': '14',\n",
       "  'references': ['3001195213',\n",
       "   '3010604545',\n",
       "   '2105275554',\n",
       "   '3011969828',\n",
       "   '2770752141',\n",
       "   '2263084061',\n",
       "   '2175815746',\n",
       "   '1991420168',\n",
       "   '2073600962',\n",
       "   '2084576921']},\n",
       " {'id': '3036958556',\n",
       "  'title': 'Involvement of digestive system in COVID-19: manifestations, pathology, management and challenges',\n",
       "  'abstract': 'The pandemic of novel coronavirus disease (COVID-19) has developed as a tremendous threat to global health. Although most COVID-19 patients present with respiratory symptoms, some present with gastrointestinal (GI) symptoms like diarrhoea, loss of appetite, nausea/vomiting and abdominal pain as the major complaints. These features may be attributable to the following facts: (a) COVID-19 is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and its receptor angiotensin converting enzyme 2 (ACE2) was found to be highly expressed in GI epithelial cells, providing a prerequisite for SARS-CoV-2 infection; (b) SARS-CoV-2 viral RNA has been found in stool specimens of infected patients, and 20% of patients showed prolonged presence of SARS-CoV-2 RNA in faecal samples after the virus converting to negative in the respiratory system. These findings suggest that SARS-CoV-2 may be able to actively infect and replicate in the GI tract. Moreover, GI infection could be the first manifestation antedating respiratory symptoms; patients suffering only digestive symptoms but no respiratory symptoms as clinical manifestation have also been reported. Thus, the implications of digestive symptoms in patients with COVID-19 is of great importance. In this review, we summarise recent findings on the epidemiology of GI tract involvement, potential mechanisms of faecal-oral transmission, GI and liver manifestation, pathological/histological features in patients with COVID-19 and the diagnosis, management of patients with pre-existing GI and liver diseases as well as precautions for preventing SARS-CoV-2 infection during GI endoscopy procedures.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Song Su',\n",
       "   'Jun Shen',\n",
       "   'Liangru Zhu',\n",
       "   'Yun Qiu',\n",
       "   'Jin-Shen He',\n",
       "   'Jin-Yu Tan',\n",
       "   'Marietta Iacucci',\n",
       "   'Siew C Ng',\n",
       "   'Subrata Ghosh',\n",
       "   'Ren Mao',\n",
       "   'Jie Liang'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Vomiting',\n",
       "   'Disease',\n",
       "   'Abdominal pain',\n",
       "   'Inflammatory bowel disease',\n",
       "   'Nausea',\n",
       "   'Transmission (medicine)',\n",
       "   'Respiratory system',\n",
       "   'Gastroenterology',\n",
       "   'Medicine',\n",
       "   'Internal medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '20',\n",
       "  'reference_count': '47',\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3002539152',\n",
       "   '3003465021',\n",
       "   '3008090866',\n",
       "   '3007940623',\n",
       "   '3010604545',\n",
       "   '3011242477']},\n",
       " {'id': '3035464429',\n",
       "  'title': 'Sampling and detection of corona viruses in air: A mini review.',\n",
       "  'abstract': \"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a strain of coronaviruses that causes coronavirus disease 2019 (COVID-19). In these days, the spread of the SARS-CoV-2 virus through the air has become a controversial topic among scientists. Various organizations provide standard methods for monitoring biological agents in the air. Nevertheless, there has been no standard recommended method for sampling and determination of viruses in air. This manuscript aimed at reviewing published papers for sampling and detection of corona viruses, especially SARS-Cov-2 as a global health concern. It was found that SARS-Cov 2 was present in some air samples that were collected from patient's rooms in hospitals. This result warrants its airborne transmission potential. However, due to the fact that in the most reviewed studies, sampling was performed in the patient's room, it seems difficult to discriminate whether it is airborne or is transmitted through respiratory droplets. Moreover, some other disrupting factors such as patient distance from the sampler, using protective or oxygen masks by patients, patient activities, coughing and sneezing during sampling time, air movement, air conditioning, sampler type, sampling conditions, storage and transferring conditions, can affect the results. About the sampling methods, most of the used samplers such as PTFE filters, gelatin filers and cyclones showed suitable performance for trapping SARS-Co and MERS-Cov viruses followed by PCR analysis.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Ali Reza Rahmani',\n",
       "   'Mostafa Leili',\n",
       "   'Ghasem Azarian',\n",
       "   'Ali Poormohammadi'],\n",
       "  'related_topics': ['Airborne transmission',\n",
       "   'Sampling (statistics)',\n",
       "   'Air conditioning',\n",
       "   'Emergency medicine',\n",
       "   'Environmental science',\n",
       "   'Air movement',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Mini review',\n",
       "   'Pcr analysis',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '22',\n",
       "  'reference_count': '41',\n",
       "  'references': ['3010604545',\n",
       "   '2132260239',\n",
       "   '3010449299',\n",
       "   '3018334611',\n",
       "   '2103503670',\n",
       "   '3015704123',\n",
       "   '3030968929',\n",
       "   '2158121945',\n",
       "   '3015636815',\n",
       "   '3018724240']},\n",
       " {'id': '3028749392',\n",
       "  'title': 'A Collaborative Multidisciplinary Approach to the Management of Coronavirus Disease 2019 in the Hospital Setting.',\n",
       "  'abstract': 'The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19), which presents an unprecedented challenge to medical providers worldwide. Although most SARS-CoV-2-infected individuals manifest with a self-limited mild disease that resolves with supportive care in the outpatient setting, patients with moderate to severe COVID-19 will require a multidisciplinary collaborative management approach for optimal care in the hospital setting. Laboratory and radiologic studies provide critical information on disease severity, management options, and overall prognosis. Medical management is mostly supportive with antipyretics, hydration, oxygen supplementation, and other measures as dictated by clinical need. Among its medical complications is a characteristic proinflammatory cytokine storm often associated with end-organ dysfunction, including respiratory failure, liver and renal insufficiency, cardiac injury, and coagulopathy. Specific recommendations for the management of these medical complications are discussed. Despite the issuance of emergency use authorization for remdesivir, there are still no proven effective antiviral and immunomodulatory therapies, and their use in COVID-19 management should be guided by clinical trial protocols or treatment registries. The medical care of patients with COVID-19 extends beyond their hospitalization. Postdischarge follow-up and monitoring should be performed, preferably using telemedicine, until the patients have fully recovered from their illness and are released from home quarantine protocols.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Raymund R. Razonable',\n",
       "   'Kelly M. Pennington',\n",
       "   'Anne M. Meehan',\n",
       "   'John W. Wilson',\n",
       "   'Adam T. Froemming',\n",
       "   'Courtney E. Bennett',\n",
       "   'Ariela L. Marshall',\n",
       "   'Abinash Virk',\n",
       "   'Eva M. Carmona'],\n",
       "  'related_topics': ['Clinical trial',\n",
       "   'Intensive care unit',\n",
       "   'Telemedicine',\n",
       "   'Emergency Use Authorization',\n",
       "   'Intensive care medicine',\n",
       "   'Respiratory failure',\n",
       "   'Liver function tests',\n",
       "   'ARDS',\n",
       "   'Medicine',\n",
       "   'Coagulopathy',\n",
       "   'View Less'],\n",
       "  'citation_count': '15',\n",
       "  'reference_count': '136',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3002539152',\n",
       "   '3004318991']},\n",
       " {'id': '3032185657',\n",
       "  'title': 'Trajectory of the COVID-19 pandemic: chasing a moving target',\n",
       "  'abstract': 'The spread of COVID-19 has already taken a pandemic form, affecting over 180 countries in a matter of three months. The full continuum of disease ranges from mild, self-limiting illness to severe progressive COVID-19 pneumonia, multiorgan failure, cytokine storm and death. Younger and healthy population is now getting affected than before. Possibilities of airborne and fecal oral routes of transmission has increased the concern. In the absence of any specific therapeutic agent for coronavirus infections, the most effective manner to contain this pandemic is probably the non-pharmacological interventions (NPIs). The damage due to the pandemic disease is multifaceted and crippling to economy, trade, and health of the citizens of the countries. The extent of damage in such scenarios is something that is beyond calculation by Gross Domestic Product rate or currency value of the country. Unfortunately, unlike many other diseases, we are still away from the target antiviral drug and vaccine for severe acute respiratory syndrome (SARS-CoV-2) infection. The prime importance of NPIs like social distancing, staying in home, work from home, self-monitoring, public awareness, self-quarantine, etc. are constantly being emphasized by CDC, WHO, health ministries of all countries and social media houses. This is time of introspection and learning from our mistakes. Countries like China and South Korea who were initially the most hit countries could contain the disease spread by liberal testing of their population, stringent quarantine of people under investigation and isolation of the positive cases. Rest of the countries need to act urgently as well to bring an immediate halt in the community transmission.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Kamal Kant Sahu', 'Ajay Kumar Mishra', 'Amos Lal'],\n",
       "  'related_topics': ['Population',\n",
       "   'Pandemic',\n",
       "   'Isolation (health care)',\n",
       "   'Quarantine',\n",
       "   'Disease',\n",
       "   'Transmission (medicine)',\n",
       "   'Psychological intervention',\n",
       "   'Gross domestic product',\n",
       "   'Development economics',\n",
       "   'Business',\n",
       "   'View Less'],\n",
       "  'citation_count': '18',\n",
       "  'reference_count': '75',\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3008090866',\n",
       "   '3007497549',\n",
       "   '3010930696',\n",
       "   '3014294089']},\n",
       " {'id': '3042098369',\n",
       "  'title': 'Laboratory Tests for COVID-19: A Review of Peer-Reviewed Publications and Implications for Clinical Use',\n",
       "  'abstract': 'Diagnostic tests for the coronavirus infection 2019 (COVID-19) are critical for prompt diagnosis, treatment and isolation to break the cycle of transmission. A positive real-time reverse-transcriptase polymerase chain reaction (RT-PCR), in conjunction with clinical and epidemiologic data, is the current standard for diagnosis, but several challenges still exist. Serological assays help to understand epidemiology better and to evaluate vaccine responses but they are unreliable for diagnosis in the acute phase of illness or assuming protective immunity. Serology is gaining attention, mainly because of convalescent plasma gaining importance as treatment for clinically worsening COVID-19 patients. We provide a narrative review of peer-reviewed research studies on RT-PCR, serology and antigen immune-assays for COVID-19, briefly describe their lab methods and discuss their limitations for clinical practice.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Daniel Shyu',\n",
       "   'James Dorroh',\n",
       "   'Caleb Holtmeyer',\n",
       "   'Detlef Ritter',\n",
       "   'Anandhi Upendran',\n",
       "   'Raghuraman Kannan',\n",
       "   'Dima Dandachi',\n",
       "   'Christian Rojas-Moreno',\n",
       "   'Stevan P Whitt',\n",
       "   'Hariharan Regunath'],\n",
       "  'related_topics': ['Serology',\n",
       "   'Peer review',\n",
       "   'Epidemiology',\n",
       "   'Isolation (health care)',\n",
       "   'Intensive care medicine',\n",
       "   'MEDLINE',\n",
       "   'Transmission (medicine)',\n",
       "   'Medicine',\n",
       "   'Pandemic',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'View Less'],\n",
       "  'citation_count': '10',\n",
       "  'reference_count': '70',\n",
       "  'references': ['3001118548',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '3001195213',\n",
       "   '3007497549',\n",
       "   '3013893137',\n",
       "   '3010604545']},\n",
       " {'id': '3037255629',\n",
       "  'title': 'COVID-19 in children: An ample review',\n",
       "  'abstract': 'The aim of this review was to describe the current knowledge about coronavirus disease 2019 (COVID-19, which is caused by severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) in children, from epidemiological, clinical, and laboratory perspectives, including knowledge on the disease course, treatment, and prognosis. An extensive literature search was performed to identify papers on COVID-19 (SARS-CoV-2 infection) in children, published between January 1, 2020 and April 1, 2020. There were 44 relevant papers on COVID-19 in children. The results showed that COVID-19 occurs in 0.39–12.3% of children. Clinical signs and symptoms are comparable to those in adults, but milder forms and a large percentage of asymptomatic carriers are found among children. Elevated inflammatory markers are associated with complications and linked to various co-infections. Chest computed tomography (CT) scans in children revealed structural changes similar to those found in adults, with consolidations surrounded by halos being somewhat specific for children with COVID-19. The recommended treatment includes providing symptomatic therapy, with no specific drug recommendations for children. The prognosis is much better for children compared to adults. This review highlights that COVID-19 in children is similar to the disease in the adult population, but with particularities regarding clinical manifestations, laboratory test results, chest imaging, and treatment. The prognosis is much better for children compared to adults, but with the progression of the pandemic; the cases in children might change in the future.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ioana M Ciuca'],\n",
       "  'related_topics': ['Epidemiology',\n",
       "   'Asymptomatic carrier',\n",
       "   'Disease',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'Pandemic',\n",
       "   'Adult population',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Signs and symptoms',\n",
       "   'View Less'],\n",
       "  'citation_count': '12',\n",
       "  'reference_count': '63',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3008028633',\n",
       "   '3007940623',\n",
       "   '3007497549',\n",
       "   '3010604545',\n",
       "   '3010930696']},\n",
       " {'id': '2168356304',\n",
       "  'title': 'Object Detection with Discriminatively Trained Part-Based Models',\n",
       "  'abstract': 'We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.',\n",
       "  'date': '2010',\n",
       "  'authors': ['P F Felzenszwalb', 'R B Girshick', 'D McAllester', 'D Ramanan'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Object detection',\n",
       "   'Support vector machine',\n",
       "   'Discriminative model',\n",
       "   'Pascal (programming language)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Linear discriminant analysis',\n",
       "   'Pedestrian detection',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,895',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '3097096317',\n",
       "   '2154422044',\n",
       "   '2120419212',\n",
       "   '2152826865',\n",
       "   '2145072179',\n",
       "   '1576520375',\n",
       "   '2030536784',\n",
       "   '2115763357']},\n",
       " {'id': '1849277567',\n",
       "  'title': 'Visualizing and Understanding Convolutional Networks',\n",
       "  'abstract': 'Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Matthew D. Zeiler', 'Rob Fergus'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Softmax function',\n",
       "   'Classifier (UML)',\n",
       "   'Network model',\n",
       "   'Machine learning',\n",
       "   'Stochastic gradient descent',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,255',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2136922672',\n",
       "   '1904365287',\n",
       "   '2155541015',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '2161381512']},\n",
       " {'id': '1959608418',\n",
       "  'title': 'Auto-Encoding Variational Bayes',\n",
       "  'abstract': 'Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Diederik P Kingma', 'Max Welling'],\n",
       "  'related_topics': ['Approximate inference',\n",
       "   'Inference',\n",
       "   'Estimator',\n",
       "   'Upper and lower bounds',\n",
       "   \"Bayes' theorem\",\n",
       "   'Latent variable',\n",
       "   'Autoencoder',\n",
       "   'Probabilistic logic',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '13,251',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2146502635',\n",
       "   '2163922914',\n",
       "   '2145094598',\n",
       "   '2166851633',\n",
       "   '2097268041',\n",
       "   '2963173382',\n",
       "   '2951493172',\n",
       "   '2171490498',\n",
       "   '2119196781',\n",
       "   '3104819538']},\n",
       " {'id': '1904365287',\n",
       "  'title': 'Improving neural networks by preventing co-adaptation of feature detectors',\n",
       "  'abstract': 'When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Geoffrey E. Hinton',\n",
       "   'Nitish',\n",
       "   'Alex',\n",
       "   'Ilya',\n",
       "   'Ruslan R. Salakhutdinov'],\n",
       "  'related_topics': ['Dropout (neural networks)',\n",
       "   'Feature (computer vision)',\n",
       "   'Overfitting',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial neural network',\n",
       "   'Context (language use)',\n",
       "   'Benchmark (computing)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,094',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2108598243',\n",
       "   '2911964244',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2912934387',\n",
       "   '2116064496',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '4919037']},\n",
       " {'id': '2964153729',\n",
       "  'title': 'Intriguing properties of neural networks',\n",
       "  'abstract': \"Abstract: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \\r\\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \\r\\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.\",\n",
       "  'date': '2014',\n",
       "  'authors': ['Christian Szegedy',\n",
       "   'Wojciech Zaremba',\n",
       "   'Ilya Sutskever',\n",
       "   'Joan Bruna',\n",
       "   'Dumitru Erhan',\n",
       "   'Ian Goodfellow',\n",
       "   'Rob Fergus',\n",
       "   ''],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Adversarial machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Linear combination',\n",
       "   'Computer science',\n",
       "   'Uninterpretable',\n",
       "   'Backdoor',\n",
       "   'Artificial intelligence',\n",
       "   'Deep neural networks',\n",
       "   'Mean squared prediction error',\n",
       "   'Semantic information',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,962',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '1614298861',\n",
       "   '2108598243',\n",
       "   '2160815625',\n",
       "   '2072128103',\n",
       "   '2206858481',\n",
       "   '2120419212',\n",
       "   '2120480077',\n",
       "   '2150165932']},\n",
       " {'id': '1901129140',\n",
       "  'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "  'abstract': 'There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .',\n",
       "  'date': '2015',\n",
       "  'authors': ['Olaf Ronneberger', 'Philipp Fischer', 'Thomas Brox'],\n",
       "  'related_topics': ['Brain segmentation',\n",
       "   'Deep learning',\n",
       "   'Segmentation',\n",
       "   'Image processing',\n",
       "   'Context (language use)',\n",
       "   'Artificial neural network',\n",
       "   'Image translation',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '26,451',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1677182931',\n",
       "   '1948751323',\n",
       "   '2167510172',\n",
       "   '1893585201',\n",
       "   '2148349024',\n",
       "   '2147800946']},\n",
       " {'id': '2133665775',\n",
       "  'title': 'Image quality assessment: from error visibility to structural similarity',\n",
       "  'abstract': 'Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Zhou Wang', 'A.C. Bovik', 'H.R. Sheikh', 'E.P. Simoncelli'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Subjective video quality',\n",
       "   'Human visual system model',\n",
       "   'Image processing',\n",
       "   'Visibility (geometry)',\n",
       "   'Cyclopean image',\n",
       "   'JPEG',\n",
       "   'JPEG 2000',\n",
       "   'Image compression',\n",
       "   'Image translation',\n",
       "   'Compression artifact',\n",
       "   'PEVQ',\n",
       "   'Data compression',\n",
       "   'View synthesis',\n",
       "   'Transform coding',\n",
       "   'Ringing artifacts',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Structural similarity',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '32,918',\n",
       "  'reference_count': '53',\n",
       "  'references': ['2159269332',\n",
       "   '2142276208',\n",
       "   '2118217749',\n",
       "   '2053691921',\n",
       "   '2153777140',\n",
       "   '2912116903',\n",
       "   '2107790757',\n",
       "   '2158564760',\n",
       "   '2124731682',\n",
       "   '2115838129']},\n",
       " {'id': '2340897893',\n",
       "  'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding',\n",
       "  'abstract': 'Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Marius Cordts',\n",
       "   'Mohamed Omran',\n",
       "   'Sebastian Ramos',\n",
       "   'Timo Rehfeld',\n",
       "   'Markus Enzweiler',\n",
       "   'Rodrigo Benenson',\n",
       "   'Uwe Franke',\n",
       "   'Stefan Roth',\n",
       "   'Bernt Schiele'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Deep learning',\n",
       "   'Machine learning',\n",
       "   'Leverage (statistics)',\n",
       "   'Annotation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,675',\n",
       "  'reference_count': '81',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2919115771',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '2168356304',\n",
       "   '1861492603']},\n",
       " {'id': '2271840356',\n",
       "  'title': 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems',\n",
       "  'abstract': 'TensorFlow is an interface for expressing machine learning algorithms, and an\\nimplementation for executing such algorithms. A computation expressed using\\nTensorFlow can be executed with little or no change on a wide variety of\\nheterogeneous systems, ranging from mobile devices such as phones and tablets\\nup to large-scale distributed systems of hundreds of machines and thousands of\\ncomputational devices such as GPU cards. The system is flexible and can be used\\nto express a wide variety of algorithms, including training and inference\\nalgorithms for deep neural network models, and it has been used for conducting\\nresearch and for deploying machine learning systems into production across more\\nthan a dozen areas of computer science and other fields, including speech\\nrecognition, computer vision, robotics, information retrieval, natural language\\nprocessing, geographic information extraction, and computational drug\\ndiscovery. This paper describes the TensorFlow interface and an implementation\\nof that interface that we have built at Google. The TensorFlow API and a\\nreference implementation were released as an open-source package under the\\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Martín',\n",
       "   'Ashish',\n",
       "   'Paul',\n",
       "   'Eugene',\n",
       "   'Zhifeng',\n",
       "   'Craig',\n",
       "   'Gregory S.',\n",
       "   'Andy',\n",
       "   'Jeffrey',\n",
       "   'Matthieu',\n",
       "   'Sanjay',\n",
       "   'Ian J.',\n",
       "   'Andrew',\n",
       "   'Geoffrey',\n",
       "   'Michael',\n",
       "   'Yangqing',\n",
       "   'Rafal',\n",
       "   'Lukasz',\n",
       "   'Manjunath',\n",
       "   'Josh',\n",
       "   'Dan',\n",
       "   'Rajat',\n",
       "   'Sherry',\n",
       "   'Derek Gordon',\n",
       "   'Chris',\n",
       "   'Mike',\n",
       "   'Jonathon',\n",
       "   'Benoit',\n",
       "   'Ilya',\n",
       "   'Kunal',\n",
       "   'Paul A.',\n",
       "   'Vincent',\n",
       "   'Vijay',\n",
       "   'Fernanda B.',\n",
       "   'Oriol',\n",
       "   'Pete',\n",
       "   'Martin',\n",
       "   'Martin',\n",
       "   'Yuan',\n",
       "   'Xiaoqiang'],\n",
       "  'related_topics': ['Interface (computing)',\n",
       "   'Deep learning',\n",
       "   'Information extraction',\n",
       "   'Artificial neural network',\n",
       "   'Mobile device',\n",
       "   'CUDA',\n",
       "   'Robotics',\n",
       "   'Distributed computing',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Inference',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,248',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2097117768',\n",
       "   '1836465849',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2155893237',\n",
       "   '2064675550',\n",
       "   '2160815625',\n",
       "   '2168231600',\n",
       "   '2016053056',\n",
       "   '2131975293']},\n",
       " {'id': '648143168',\n",
       "  'title': 'Deep generative image models using a Laplacian pyramid of adversarial networks',\n",
       "  'abstract': 'In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Emily Denton',\n",
       "   'Soumith Chintala',\n",
       "   'Arthur Szlam',\n",
       "   'Rob Fergus'],\n",
       "  'related_topics': ['Pyramid',\n",
       "   'Real image',\n",
       "   'Parametric model',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Generative grammar',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Laplacian pyramid',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,923',\n",
       "  'reference_count': '31',\n",
       "  'references': ['1836465849',\n",
       "   '2099471712',\n",
       "   '2108598243',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2025768430',\n",
       "   '2125389028',\n",
       "   '2118858186',\n",
       "   '189596042']},\n",
       " {'id': '2963685250',\n",
       "  'title': 'Weight normalization: a simple reparameterization to accelerate training of deep neural networks',\n",
       "  'abstract': 'We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Tim Salimans', 'Diederik P. Kingma'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Artificial neural network',\n",
       "   'Stochastic gradient descent',\n",
       "   'Reinforcement learning',\n",
       "   'Optimization problem',\n",
       "   'Speedup',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Deep neural networks',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,061',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2194775991',\n",
       "   '1836465849',\n",
       "   '2145339207',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2064675550',\n",
       "   '2963911037',\n",
       "   '1533861849',\n",
       "   '104184427',\n",
       "   '2962897886']},\n",
       " {'id': '2949416428',\n",
       "  'title': 'Semi-Supervised Learning with Deep Generative Models',\n",
       "  'abstract': 'The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Diederik P.', 'Danilo J.', 'Shakir', 'Max'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Bayesian inference',\n",
       "   'Generative grammar',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Generalization',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,679',\n",
       "  'reference_count': '20',\n",
       "  'references': ['1959608418',\n",
       "   '2146502635',\n",
       "   '2962897886',\n",
       "   '2335728318',\n",
       "   '2136504847',\n",
       "   '2107008379',\n",
       "   '1676820704',\n",
       "   '2407712691',\n",
       "   '2158049734',\n",
       "   '2122457239']},\n",
       " {'id': '830076066',\n",
       "  'title': 'Semi-supervised learning with Ladder networks',\n",
       "  'abstract': 'We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Antti Rasmus',\n",
       "   'Harri Valpola',\n",
       "   'Mikko Honkala',\n",
       "   'Mathias Berglund',\n",
       "   'Tapani Raiko'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Deep learning',\n",
       "   'Competitive learning',\n",
       "   'Artificial neural network',\n",
       "   'Supervised learning',\n",
       "   'MNIST database',\n",
       "   'Backpropagation',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '927',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2964121744',\n",
       "   '1836465849',\n",
       "   '2095705004',\n",
       "   '2963207607',\n",
       "   '2100495367',\n",
       "   '2145094598',\n",
       "   '2294059674',\n",
       "   '2963382180',\n",
       "   '1479807131',\n",
       "   '1606347560']},\n",
       " {'id': '1487641199',\n",
       "  'title': 'Generative Moment Matching Networks',\n",
       "  'abstract': 'We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Yujia Li', 'Kevin Swersky', 'Rich Zemel', ''],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Generative topographic map',\n",
       "   'Multilayer perceptron',\n",
       "   'MNIST database',\n",
       "   'Backpropagation',\n",
       "   'Statistical hypothesis testing',\n",
       "   'Matching (statistics)',\n",
       "   'Minimax',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '532',\n",
       "  'reference_count': '48',\n",
       "  'references': ['2618530766',\n",
       "   '2097117768',\n",
       "   '2099471712',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2963542991',\n",
       "   '1959608418',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '1665214252']},\n",
       " {'id': '2911964244',\n",
       "  'title': 'Random Forests',\n",
       "  'abstract': 'Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'related_topics': ['Random forest',\n",
       "   'Multivariate random variable',\n",
       "   'Random subspace method',\n",
       "   'AdaBoost',\n",
       "   'Alternating decision tree',\n",
       "   'Tree (graph theory)',\n",
       "   'Ensemble learning',\n",
       "   'Gradient boosting',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '74,573',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2912934387',\n",
       "   '2112076978',\n",
       "   '1975846642',\n",
       "   '2152761983',\n",
       "   '2113242816',\n",
       "   '1605688901',\n",
       "   '2120240539',\n",
       "   '2099968818',\n",
       "   '2067885219',\n",
       "   '1580948147']},\n",
       " {'id': '2130325614',\n",
       "  'title': 'Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations',\n",
       "  'abstract': 'There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Honglak Lee',\n",
       "   'Roger Grosse',\n",
       "   'Rajesh Ranganath',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Convolutional Deep Belief Networks',\n",
       "   'Deep belief network',\n",
       "   'Deep learning',\n",
       "   'Unsupervised learning',\n",
       "   'Generative model',\n",
       "   'Probabilistic logic',\n",
       "   'Inference',\n",
       "   'Pattern recognition',\n",
       "   'Scalability',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,975',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2166049352',\n",
       "   '2147800946',\n",
       "   '2145889472',\n",
       "   '2122922389',\n",
       "   '2139427956']},\n",
       " {'id': '2250539671',\n",
       "  'title': 'Glove: Global Vectors for Word Representation',\n",
       "  'abstract': 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Jeffrey Pennington', 'Richard Socher', 'Christopher Manning'],\n",
       "  'related_topics': ['Word2vec',\n",
       "   'Word embedding',\n",
       "   'Sparse matrix',\n",
       "   'SemEval',\n",
       "   'Sequence labeling',\n",
       "   'Word (computer architecture)',\n",
       "   'Context (language use)',\n",
       "   'Distributional semantics',\n",
       "   'Named-entity recognition',\n",
       "   'Sememe',\n",
       "   'Vocabulary mismatch',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '21,265',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2146502635',\n",
       "   '2158899491',\n",
       "   '2072128103',\n",
       "   '2141599568',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2118020653',\n",
       "   '2158139315']},\n",
       " {'id': '2962739339',\n",
       "  'title': 'Deep contextualized word representations',\n",
       "  'abstract': 'We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Matthew E. Peters',\n",
       "   'Mark Neumann',\n",
       "   'Mohit Iyyer',\n",
       "   'Matt Gardner',\n",
       "   'Christopher Clark',\n",
       "   'Kenton Lee',\n",
       "   'Luke Zettlemoyer'],\n",
       "  'related_topics': ['Textual entailment',\n",
       "   'Text corpus',\n",
       "   'Syntax',\n",
       "   'Language model',\n",
       "   'Polysemy',\n",
       "   'Semantics',\n",
       "   'Question answering',\n",
       "   'Sentiment analysis',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,430',\n",
       "  'reference_count': '55',\n",
       "  'references': ['2964121744',\n",
       "   '2153579005',\n",
       "   '2095705004',\n",
       "   '2250539671',\n",
       "   '2064675550',\n",
       "   '2158899491',\n",
       "   '2493916176',\n",
       "   '2251939518',\n",
       "   '2147880316',\n",
       "   '2963748441']},\n",
       " {'id': '2131744502',\n",
       "  'title': 'Distributed Representations of Sentences and Documents',\n",
       "  'abstract': 'Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Quoc Le', 'Tomas Mikolov'],\n",
       "  'related_topics': ['Topic model',\n",
       "   'Feature vector',\n",
       "   'Feature (machine learning)',\n",
       "   'Sentiment analysis',\n",
       "   'Paragraph',\n",
       "   'Semantics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,019',\n",
       "  'reference_count': '41',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2158899491',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2141599568',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '2113459411']},\n",
       " {'id': '2251939518',\n",
       "  'title': 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank',\n",
       "  'abstract': 'Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Richard Socher',\n",
       "   'Alex',\n",
       "   'Jean Wu',\n",
       "   'Jason Chuang',\n",
       "   'Christopher D. Manning',\n",
       "   'Andrew Ng',\n",
       "   'Christopher Potts'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Principle of compositionality',\n",
       "   'Parsing',\n",
       "   'Natural language processing',\n",
       "   'Tree (data structure)',\n",
       "   'Machine learning',\n",
       "   'Negation',\n",
       "   'Word (computer architecture)',\n",
       "   'Computer science',\n",
       "   'Scope (computer science)',\n",
       "   'Meaning (non-linguistic)',\n",
       "   'Artificial intelligence',\n",
       "   'Single sentence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,158',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2146502635',\n",
       "   '2097726431',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '1662133657',\n",
       "   '1889268436',\n",
       "   '2164019165',\n",
       "   '2097606805']},\n",
       " {'id': '2963748441',\n",
       "  'title': 'SQuAD: 100,000+ Questions for Machine Comprehension of Text',\n",
       "  'abstract': 'We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \\r\\nThe dataset is freely available at this https URL',\n",
       "  'date': '2016',\n",
       "  'authors': ['Pranav Rajpurkar',\n",
       "   'Jian Zhang',\n",
       "   'Konstantin Lopyrev',\n",
       "   'Percy Liang'],\n",
       "  'related_topics': ['Question answering',\n",
       "   'Reading comprehension',\n",
       "   'Reading (process)',\n",
       "   'Comprehension',\n",
       "   'F1 score',\n",
       "   'Natural language processing',\n",
       "   'Set (abstract data type)',\n",
       "   'Computer science',\n",
       "   'Baseline (configuration management)',\n",
       "   'Dependency (UML)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,862',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2108598243',\n",
       "   '1544827683',\n",
       "   '1632114991',\n",
       "   '2125436846',\n",
       "   '2964267515',\n",
       "   '2962809918',\n",
       "   '2171278097',\n",
       "   '2962790689',\n",
       "   '2251818205',\n",
       "   '2251349042']},\n",
       " {'id': '2096192494',\n",
       "  'title': 'On the quantitative analysis of deep belief networks',\n",
       "  'abstract': \"Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.\",\n",
       "  'date': '2008',\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Iain Murray'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Approximate inference',\n",
       "   'Graphical model',\n",
       "   'Model selection',\n",
       "   'Importance sampling',\n",
       "   'Greedy algorithm',\n",
       "   'Hidden variable theory',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '493',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2099866409',\n",
       "   '2169415915',\n",
       "   '2158164339',\n",
       "   '66838807',\n",
       "   '2064630666',\n",
       "   '1513873506',\n",
       "   '2135094946']},\n",
       " {'id': '2081580037',\n",
       "  'title': 'WordNet: a lexical database for English',\n",
       "  'abstract': 'Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].',\n",
       "  'date': '1995',\n",
       "  'authors': ['George A. Miller'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Lexical database',\n",
       "   'eXtended WordNet',\n",
       "   'Machine-readable dictionary',\n",
       "   'Natural language',\n",
       "   'VerbNet',\n",
       "   'Noun',\n",
       "   'Ontology merging',\n",
       "   'Natural language processing',\n",
       "   'Synonym',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word-sense induction',\n",
       "   'View Less'],\n",
       "  'citation_count': '21,849',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2102381086',\n",
       "   '2103318667',\n",
       "   '2065157922',\n",
       "   '1483126227',\n",
       "   '2017668967',\n",
       "   '1518768680',\n",
       "   '13823885']},\n",
       " {'id': '2165225968',\n",
       "  'title': 'Unsupervised learning of distributions on binary vectors using two layer networks',\n",
       "  'abstract': 'We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Yoav Freund', 'David Haussler'],\n",
       "  'related_topics': ['Wake-sleep algorithm',\n",
       "   'Feature learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Unsupervised learning',\n",
       "   'Online machine learning',\n",
       "   'Boltzmann machine',\n",
       "   'Semi-supervised learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '401',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2159080219',\n",
       "   '1652505363',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2293063825',\n",
       "   '1507849272',\n",
       "   '1964724001',\n",
       "   '2121407732',\n",
       "   '2725061391',\n",
       "   '2315016682']},\n",
       " {'id': '2806070179',\n",
       "  'title': 'Mask R-CNN',\n",
       "  'abstract': 'We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in  parallel  with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at:  https://github.com/facebookresearch/Detectron .',\n",
       "  'date': '2020',\n",
       "  'authors': ['Kaiming He',\n",
       "   'Georgia Gkioxari',\n",
       "   'Piotr Dollar',\n",
       "   'Ross Girshick'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Image segmentation',\n",
       "   'Minimum bounding box',\n",
       "   'Pose',\n",
       "   'Segmentation',\n",
       "   'Feature extraction',\n",
       "   'Object (computer science)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,729',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '639708223',\n",
       "   '2102605133',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '2806070179',\n",
       "   '1861492603',\n",
       "   '2109255472',\n",
       "   '2565639579']},\n",
       " {'id': '1861492603',\n",
       "  'title': 'Microsoft COCO: Common Objects in Context',\n",
       "  'abstract': 'We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Tsung-Yi Lin',\n",
       "   'Michael Maire',\n",
       "   'Serge J. Belongie',\n",
       "   'James Hays',\n",
       "   'Pietro Perona',\n",
       "   'Deva Ramanan',\n",
       "   'Piotr Dollár',\n",
       "   'C. Lawrence Zitnick'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Minimum bounding box',\n",
       "   'Segmentation',\n",
       "   'User interface',\n",
       "   'Pascal (programming language)',\n",
       "   'Computer vision',\n",
       "   'Spotting',\n",
       "   'Closed captioning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '16,063',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2963542991',\n",
       "   '3118608800',\n",
       "   '2031489346',\n",
       "   '2110158442',\n",
       "   '2038721957']},\n",
       " {'id': '2565639579',\n",
       "  'title': 'Feature Pyramid Networks for Object Detection',\n",
       "  'abstract': 'Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Tsung-Yi Lin',\n",
       "   'Piotr Dollar',\n",
       "   'Ross Girshick',\n",
       "   'Kaiming He',\n",
       "   'Bharath Hariharan',\n",
       "   'Serge Belongie'],\n",
       "  'related_topics': ['Feature (computer vision)',\n",
       "   'Pyramid',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Semantic feature',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Artificial neural network',\n",
       "   'Robustness (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Detector',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,099',\n",
       "  'reference_count': '41',\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '2161969291']},\n",
       " {'id': '2053186076',\n",
       "  'title': 'Nonlinear dimensionality reduction by locally linear embedding.',\n",
       "  'abstract': 'Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Sam T. Roweis', 'Lawrence K. Saul'],\n",
       "  'related_topics': ['Diffusion map',\n",
       "   'Dimensionality reduction',\n",
       "   't-distributed stochastic neighbor embedding',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Isomap',\n",
       "   'Elastic map',\n",
       "   'Curse of dimensionality',\n",
       "   'Local tangent space alignment',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '16,281',\n",
       "  'reference_count': '12',\n",
       "  'references': ['1902027874',\n",
       "   '2610857016',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '2121122425',\n",
       "   '2122538988',\n",
       "   '2047870719',\n",
       "   '2070320140',\n",
       "   '1513400187',\n",
       "   '2019020850']},\n",
       " {'id': '2001141328',\n",
       "  'title': 'A Global Geometric Framework for Nonlinear Dimensionality Reduction',\n",
       "  'abstract': 'Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.',\n",
       "  'date': '2000',\n",
       "  'authors': ['J. B. Tenenbaum', 'V. de Silva', 'J. C. Langford'],\n",
       "  'related_topics': ['Diffusion map',\n",
       "   'Dimensionality reduction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   't-distributed stochastic neighbor embedding',\n",
       "   'Isomap',\n",
       "   'Curse of dimensionality',\n",
       "   'Sammon mapping',\n",
       "   'Principal component analysis',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,796',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2138451337',\n",
       "   '2099741732',\n",
       "   '2108384452',\n",
       "   '2123977795',\n",
       "   '2587818897',\n",
       "   '2107636931',\n",
       "   '2122538988',\n",
       "   '2047870719',\n",
       "   '2070320140',\n",
       "   '2032647857']},\n",
       " {'id': '2293063825',\n",
       "  'title': 'Neural networks and physical systems with emergent collective computational abilities',\n",
       "  'abstract': 'Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.',\n",
       "  'date': '1999',\n",
       "  'authors': ['John J.'],\n",
       "  'related_topics': ['Physical system',\n",
       "   'Artificial neural network',\n",
       "   'Asynchronous communication',\n",
       "   'Generalization',\n",
       "   'State (computer science)',\n",
       "   'Parallel processing (DSP implementation)',\n",
       "   'Error detection and correction',\n",
       "   'Categorization',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '26,670',\n",
       "  'reference_count': '4',\n",
       "  'references': ['1594524188', '2086789740', '2970228278', '2076870593']},\n",
       " {'id': '2121122425',\n",
       "  'title': 'Dimension reduction by local principal component analysis',\n",
       "  'abstract': 'Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Nandakishore Kambhatla', 'Todd K. Leen'],\n",
       "  'related_topics': ['Sparse PCA',\n",
       "   'Principal component analysis',\n",
       "   'Dimensionality reduction',\n",
       "   'Artificial neural network',\n",
       "   'Redundancy (engineering)',\n",
       "   'Representation (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Nonlinear system',\n",
       "   'Algorithm',\n",
       "   'Image (mathematics)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '787',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2137983211',\n",
       "   '3004157836',\n",
       "   '2140196014',\n",
       "   '1634005169',\n",
       "   '3146803896',\n",
       "   '1971735090',\n",
       "   '2913399920',\n",
       "   '2122538988',\n",
       "   '2096710051',\n",
       "   '2017977879']},\n",
       " {'id': '2032647857',\n",
       "  'title': 'Replicator neural networks for universal optimal source coding.',\n",
       "  'abstract': 'Replicator neural networks self-organize by using their inputs as desired outputs; they internally form a compressed representation for the input data. A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error (for instance, by training on raw data examples), carry out optimal data compression for arbitrary data vector sources. Data manifolds, a new general model of data sources, are then introduced and a second theorem shows that, in a practically important limiting case, optimal-compression replicator networks operate by creating an essentially unique natural coordinate system for the manifold.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Robert'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Data compression',\n",
       "   'Manifold',\n",
       "   'Backpropagation',\n",
       "   'Minification',\n",
       "   'Algorithm',\n",
       "   'Source code',\n",
       "   'Coding (social sciences)',\n",
       "   'Essentially unique',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '179',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2137983211',\n",
       "   '2166116275',\n",
       "   '1993845689',\n",
       "   '2122538988',\n",
       "   '5731987',\n",
       "   '2142228262',\n",
       "   '2063971957',\n",
       "   '2079782346',\n",
       "   '2089419199',\n",
       "   '2162604518']},\n",
       " {'id': '2021774695',\n",
       "  'title': 'Learning sets of filters using back-propagation',\n",
       "  'abstract': 'Abstract   A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in “weight space”. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. Some preliminary results on scaling are reported and it is shown how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results show how the amount of interaction between the weights affects the learning speed. The paper is concluded with a discussion of the difficulties that are likely to be encounted in applying back-propagation to more realistic problems in speech recognition, and some promising approaches to overcoming these difficulties.',\n",
       "  'date': '1987',\n",
       "  'authors': ['David C. Plaut', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Semi-supervised learning',\n",
       "   'Backpropagation',\n",
       "   'Edge detection',\n",
       "   'Noise (video)',\n",
       "   'Task (project management)',\n",
       "   'Set (psychology)',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '156',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '1507849272',\n",
       "   '2155487652',\n",
       "   '1995169133',\n",
       "   '2591802459',\n",
       "   '2010581677']},\n",
       " {'id': '2156718197',\n",
       "  'title': 'Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering',\n",
       "  'abstract': 'Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Mikhail Belkin', 'Partha Niyogi'],\n",
       "  'related_topics': ['Spectral clustering',\n",
       "   'Manifold alignment',\n",
       "   'Laplacian matrix',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Laplacian smoothing',\n",
       "   'Isomap',\n",
       "   'Cluster analysis',\n",
       "   'Laplace operator',\n",
       "   'Topology',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,960',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328',\n",
       "   '1578099820',\n",
       "   '108654854']},\n",
       " {'id': '2125637308',\n",
       "  'title': 'Random Walks for Image Segmentation',\n",
       "  'abstract': 'A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs',\n",
       "  'date': '2006',\n",
       "  'authors': ['L. Grady'],\n",
       "  'related_topics': ['Random walker algorithm',\n",
       "   'Image segmentation',\n",
       "   'Scale-space segmentation',\n",
       "   'Pixel',\n",
       "   'Graph theory',\n",
       "   'Image processing',\n",
       "   'Cut',\n",
       "   'Discrete space',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,812',\n",
       "  'reference_count': '76',\n",
       "  'references': ['2296319761',\n",
       "   '3029645440',\n",
       "   '2177274842',\n",
       "   '2121947440',\n",
       "   '2124351162',\n",
       "   '2143516773',\n",
       "   '2104095591',\n",
       "   '1578099820',\n",
       "   '2169551590',\n",
       "   '2150134853']},\n",
       " {'id': '2139823104',\n",
       "  'title': 'Semi-supervised learning using Gaussian fields and harmonic functions',\n",
       "  'abstract': \"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.\",\n",
       "  'date': '2003',\n",
       "  'authors': ['Xiaojin Zhu', 'Zoubin Ghahramani', 'John Lafferty'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Empirical risk minimization',\n",
       "   'Stability (learning theory)',\n",
       "   'Supervised learning',\n",
       "   'Gaussian random field',\n",
       "   'Random graph',\n",
       "   'Online machine learning',\n",
       "   'Belief propagation',\n",
       "   'Spectral graph theory',\n",
       "   'Gaussian',\n",
       "   'Feature selection',\n",
       "   'Random walk',\n",
       "   'Prior probability',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization error',\n",
       "   'Graph',\n",
       "   'Manifold regularization',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,300',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2121947440',\n",
       "   '2143516773',\n",
       "   '2165874743',\n",
       "   '2154579312',\n",
       "   '2122837498',\n",
       "   '1511160855',\n",
       "   '1585385982',\n",
       "   '1979711143',\n",
       "   '2113592823',\n",
       "   '200434350']},\n",
       " {'id': '2137570937',\n",
       "  'title': 'Dimensionality Reduction: A Comparative Review',\n",
       "  'abstract': 'In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Laurens van der', 'Eric Postma', 'Jaap van den Herik'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Nonlinear system',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Variety (cybernetics)',\n",
       "   'Scaling',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,358',\n",
       "  'reference_count': '160',\n",
       "  'references': ['2296319761',\n",
       "   '2136922672',\n",
       "   '1880262756',\n",
       "   '2100495367',\n",
       "   '2187089797',\n",
       "   '1497256448',\n",
       "   '2072128103',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '2116064496']},\n",
       " {'id': '2157444450',\n",
       "  'title': 'Stochastic Neighbor Embedding',\n",
       "  'abstract': 'We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional \"images\" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \"bank\", to have versions close to the images of both \"river\" and \"finance\" without forcing the images of outdoor concepts to be located close to those of corporate concepts.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Geoffrey E. Hinton', 'Sam T. Roweis'],\n",
       "  'related_topics': ['t-distributed stochastic neighbor embedding',\n",
       "   'Dimensionality reduction',\n",
       "   'Object (grammar)',\n",
       "   'Probabilistic logic',\n",
       "   'Probability distribution',\n",
       "   'Embedding',\n",
       "   'Forcing (recursion theory)',\n",
       "   'Gaussian',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,538',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2053186076',\n",
       "   '2001141328',\n",
       "   '2148694408',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '23758216',\n",
       "   '2100659887',\n",
       "   '2159174312',\n",
       "   '2106346128']},\n",
       " {'id': '1742512077',\n",
       "  'title': 'Nonlinear Dimensionality Reduction',\n",
       "  'abstract': 'Methods of dimensionality reduction provide a way to understand and visualize the structure of complex data sets. Traditional methods like principal component analysis and classical metric multidimensional scaling suffer from being based on linear models. Until recently, very few methods were able to reduce the data dimensionality in a nonlinear way. However, since the late nineties, many new methods have been developed and nonlinear dimensionality reduction, also called manifold learning, has become a hot topic. New advances that account for this rapid growth are, e.g. the use of graphs to represent the manifold topology, and the use of new metrics like the geodesic distance. In addition, new optimization schemes, based on kernel techniques and spectral decomposition, have lead to spectral embedding, which encompasses many of the recently developed methods. This book describes existing and advanced methods to reduce the dimensionality of numerical databases. For each method, the description starts from intuitive ideas, develops the necessary mathematical details, and ends by outlining the algorithmic implementation. Methods are compared with each other with the help of different illustrative examples. The purpose of the book is to summarize clear facts and ideas about well-known methods as well as recent developments in the topic of nonlinear dimensionality reduction. With this goal in mind, methods are all described from a unifying point of view, in order to highlight their respective strengths and shortcomings. The book is primarily intended for statisticians, computer scientists and data analysts. It is also accessible to other practitioners having a basic background in statistics and/or computational learning, like psychologists (in psychometry) and economists.',\n",
       "  'date': '2007',\n",
       "  'authors': ['John A.', 'Michel'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Curse of dimensionality',\n",
       "   'Linear model',\n",
       "   'Metric (mathematics)',\n",
       "   'Kernel (statistics)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Principal component analysis',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,682',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2116064496',\n",
       "  'title': 'Training products of experts by minimizing contrastive divergence',\n",
       "  'abstract': 'It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \"expert\" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \"contrastive divergence\" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Product of experts',\n",
       "   'Deep belief network',\n",
       "   'Conditional independence',\n",
       "   'Latent variable',\n",
       "   'Inference',\n",
       "   'Probability distribution',\n",
       "   'Boltzmann machine',\n",
       "   'Data type',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,168',\n",
       "  'reference_count': '23',\n",
       "  'references': ['1652505363',\n",
       "   '1997063559',\n",
       "   '2096175520',\n",
       "   '1746680969',\n",
       "   '1993845689',\n",
       "   '2165225968',\n",
       "   '2083380015',\n",
       "   '2114153178',\n",
       "   '1547224907',\n",
       "   '2101706260']},\n",
       " {'id': '2134557905',\n",
       "  'title': 'Learning methods for generic object recognition with invariance to pose and lighting',\n",
       "  'abstract': 'We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Y. LeCun', 'Fu Jie Huang', 'L. Bottou'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Image segmentation',\n",
       "   'Support vector machine',\n",
       "   'Feature extraction',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Clutter',\n",
       "   'Grayscale',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,463',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2217896605',\n",
       "   '2124087378',\n",
       "   '2124351082',\n",
       "   '2123977795',\n",
       "   '2155511848',\n",
       "   '2160225842',\n",
       "   '2295106276',\n",
       "   '2141376824']},\n",
       " {'id': '2099866409',\n",
       "  'title': 'Restricted Boltzmann machines for collaborative filtering',\n",
       "  'abstract': \"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.\",\n",
       "  'date': '2007',\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Andriy Mnih', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'Boltzmann machine',\n",
       "   'Collaborative filtering',\n",
       "   'Graphical model',\n",
       "   'Word error rate',\n",
       "   'Inference',\n",
       "   'Machine learning',\n",
       "   'Data set',\n",
       "   'Singular value decomposition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,063',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1612003148',\n",
       "   '2122090912',\n",
       "   '2158164339',\n",
       "   '2124914669',\n",
       "   '205159212',\n",
       "   '2165395308']},\n",
       " {'id': '2536626143',\n",
       "  'title': 'Attribute and simile classifiers for face verification',\n",
       "  'abstract': 'We present two novel methods for face verification. Our first method - “attribute” classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - “simile” classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Neeraj Kumar',\n",
       "   'Alexander C. Berg',\n",
       "   'Peter N. Belhumeur',\n",
       "   'Shree K. Nayar'],\n",
       "  'related_topics': ['Feature extraction',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Data set',\n",
       "   'Visualization',\n",
       "   'Pattern recognition',\n",
       "   'Expression (mathematics)',\n",
       "   'Similarity (geometry)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,675',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2151103935',\n",
       "   '2119821739',\n",
       "   '1782590233',\n",
       "   '1989702938',\n",
       "   '2112076978',\n",
       "   '2033419168',\n",
       "   '2123921160',\n",
       "   '2098947662',\n",
       "   '2905573712',\n",
       "   '2155759509']},\n",
       " {'id': '2157364932',\n",
       "  'title': 'Learning a similarity metric discriminatively, with application to face verification',\n",
       "  'abstract': 'We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.',\n",
       "  'date': '2005',\n",
       "  'authors': ['S. Chopra', 'R. Hadsell', 'Y. LeCun'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Norm (mathematics)',\n",
       "   'Discriminative model',\n",
       "   'Robustness (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Face verification',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,924',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2310919327',\n",
       "   '2053186076',\n",
       "   '2138451337',\n",
       "   '2121647436',\n",
       "   '2095757522',\n",
       "   '2994340921',\n",
       "   '2144354855',\n",
       "   '2107369107',\n",
       "   '1802356529',\n",
       "   '10021998']},\n",
       " {'id': '2156909104',\n",
       "  'title': 'The Nature of Statistical Learning Theory',\n",
       "  'abstract': 'Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Vladimir N. Vapnik'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Statistical learning theory',\n",
       "   'Computational learning theory',\n",
       "   'Instance-based learning',\n",
       "   'Proactive learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Unsupervised learning',\n",
       "   'Probably approximately correct learning',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '67,838',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2296616510',\n",
       "  'title': 'Compressed sensing',\n",
       "  'abstract': 'Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel\\'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel\\'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces',\n",
       "  'date': '2004',\n",
       "  'authors': ['D.L. Donoho'],\n",
       "  'related_topics': ['Basis pursuit',\n",
       "   'Orthonormal basis',\n",
       "   'Linear combination',\n",
       "   'Matching pursuit',\n",
       "   'Basis pursuit denoising',\n",
       "   'Wavelet',\n",
       "   'Linear subspace',\n",
       "   'Restricted isometry property',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '17,696',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2145096794',\n",
       "   '2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2097323375',\n",
       "   '2136235822',\n",
       "   '2147656689',\n",
       "   '2012365979',\n",
       "   '2096613063',\n",
       "   '2050880896']},\n",
       " {'id': '2129131372',\n",
       "  'title': 'Decoding by linear programming',\n",
       "  'abstract': 'This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.',\n",
       "  'date': '2005',\n",
       "  'authors': ['E.J. Candes', 'T. Tao'],\n",
       "  'related_topics': ['Underdetermined system',\n",
       "   'Linear code',\n",
       "   'Sigma',\n",
       "   'Restricted isometry property',\n",
       "   'Linear system',\n",
       "   'Convex optimization',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Matching pursuit',\n",
       "   'System of linear equations',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,866',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2296319761',\n",
       "   '2296616510',\n",
       "   '2145096794',\n",
       "   '2129638195',\n",
       "   '2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2050834445',\n",
       "   '2154332973',\n",
       "   '2136235822']},\n",
       " {'id': '2119821739',\n",
       "  'title': 'Support-Vector Networks',\n",
       "  'abstract': 'The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.\\r\\n\\r\\nHigh generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Corinna Cortes', 'Vladimir Vapnik'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Feature vector',\n",
       "   'Computational learning theory',\n",
       "   'Online machine learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Linear classifier',\n",
       "   'Relevance vector machine',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '48,415',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2154642048',\n",
       "   '1498436455',\n",
       "   '2087347434',\n",
       "   '2154579312',\n",
       "   '1530699444',\n",
       "   '2168228682',\n",
       "   '2504871398',\n",
       "   '1568787085',\n",
       "   '5594912',\n",
       "   '2322002063']},\n",
       " {'id': '2161969291',\n",
       "  'title': 'Histograms of oriented gradients for human detection',\n",
       "  'abstract': 'We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.',\n",
       "  'date': '2005',\n",
       "  'authors': ['N. Dalal', 'B. Triggs'],\n",
       "  'related_topics': ['Histogram of oriented gradients',\n",
       "   'Local binary patterns',\n",
       "   'GLOH',\n",
       "   'Object detection',\n",
       "   'Feature (computer vision)',\n",
       "   'Pedestrian detection',\n",
       "   'Feature extraction',\n",
       "   'Normalization (image processing)',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Implicit Shape Model',\n",
       "   'Normalization (statistics)',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Caltech 101',\n",
       "   'Haar-like features',\n",
       "   'LabelMe',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Support vector machine',\n",
       "   'Histogram',\n",
       "   'Motion History Images',\n",
       "   'Pattern recognition',\n",
       "   'Traffic sign recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Feature descriptor',\n",
       "   'View Less'],\n",
       "  'citation_count': '36,483',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '2145072179',\n",
       "   '1576520375',\n",
       "   '2172188317',\n",
       "   '2152473410',\n",
       "   '1992825118',\n",
       "   '1608462934',\n",
       "   '2295106276',\n",
       "   '2156539399']},\n",
       " {'id': '2162915993',\n",
       "  'title': 'Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories',\n",
       "  'abstract': 'This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\\x92s \"gist\" and Lowe\\x92s SIFT descriptors.',\n",
       "  'date': '2006',\n",
       "  'authors': ['S. Lazebnik', 'C. Schmid', 'J. Ponce'],\n",
       "  'related_topics': ['Pyramid (image processing)',\n",
       "   'Pyramid',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Image texture',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image segmentation',\n",
       "   'Caltech 101',\n",
       "   'Visual dictionary',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Histogram',\n",
       "   'LabelMe',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,928',\n",
       "  'reference_count': '28',\n",
       "  'references': ['1880262756',\n",
       "   '2154422044',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2104978738',\n",
       "   '2914885528',\n",
       "   '2168002178',\n",
       "   '2134731454',\n",
       "   '2165828254']},\n",
       " {'id': '2097018403',\n",
       "  'title': 'Linear spatial pyramid matching using sparse coding for image classification',\n",
       "  'abstract': 'Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Jianchao Yang', 'Kai Yu', 'Yihong Gong', 'Thomas Huang'],\n",
       "  'related_topics': ['Support vector machine',\n",
       "   'Kernel (image processing)',\n",
       "   'Contextual image classification',\n",
       "   'Vector quantization',\n",
       "   'Caltech 101',\n",
       "   'Neural coding',\n",
       "   'Image segmentation',\n",
       "   'Histogram',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,781',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2153635508',\n",
       "   '2162915993',\n",
       "   '1576445103',\n",
       "   '2153663612',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2113606819',\n",
       "   '2161516371',\n",
       "   '1624854622']},\n",
       " {'id': '2166049352',\n",
       "  'title': 'Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories',\n",
       "  'abstract': 'Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Li Fei-Fei', 'Rob Fergus', 'Pietro Perona'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Learning object',\n",
       "   'Caltech 101',\n",
       "   'Bayesian inference',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Object (computer science)',\n",
       "   'Categorization',\n",
       "   'Bayesian probability',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,240',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2164598857',\n",
       "   '2124386111',\n",
       "   '2154422044',\n",
       "   '2124087378',\n",
       "   '2155511848',\n",
       "   '1516111018',\n",
       "   '1949116567',\n",
       "   '1746680969',\n",
       "   '2567948266',\n",
       "   '1699734612']},\n",
       " {'id': '1639032689',\n",
       "  'title': 'Genetic algorithms in search, optimization, and machine learning',\n",
       "  'abstract': 'From the Publisher:\\r\\nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \\r\\n\\r\\nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.',\n",
       "  'date': '1988',\n",
       "  'authors': ['David E.'],\n",
       "  'related_topics': ['Genetic representation',\n",
       "   'Genetic programming',\n",
       "   'Pascal (programming language)',\n",
       "   'Evolutionary computation',\n",
       "   'Computer programming',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Computer techniques',\n",
       "   'View Less'],\n",
       "  'citation_count': '108,858',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2154642048',\n",
       "  'title': 'Learning internal representations by error propagation',\n",
       "  'abstract': 'This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion',\n",
       "  'date': '1988',\n",
       "  'authors': ['D. E.', 'G. E.', 'R. J.'],\n",
       "  'related_topics': ['Delta rule',\n",
       "   'Semi-supervised learning',\n",
       "   'Backpropagation',\n",
       "   'Artificial neural network',\n",
       "   'Propagation of uncertainty',\n",
       "   'Online machine learning',\n",
       "   'Algebra',\n",
       "   'Stochastic gradient descent',\n",
       "   'Descent (mathematics)',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '31,294',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1535810436',\n",
       "   '1507849272',\n",
       "   '2101926813',\n",
       "   '2073257493',\n",
       "   '2021878536',\n",
       "   '1490454746',\n",
       "   '2115647291',\n",
       "   '1505136099']},\n",
       " {'id': '3017143921',\n",
       "  'title': 'Pattern classification and scene analysis',\n",
       "  'abstract': 'Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.',\n",
       "  'date': '1973',\n",
       "  'authors': ['Richard O.', 'Peter E.'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Cluster analysis',\n",
       "   'Linear discriminant analysis',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Perspective (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Nonparametric statistics',\n",
       "   'Bayes estimator',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Scene analysis',\n",
       "   'View Less'],\n",
       "  'citation_count': '21,861',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2100677568',\n",
       "  'title': 'Learning to Predict by the Methods of Temporal Differences',\n",
       "  'abstract': \"This article introduces a class of incremental learning procedures specialized for prediction – that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.\",\n",
       "  'date': '1988',\n",
       "  'authors': ['Richard S. Sutton'],\n",
       "  'related_topics': ['Temporal difference learning',\n",
       "   'Supervised learning',\n",
       "   'Heuristic',\n",
       "   'sort',\n",
       "   'Connectionism',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Class (computer programming)',\n",
       "   'Convergence (routing)',\n",
       "   'Computer science',\n",
       "   'Computation',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,363',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2895674046',\n",
       "   '1535810436',\n",
       "   '1507849272',\n",
       "   '2178806388',\n",
       "   '1596324102',\n",
       "   '1583833196',\n",
       "   '1569296262',\n",
       "   '2075379212']},\n",
       " {'id': '1535810436',\n",
       "  'title': 'Adaptive switching circuits',\n",
       "  'abstract': '',\n",
       "  'date': '1988',\n",
       "  'authors': ['Bernard', 'Marcian E.'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Electronic circuit',\n",
       "   'Electronic engineering',\n",
       "   'Adaptive switching',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,028',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1603765807',\n",
       "  'title': 'Parallel and Distributed Computation: Numerical Methods',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['Dimitri P. Bertsekas', 'John N. Tsitsiklis'],\n",
       "  'related_topics': ['Distributed design patterns',\n",
       "   'Distributed algorithm',\n",
       "   'Computation',\n",
       "   'Dynamic programming',\n",
       "   'Computational science',\n",
       "   'Computer science',\n",
       "   'Numerical analysis',\n",
       "   'Asynchronous algorithms',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,462',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3011120880',\n",
       "  'title': 'Learning from delayed rewards',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['C. J. C. H.'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Cognitive psychology',\n",
       "   'Q learning algorithm'],\n",
       "  'citation_count': '8,088',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1569320505',\n",
       "  'title': 'Adaptive filtering prediction and control',\n",
       "  'abstract': 'This unified survey focuses on linear discrete-time systems and explores the natural extensions to nonlinear systems. In keeping with the importance of computers to practical applications, the authors emphasize discrete-time systems. Their approach summarizes the theoretical and practical aspects of a large class of adaptive algorithms.1984 edition.',\n",
       "  'date': '1984',\n",
       "  'authors': ['Graham C.', 'Kwai Sang'],\n",
       "  'related_topics': ['Control theory',\n",
       "   'Adaptive filter',\n",
       "   'Nonlinear system',\n",
       "   'Control engineering',\n",
       "   'Computer science',\n",
       "   'Control (linguistics)',\n",
       "   'Natural (music)',\n",
       "   'Large class',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,699',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '94523489',\n",
       "  'title': 'Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks',\n",
       "  'abstract': \"Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain.\",\n",
       "  'date': '1988',\n",
       "  'authors': ['David S.', 'David'],\n",
       "  'related_topics': ['Interpolation',\n",
       "   'Linear interpolation',\n",
       "   'Bilinear interpolation',\n",
       "   'Trilinear interpolation',\n",
       "   'Learning rule',\n",
       "   'Radial basis function network',\n",
       "   'Generalization',\n",
       "   'Hierarchical RBF',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,007',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2178806388',\n",
       "  'title': 'Some studies in machine learning using the game of checkers',\n",
       "  'abstract': 'Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done by verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Arthur L.'],\n",
       "  'related_topics': ['Rote learning',\n",
       "   'Artificial intelligence',\n",
       "   'Period (music)',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Course (navigation)',\n",
       "   'Work (physics)',\n",
       "   'Sense of direction',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,436',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2126316555',\n",
       "  'title': 'A Survey of Monte Carlo Tree Search Methods',\n",
       "  'abstract': \"Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.\",\n",
       "  'date': '2012',\n",
       "  'authors': ['C. B. Browne',\n",
       "   'E. Powley',\n",
       "   'D. Whitehouse',\n",
       "   'S. M. Lucas',\n",
       "   'P. I. Cowling',\n",
       "   'P. Rohlfshagen',\n",
       "   'S. Tavener',\n",
       "   'D. Perez',\n",
       "   'S. Samothrakis',\n",
       "   'S. Colton'],\n",
       "  'related_topics': ['Monte Carlo tree search',\n",
       "   'Computer Go',\n",
       "   'General game playing',\n",
       "   'Monte Carlo method',\n",
       "   'General video game playing',\n",
       "   'Decision theory',\n",
       "   'Machine learning',\n",
       "   'Open research',\n",
       "   'Computer science',\n",
       "   'Game theory',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,344',\n",
       "  'reference_count': '214',\n",
       "  'references': ['2122410182',\n",
       "   '2168405694',\n",
       "   '1625390266',\n",
       "   '1714211023',\n",
       "   '131069610',\n",
       "   '2171084228',\n",
       "   '2020135152',\n",
       "   '1888434271',\n",
       "   '1510812122',\n",
       "   '1500868819']},\n",
       " {'id': '1515851193',\n",
       "  'title': 'Introduction to Reinforcement Learning',\n",
       "  'abstract': \"From the Publisher:\\r\\nIn Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.\",\n",
       "  'date': '1998',\n",
       "  'authors': ['Richard S.', 'Andrew G.'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Temporal difference learning',\n",
       "   'Q-learning',\n",
       "   'AIXI',\n",
       "   'Cognitive science',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,402',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1625390266',\n",
       "  'title': 'Bandit based monte-carlo planning',\n",
       "  'abstract': 'For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Levente Kocsis', 'Csaba Szepesvári'],\n",
       "  'related_topics': ['Monte Carlo tree search',\n",
       "   'Monte Carlo method',\n",
       "   'Decision problem',\n",
       "   'Sampling (statistics)',\n",
       "   'Markov process',\n",
       "   'Mathematical optimization',\n",
       "   'Computer Go',\n",
       "   'Sample (statistics)',\n",
       "   'General game playing',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,101',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2168405694',\n",
       "   '2077902449',\n",
       "   '2101861158',\n",
       "   '2009551863',\n",
       "   '1512919909',\n",
       "   '1551466210',\n",
       "   '1863869622',\n",
       "   '2016647253',\n",
       "   '1562282139',\n",
       "   '2135997697']},\n",
       " {'id': '1502916507',\n",
       "  'title': 'Similarity Search in High Dimensions via Hashing',\n",
       "  'abstract': 'The nearestor near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the \\\\curse of dimensionality.\" That is, the data structures scale poorly with data dimensionality; in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should su ce for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points Supported by NAVY N00014-96-1-1221 grant and NSF Grant IIS-9811904. Supported by Stanford Graduate Fellowship and NSF NYI Award CCR-9357849. Supported by ARO MURI Grant DAAH04-96-1-0007, NSF Grant IIS-9811904, and NSF Young Investigator Award CCR9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 25th VLDB Conference, Edinburgh, Scotland, 1999. from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives signi cant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).',\n",
       "  'date': '1999',\n",
       "  'authors': ['Aristides Gionis', 'Piotr Indyk', 'Rajeev Motwani'],\n",
       "  'related_topics': ['Nearest neighbor search',\n",
       "   'Locality-sensitive hashing',\n",
       "   'Very large database',\n",
       "   'Open addressing',\n",
       "   'Dynamic perfect hashing',\n",
       "   'Cover tree',\n",
       "   'Hash function',\n",
       "   'Ball tree',\n",
       "   'Feature hashing',\n",
       "   'Linear search',\n",
       "   'Universal hashing',\n",
       "   'K-independent hashing',\n",
       "   'Hopscotch hashing',\n",
       "   'Metric (mathematics)',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Tree decomposition',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,138',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2147152072',\n",
       "   '2147717514',\n",
       "   '1634005169',\n",
       "   '2295428206',\n",
       "   '1956559956',\n",
       "   '2125148312',\n",
       "   '2160066518',\n",
       "   '3017143921',\n",
       "   '2074429597',\n",
       "   '1541459201']},\n",
       " {'id': '2099587183',\n",
       "  'title': 'General Game Playing: Overview of the AAAI Competition',\n",
       "  'abstract': \"A general game playing system is one that can accept a formal description of a game and play the game effectively without human intervention. Unlike specialized game players, such as Deep Blue, general game players do not rely on algorithms designed in advance for specific games; and, unlike Deep Blue, they are able to play different kinds of games. In order to promote work in this area, the AAAI is sponsoring an open competition at this summer's Twentieth National Conference on Artificial Intelligence. This article is an overview of the technical issues and logistics associated with this summer's competition, as well as the relevance of general game playing to the long range-goals of artificial intelligence.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['Michael R.', 'Nathaniel Love', 'Barney'],\n",
       "  'related_topics': ['General game playing',\n",
       "   'General video game playing',\n",
       "   'Game design',\n",
       "   'Video game design',\n",
       "   'Game Developer',\n",
       "   'Non-cooperative game',\n",
       "   'Game mechanics',\n",
       "   'Repeated game',\n",
       "   'Human–computer interaction',\n",
       "   'Operations research',\n",
       "   'Engineering',\n",
       "   'View Less'],\n",
       "  'citation_count': '616',\n",
       "  'reference_count': '4',\n",
       "  'references': ['1981627423', '1572038152', '2119409989', '3025398977']},\n",
       " {'id': '2013391942',\n",
       "  'title': 'Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability',\n",
       "  'abstract': 'This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environment, the latter is suited for passive prediction in unknown environment. The book introduces these two well-known but very different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment. Most AI problems can easily be formulated within this theory, which reduces the conceptual problems to pure computational problems. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches to AI. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Marcus'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Reinforcement learning',\n",
       "   'Algorithmic probability',\n",
       "   'AIXI',\n",
       "   'Algorithmic information theory',\n",
       "   \"Solomonoff's theory of inductive inference\",\n",
       "   'Kolmogorov complexity',\n",
       "   'Computational problem',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '709',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2101355568',\n",
       "  'title': 'An object-oriented representation for efficient reinforcement learning',\n",
       "  'abstract': 'Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Carlos Diuk', 'Andre Cohen', 'Michael L. Littman'],\n",
       "  'related_topics': ['Learning classifier system',\n",
       "   'Reinforcement learning',\n",
       "   'Robot learning',\n",
       "   'Representation (mathematics)',\n",
       "   'Generalization',\n",
       "   'Polynomial',\n",
       "   'Artificial intelligence',\n",
       "   'Domain (software engineering)',\n",
       "   'Computer science',\n",
       "   'Markov process',\n",
       "   'View Less'],\n",
       "  'citation_count': '270',\n",
       "  'reference_count': '102',\n",
       "  'references': ['2122410182',\n",
       "   '1506806321',\n",
       "   '2121863487',\n",
       "   '2168405694',\n",
       "   '1515851193',\n",
       "   '1625390266',\n",
       "   '2119567691',\n",
       "   '2107726111',\n",
       "   '1576452626',\n",
       "   '2168359464']},\n",
       " {'id': '2132622533',\n",
       "  'title': 'Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction',\n",
       "  'abstract': \"Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction.\",\n",
       "  'date': '2011',\n",
       "  'authors': ['Richard S. Sutton',\n",
       "   'Joseph Modayil',\n",
       "   'Michael Delp',\n",
       "   'Thomas Degris',\n",
       "   'Patrick M. Pilarski',\n",
       "   'Adam White',\n",
       "   'Doina Precup'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Robot learning',\n",
       "   'Reinforcement learning',\n",
       "   'Knowledge representation and reasoning',\n",
       "   'Temporal difference learning',\n",
       "   'Function approximation',\n",
       "   'General knowledge',\n",
       "   'Mobile robot',\n",
       "   'Robot',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '368',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2121863487',\n",
       "   '1515851193',\n",
       "   '2100677568',\n",
       "   '2109910161',\n",
       "   '2075268401',\n",
       "   '2062937587',\n",
       "   '1491843047',\n",
       "   '2149390907',\n",
       "   '2134042548',\n",
       "   '13294968']},\n",
       " {'id': '179875071',\n",
       "  'title': 'Recurrent neural network based language model',\n",
       "  'abstract': 'A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition',\n",
       "  'date': '2010',\n",
       "  'authors': ['Tomas Mikolov',\n",
       "   'Martin Karafiát',\n",
       "   'Lukás Burget',\n",
       "   'Jan',\n",
       "   'Sanjeev Khudanpur'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Language model',\n",
       "   'Recurrent neural network',\n",
       "   'Word error rate',\n",
       "   'Perplexity',\n",
       "   'Connectionism',\n",
       "   'Reduction (complexity)',\n",
       "   'Speech recognition',\n",
       "   'NIST',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,539',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2132339004',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '36903255',\n",
       "   '2096072088',\n",
       "   '2468573742',\n",
       "   '2152808281',\n",
       "   '2292896937',\n",
       "   '2027499299',\n",
       "   '2437096199']},\n",
       " {'id': '2147152072',\n",
       "  'title': 'Indexing by Latent Semantic Analysis',\n",
       "  'abstract': 'A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Scott Deerwester',\n",
       "   'Susan T. Dumais',\n",
       "   'George W. Furnas',\n",
       "   'Thomas K. Landauer',\n",
       "   'Richard Harshman'],\n",
       "  'related_topics': ['Document-term matrix',\n",
       "   'Latent semantic analysis',\n",
       "   'Vector space model',\n",
       "   'Automatic indexing',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Explicit semantic analysis',\n",
       "   'Random indexing',\n",
       "   'Document retrieval',\n",
       "   'Information retrieval',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '17,225',\n",
       "  'reference_count': '26',\n",
       "  'references': ['1956559956',\n",
       "   '1984565341',\n",
       "   '1964262399',\n",
       "   '2000215628',\n",
       "   '2114804204',\n",
       "   '2151561903',\n",
       "   '3012395598',\n",
       "   '1965061793',\n",
       "   '2024683548',\n",
       "   '2096411881']},\n",
       " {'id': '1632114991',\n",
       "  'title': 'Building a large annotated corpus of English: the penn treebank',\n",
       "  'abstract': 'Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Mitchell P. Marcus',\n",
       "   'Mary Ann Marcinkiewicz',\n",
       "   'Beatrice Santorini'],\n",
       "  'related_topics': ['Corpus linguistics',\n",
       "   'Brown Corpus',\n",
       "   'Treebank',\n",
       "   'Text corpus',\n",
       "   'PropBank',\n",
       "   'Trigram tagger',\n",
       "   'Computational linguistics',\n",
       "   'Shallow parsing',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,170',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2099247782',\n",
       "   '1483126227',\n",
       "   '2439178139',\n",
       "   '2334801970',\n",
       "   '900993354',\n",
       "   '2110190189',\n",
       "   '2012837062',\n",
       "   '2121407024',\n",
       "   '2076526090',\n",
       "   '2166675302']},\n",
       " {'id': '1970689298',\n",
       "  'title': 'Continuous space language models',\n",
       "  'abstract': 'This paper describes the use of a neural network language model for large vocabulary continuous speech recognition. The underlying idea of this approach is to attack the data sparseness problem by performing the language model probability estimation in a continuous space. Highly efficient learning algorithms are described that enable the use of training corpora of several hundred million words. It is also shown that this approach can be incorporated into a large vocabulary continuous speech recognizer using a lattice rescoring framework at a very low additional processing time. The neural network language model was thoroughly evaluated in a state-of-the-art large vocabulary continuous speech recognizer for several international benchmark tasks, in particular the Nist evaluations on broadcast news and conversational speech recognition. The new approach is compared to four-gram back-off language models trained with modified Kneser-Ney smoothing which has often been reported to be the best known smoothing method. Usually the neural network language model is interpolated with the back-off language model. In that way, consistent word error rate reductions for all considered tasks and languages were achieved, ranging from 0.4% to almost 1% absolute.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Holger Schwenk'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Time delay neural network',\n",
       "   'Computational linguistics',\n",
       "   'n-gram',\n",
       "   'Word error rate',\n",
       "   'Factored language model',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '604',\n",
       "  'reference_count': '63',\n",
       "  'references': ['2148603752',\n",
       "   '1554663460',\n",
       "   '2912934387',\n",
       "   '2132339004',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '36903255',\n",
       "   '2158195707']},\n",
       " {'id': '2130903752',\n",
       "  'title': 'A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data',\n",
       "  'abstract': \"One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['Rie Kubota', 'Tong'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Multi-task learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Online machine learning',\n",
       "   'Learning classifier system',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,514',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2148603752',\n",
       "   '1480376833',\n",
       "   '2154455818',\n",
       "   '2139823104',\n",
       "   '2048679005',\n",
       "   '2097089247',\n",
       "   '2107008379',\n",
       "   '2914746235',\n",
       "   '2010353172',\n",
       "   '2101210369']},\n",
       " {'id': '2158847908',\n",
       "  'title': 'The Proposition Bank: An Annotated Corpus of Semantic Roles',\n",
       "  'abstract': \"The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.\\r\\n\\r\\nWe describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['Martha Palmer', 'Daniel Gildea', 'Paul Kingsbury'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Semantic role labeling',\n",
       "   'Explicit semantic analysis',\n",
       "   'Semantic computing',\n",
       "   'Semantic similarity',\n",
       "   'PropBank',\n",
       "   'FrameNet',\n",
       "   'VerbNet',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,678',\n",
       "  'reference_count': '57',\n",
       "  'references': ['1632114991',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '2115792525',\n",
       "   '2151170651',\n",
       "   '3021452258',\n",
       "   '2039217078',\n",
       "   '1567277581',\n",
       "   '2126851059',\n",
       "   '2154626406']},\n",
       " {'id': '2107008379',\n",
       "  'title': 'Transductive Inference for Text Classification using Support Vector Machines',\n",
       "  'abstract': '',\n",
       "  'date': '1999',\n",
       "  'authors': ['Thorsten'],\n",
       "  'related_topics': ['Relevance vector machine',\n",
       "   'Transduction (machine learning)',\n",
       "   'Structured support vector machine',\n",
       "   'Support vector machine',\n",
       "   'Co-training',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,773',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2914746235',\n",
       "  'title': 'Multitask learning',\n",
       "  'abstract': 'Multitask Learning is an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate multitask learning for a dozen problems. We explain how multitask learning works and show that there are many opportunities for multitask learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as multitask outputs instead. We present suggestions for how to get the most out of multitask learning in artificial neural nets, present an algorithm for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Multitask learning improves generalization performance, can be applied in many different kinds of domains, and can be used with different learning algorithms. We conjecture there will be many opportunities for its use on real-world problems.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Rich'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Inductive transfer',\n",
       "   'Artificial neural network',\n",
       "   'Decision tree',\n",
       "   'Task (project management)',\n",
       "   'Generalization (learning)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Sketch',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,123',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2173629880',\n",
       "  'title': 'Phoneme recognition using time-delay neural networks',\n",
       "  'abstract': 'The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >',\n",
       "  'date': '1995',\n",
       "  'authors': ['Alexander', 'Toshiyuki', 'Geoffrey', 'Kiyohiro', 'Kevin J.'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Hidden Markov model',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Speech recognition',\n",
       "   'Task (computing)',\n",
       "   'Hierarchy (mathematics)',\n",
       "   'Position (vector)',\n",
       "   'Computer science',\n",
       "   'Nonlinear system',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,360',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2885050925',\n",
       "  'title': 'Shallow Semantic Parsing using Support Vector Machines.',\n",
       "  'abstract': '',\n",
       "  'date': '2004',\n",
       "  'authors': ['Sameer S.', 'Wayne H.', 'Kadri', 'James H.', 'Daniel'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Support vector machine',\n",
       "   'Natural language processing',\n",
       "   'PropBank',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '498',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2092654472',\n",
       "   '2151170651',\n",
       "   '2166776180',\n",
       "   '1520377376',\n",
       "   '1988995507',\n",
       "   '2145310422',\n",
       "   '2155693943',\n",
       "   '2093647425',\n",
       "   '2040909025',\n",
       "   '2150203234']},\n",
       " {'id': '2158823144',\n",
       "  'title': 'Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data',\n",
       "  'abstract': 'In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Charles Sutton', 'Khashayar Rohanimanesh', 'Andrew McCallum'],\n",
       "  'related_topics': ['Approximate inference',\n",
       "   'Variable elimination',\n",
       "   'Bayesian network',\n",
       "   'Belief propagation',\n",
       "   'Graphical model',\n",
       "   'Conditional random field',\n",
       "   'Dynamic Bayesian network',\n",
       "   'Inference',\n",
       "   'Probabilistic logic',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,059',\n",
       "  'reference_count': '57',\n",
       "  'references': ['2147880316',\n",
       "   '2116064496',\n",
       "   '2125838338',\n",
       "   '1574901103',\n",
       "   '1632114991',\n",
       "   '2008652694',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '2169415915',\n",
       "   '2156515921']},\n",
       " {'id': '2163568299',\n",
       "  'title': 'Effective Self-Training for Parsing',\n",
       "  'abstract': 'We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.',\n",
       "  'date': '2006',\n",
       "  'authors': ['David McClosky', 'Eugene Charniak', 'Mark Johnson'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Discriminative model',\n",
       "   'Bootstrapping',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Error reduction',\n",
       "   'Self training',\n",
       "   'View Less'],\n",
       "  'citation_count': '602',\n",
       "  'reference_count': '20',\n",
       "  'references': ['1632114991',\n",
       "   '2048679005',\n",
       "   '1535015163',\n",
       "   '2158195707',\n",
       "   '3021452258',\n",
       "   '1567570606',\n",
       "   '2125712079',\n",
       "   '2729906263',\n",
       "   '2098379588',\n",
       "   '2037894654']},\n",
       " {'id': '1631260214',\n",
       "  'title': 'SRILM – An Extensible Language Modeling Toolkit',\n",
       "  'abstract': '',\n",
       "  'date': '2002',\n",
       "  'authors': ['Andreas'],\n",
       "  'related_topics': ['Modeling language',\n",
       "   'High-level programming language',\n",
       "   'Computer science',\n",
       "   'Language model',\n",
       "   'Programming language',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Inversion transduction grammars',\n",
       "   'Machine translation system',\n",
       "   'Recurrent neural network language models',\n",
       "   'Speech transcription',\n",
       "   'System combination',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,402',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2158195707',\n",
       "   '2121227244',\n",
       "   '1904457459',\n",
       "   '2594610113',\n",
       "   '1549285799',\n",
       "   '2100506586',\n",
       "   '1797288984',\n",
       "   '2097978681',\n",
       "   '1528470941',\n",
       "   '2127836646']},\n",
       " {'id': '2096175520',\n",
       "  'title': 'A maximum entropy approach to natural language processing',\n",
       "  'abstract': 'The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Adam L. Berger',\n",
       "   'Vincent J. Della Pietra',\n",
       "   'Stephen A. Della Pietra'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Statistical model',\n",
       "   'Generalized iterative scaling',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,915',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2099111195',\n",
       "   '2049633694',\n",
       "   '2006969979',\n",
       "   '2121227244',\n",
       "   '2160842254',\n",
       "   '2097333193',\n",
       "   '1597533204',\n",
       "   '2099345940',\n",
       "   '2167434254',\n",
       "   '1976241232']},\n",
       " {'id': '2110485445',\n",
       "  'title': 'Finding Structure in Time',\n",
       "  'abstract': 'Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Jeffrey L. Elman'],\n",
       "  'related_topics': ['Task (project management)',\n",
       "   'Context (language use)',\n",
       "   'Semantics',\n",
       "   'Connectionism',\n",
       "   'Set (psychology)',\n",
       "   'Artificial grammar learning',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Prediction in language comprehension',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,889',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2173629880',\n",
       "   '2016589492',\n",
       "   '3036751298',\n",
       "   '2118373646',\n",
       "   '2046432185',\n",
       "   '2122988375',\n",
       "   '2170716495',\n",
       "   '2094249282']},\n",
       " {'id': '1575350781',\n",
       "  'title': 'MPI: A Message-Passing Interface Standard',\n",
       "  'abstract': 'The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Message P'],\n",
       "  'related_topics': ['Message passing',\n",
       "   'Message Passing Interface',\n",
       "   'Interface (Java)',\n",
       "   'Standards organization',\n",
       "   'MPICH',\n",
       "   'Set (abstract data type)',\n",
       "   'World Wide Web',\n",
       "   'Computer science',\n",
       "   'Message passing interface standard',\n",
       "   'Remote memory access',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,616',\n",
       "  'reference_count': '21',\n",
       "  'references': ['1480928214',\n",
       "   '1521571223',\n",
       "   '1964564149',\n",
       "   '1978513924',\n",
       "   '2083200599',\n",
       "   '2090683636',\n",
       "   '2010542899',\n",
       "   '2294265735',\n",
       "   '1843937266',\n",
       "   '2010269868']},\n",
       " {'id': '2158195707',\n",
       "  'title': 'An empirical study of smoothing techniques for language modeling',\n",
       "  'abstract': 'We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser?Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Stanley F. Chen', 'Joshua Goodman'],\n",
       "  'related_topics': ['Kneser–Ney smoothing',\n",
       "   'Smoothing',\n",
       "   'Bigram',\n",
       "   'Trigram',\n",
       "   'Language model',\n",
       "   'Test data',\n",
       "   'Speech recognition',\n",
       "   'Empirical research',\n",
       "   'Gram',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,698',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2170120409',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '1966812932',\n",
       "   '2611071497',\n",
       "   '2166637769',\n",
       "   '2134237567',\n",
       "   '2075201173']},\n",
       " {'id': '2121227244',\n",
       "  'title': 'Class-based n -gram models of natural language',\n",
       "  'abstract': 'We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Peter F. Brown',\n",
       "   'Peter V. deSouza',\n",
       "   'Robert L. Mercer',\n",
       "   'Vincent J. Della Pietra',\n",
       "   'Jenifer C. Lai'],\n",
       "  'related_topics': ['n-gram',\n",
       "   'Natural language',\n",
       "   'Word (group theory)',\n",
       "   'Factored language model',\n",
       "   'Natural language processing',\n",
       "   'Class (biology)',\n",
       "   'Sample (statistics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,024',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2049633694',\n",
       "   '2097333193',\n",
       "   '1966812932',\n",
       "   '2142901448',\n",
       "   '2751862591',\n",
       "   '1597533204',\n",
       "   '1575431606',\n",
       "   '2007780422',\n",
       "   '2016871293',\n",
       "   '1628850721']},\n",
       " {'id': '2914484425',\n",
       "  'title': 'Efficient BackProp',\n",
       "  'abstract': '',\n",
       "  'date': '1998',\n",
       "  'authors': ['Yann', 'Léon', 'Genevieve B.', 'Klaus-Robert'],\n",
       "  'related_topics': ['Computer science'],\n",
       "  'citation_count': '4,270',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1880262756',\n",
       "  'title': 'Latent dirichlet allocation',\n",
       "  'abstract': 'We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.',\n",
       "  'date': '2003',\n",
       "  'authors': ['David M. Blei', 'Andrew Y. Ng', 'Michael I. Jordan'],\n",
       "  'related_topics': ['Latent Dirichlet allocation',\n",
       "   'Dynamic topic model',\n",
       "   'Hierarchical Dirichlet process',\n",
       "   'Topic model',\n",
       "   'Pachinko allocation',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Variational message passing',\n",
       "   'Dirichlet-multinomial distribution',\n",
       "   'Data mining',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '38,465',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2045656233',\n",
       "   '2147152072',\n",
       "   '2107743791',\n",
       "   '2097089247',\n",
       "   '1956559956',\n",
       "   '1516111018',\n",
       "   '1508165687',\n",
       "   '1746680969',\n",
       "   '2020842694',\n",
       "   '2063392856']},\n",
       " {'id': '168564468',\n",
       "  'title': 'Software Framework for Topic Modelling with Large Corpora',\n",
       "  'abstract': \"Large corpora are ubiquitous in today's world and memory\\nquickly becomes the limiting factor in practical applications\\nof the Vector Space Model (VSM). We identify gap in existing\\nVSM implementations, which is their scalability and ease of\\nuse. We describe a Natural Language Processing software\\nframework which is based on the idea of document streaming,\\ni.e. processing corpora document after document, in a memory\\nindependent fashion. In this framework, we implement several\\npopular algorithms for topical inference, including Latent\\nSemantic Analysis and Latent Dirichlet Allocation, in a way\\nthat makes them completely independent of the training corpus\\nsize. Particular emphasis is placed on straightforward and\\nintuitive framework design, so that modifications and\\nextensions of the methods and/or their application by\\ninterested practitioners are effortless. We demonstrate the\\nusefulness of our approach on a real-world scenario of\\ncomputing document similarities within an existing digital\\nlibrary DML-CZ.\",\n",
       "  'date': '2010',\n",
       "  'authors': ['Radim Řehůřek', 'Petr Sojka'],\n",
       "  'related_topics': ['Latent Dirichlet allocation',\n",
       "   'Topic model',\n",
       "   'Software framework',\n",
       "   'Latent semantic analysis',\n",
       "   'Vector space model',\n",
       "   'Scalability',\n",
       "   'Software',\n",
       "   'Python (programming language)',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,231',\n",
       "  'reference_count': '25',\n",
       "  'references': ['1880262756',\n",
       "   '2170120409',\n",
       "   '2001082470',\n",
       "   '2147152072',\n",
       "   '2158266063',\n",
       "   '2047804403',\n",
       "   '2143017621',\n",
       "   '2159426623',\n",
       "   '2334889010',\n",
       "   '2063392856']},\n",
       " {'id': '2296073425',\n",
       "  'title': 'Curriculum learning',\n",
       "  'abstract': 'Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).',\n",
       "  'date': '2009',\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Jérôme Louradour',\n",
       "   'Ronan Collobert',\n",
       "   'Jason Weston'],\n",
       "  'related_topics': ['Active learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Instance-based learning',\n",
       "   'Curriculum',\n",
       "   'Context (language use)',\n",
       "   'Generalization (learning)',\n",
       "   'Stochastic neural network',\n",
       "   'Process (engineering)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,862',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '2099866409',\n",
       "   '1994197834',\n",
       "   '2172174689',\n",
       "   '205159212']},\n",
       " {'id': '2158997610',\n",
       "  'title': 'An introduction to latent semantic analysis',\n",
       "  'abstract': \"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual‐usage meaning of words by statistical computations applied to a large corpus of text (Landauer & Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA's reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word‐word and passage‐word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.\",\n",
       "  'date': '1998',\n",
       "  'authors': ['Thomas K Landauer', 'Peter W. Foltz', 'Darrell Laham'],\n",
       "  'related_topics': ['Latent semantic analysis',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Vocabulary',\n",
       "   'Similarity (psychology)',\n",
       "   'Semantics',\n",
       "   'Learnability',\n",
       "   'Priming (psychology)',\n",
       "   'Automated essay scoring',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,465',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2147152072',\n",
       "   '1983578042',\n",
       "   '1674947250',\n",
       "   '2072773380',\n",
       "   '2056029990',\n",
       "   '1981617416',\n",
       "   '2059086756',\n",
       "   '2092919341',\n",
       "   '2928502135',\n",
       "   '1996650435']},\n",
       " {'id': '2004763266',\n",
       "  'title': 'Design Challenges and Misconceptions in Named Entity Recognition',\n",
       "  'abstract': 'We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Lev Ratinov', 'Dan Roth'],\n",
       "  'related_topics': ['Named-entity recognition',\n",
       "   'Inference',\n",
       "   'F1 score',\n",
       "   'Process (engineering)',\n",
       "   'Machine learning',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Representation (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,567',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2147880316',\n",
       "   '2125838338',\n",
       "   '2096765155',\n",
       "   '2008652694',\n",
       "   '1996430422',\n",
       "   '2148540243',\n",
       "   '2144578941',\n",
       "   '2121227244',\n",
       "   '2128634885',\n",
       "   '1979711143']},\n",
       " {'id': '2156515921',\n",
       "  'title': 'Shallow parsing with conditional random fields',\n",
       "  'abstract': 'Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Fei Sha', 'Fernando Pereira'],\n",
       "  'related_topics': ['Conditional random field',\n",
       "   'Shallow parsing',\n",
       "   'Sequence labeling',\n",
       "   'Chunking (psychology)',\n",
       "   'Machine learning',\n",
       "   'Natural language processing',\n",
       "   'Sequence',\n",
       "   'Computer science',\n",
       "   'Task (computing)',\n",
       "   'Generative grammar',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,924',\n",
       "  'reference_count': '33',\n",
       "  'references': ['1995945562',\n",
       "   '2147880316',\n",
       "   '2008652694',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '1773803948',\n",
       "   '2160842254',\n",
       "   '2962735828',\n",
       "   '2117400858',\n",
       "   '1520377376']},\n",
       " {'id': '2067191022',\n",
       "  'title': 'Mean shift: a robust approach toward feature space analysis',\n",
       "  'abstract': 'A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.',\n",
       "  'date': '2002',\n",
       "  'authors': ['D. Comaniciu', 'P. Meer'],\n",
       "  'related_topics': ['Mean-shift',\n",
       "   'Smoothing',\n",
       "   'Estimator',\n",
       "   'Kernel (statistics)',\n",
       "   'Feature vector',\n",
       "   'Image segmentation',\n",
       "   'Estimation theory',\n",
       "   'Kernel regression',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,663',\n",
       "  'reference_count': '69',\n",
       "  'references': ['2798766386',\n",
       "   '2140235142',\n",
       "   '2099244020',\n",
       "   '2132549764',\n",
       "   '2159128898',\n",
       "   '2150134853',\n",
       "   '1971784203',\n",
       "   '2129905273',\n",
       "   '2999729612',\n",
       "   '2482402870']},\n",
       " {'id': '1574901103',\n",
       "  'title': 'Foundations of Statistical Natural Language Processing',\n",
       "  'abstract': 'Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Christopher D. Manning', 'Hinrich Schütze'],\n",
       "  'related_topics': ['Computational linguistics',\n",
       "   'Natural language',\n",
       "   'Parsing',\n",
       "   'Collocation extraction',\n",
       "   'Data-oriented parsing',\n",
       "   'Probabilistic logic',\n",
       "   'Collocation',\n",
       "   'Natural language processing',\n",
       "   'Construct (python library)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,331',\n",
       "  'reference_count': '9',\n",
       "  'references': ['1508165687',\n",
       "   '182831726',\n",
       "   '1994851566',\n",
       "   '2108321481',\n",
       "   '1549026077',\n",
       "   '2949237929',\n",
       "   '1795234945',\n",
       "   '1746620543',\n",
       "   '1736036918']},\n",
       " {'id': '1566135517',\n",
       "  'title': 'Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope',\n",
       "  'abstract': 'In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Aude Oliva', 'Antonio Torralba'],\n",
       "  'related_topics': ['Scene statistics',\n",
       "   'Representation (systemics)',\n",
       "   'Envelope (motion)',\n",
       "   'Naturalness',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Categorization',\n",
       "   'LabelMe',\n",
       "   'Segmentation',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,505',\n",
       "  'reference_count': '45',\n",
       "  'references': ['2117812871',\n",
       "   '2128716185',\n",
       "   '2012352340',\n",
       "   '2130259898',\n",
       "   '2156406284',\n",
       "   '1524408959',\n",
       "   '2180838288',\n",
       "   '2142796031',\n",
       "   '2104825706',\n",
       "   '2167034998']},\n",
       " {'id': '2536208356',\n",
       "  'title': 'Decomposing a scene into geometric and semantically consistent regions',\n",
       "  'abstract': 'High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Stephen Gould', 'Richard Fulton', 'Daphne Koller'],\n",
       "  'related_topics': ['Orientation (computer vision)',\n",
       "   'Image segmentation',\n",
       "   'Object (computer science)',\n",
       "   'Solid modeling',\n",
       "   '3D reconstruction',\n",
       "   'Representation (mathematics)',\n",
       "   'Pixel',\n",
       "   'Pattern recognition',\n",
       "   'Inference',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '810',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2161969291',\n",
       "   '2033819227',\n",
       "   '3097096317',\n",
       "   '2067191022',\n",
       "   '2110764733',\n",
       "   '1528789833',\n",
       "   '2132947399',\n",
       "   '2125310925',\n",
       "   '2116877738',\n",
       "   '2112301665']},\n",
       " {'id': '2322002063',\n",
       "  'title': 'Principles of neurodynamics',\n",
       "  'abstract': '',\n",
       "  'date': '1962',\n",
       "  'authors': ['A. A.', 'Frank'],\n",
       "  'related_topics': ['Computer science'],\n",
       "  'citation_count': '2,760',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2173213060',\n",
       "  'title': 'MapReduce: simplified data processing on large clusters',\n",
       "  'abstract': \"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.\",\n",
       "  'date': '2008',\n",
       "  'authors': ['Jeffrey Dean', 'Sanjay Ghemawat'],\n",
       "  'related_topics': ['Data-intensive computing',\n",
       "   'Runtime system',\n",
       "   'Many-task computing',\n",
       "   'Programming with Big Data in R',\n",
       "   'Jaql',\n",
       "   'Programming paradigm',\n",
       "   'NewSQL',\n",
       "   'Data center network architectures',\n",
       "   'Parallel computing',\n",
       "   'Petabyte',\n",
       "   'Distributed computing',\n",
       "   'Computer science',\n",
       "   'Massively parallel computation',\n",
       "   'View Less'],\n",
       "  'citation_count': '30,857',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2173213060',\n",
       "   '2119565742',\n",
       "   '2148317584',\n",
       "   '2073965851',\n",
       "   '2109722477',\n",
       "   '2104644701',\n",
       "   '1510543252',\n",
       "   '2044534358',\n",
       "   '1988243929',\n",
       "   '2045271686']},\n",
       " {'id': '1532325895',\n",
       "  'title': 'Introduction to Information Retrieval',\n",
       "  'abstract': 'Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Christopher D. Manning',\n",
       "   'Prabhakar Raghavan',\n",
       "   'Hinrich Schütze'],\n",
       "  'related_topics': ['Human–computer information retrieval',\n",
       "   'Cognitive models of information retrieval',\n",
       "   'Concept search',\n",
       "   'Relevance (information retrieval)',\n",
       "   'Information science',\n",
       "   'Document retrieval',\n",
       "   'Adversarial information retrieval',\n",
       "   'Search engine indexing',\n",
       "   'Multimedia',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,614',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3013264884',\n",
       "  'title': 'The Anatomy of a Large-Scale Hypertextual Web Search Engine.',\n",
       "  'abstract': 'In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The  prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/. To engineer a search  engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search  engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Sergey', 'Lawrence'],\n",
       "  'related_topics': ['Web search engine',\n",
       "   'Web page',\n",
       "   'Hypertext',\n",
       "   'Hyperlink',\n",
       "   'Index (publishing)',\n",
       "   'Search engine',\n",
       "   'World Wide Web',\n",
       "   'Exploit',\n",
       "   'Public Description',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '20,977',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2024165284',\n",
       "  'title': 'Tensor Decompositions and Applications',\n",
       "  'abstract': 'This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\\\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Tamara G. Kolda', 'Brett W. Bader'],\n",
       "  'related_topics': ['Tensor contraction',\n",
       "   'Cartesian tensor',\n",
       "   'Symmetric tensor',\n",
       "   'Tensor',\n",
       "   'Matricization',\n",
       "   'Tensor product of Hilbert spaces',\n",
       "   'Tucker decomposition',\n",
       "   'Tensor product',\n",
       "   'Algebra',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,886',\n",
       "  'reference_count': '223',\n",
       "  'references': ['1902027874',\n",
       "   '2798909945',\n",
       "   '2099741732',\n",
       "   '2147152072',\n",
       "   '2013912476',\n",
       "   '2090208105',\n",
       "   '2752853835',\n",
       "   '2072773380',\n",
       "   '2113722075',\n",
       "   '2132267493']},\n",
       " {'id': '1660390307',\n",
       "  'title': 'Modern Information Retrieval',\n",
       "  'abstract': 'From the Publisher:\\r\\nThis is a rigorous and complete textbook for a first course on information retrieval from the computer science (as opposed to a user-centred) perspective. The advent of the Internet and the enormous increase in volume of electronically stored information generally has led to substantial work on IR from the computer science perspective - this book provides an up-to-date student oriented treatment of the subject.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Ricardo A.', 'Berthier'],\n",
       "  'related_topics': ['Human–computer information retrieval',\n",
       "   'Information science',\n",
       "   'Electronically stored information',\n",
       "   'Adversarial information retrieval',\n",
       "   'The Internet',\n",
       "   'Okapi BM25',\n",
       "   'Subject (documents)',\n",
       "   'World Wide Web',\n",
       "   'Multimedia',\n",
       "   'Perspective (graphical)',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,681',\n",
       "  'reference_count': '5',\n",
       "  'references': ['1499900670',\n",
       "   '166263196',\n",
       "   '2107745473',\n",
       "   '2122962290',\n",
       "   '2037959956']},\n",
       " {'id': '2166706824',\n",
       "  'title': 'Thumbs up? Sentiment Classification using Machine Learning Techniques',\n",
       "  'abstract': 'We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Bo Pang', 'Lillian Lee', 'Shivakumar Vaithyanathan'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Multiclass classification',\n",
       "   'Relevance vector machine',\n",
       "   'Structured support vector machine',\n",
       "   'Naive Bayes classifier',\n",
       "   'Support vector machine',\n",
       "   'Categorization',\n",
       "   'Machine learning',\n",
       "   'Principle of maximum entropy',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,502',\n",
       "  'reference_count': '32',\n",
       "  'references': ['3146306708',\n",
       "   '2149684865',\n",
       "   '1576520375',\n",
       "   '2096175520',\n",
       "   '1550206324',\n",
       "   '2140785063',\n",
       "   '2155328222',\n",
       "   '2199803028',\n",
       "   '2160842254',\n",
       "   '1924689489']},\n",
       " {'id': '1992419399',\n",
       "  'title': 'Data clustering: a review',\n",
       "  'abstract': 'Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.',\n",
       "  'date': '1999',\n",
       "  'authors': ['A. K. Jain', 'M. N. Murty', 'P. J. Flynn'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Correlation clustering',\n",
       "   'Single-linkage clustering',\n",
       "   'Conceptual clustering',\n",
       "   'Constrained clustering',\n",
       "   'Consensus clustering',\n",
       "   'Brown clustering',\n",
       "   'Data stream clustering',\n",
       "   'Information retrieval',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '18,116',\n",
       "  'reference_count': '191',\n",
       "  'references': ['2912565176',\n",
       "   '1639032689',\n",
       "   '1497256448',\n",
       "   '2581275558',\n",
       "   '2152150600',\n",
       "   '2049633694',\n",
       "   '2133671888',\n",
       "   '1971784203',\n",
       "   '2095897464',\n",
       "   '2138745909']},\n",
       " {'id': '71795751',\n",
       "  'title': 'Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions',\n",
       "  'abstract': \"We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.\",\n",
       "  'date': '2011',\n",
       "  'authors': ['Richard Socher',\n",
       "   'Jeffrey Pennington',\n",
       "   'Eric H. Huang',\n",
       "   'Andrew Y. Ng',\n",
       "   'Christopher D. Manning'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Machine learning',\n",
       "   'Multinomial distribution',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,477',\n",
       "  'reference_count': '35',\n",
       "  'references': ['1880262756',\n",
       "   '2097726431',\n",
       "   '2117130368',\n",
       "   '2166706824',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '3146306708',\n",
       "   '1423339008',\n",
       "   '2114524997',\n",
       "   '2022204871']},\n",
       " {'id': '2097606805',\n",
       "  'title': 'Accurate Unlexicalized Parsing',\n",
       "  'abstract': 'We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Dan Klein', 'Christopher D. Manning'],\n",
       "  'related_topics': ['Statistical parsing',\n",
       "   'Parsing',\n",
       "   'Treebank',\n",
       "   'Natural language processing',\n",
       "   'Independence (mathematical logic)',\n",
       "   'Grammar',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,786',\n",
       "  'reference_count': '18',\n",
       "  'references': ['1535015163',\n",
       "   '2092654472',\n",
       "   '2110882317',\n",
       "   '1567570606',\n",
       "   '2153439141',\n",
       "   '1551104980',\n",
       "   '2155693943',\n",
       "   '2161204834',\n",
       "   '1859173823',\n",
       "   '2170716495']},\n",
       " {'id': '2103305545',\n",
       "  'title': 'Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection',\n",
       "  'abstract': 'Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Richard Socher',\n",
       "   'Eric H. Huang',\n",
       "   'Jeffrey Pennin',\n",
       "   'Christopher D Manning',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Paraphrase',\n",
       "   'Feature vector',\n",
       "   'Pooling',\n",
       "   'Classifier (UML)',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Matrix (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '979',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2117130368',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '2296073425',\n",
       "   '2097606805',\n",
       "   '2140833774',\n",
       "   '1566018662',\n",
       "   '2095739681']},\n",
       " {'id': '2163455955',\n",
       "  'title': 'Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales',\n",
       "  'abstract': 'We address the rating-inference problem, wherein rather than simply decide whether a review is \"thumbs up\" or \"thumbs down\", as in previous sentiment analysis work, one must determine an author\\'s evaluation with respect to a multi-point scale (e.g., one to five \"stars\"). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, \"three stars\" is intuitively closer to \"four stars\" than to \"one star\".We first evaluate human performance at the task. Then, we apply a meta-algorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier\\'s output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Bo Pang', 'Lillian Lee'],\n",
       "  'related_topics': ['Semantic similarity',\n",
       "   'Class (philosophy)',\n",
       "   'Sentiment analysis',\n",
       "   'Categorization',\n",
       "   'Similarity measure',\n",
       "   'Classifier (linguistics)',\n",
       "   'Similarity (psychology)',\n",
       "   'Metric (mathematics)',\n",
       "   'Support vector machine',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,401',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2156909104',\n",
       "   '2166706824',\n",
       "   '1964357740',\n",
       "   '2143516773',\n",
       "   '3146306708',\n",
       "   '2114524997',\n",
       "   '1576520375',\n",
       "   '2115023510',\n",
       "   '2053463056',\n",
       "   '2155328222']},\n",
       " {'id': '1984052055',\n",
       "  'title': 'Composition in distributional models of semantics.',\n",
       "  'abstract': 'Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Jeff Mitchell', 'Mirella Lapata'],\n",
       "  'related_topics': ['Distributional semantics',\n",
       "   'Semantic similarity',\n",
       "   'Semantics',\n",
       "   'Phrase',\n",
       "   'Similarity (psychology)',\n",
       "   'Priming (psychology)',\n",
       "   'Principle of compositionality',\n",
       "   'Meaning (non-linguistic)',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '994',\n",
       "  'reference_count': '217',\n",
       "  'references': ['2156909104',\n",
       "   '1880262756',\n",
       "   '2038721957',\n",
       "   '2132339004',\n",
       "   '1652505363',\n",
       "   '2147152072',\n",
       "   '1498436455',\n",
       "   '1662133657',\n",
       "   '1587094587',\n",
       "   '1631260214']},\n",
       " {'id': '2151048449',\n",
       "  'title': 'Local and Global Algorithms for Disambiguation to Wikipedia',\n",
       "  'abstract': 'Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call \"global\" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Lev Ratinov', 'Dan Roth', 'Doug Downey', 'Mike Anderson'],\n",
       "  'related_topics': ['Entity linking',\n",
       "   'Online encyclopedia',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word-sense disambiguation',\n",
       "   'View Less'],\n",
       "  'citation_count': '790',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2120779048',\n",
       "   '2100341149',\n",
       "   '86887328',\n",
       "   '2096152098',\n",
       "   '2131357087',\n",
       "   '1548663377',\n",
       "   '2165897980',\n",
       "   '158057341',\n",
       "   '1960027552',\n",
       "   '2123142779']},\n",
       " {'id': '36903255',\n",
       "  'title': 'Hierarchical Probabilistic Neural Network Language Model.',\n",
       "  'abstract': 'In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Frederic Morin', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Hierarchical network model',\n",
       "   'Language model',\n",
       "   'Time delay neural network',\n",
       "   'Hierarchical clustering',\n",
       "   'Probabilistic neural network',\n",
       "   'WordNet',\n",
       "   'Nervous system network models',\n",
       "   'Component (UML)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,118',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2038721957',\n",
       "   '2116064496',\n",
       "   '2132339004',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '1978394996',\n",
       "   '2121227244',\n",
       "   '2127314673']},\n",
       " {'id': '2091812280',\n",
       "  'title': 'Three new graphical models for statistical language modelling',\n",
       "  'abstract': 'The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Andriy Mnih', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Graphical model',\n",
       "   'Word (computer architecture)',\n",
       "   'Probabilistic logic',\n",
       "   'Parametric model',\n",
       "   'Theoretical computer science',\n",
       "   'Sequence',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Large set (Ramsey theory)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '699',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2136922672',\n",
       "   '2116064496',\n",
       "   '2132339004',\n",
       "   '1631260214',\n",
       "   '36903255',\n",
       "   '2158195707',\n",
       "   '2147010501',\n",
       "   '145476170',\n",
       "   '2437096199',\n",
       "   '1558797106']},\n",
       " {'id': '2127314673',\n",
       "  'title': 'DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS',\n",
       "  'abstract': 'We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Fernando Pereira', 'Naftali Tishby', 'Lillian Lee'],\n",
       "  'related_topics': ['Single-linkage clustering',\n",
       "   'Complete-linkage clustering',\n",
       "   'Correlation clustering',\n",
       "   'Brown clustering',\n",
       "   'Cluster analysis',\n",
       "   'k-medians clustering',\n",
       "   'CURE data clustering algorithm',\n",
       "   'Consensus clustering',\n",
       "   'Fuzzy clustering',\n",
       "   'Similarity measure',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,456',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2099111195',\n",
       "   '2049633694',\n",
       "   '3017143921',\n",
       "   '2121227244',\n",
       "   '2099247782',\n",
       "   '2123084125',\n",
       "   '2025887562',\n",
       "   '2059800182',\n",
       "   '2016001305',\n",
       "   '1982944197']},\n",
       " {'id': '2056590938',\n",
       "  'title': 'Connectionist language modeling for large vocabulary continuous speech recognition',\n",
       "  'abstract': 'This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition. Almost all state.. o. f-the-art systems use statistical n-gram language models estimated on text corpora. One principle problem with such language models is the fact that many of the n-grams are never observed even in very large training corpora, and therefore it is common to back-off to a lower-order model. In this paper we propose to address this problem by carrying out the estimation task in a continuous space, enabling a smooth interpolation of the probabilities. A neural network is used to learn the projection of the words onto a continuous space and to estimate the n-gram probabilities. The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Holger Schwenk', 'Jean-Luc Gauvain'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Perplexity',\n",
       "   'Word error rate',\n",
       "   'Text corpus',\n",
       "   'Connectionism',\n",
       "   'Artificial neural network',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '172',\n",
       "  'reference_count': '6',\n",
       "  'references': ['1554663460',\n",
       "   '2132339004',\n",
       "   '2016243284',\n",
       "   '2140679639',\n",
       "   '82490022',\n",
       "   '17500809']},\n",
       " {'id': '2111305191',\n",
       "  'title': 'A bit of progress in language modeling',\n",
       "  'abstract': 'In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser?Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Joshua T. Goodman'],\n",
       "  'related_topics': ['Perplexity',\n",
       "   'Trigram',\n",
       "   'Word error rate',\n",
       "   'Smoothing',\n",
       "   'Language model',\n",
       "   'Mixture model',\n",
       "   'Cluster analysis',\n",
       "   'Entropy (information theory)',\n",
       "   'Statistics',\n",
       "   'Speech recognition',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '631',\n",
       "  'reference_count': '50',\n",
       "  'references': ['2170120409',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '1996764654',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '2127314673',\n",
       "   '2595741664',\n",
       "   '2134237567',\n",
       "   '47415966']},\n",
       " {'id': '1558797106',\n",
       "  'title': 'Quick Training of Probabilistic Neural Nets by Importance Sampling.',\n",
       "  'abstract': '',\n",
       "  'date': '2003',\n",
       "  'authors': ['Yoshua', 'Jean-Sébastien'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Artificial neural network',\n",
       "   'Probabilistic logic',\n",
       "   'Importance sampling',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Training (meteorology)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '205',\n",
       "  'reference_count': '16',\n",
       "  'references': []},\n",
       " {'id': '2963542991',\n",
       "  'title': 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks',\n",
       "  'abstract': 'Abstract: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Pierre Sermanet',\n",
       "   'David Eigen',\n",
       "   'Xiang Zhang',\n",
       "   'Michael Mathieu',\n",
       "   'Rob Fergus',\n",
       "   'Yann LeCun'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Feature (computer vision)',\n",
       "   'Sliding window protocol',\n",
       "   'Pattern recognition',\n",
       "   'Task (project management)',\n",
       "   'Bounding overwatch',\n",
       "   'Object (computer science)',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Scale (ratio)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,460',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2963911037',\n",
       "  'title': 'Network In Network',\n",
       "  'abstract': 'Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Min Lin', 'Qiang Chen', 'Shuicheng Yan'],\n",
       "  'related_topics': ['Multilayer perceptron',\n",
       "   'Activation function',\n",
       "   'Artificial neural network',\n",
       "   'MNIST database',\n",
       "   'Overfitting',\n",
       "   'Feature (machine learning)',\n",
       "   'Linear filter',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,773',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2068730032',\n",
       "  'title': 'Scalable Object Detection Using Deep Neural Networks',\n",
       "  'abstract': 'Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Dumitru Erhan',\n",
       "   'Christian Szegedy',\n",
       "   'Alexander Toshev',\n",
       "   'Dragomir Anguelov'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Object detection',\n",
       "   'Minimum bounding box',\n",
       "   'Convolutional neural network',\n",
       "   '3D single-object recognition',\n",
       "   'Artificial neural network',\n",
       "   'Object (computer science)',\n",
       "   'Context (language use)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,061',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2168356304',\n",
       "   '2031489346',\n",
       "   '2088049833',\n",
       "   '2130306094',\n",
       "   '2129305389',\n",
       "   '2017691720',\n",
       "   '2128715914',\n",
       "   '2122146326']},\n",
       " {'id': '2031489346',\n",
       "  'title': 'The Pascal Visual Object Classes (VOC) Challenge',\n",
       "  'abstract': 'The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.\\r\\n\\r\\nThis paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Mark Everingham',\n",
       "   'Luc Gool',\n",
       "   'Christopher K. Williams',\n",
       "   'John Winn',\n",
       "   'Andrew Zisserman'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pascal (programming language)',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Image processing',\n",
       "   'Annotation',\n",
       "   'Object category recognition',\n",
       "   'Object detector',\n",
       "   'Statistical analysis',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,248',\n",
       "  'reference_count': '57',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '3097096317',\n",
       "   '2162915993',\n",
       "   '2038721957',\n",
       "   '2131846894',\n",
       "   '2110764733',\n",
       "   '2104974755',\n",
       "   '1576445103',\n",
       "   '1565746575']},\n",
       " {'id': '2088049833',\n",
       "  'title': 'Selective Search for Object Recognition',\n",
       "  'abstract': 'This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).',\n",
       "  'date': '2013',\n",
       "  'authors': ['J. R. Uijlings', 'K. E. Sande', 'T. Gevers', 'A. W. Smeulders'],\n",
       "  'related_topics': ['Beam search',\n",
       "   'Brute-force search',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Segmentation',\n",
       "   'Software',\n",
       "   'Pattern recognition',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,484',\n",
       "  'reference_count': '37',\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2164598857',\n",
       "   '2031489346',\n",
       "   '3097096317',\n",
       "   '2162915993',\n",
       "   '2163352848',\n",
       "   '2121947440',\n",
       "   '2110158442']},\n",
       " {'id': '2155541015',\n",
       "  'title': 'DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition',\n",
       "  'abstract': 'We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Jeff Donahue',\n",
       "   'Yangqing Jia',\n",
       "   'Oriol Vinyals',\n",
       "   'Judy Hoffman',\n",
       "   'Ning Zhang',\n",
       "   'Eric Tzeng',\n",
       "   'Trevor Darrell'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Machine learning',\n",
       "   'Set (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Concept learning',\n",
       "   'Computer science',\n",
       "   'Range (mathematics)',\n",
       "   'Variety (cybernetics)',\n",
       "   'Artificial intelligence',\n",
       "   'Visual recognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,369',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2618530766',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '1904365287',\n",
       "   '1677409904',\n",
       "   '2187089797',\n",
       "   '2546302380']},\n",
       " {'id': '1663973292',\n",
       "  'title': 'Pattern Recognition and Machine Learning',\n",
       "  'abstract': 'Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Christopher M. Bishop'],\n",
       "  'related_topics': ['Kernel method',\n",
       "   'Kernel (statistics)',\n",
       "   'Graphical model',\n",
       "   'Variational Bayesian methods',\n",
       "   'Approximate inference',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Linear model',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '52,177',\n",
       "  'reference_count': '2',\n",
       "  'references': ['2117812871', '1496357020']},\n",
       " {'id': '2109255472',\n",
       "  'title': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition',\n",
       "  'abstract': 'Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224  $\\\\times$      224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102   $\\\\times$       faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Contextual image classification',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Pooling',\n",
       "   'Pascal (programming language)',\n",
       "   'Pattern recognition',\n",
       "   'Image resolution',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,651',\n",
       "  'reference_count': '45',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2153635508',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304']},\n",
       " {'id': '753012316',\n",
       "  'title': 'Torch7: A Matlab-like Environment for Machine Learning',\n",
       "  'abstract': 'Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua’s light interface.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Ronan Collobert', 'Koray Kavukcuoglu', 'Clément Farabet'],\n",
       "  'related_topics': ['Scripting language',\n",
       "   'CUDA',\n",
       "   'Interface (Java)',\n",
       "   'MATLAB',\n",
       "   'Software',\n",
       "   'Theano',\n",
       "   'Computer science',\n",
       "   'Flexibility (engineering)',\n",
       "   'Machine learning',\n",
       "   'Implementation',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,761',\n",
       "  'reference_count': '1',\n",
       "  'references': ['2606594511']},\n",
       " {'id': '1825604117',\n",
       "  'title': 'Open-vocabulary Object Retrieval',\n",
       "  'abstract': '',\n",
       "  'date': '2014',\n",
       "  'authors': ['Sergio Guadarrama',\n",
       "   'Erik Rodner',\n",
       "   'Kate Saenko',\n",
       "   'Ning Zhang',\n",
       "   'Ryan Farrell',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Deep-sky object',\n",
       "   'Artificial intelligence',\n",
       "   'Vocabulary Object',\n",
       "   'View Less'],\n",
       "  'citation_count': '81',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2088049833',\n",
       "   '2131846894',\n",
       "   '2128017662',\n",
       "   '2141362318',\n",
       "   '2094728533',\n",
       "   '1889268436',\n",
       "   '1618905105',\n",
       "   '21006490',\n",
       "   '1897761818',\n",
       "   '2066134726']},\n",
       " {'id': '2147414309',\n",
       "  'title': 'PANDA: Pose Aligned Networks for Deep Attribute Modeling',\n",
       "  'abstract': 'We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Ning Zhang',\n",
       "   'Manohar Paluri',\n",
       "   \"Marc'Aurelio Ranzato\",\n",
       "   'Trevor Darrell',\n",
       "   'Lubomir Bourdev'],\n",
       "  'related_topics': ['3D pose estimation',\n",
       "   'Deep learning',\n",
       "   'Context (language use)',\n",
       "   'Minimum bounding box',\n",
       "   'Artificial neural network',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Expression (mathematics)',\n",
       "   'Variation (game tree)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '511',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2618530766',\n",
       "   '2168356304',\n",
       "   '2310919327',\n",
       "   '2155541015',\n",
       "   '2162915993',\n",
       "   '2546302380',\n",
       "   '1498436455',\n",
       "   '2536626143',\n",
       "   '2253807446',\n",
       "   '2098411764']},\n",
       " {'id': '1872489089',\n",
       "  'title': 'Pylearn2: a machine learning research library',\n",
       "  'abstract': \"Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.\",\n",
       "  'date': '2013',\n",
       "  'authors': ['Ian J.',\n",
       "   'David',\n",
       "   'Pascal',\n",
       "   'Vincent',\n",
       "   'Mehdi',\n",
       "   'Razvan',\n",
       "   'James',\n",
       "   'Frédéric',\n",
       "   'Yoshua'],\n",
       "  'related_topics': ['Flexibility (engineering)',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Order (business)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '307',\n",
       "  'reference_count': '44',\n",
       "  'references': ['2618530766',\n",
       "   '2101234009',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2168231600',\n",
       "   '2119821739',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2335728318']},\n",
       " {'id': '2962883796',\n",
       "  'title': 'Recognizing Image Style.',\n",
       "  'abstract': 'The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best – even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Sergey Karayev',\n",
       "   'Matthew Trentacoste',\n",
       "   'Helen',\n",
       "   'Aseem Agarwala',\n",
       "   'Trevor Darrell',\n",
       "   'Aaron Hertzmann',\n",
       "   'Holger Winnemoeller'],\n",
       "  'related_topics': ['Style (sociolinguistics)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Image (mathematics)',\n",
       "   'Painting',\n",
       "   'Artificial intelligence',\n",
       "   'Learning methods',\n",
       "   'Object Class',\n",
       "   'View Less'],\n",
       "  'citation_count': '389',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2618530766',\n",
       "   '2108598243',\n",
       "   '2146502635',\n",
       "   '2155541015',\n",
       "   '1566135517',\n",
       "   '2135957164',\n",
       "   '1511924373',\n",
       "   '2075456404',\n",
       "   '2078807908',\n",
       "   '2157462866']},\n",
       " {'id': '2164598857',\n",
       "  'title': 'Rapid object detection using a boosted cascade of simple features',\n",
       "  'abstract': 'This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.',\n",
       "  'date': '2001',\n",
       "  'authors': ['P. Viola', 'M. Jones'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Object-class detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Haar-like features',\n",
       "   'Face detection',\n",
       "   'Image differencing',\n",
       "   'Cascading classifiers',\n",
       "   'Image processing',\n",
       "   'Feature extraction',\n",
       "   'Contextual image classification',\n",
       "   'Pedestrian detection',\n",
       "   'Facial motion capture',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '23,739',\n",
       "  'reference_count': '18',\n",
       "  'references': ['3124955340',\n",
       "   '2128272608',\n",
       "   '2217896605',\n",
       "   '2115763357',\n",
       "   '1975846642',\n",
       "   '2124351082',\n",
       "   '2159686933',\n",
       "   '2155511848',\n",
       "   '2101522199',\n",
       "   '3146003712']},\n",
       " {'id': '2135046866',\n",
       "  'title': 'Regression Shrinkage and Selection via the Lasso',\n",
       "  'abstract': \"SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.\",\n",
       "  'date': '1996',\n",
       "  'authors': ['Robert Tibshirani'],\n",
       "  'related_topics': ['Lasso (statistics)',\n",
       "   'Elastic net regularization',\n",
       "   'Residual sum of squares',\n",
       "   'Least-angle regression',\n",
       "   'Linear model',\n",
       "   'g-prior',\n",
       "   'Design matrix',\n",
       "   'Shrinkage estimator',\n",
       "   'Applied mathematics',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '40,049',\n",
       "  'reference_count': '19',\n",
       "  'references': ['1995945562',\n",
       "   '1594031697',\n",
       "   '2158940042',\n",
       "   '2797583072',\n",
       "   '2106706098',\n",
       "   '2102201073',\n",
       "   '2117897510',\n",
       "   '191129667',\n",
       "   '2954064014',\n",
       "   '2007069447']},\n",
       " {'id': '2145094598',\n",
       "  'title': 'Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion',\n",
       "  'abstract': 'We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Pascal', 'Hugo', 'Isabelle', 'Yoshua', 'Pierre-Antoine'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Support vector machine',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Noise reduction',\n",
       "   'Bridging (networking)',\n",
       "   'Image (mathematics)',\n",
       "   'Enhanced Data Rates for GSM Evolution',\n",
       "   'Variation (game tree)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,087',\n",
       "  'reference_count': '55',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '1652505363',\n",
       "   '2108384452',\n",
       "   '1479807131',\n",
       "   '1994197834']},\n",
       " {'id': '2131241448',\n",
       "  'title': 'Practical Bayesian Optimization of Machine Learning Algorithms',\n",
       "  'abstract': 'The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\\'s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Jasper Snoek', 'Hugo Larochelle', 'Ryan P Adams'],\n",
       "  'related_topics': ['Weighted Majority Algorithm',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Bayesian optimization',\n",
       "   'Hyperparameter optimization',\n",
       "   'Support vector machine',\n",
       "   'Convolutional neural network',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Hyperparameter',\n",
       "   'Gaussian process',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,039',\n",
       "  'reference_count': '23',\n",
       "  'references': ['3118608800',\n",
       "   '1746819321',\n",
       "   '2141125852',\n",
       "   '2097998348',\n",
       "   '2106411961',\n",
       "   '2951665052',\n",
       "   '60686164',\n",
       "   '2165599843',\n",
       "   '2099201756',\n",
       "   '1973333099']},\n",
       " {'id': '2335728318',\n",
       "  'title': 'Reading Digits in Natural Images with Unsupervised Feature Learning',\n",
       "  'abstract': 'Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Yuval Netzer',\n",
       "   'Tao Wang',\n",
       "   'Adam Coates',\n",
       "   'Alessandro Bissacco',\n",
       "   'Bo Wu',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Reading (process)',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Natural (music)',\n",
       "   'Variety (linguistics)',\n",
       "   'Artificial intelligence',\n",
       "   'Character recognition',\n",
       "   'Research use',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,782',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2161969291',\n",
       "   '2122410182',\n",
       "   '2145094598',\n",
       "   '2147768505',\n",
       "   '2145607950',\n",
       "   '2097018403',\n",
       "   '2118858186',\n",
       "   '2144161366',\n",
       "   '1998042868',\n",
       "   '2132424367']},\n",
       " {'id': '2296319761',\n",
       "  'title': 'Convex Optimization',\n",
       "  'abstract': 'Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Stephen Boyd', 'Lieven Vandenberghe'],\n",
       "  'related_topics': ['Conic optimization',\n",
       "   'Convex optimization',\n",
       "   'Nonlinear programming',\n",
       "   'Engineering optimization',\n",
       "   'Multi-objective optimization',\n",
       "   'Drift plus penalty',\n",
       "   'Semidefinite programming',\n",
       "   'Geometric programming',\n",
       "   'Management science',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '59,058',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2030723843',\n",
       "   '2000296233',\n",
       "   '2020830442',\n",
       "   '2006980285',\n",
       "   '82689443',\n",
       "   '2611147814']},\n",
       " {'id': '3120740533',\n",
       "  'title': 'UCI Machine Learning Repository',\n",
       "  'abstract': '',\n",
       "  'date': '2007',\n",
       "  'authors': ['A.'],\n",
       "  'related_topics': ['Ensembles of classifiers',\n",
       "   'LPBoost',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Associative classifier',\n",
       "   'Concept drifting',\n",
       "   'Ensemble diversity',\n",
       "   'Ensemble selection',\n",
       "   'Instance selection',\n",
       "   'View Less'],\n",
       "  'citation_count': '33,352',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2798766386',\n",
       "  'title': 'Nonlinear Programming',\n",
       "  'abstract': '',\n",
       "  'date': '1995',\n",
       "  'authors': ['Dimitri'],\n",
       "  'related_topics': ['Nonlinear programming',\n",
       "   'Fritz John conditions',\n",
       "   'Computer science',\n",
       "   'Mathematical optimization',\n",
       "   'Random coordinate descent',\n",
       "   'View Less'],\n",
       "  'citation_count': '16,962',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2610857016',\n",
       "  'title': 'Matrix Analysis',\n",
       "  'abstract': 'Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.',\n",
       "  'date': '1985',\n",
       "  'authors': ['Roger A. Horn', 'Charles R. Johnson'],\n",
       "  'related_topics': ['Weyr canonical form',\n",
       "   'Canonical form',\n",
       "   'Matrix (mathematics)',\n",
       "   'Adjugate matrix',\n",
       "   'Commuting matrices',\n",
       "   'Involutory matrix',\n",
       "   'Matrix analysis',\n",
       "   'Hermitian matrix',\n",
       "   'Algebra',\n",
       "   'Pure mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '45,196',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2150102617',\n",
       "  'title': 'RCV1: A New Benchmark Collection for Text Categorization Research',\n",
       "  'abstract': \"Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.\",\n",
       "  'date': '2004',\n",
       "  'authors': ['David D.', 'Yiming', 'Tony G.', 'Fan'],\n",
       "  'related_topics': ['Documentation',\n",
       "   'Supervised learning',\n",
       "   'Information retrieval',\n",
       "   'Coding (social sciences)',\n",
       "   'Computer science',\n",
       "   'Quality control',\n",
       "   'Future studies',\n",
       "   'Original data',\n",
       "   'Text categorization',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,008',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2118020653',\n",
       "   '2149684865',\n",
       "   '2118202495',\n",
       "   '2098162425',\n",
       "   '2435251607',\n",
       "   '2114535528',\n",
       "   '2005422315',\n",
       "   '2107008379',\n",
       "   '2000672666',\n",
       "   '1620204465']},\n",
       " {'id': '2167732364',\n",
       "  'title': 'Smooth minimization of non-smooth functions',\n",
       "  'abstract': 'In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from ** keeping basically the complexity of each iteration unchanged.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Yu Nesterov'],\n",
       "  'related_topics': ['Convex optimization',\n",
       "   'Proximal Gradient Methods',\n",
       "   'Random coordinate descent',\n",
       "   'Smoothing',\n",
       "   'Functional decomposition',\n",
       "   'Minification',\n",
       "   'Mathematical optimization',\n",
       "   'Numerical analysis',\n",
       "   'Mathematics',\n",
       "   'Non smooth',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,029',\n",
       "  'reference_count': '10',\n",
       "  'references': ['3141595720',\n",
       "   '1669104078',\n",
       "   '1568307856',\n",
       "   '2124541940',\n",
       "   '1553702074',\n",
       "   '1568288633',\n",
       "   '2015263936',\n",
       "   '2150126561',\n",
       "   '2969945254',\n",
       "   '2008164266']},\n",
       " {'id': '1992208280',\n",
       "  'title': 'Robust Stochastic Approximation Approach to Stochastic Programming',\n",
       "  'abstract': 'In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.',\n",
       "  'date': '2008',\n",
       "  'authors': ['A. Nemirovski', 'A. Juditsky', 'G. Lan', 'A. Shapiro'],\n",
       "  'related_topics': ['Stochastic optimization',\n",
       "   'Stochastic approximation',\n",
       "   'Stochastic programming',\n",
       "   'Subgradient method',\n",
       "   'Optimization problem',\n",
       "   'Monte Carlo method',\n",
       "   'Mathematical optimization',\n",
       "   'Saddle point',\n",
       "   'Structure (category theory)',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,868',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2038669746',\n",
       "   '2169713291',\n",
       "   '203276351',\n",
       "   '2064076655',\n",
       "   '1983916623',\n",
       "   '2090359754',\n",
       "   '2000257769',\n",
       "   '2086161653',\n",
       "   '1490324987',\n",
       "   '2000953623']},\n",
       " {'id': '2160218441',\n",
       "  'title': 'Online Passive-Aggressive Algorithms',\n",
       "  'abstract': 'We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Koby Crammer',\n",
       "   '',\n",
       "   'Ofer Dekel',\n",
       "   'Joseph Keshet',\n",
       "   'Shai Shalev-Shwartz',\n",
       "   'Yoram Singer'],\n",
       "  'related_topics': ['Margin Infused Relaxed Algorithm',\n",
       "   'Margin (machine learning)',\n",
       "   'Decision problem',\n",
       "   'Lemma (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Series (mathematics)',\n",
       "   'Binary number',\n",
       "   'Categorization',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,130',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2296319761',\n",
       "   '2148603752',\n",
       "   '1563088657',\n",
       "   '1560724230',\n",
       "   '1601740268',\n",
       "   '2053463056',\n",
       "   '2032210760',\n",
       "   '1978394996',\n",
       "   '2101276256',\n",
       "   '2157791002']},\n",
       " {'id': '1978394996',\n",
       "  'title': 'Term Weighting Approaches in Automatic Text Retrieval',\n",
       "  'abstract': 'The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Gerard Salton', 'Christopher Buckley'],\n",
       "  'related_topics': ['Term Discrimination',\n",
       "   'Weighting',\n",
       "   'Term (time)',\n",
       "   'tf–idf',\n",
       "   'Term indexing',\n",
       "   'Automatic indexing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Probability distribution',\n",
       "   'Baseline (configuration management)',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,455',\n",
       "  'reference_count': '48',\n",
       "  'references': ['1956559956',\n",
       "   '2043909051',\n",
       "   '2083605078',\n",
       "   '2068632118',\n",
       "   '3090556797',\n",
       "   '2095396650',\n",
       "   '2075006521',\n",
       "   '11171803',\n",
       "   '3091372544',\n",
       "   '1557757161']},\n",
       " {'id': '2141125852',\n",
       "  'title': 'Multi-column deep neural networks for image classification',\n",
       "  'abstract': 'Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Dan Cireşan', 'Ueli Meier', 'Juergen Schmidhuber'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'MNIST database',\n",
       "   'Traffic sign recognition',\n",
       "   'Contextual image classification',\n",
       "   'Pattern recognition',\n",
       "   'Receptive field',\n",
       "   'Machine learning',\n",
       "   'Visual cortex',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,678',\n",
       "  'reference_count': '39',\n",
       "  'references': ['3118608800',\n",
       "   '2310919327',\n",
       "   '2110798204',\n",
       "   '2154642048',\n",
       "   '2134557905',\n",
       "   '2156163116',\n",
       "   '2138857742',\n",
       "   '2148461049',\n",
       "   '2144982973',\n",
       "   '1991848143']},\n",
       " {'id': '2184045248',\n",
       "  'title': 'Deep Neural Networks for Acoustic Modeling in Speech Recognition',\n",
       "  'abstract': 'Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Geoffrey',\n",
       "   'Li',\n",
       "   'Dong',\n",
       "   'George',\n",
       "   'Abdel-rahman',\n",
       "   'Navdeep',\n",
       "   'Andrew',\n",
       "   'Vincent',\n",
       "   'Patrick',\n",
       "   'Tara',\n",
       "   'Brian'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Hidden Markov model',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Speech recognition',\n",
       "   'Margin (machine learning)',\n",
       "   'Posterior probability',\n",
       "   'Frame (networking)',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,940',\n",
       "  'reference_count': '64',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2116064496',\n",
       "   '2145094598',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '2159080219',\n",
       "   '44815768',\n",
       "   '1498436455']},\n",
       " {'id': '2118858186',\n",
       "  'title': 'An analysis of single-layer networks in unsupervised feature learning',\n",
       "  'abstract': 'A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (“stride”) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance—so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).',\n",
       "  'date': '2011',\n",
       "  'authors': ['Adam Coates', 'Andrew Y. Ng', 'Honglak Lee'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Feature learning',\n",
       "   'Cluster analysis',\n",
       "   'Feature extraction',\n",
       "   'Benchmark (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Hyperparameter',\n",
       "   'Gaussian',\n",
       "   'Computer science',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,577',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2162915993',\n",
       "   '2546302380',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2130325614',\n",
       "   '2097018403',\n",
       "   '2107034620',\n",
       "   '1625255723']},\n",
       " {'id': '3141595720',\n",
       "  'title': 'Introductory Lectures on Convex Optimization: A Basic Course',\n",
       "  'abstract': 'It was in the middle of the 1980s, when the seminal paper by Kar- markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op- timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre- diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc- tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop- ing field, which got the name \"polynomial-time interior-point methods\", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].',\n",
       "  'date': '2014',\n",
       "  'authors': ['I︠u︡. E.'],\n",
       "  'related_topics': ['Nonlinear programming',\n",
       "   'Convex optimization',\n",
       "   'Linear programming',\n",
       "   'Proximal Gradient Methods',\n",
       "   'Random coordinate descent',\n",
       "   'Diction',\n",
       "   'Field (computer science)',\n",
       "   'Calculus',\n",
       "   'Computer science',\n",
       "   'Epoch (reference date)',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,543',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2120420045',\n",
       "  'title': 'No more pesky learning rates',\n",
       "  'abstract': 'The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Tom Schaul', 'Sixin Zhang', 'Yann LeCun'],\n",
       "  'related_topics': ['Online machine learning',\n",
       "   'Stochastic gradient descent',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '402',\n",
       "  'reference_count': '21',\n",
       "  'references': ['3118608800',\n",
       "   '2146502635',\n",
       "   '1533861849',\n",
       "   '2113651538',\n",
       "   '2914484425',\n",
       "   '2156779765',\n",
       "   '2137515395',\n",
       "   '1568229137',\n",
       "   '1598497354',\n",
       "   '2130984546']},\n",
       " {'id': '19621276',\n",
       "  'title': 'Improving the convergence of back-propagation learning with second-order methods',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['S.', 'Yann'],\n",
       "  'related_topics': ['Convergence (routing)',\n",
       "   'Order (business)',\n",
       "   'Backpropagation',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '505',\n",
       "  'reference_count': '2',\n",
       "  'references': ['2160699933', '1526055535']},\n",
       " {'id': '1994616650',\n",
       "  'title': 'A Stochastic Approximation Method',\n",
       "  'abstract': 'Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where a is a given constant. We give a method for making successive experiments at levels x1, x2, ··· in such a way that xn will tend to θ in probability.',\n",
       "  'date': '1951',\n",
       "  'authors': ['Herbert Robbins', 'Sutton Monro'],\n",
       "  'related_topics': ['Minimax approximation algorithm',\n",
       "   'Stochastic approximation',\n",
       "   'Constant (mathematics)',\n",
       "   'Continuous-time stochastic process',\n",
       "   'Approximation error',\n",
       "   'Stochastic optimization',\n",
       "   'Expected value',\n",
       "   'Monotonic function',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,733',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2171928131',\n",
       "  'title': 'Extensions of recurrent neural network language model',\n",
       "  'abstract': 'We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Tomas Mikolov',\n",
       "   'Stefan Kombrink',\n",
       "   'Lukas Burget',\n",
       "   'Jan Cernocky',\n",
       "   'Sanjeev Khudanpur'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Recurrent neural network',\n",
       "   'Language model',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Speedup',\n",
       "   'Computational complexity theory',\n",
       "   'Feed forward',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Probability distribution',\n",
       "   'Computer science',\n",
       "   'Recurrent neural nets',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,404',\n",
       "  'reference_count': '20',\n",
       "  'references': ['179875071',\n",
       "   '2132339004',\n",
       "   '1498436455',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '2613634265',\n",
       "   '36903255',\n",
       "   '2096072088',\n",
       "   '2111305191',\n",
       "   '2152808281']},\n",
       " {'id': '2251222643',\n",
       "  'title': 'Continuous Space Translation Models for Phrase-Based Statistical Machine Translation',\n",
       "  'abstract': 'This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Holger Schwenk'],\n",
       "  'related_topics': ['Phrase',\n",
       "   'Example-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Phrase search',\n",
       "   'Transfer-based machine translation',\n",
       "   'Rule-based machine translation',\n",
       "   'Evaluation of machine translation',\n",
       "   'Machine translation software usability',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'BLEU',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '143',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2132339004',\n",
       "   '2156985047',\n",
       "   '2146574666',\n",
       "   '1970689298',\n",
       "   '2109664771',\n",
       "   '2251098065',\n",
       "   '2143719855',\n",
       "   '2140679639',\n",
       "   '2250379827',\n",
       "   '2103078213']},\n",
       " {'id': '2006969979',\n",
       "  'title': 'The mathematics of statistical machine translation: parameter estimation',\n",
       "  'abstract': 'We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Peter F. Brown',\n",
       "   'Vincent J. Della Pietra',\n",
       "   'Stephen A. Della Pietra',\n",
       "   'Robert L. Mercer'],\n",
       "  'related_topics': ['Hybrid machine translation',\n",
       "   'Example-based machine translation',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Machine translation',\n",
       "   'Interactive machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Computer-assisted translation',\n",
       "   'Machine translation software usability',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,809',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2049633694',\n",
       "   '2121227244',\n",
       "   '2097333193',\n",
       "   '1489181569',\n",
       "   '2117652747',\n",
       "   '2129139611',\n",
       "   '2154384676',\n",
       "   '2138584836',\n",
       "   '1575431606',\n",
       "   '2048390999']},\n",
       " {'id': '196214544',\n",
       "  'title': 'Generating Text with Recurrent Neural Networks',\n",
       "  'abstract': 'Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling – a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Ilya Sutskever', 'James Martens', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Language model',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'State vector',\n",
       "   'Sequence',\n",
       "   'Computer science',\n",
       "   'Character (mathematics)',\n",
       "   'Stochastic matrix',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,439',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2064675550',\n",
       "   '179875071',\n",
       "   '1498436455',\n",
       "   '196761320',\n",
       "   '2131462252',\n",
       "   '2118706537',\n",
       "   '2107878631',\n",
       "   '2110575115',\n",
       "   '1408639475',\n",
       "   '2170942820']},\n",
       " {'id': '2912934387',\n",
       "  'title': 'Bagging predictors',\n",
       "  'abstract': '',\n",
       "  'date': '1996',\n",
       "  'authors': ['Leo'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Bootstrap aggregating',\n",
       "   'Classifier fusion',\n",
       "   'Diversity measure',\n",
       "   'Ensemble diversity',\n",
       "   'Ensemble selection',\n",
       "   'Ensemble systems',\n",
       "   'Multiple classifier',\n",
       "   'Rotation forest',\n",
       "   'View Less'],\n",
       "  'citation_count': '27,423',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '189596042',\n",
       "  'title': 'Deep Boltzmann machines',\n",
       "  'abstract': 'We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'MNIST database',\n",
       "   'Markov chain',\n",
       "   'Hidden variable theory',\n",
       "   'Boltzmann constant',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Inference',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,550',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2134557905',\n",
       "   '2096192494',\n",
       "   '2613634265',\n",
       "   '2116825644',\n",
       "   '2567948266',\n",
       "   '2159737176',\n",
       "   '2124914669']},\n",
       " {'id': '1554663460',\n",
       "  'title': 'Neural networks for pattern recognition',\n",
       "  'abstract': 'From the Publisher:\\r\\nThis is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Christopher M. Bishop'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Feature (machine learning)',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Neural gas',\n",
       "   'Feedforward neural network',\n",
       "   'Rectifier (neural networks)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '33,877',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2143612262',\n",
       "  'title': 'Speech recognition with deep recurrent neural networks',\n",
       "  'abstract': 'Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Alex Graves', 'Abdel-rahman Mohamed', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Deep learning',\n",
       "   'Time delay neural network',\n",
       "   'TIMIT',\n",
       "   'Context (language use)',\n",
       "   'Connectionism',\n",
       "   'Speech recognition',\n",
       "   'Test set',\n",
       "   'Machine learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,525',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2064675550',\n",
       "   '2160815625',\n",
       "   '1993882792',\n",
       "   '2184045248',\n",
       "   '2127141656',\n",
       "   '2144499799',\n",
       "   '2108677974',\n",
       "   '3023071679',\n",
       "   '2155273149',\n",
       "   '1828163288']},\n",
       " {'id': '44815768',\n",
       "  'title': 'A Practical Guide to Training Restricted Boltzmann Machines',\n",
       "  'abstract': 'Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Deep belief network',\n",
       "   'Artificial intelligence',\n",
       "   'Data type',\n",
       "   'Computer science',\n",
       "   'Set (abstract data type)',\n",
       "   'Generative grammar',\n",
       "   'Training (civil)',\n",
       "   'Contrastive divergence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,207',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2136922672',\n",
       "   '1665214252',\n",
       "   '2116064496',\n",
       "   '2099866409',\n",
       "   '2096192494',\n",
       "   '2293063825',\n",
       "   '2029949252',\n",
       "   '2116825644',\n",
       "   '2158164339',\n",
       "   '2124914669']},\n",
       " {'id': '2108677974',\n",
       "  'title': 'Practical Variational Inference for Neural Networks',\n",
       "  'abstract': 'Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Alex Graves'],\n",
       "  'related_topics': ['Stochastic neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Minimum description length',\n",
       "   'Bayesian inference',\n",
       "   'TIMIT',\n",
       "   'Inference',\n",
       "   'Variational method',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '904',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2064675550',\n",
       "   '1498436455',\n",
       "   '2127141656',\n",
       "   '2129652681',\n",
       "   '3161062409',\n",
       "   '2170942820',\n",
       "   '2103359087',\n",
       "   '2114766824',\n",
       "   '2150218618',\n",
       "   '2054658115']},\n",
       " {'id': '2120861206',\n",
       "  'title': 'A fast and simple algorithm for training neural probabilistic language models',\n",
       "  'abstract': 'In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.\\r\\n\\r\\nWe propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well.\\r\\n\\r\\nWe demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Andriy Mnih', 'Yee W. Teh'],\n",
       "  'related_topics': ['Probabilistic logic',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Importance sampling',\n",
       "   'Treebank',\n",
       "   'SIMPLE algorithm',\n",
       "   'Machine learning',\n",
       "   'Noise (video)',\n",
       "   'Normalization (statistics)',\n",
       "   'Computer science',\n",
       "   'Sentence completion tests',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '494',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '1631260214',\n",
       "   '1521626219',\n",
       "   '2131462252',\n",
       "   '2138204974',\n",
       "   '2096175520']},\n",
       " {'id': '3023071679',\n",
       "  'title': 'Framewise phoneme classification with bidirectional LSTM and other neural network architectures',\n",
       "  'abstract': \"In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it'.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['Alex', 'Jürgen'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Perceptron',\n",
       "   'Speech processing',\n",
       "   'Benchmark (computing)',\n",
       "   'Network architecture',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Recurrent neural nets',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,038',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2130942839',\n",
       "  'title': 'Sequence to Sequence Learning with Neural Networks',\n",
       "  'abstract': \"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.\",\n",
       "  'date': '2014',\n",
       "  'authors': ['Ilya Sutskever', 'Oriol Vinyals', 'Quoc V. Le'],\n",
       "  'related_topics': ['Sequence learning',\n",
       "   'Phrase',\n",
       "   'Sentence',\n",
       "   'Artificial neural network',\n",
       "   'Test set',\n",
       "   'Speech recognition',\n",
       "   'Sequence',\n",
       "   'Natural language processing',\n",
       "   'Word order',\n",
       "   'Term (logic)',\n",
       "   'Computer science',\n",
       "   'BLEU',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,925',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2618530766',\n",
       "   '2964308564',\n",
       "   '2157331557',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '2101105183',\n",
       "   '179875071',\n",
       "   '2147768505',\n",
       "   '2132339004',\n",
       "   '1753482797']},\n",
       " {'id': '2395935897',\n",
       "  'title': 'Audio Chord Recognition with Recurrent Neural Networks.',\n",
       "  'abstract': 'In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Nicolas Boulanger-Lewandowski',\n",
       "   'Yoshua Bengio',\n",
       "   'Pascal Vincent'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Chord (music)',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Chord recognition',\n",
       "   'Efficient algorithm',\n",
       "   'View Less'],\n",
       "  'citation_count': '179',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2136922672',\n",
       "   '2072128103',\n",
       "   '2147768505',\n",
       "   '2154642048',\n",
       "   '2184045248',\n",
       "   '2107878631',\n",
       "   '2962968839',\n",
       "   '1828163288',\n",
       "   '2108563286',\n",
       "   '2162911105']},\n",
       " {'id': '1828163288',\n",
       "  'title': 'Sequence Transduction with Recurrent Neural Networks',\n",
       "  'abstract': 'Many machine learning tasks can be expressed as the transformation---or \\\\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\\\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Alex Graves'],\n",
       "  'related_topics': ['Sequence learning',\n",
       "   'Recurrent neural network',\n",
       "   'Transduction (machine learning)',\n",
       "   'TIMIT',\n",
       "   'Speech corpus',\n",
       "   'Probabilistic logic',\n",
       "   'Machine translation',\n",
       "   'Speech recognition',\n",
       "   'Invariant (mathematics)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,022',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2310919327',\n",
       "   '2064675550',\n",
       "   '2147880316',\n",
       "   '179875071',\n",
       "   '2127141656',\n",
       "   '196214544',\n",
       "   '3023071679',\n",
       "   '2131774270',\n",
       "   '2079735306',\n",
       "   '2170942820']},\n",
       " {'id': '1905522558',\n",
       "  'title': 'Domain Adaptation via Pseudo In-Domain Data Selection',\n",
       "  'abstract': 'We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora -- 1% the size of the original -- can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Amittai Axelrod', 'Xiaodong He', 'Jianfeng Gao'],\n",
       "  'related_topics': ['Rule-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Domain (software engineering)',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Task (computing)',\n",
       "   'Computer science',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'Data selection',\n",
       "   'Domain adaptation',\n",
       "   'View Less'],\n",
       "  'citation_count': '533',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2124807415',\n",
       "   '2156985047',\n",
       "   '1631260214',\n",
       "   '2146574666',\n",
       "   '2158195707',\n",
       "   '2117278770',\n",
       "   '2137387514',\n",
       "   '2132001515',\n",
       "   '2130450156',\n",
       "   '2148861208']},\n",
       " {'id': '2341457423',\n",
       "  'title': 'BLEU Deconstructed: Designing a Better MT Evaluation Metric',\n",
       "  'abstract': 'BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative training. Our best metric has better correlation with human judgements than standard BLEU, despite using a simpler formulation. Moreover, these improvements carry over to a system tuned for our new metric.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Xingyi', 'Trevor', 'Lucia'],\n",
       "  'related_topics': ['Evaluation of machine translation',\n",
       "   'Metric (mathematics)',\n",
       "   'BLEU',\n",
       "   'Machine translation',\n",
       "   'De facto standard',\n",
       "   'Discriminative model',\n",
       "   'Natural language processing',\n",
       "   'Sentence',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '20',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2101105183',\n",
       "   '2124807415',\n",
       "   '2146574666',\n",
       "   '2123301721',\n",
       "   '2078861931',\n",
       "   '2087735403',\n",
       "   '222053410',\n",
       "   '1489525520',\n",
       "   '2115081467',\n",
       "   '2895810819']},\n",
       " {'id': '1606347560',\n",
       "  'title': 'Theano: new features and speed improvements',\n",
       "  'abstract': \"Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.\",\n",
       "  'date': '2012',\n",
       "  'authors': ['Frédéric',\n",
       "   'Pascal Lamblin',\n",
       "   'Razvan',\n",
       "   'James',\n",
       "   'Ian J. Goodfellow',\n",
       "   'Arnaud Bergeron',\n",
       "   'Nicolas',\n",
       "   'David Warde-Farley',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Theano',\n",
       "   'Compiler',\n",
       "   'Recurrent neural network',\n",
       "   'Linear algebra',\n",
       "   'Programming language',\n",
       "   'Theoretical computer science',\n",
       "   'Computation',\n",
       "   'Implementation',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,557',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2011301426',\n",
       "   '2154642048',\n",
       "   '2061939373',\n",
       "   '753012316',\n",
       "   '3005347330',\n",
       "   '1408639475',\n",
       "   '2110114082',\n",
       "   '2185726469',\n",
       "   '2254715784',\n",
       "   '2006903949']},\n",
       " {'id': '2171865010',\n",
       "  'title': 'Survey: Reservoir computing approaches to recurrent neural network training',\n",
       "  'abstract': \"Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ''brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ''map'' of it.\",\n",
       "  'date': '2009',\n",
       "  'authors': ['Mantas Lukoševičius', 'Herbert Jaeger'],\n",
       "  'related_topics': ['Reservoir computing',\n",
       "   'Echo state network',\n",
       "   'Recurrent neural network',\n",
       "   'Liquid state machine',\n",
       "   'Field (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Adaptation (computer science)',\n",
       "   'State (computer science)',\n",
       "   'Natural (music)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,853',\n",
       "  'reference_count': '144',\n",
       "  'references': ['2100495367',\n",
       "   '2112090702',\n",
       "   '2008620264',\n",
       "   '2064675550',\n",
       "   '1497256448',\n",
       "   '2154642048',\n",
       "   '2108384452',\n",
       "   '2293063825',\n",
       "   '1659842140',\n",
       "   '2118706537']},\n",
       " {'id': '2122585011',\n",
       "  'title': 'A Novel Connectionist System for Unconstrained Handwriting Recognition',\n",
       "  'abstract': \"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.\",\n",
       "  'date': '2009',\n",
       "  'authors': ['A. Graves',\n",
       "   'M. Liwicki',\n",
       "   'S. Fernandez',\n",
       "   'R. Bertolami',\n",
       "   'H. Bunke',\n",
       "   'J. Schmidhuber'],\n",
       "  'related_topics': ['Intelligent character recognition',\n",
       "   'Handwriting recognition',\n",
       "   'Language model',\n",
       "   'Recurrent neural network',\n",
       "   'Sequence labeling',\n",
       "   'Handwriting',\n",
       "   'Hidden Markov model',\n",
       "   'Word recognition',\n",
       "   'Artificial neural network',\n",
       "   'Cursive',\n",
       "   'Speech recognition',\n",
       "   'Machine learning',\n",
       "   'Image segmentation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,876',\n",
       "  'reference_count': '54',\n",
       "  'references': ['2064675550',\n",
       "   '2125838338',\n",
       "   '2127141656',\n",
       "   '3023071679',\n",
       "   '2107878631',\n",
       "   '2142069714',\n",
       "   '2131774270',\n",
       "   '2079735306',\n",
       "   '1578856370',\n",
       "   '2147568880']},\n",
       " {'id': '2118706537',\n",
       "  'title': 'Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication',\n",
       "  'abstract': 'We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Herbert Jaeger', 'Harald Haas'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Echo state network',\n",
       "   'Reservoir computing',\n",
       "   'Chaotic',\n",
       "   'Benchmark (computing)',\n",
       "   'Wireless',\n",
       "   'Word error rate',\n",
       "   'Energy (signal processing)',\n",
       "   'Computer engineering',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,644',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2103179919',\n",
       "   '2016589492',\n",
       "   '1543237449',\n",
       "   '2166322089',\n",
       "   '2134514463',\n",
       "   '2094631910',\n",
       "   '2058580716',\n",
       "   '2143879519',\n",
       "   '2045182040',\n",
       "   '1943433854']},\n",
       " {'id': '2101105183',\n",
       "  'title': 'Bleu: a Method for Automatic Evaluation of Machine Translation',\n",
       "  'abstract': 'Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Kishore Papineni', 'Salim Roukos', 'Todd Ward', 'Wei-Jing Zhu'],\n",
       "  'related_topics': ['Evaluation of machine translation',\n",
       "   'Interactive machine translation',\n",
       "   'Hybrid machine translation',\n",
       "   'Postediting',\n",
       "   'Example-based machine translation',\n",
       "   'Computer-assisted translation',\n",
       "   'Machine translation software usability',\n",
       "   'Machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Translation memory',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Speech translation',\n",
       "   'Arabic machine translation',\n",
       "   'Text simplification',\n",
       "   'Pivot language',\n",
       "   'Natural language generation',\n",
       "   'Machine learning',\n",
       "   'Closed captioning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'BLEU',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,214',\n",
       "  'reference_count': '3',\n",
       "  'references': ['2001810881', '3037252522', '2732923061']},\n",
       " {'id': '1508165687',\n",
       "  'title': 'Statistical methods for speech recognition',\n",
       "  'abstract': 'The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Frederick Jelinek'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Hidden Markov model',\n",
       "   'Acoustic model',\n",
       "   'Information theory',\n",
       "   'Decision tree',\n",
       "   'Tree (data structure)',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,874',\n",
       "  'reference_count': '1',\n",
       "  'references': ['1980862600']},\n",
       " {'id': '1973923101',\n",
       "  'title': 'Improved statistical alignment models',\n",
       "  'abstract': 'In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Franz Josef Och', 'Hermann Ney'],\n",
       "  'related_topics': ['Viterbi algorithm',\n",
       "   'Machine translation',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Quality (business)',\n",
       "   'Artificial intelligence',\n",
       "   'Statistical alignment',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,319',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2006969979',\n",
       "   '2038698865',\n",
       "   '1979102019',\n",
       "   '1575431606',\n",
       "   '3104029765',\n",
       "   '1811404221',\n",
       "   '2030750105',\n",
       "   '1525706028',\n",
       "   '136130055']},\n",
       " {'id': '1986543644',\n",
       "  'title': 'Three Generative, Lexicalised Models for Statistical Parsing',\n",
       "  'abstract': 'In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).',\n",
       "  'date': '1997',\n",
       "  'authors': ['Michael Collins'],\n",
       "  'related_topics': ['Parser combinator',\n",
       "   'Top-down parsing',\n",
       "   'Statistical parsing',\n",
       "   'Parsing',\n",
       "   'Data-oriented parsing',\n",
       "   'Generative model',\n",
       "   'Generative grammar',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,111',\n",
       "  'reference_count': '15',\n",
       "  'references': ['1632114991',\n",
       "   '1773803948',\n",
       "   '2110882317',\n",
       "   '2153439141',\n",
       "   '2052449326',\n",
       "   '2093647425',\n",
       "   '1972573551',\n",
       "   '2087165009',\n",
       "   '2069912724',\n",
       "   '2162455891']},\n",
       " {'id': '2116316001',\n",
       "  'title': 'A Syntax-based Statistical Translation Model',\n",
       "  'abstract': 'We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Kenji Yamada', 'Kevin Knight'],\n",
       "  'related_topics': ['Parse tree',\n",
       "   'Word error rate',\n",
       "   'Word (computer architecture)',\n",
       "   'String (computer science)',\n",
       "   'Syntax (programming languages)',\n",
       "   'Syntax',\n",
       "   'Time complexity',\n",
       "   'Word order',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,046',\n",
       "  'reference_count': '315',\n",
       "  'references': ['2101105183',\n",
       "   '2122410182',\n",
       "   '1574901103',\n",
       "   '2049633694',\n",
       "   '2006969979',\n",
       "   '1916559533',\n",
       "   '2092654472',\n",
       "   '2135843243',\n",
       "   '2117400858',\n",
       "   '2101210369']},\n",
       " {'id': '1517947178',\n",
       "  'title': 'Improved Alignment Models for Statistical Machine Translation',\n",
       "  'abstract': '',\n",
       "  'date': '1999',\n",
       "  'authors': ['Franz Josef', 'Christoph', 'Hermann'],\n",
       "  'related_topics': ['Interactive machine translation',\n",
       "   'Machine translation',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '878',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2006969979',\n",
       "   '2038698865',\n",
       "   '2107551411',\n",
       "   '2113106066',\n",
       "   '2196555355',\n",
       "   '2294072136',\n",
       "   '2158164089']},\n",
       " {'id': '2161792612',\n",
       "  'title': 'A Phrase-Based,Joint Probability Model for Statistical Machine Translation',\n",
       "  'abstract': 'We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Daniel Marcu', 'Daniel Wong'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Phrase',\n",
       "   'Word (computer architecture)',\n",
       "   'Joint probability distribution',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Joint (audio engineering)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '632',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2049633694',\n",
       "   '2006969979',\n",
       "   '1916559533',\n",
       "   '2001810881',\n",
       "   '2116316001',\n",
       "   '1517947178',\n",
       "   '2129765547',\n",
       "   '1549285799',\n",
       "   '2139403546',\n",
       "   '133045130']},\n",
       " {'id': '1549285799',\n",
       "  'title': 'Statistical Language Modeling using the CMU-Cambridge Toolkit',\n",
       "  'abstract': '',\n",
       "  'date': '1997',\n",
       "  'authors': ['Philip', 'Ronald'],\n",
       "  'related_topics': ['Modeling language',\n",
       "   'Language model',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '972',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2158388102',\n",
       "  'title': 'Stochastic inversion transduction grammars and bilingual parsing of parallel corpora',\n",
       "  'abstract': \"We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.\",\n",
       "  'date': '1997',\n",
       "  'authors': ['Dekai Wu'],\n",
       "  'related_topics': ['S-attributed grammar',\n",
       "   'Parsing',\n",
       "   'Language model',\n",
       "   'Computational linguistics',\n",
       "   'Rule-based machine translation',\n",
       "   'Probabilistic logic',\n",
       "   'Transduction (machine learning)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Segmentation',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,119',\n",
       "  'reference_count': '48',\n",
       "  'references': ['2006969979',\n",
       "   '2097333193',\n",
       "   '1489181569',\n",
       "   '2117652747',\n",
       "   '1513168562',\n",
       "   '1991133427',\n",
       "   '2439178139',\n",
       "   '2138584836',\n",
       "   '201288405',\n",
       "   '2110190189']},\n",
       " {'id': '2096733369',\n",
       "  'title': 'FaceNet: A unified embedding for face recognition and clustering',\n",
       "  'abstract': 'Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Florian Schroff', 'Dmitry Kalenichenko', 'James Philbin'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Face detection',\n",
       "   'Object-class detection',\n",
       "   'Facial recognition system',\n",
       "   'Cluster analysis',\n",
       "   'Feature vector',\n",
       "   'Face (geometry)',\n",
       "   'Similarity (geometry)',\n",
       "   'Embedding',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,601',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2097117768',\n",
       "   '1849277567',\n",
       "   '2146502635',\n",
       "   '2963911037',\n",
       "   '2168231600',\n",
       "   '2145287260',\n",
       "   '1782590233',\n",
       "   '2294059674',\n",
       "   '1498436455',\n",
       "   '2296073425']},\n",
       " {'id': '2016053056',\n",
       "  'title': 'Large-Scale Video Classification with Convolutional Neural Networks',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).',\n",
       "  'date': '2014',\n",
       "  'authors': ['Andrej Karpathy',\n",
       "   'George Toderici',\n",
       "   'Sanketh Shetty',\n",
       "   'Thomas Leung',\n",
       "   'Rahul Sukthankar',\n",
       "   'Li Fei-Fei'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Feature (machine learning)',\n",
       "   'Feature extraction',\n",
       "   'Machine learning',\n",
       "   'Generalization',\n",
       "   'Class (biology)',\n",
       "   'Computer science',\n",
       "   'Scale (map)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,760',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '2168231600',\n",
       "   '2022508996',\n",
       "   '2131846894',\n",
       "   '2062118960']},\n",
       " {'id': '2097726431',\n",
       "  'title': 'Opinion Mining and Sentiment Analysis',\n",
       "  'abstract': 'An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.\\r\\n\\r\\nThis survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Bo Pang', 'Lillian Lee'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Automatic summarization',\n",
       "   'Information technology',\n",
       "   'Popularity',\n",
       "   'Data science',\n",
       "   'Economic impact analysis',\n",
       "   'Information extraction',\n",
       "   'Object (philosophy)',\n",
       "   'Subjectivity',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,811',\n",
       "  'reference_count': '314',\n",
       "  'references': ['1880262756',\n",
       "   '3013264884',\n",
       "   '2147880316',\n",
       "   '2038721957',\n",
       "   '2138621811',\n",
       "   '2166706824',\n",
       "   '2160660844',\n",
       "   '2118020653',\n",
       "   '3146306708',\n",
       "   '2114524997']},\n",
       " {'id': '2581275558',\n",
       "  'title': 'Optimization by simulated annealing',\n",
       "  'abstract': 'There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.',\n",
       "  'date': '1987',\n",
       "  'authors': ['S. Kirkpatrick', 'C. D. Gelatt', 'M. P. Vecchi'],\n",
       "  'related_topics': ['Optimization problem',\n",
       "   'Combinatorial optimization',\n",
       "   'Simulated annealing',\n",
       "   'Degrees of freedom (physics and chemistry)',\n",
       "   'Statistical mechanics',\n",
       "   'Complex system',\n",
       "   'Function (mathematics)',\n",
       "   'Connection (mathematics)',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '51,868',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2042986967',\n",
       "   '2022820481',\n",
       "   '2114552889',\n",
       "   '2022494241',\n",
       "   '2143037347',\n",
       "   '2056760934',\n",
       "   '2148673189',\n",
       "   '86906884',\n",
       "   '2014952973',\n",
       "   '2014068360']},\n",
       " {'id': '2016589492',\n",
       "  'title': 'A learning algorithm for continually running fully recurrent neural networks',\n",
       "  'abstract': 'The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Ronald J. Williams', 'David Zipser'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Recurrent neural network',\n",
       "   'Backpropagation through time',\n",
       "   'Artificial neural network',\n",
       "   'Supervised learning',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Interval (mathematics)',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Basis (linear algebra)',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,375',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2154642048',\n",
       "   '2293063825',\n",
       "   '2110485445',\n",
       "   '2143503258',\n",
       "   '1881179843',\n",
       "   '1959983357',\n",
       "   '1984205520',\n",
       "   '1984375561',\n",
       "   '2119796132',\n",
       "   '1527772862']},\n",
       " {'id': '2088978850',\n",
       "  'title': 'Minimizing multimodal functions of continuous variables with the “simulated annealing” algorithm—Corrigenda for this article is available here',\n",
       "  'abstract': 'A new global optimization algorithm for functions of continuous variables is presented, derived from the “Simulated Annealing” algorithm recently introduced in combinatorial optimization.The algorithm is essentially an iterative random search procedure with adaptive moves along the coordinate directions. It permits uphill moves under the control of a probabilistic criterion, thus tending to avoid the first local minima encountered.The algorithm has been tested against the Nelder and Mead simplex method and against a version of Adaptive Random Search. The test functions were Rosenbrock valleys and multiminima functions in 2,4, and 10 dimensions.The new method proved to be more reliable than the others, being always able to find the optimum, or at least a point very close to it. It is quite costly in term of function evaluations, but its cost can be predicted in advance, depending only slightly on the starting point.',\n",
       "  'date': '1987',\n",
       "  'authors': ['A. Corana', 'M. Marchesi', 'C. Martini', 'S. Ridella'],\n",
       "  'related_topics': ['Adaptive simulated annealing',\n",
       "   'Simulated annealing',\n",
       "   'Hill climbing',\n",
       "   'Simplex algorithm',\n",
       "   'Random search',\n",
       "   'Maxima and minima',\n",
       "   'Function (mathematics)',\n",
       "   'Probabilistic logic',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,063',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2581275558',\n",
       "   '2171074980',\n",
       "   '2022772618',\n",
       "   '2056760934',\n",
       "   '2026258334',\n",
       "   '2012231377',\n",
       "   '2152710595',\n",
       "   '2029673686',\n",
       "   '1652464192',\n",
       "   '2065606540']},\n",
       " {'id': '2148099973',\n",
       "  'title': 'Global optimization of a neural network-hidden Markov model hybrid',\n",
       "  'abstract': 'The integration of multilayered and recurrent artificial neural networks (ANNs) with hidden Markov models (HMMs) is addressed. ANNs are suitable for approximating functions that compute new acoustic parameters, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported. >',\n",
       "  'date': '1992',\n",
       "  'authors': ['Y. Bengio', 'R. De Mori', 'G. Flammia', 'R. Kompe'],\n",
       "  'related_topics': ['Hidden Markov model',\n",
       "   'Markov model',\n",
       "   'Artificial neural network',\n",
       "   'TIMIT',\n",
       "   'Markov process',\n",
       "   'Global optimization',\n",
       "   'Estimation theory',\n",
       "   'Pattern recognition',\n",
       "   'Sequence',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '349',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2125838338',\n",
       "   '2154642048',\n",
       "   '169539560',\n",
       "   '2077804127',\n",
       "   '2135622428',\n",
       "   '2086699924',\n",
       "   '2140539590',\n",
       "   '2169415433',\n",
       "   '2140766383',\n",
       "   '2125610452']},\n",
       " {'id': '1996741810',\n",
       "  'title': 'Local feedback multilayered networks',\n",
       "  'abstract': 'In this paper, we investigate the capabilities of local feedback multilayered networks, a particular class of recurrent networks, in which feedback connections are only allowed from neurons to themselves. In this class, learning can be accomplished by an algorithm that is local in both space and time. We describe the limits and properties of these networks and give some insights on their use for solving practical problems.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Paolo Frasconi', 'Marco Gori', 'Giovanni Soda'],\n",
       "  'related_topics': ['Class (computer programming)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Spacetime',\n",
       "   'View Less'],\n",
       "  'citation_count': '273',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2110485445',\n",
       "   '2016589492',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '2121553911',\n",
       "   '2140539590',\n",
       "   '2133656308',\n",
       "   '2047515372',\n",
       "   '2032164462',\n",
       "   '2094096029']},\n",
       " {'id': '2125329357',\n",
       "  'title': 'Using random weights to train multilayer networks of hard-limiting units',\n",
       "  'abstract': 'A gradient descent algorithm suitable for training multilayer feedforward networks of processing units with hard-limiting output functions is presented. The conventional backpropagation algorithm cannot be applied in this case because the required derivatives are not available. However, if the network weights are random variables with smooth distribution functions, the probability of a hard-limiting unit taking one of its two possible values is a continuously differentiable function. In the paper, this is used to develop an algorithm similar to backpropagation, but for the hard-limiting case. It is shown that the computational framework of this algorithm is similar to standard backpropagation, but there is an additional computational expense involved in the estimation of gradients. Upper bounds on this estimation penalty are given. Two examples which indicate that, when this algorithm is used to train networks of hard-limiting units, its performance is similar to that of conventional backpropagation applied to networks of units with sigmoidal characteristics are presented. >',\n",
       "  'date': '1992',\n",
       "  'authors': ['P.L. Barlett', 'T. Downs'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Gradient descent',\n",
       "   'Artificial neural network',\n",
       "   'Random variable',\n",
       "   'Sigmoid function',\n",
       "   'Function (mathematics)',\n",
       "   'Feed forward',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '55',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2154642048',\n",
       "   '2165758113',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '2176028050',\n",
       "   '2010029425',\n",
       "   '2084544490',\n",
       "   '2181111061',\n",
       "   '1529808766',\n",
       "   '2003357516']},\n",
       " {'id': '1527772862',\n",
       "  'title': 'A focused backpropagation algorithm for temporal pattern recognition',\n",
       "  'abstract': '',\n",
       "  'date': '1995',\n",
       "  'authors': ['Michael C.'],\n",
       "  'related_topics': ['Pattern recognition (psychology)',\n",
       "   'Backpropagation',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '339',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1959983357',\n",
       "  'title': 'Attractor dynamics and parallelism in a connectionist sequential machine',\n",
       "  'abstract': '',\n",
       "  'date': '1990',\n",
       "  'authors': ['Michael I.'],\n",
       "  'related_topics': ['Parallelism (grammar)',\n",
       "   'Attractor',\n",
       "   'Connectionism',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Dynamics (music)',\n",
       "   'Sequential machine',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,521',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2028629011',\n",
       "  'title': 'Learning state space trajectories in recurrent neural networks',\n",
       "  'abstract': 'A number of procedures are described for finding delta E/ delta W/sub ij/ where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and w/sub ij/ are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E, so these procedures form the kernels of connectionist learning algorithms. Simulations in which networks are taught to move through limit cycles are shown, along with some empirical perturbation sensitivity tests. The author describes a number of elaborations of the basic idea, including mutable time delays and teacher forcing. He includes a complexity analysis of the various learning procedures discussed and analyzed. Temporally continuous recurrent networks seems particularly suited for temporally continuous domains, such as signal processing, control, and speech. >',\n",
       "  'date': '1989',\n",
       "  'authors': ['Pearlmutter'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'State space',\n",
       "   'Gradient descent',\n",
       "   'Connectionism',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '251',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '1597286183',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '1971129545',\n",
       "   '1959983357',\n",
       "   '3121926921']},\n",
       " {'id': '2053127376',\n",
       "  'title': 'On the time relations of mental processes: An examination of systems of processes in cascade.',\n",
       "  'abstract': '',\n",
       "  'date': '1979',\n",
       "  'authors': ['James L. McClelland'],\n",
       "  'related_topics': ['Cognition',\n",
       "   'Perception',\n",
       "   'Cognitive science',\n",
       "   'Cognitive psychology',\n",
       "   'Computer science',\n",
       "   'Cascade',\n",
       "   'Abstract reasoning',\n",
       "   'Research methodology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,812',\n",
       "  'reference_count': '50',\n",
       "  'references': ['1784695092',\n",
       "   '2135255848',\n",
       "   '2098205603',\n",
       "   '2045597501',\n",
       "   '2094493170',\n",
       "   '1502139053',\n",
       "   '1967670055',\n",
       "   '2106654511',\n",
       "   '2147311265',\n",
       "   '1529340823']},\n",
       " {'id': '2167607759',\n",
       "  'title': 'The \"Moving Targets\" Training Algorithm',\n",
       "  'abstract': 'A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Richard Rohwer'],\n",
       "  'related_topics': ['State space',\n",
       "   'Artificial neural network',\n",
       "   'Error function',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Algorithm',\n",
       "   'Signal',\n",
       "   'Training (meteorology)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '63',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2154642048',\n",
       "   '3004157836',\n",
       "   '2077658674',\n",
       "   '2016589492',\n",
       "   '2796837256',\n",
       "   '2143503258',\n",
       "   '1984205520',\n",
       "   '1984375561',\n",
       "   '2325850497',\n",
       "   '2028629011']},\n",
       " {'id': '2177721432',\n",
       "  'title': 'Neurons with graded response have collective computational properties like those of two-state neurons',\n",
       "  'abstract': 'A model for a large network of \"neurons\" with a graded response (or sigmoid input--output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch--Pitts neurons. The content-addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.',\n",
       "  'date': '1988',\n",
       "  'authors': ['J. J.'],\n",
       "  'related_topics': ['Deterministic system',\n",
       "   'Electrical network',\n",
       "   'Sigmoid function',\n",
       "   'Stochastic modelling',\n",
       "   'Connection (algebraic framework)',\n",
       "   'Function (mathematics)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Action (physics)',\n",
       "   'Statistical physics',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,635',\n",
       "  'reference_count': '2',\n",
       "  'references': ['1973108021', '582196039']},\n",
       " {'id': '2075510082',\n",
       "  'title': 'Characteristics of Random Nets of Analog Neuron-Like Elements',\n",
       "  'abstract': 'The dynamic behavior of randomly connected analog neuron-like elements that process pulse-frequency modulated signals is investigated from the macroscopic point of view. By extracting two statistical parameters, the macroscopic state equations are derived in terms of these parameters under some hypotheses on the stochastics of microscopic states. It is shown that a random net of statistically symmetric structure is monostable or bistable, and the stability criteria are explicitly given. Random nets consisting of many different classes of elements are also analyzed. Special attention is paid to nets of randomly connected excitatory and inhibitory elements. It is shown that a stable oscillation exists in such a net?in contrast with the fact that no stable oscillations exist in a net of statistically symmetric structure even if negative as well as positive synaptic weights are permitted at a time. The results are checked by computer-simulated experiments.',\n",
       "  'date': '1972',\n",
       "  'authors': ['Shun-Ichi'],\n",
       "  'related_topics': ['Stochastic process',\n",
       "   'Stability (probability)',\n",
       "   'Bistability',\n",
       "   'Statistical parameter',\n",
       "   'Oscillation',\n",
       "   'Net (mathematics)',\n",
       "   'State (functional analysis)',\n",
       "   'Statistical physics',\n",
       "   'Contrast (statistics)',\n",
       "   'Control theory',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '390',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2159116087',\n",
       "   '2060666586',\n",
       "   '1968014724',\n",
       "   '1995030605',\n",
       "   '2159187100',\n",
       "   '2072972096',\n",
       "   '2082628174']},\n",
       " {'id': '2138484437',\n",
       "  'title': 'Identification and control of dynamical systems using neural networks',\n",
       "  'abstract': 'It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis is on models for both identification and control. Static and dynamic backpropagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations, and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout, and theoretical questions that have to be addressed are also described. >',\n",
       "  'date': '1990',\n",
       "  'authors': ['K.S. Narendra', 'K. Parthasarathy'],\n",
       "  'related_topics': ['Nonlinear system identification',\n",
       "   'Adaptive control',\n",
       "   'Dynamical systems theory',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Identification (information)',\n",
       "   'Control system',\n",
       "   'Linear system',\n",
       "   'Nonlinear system',\n",
       "   'Control engineering',\n",
       "   'Control theory',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,733',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2137983211',\n",
       "   '2293063825',\n",
       "   '3146803896',\n",
       "   '1597286183',\n",
       "   '1572161815',\n",
       "   '2174984063',\n",
       "   '3036751298',\n",
       "   '2122136962',\n",
       "   '2095425517',\n",
       "   '2007431958']},\n",
       " {'id': '2150355110',\n",
       "  'title': 'Backpropagation through time: what it does and how to do it',\n",
       "  'abstract': 'Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users. >',\n",
       "  'date': '1990',\n",
       "  'authors': ['P.J. Werbos'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Backpropagation',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Deep learning',\n",
       "   'Time delay neural network',\n",
       "   'Nonlinear system identification',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,317',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2154642048',\n",
       "   '2068484625',\n",
       "   '2143503258',\n",
       "   '3148194443',\n",
       "   '1971129545',\n",
       "   '1881179843',\n",
       "   '3121926921',\n",
       "   '2090248140',\n",
       "   '1487148666',\n",
       "   '2028629011']},\n",
       " {'id': '1583833196',\n",
       "  'title': 'Neuronlike adaptive elements that can solve difficult learning control problems',\n",
       "  'abstract': \"It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.\",\n",
       "  'date': '1990',\n",
       "  'authors': ['Andrew G.', 'Richard S.', 'Charles W.'],\n",
       "  'related_topics': ['Evaluation function',\n",
       "   'Relation (database)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Control (management)',\n",
       "   'Element (category theory)',\n",
       "   'Reinforcement',\n",
       "   'Base (topology)',\n",
       "   'Operant conditioning',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,287',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2143787696',\n",
       "  'title': 'Gradient methods for the optimization of dynamical systems containing neural networks',\n",
       "  'abstract': 'An extension of the backpropagation method, termed dynamic backpropagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed. The method is based on the fact that gradient methods used in linear dynamical systems can be combined with backpropagation methods for neural networks to obtain the gradient of a performance index of nonlinear dynamical systems. The method can be applied to any complex system which can be expressed as the interconnection of linear dynamical systems and multilayer neural networks. To facilitate the practical implementation of the proposed method, emphasis is placed on the diagrammatic representation of the system which generates the gradient of the performance function. >',\n",
       "  'date': '1991',\n",
       "  'authors': ['K.S. Narendra', 'K. Parthasarathy'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Linear dynamical system',\n",
       "   'Gradient method',\n",
       "   'Artificial neural network',\n",
       "   'Dynamical systems theory',\n",
       "   'Linear system',\n",
       "   'Nonlinear system',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '863',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2138484437',\n",
       "   '2016589492',\n",
       "   '2150355110',\n",
       "   '2076086013',\n",
       "   '2016261381',\n",
       "   '2062870975']},\n",
       " {'id': '2057653135',\n",
       "  'title': 'An efficient gradient-based algorithm for on-line training of recurrent network trajectories',\n",
       "  'abstract': 'A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Ronald J. Williams', 'Jing Peng'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Reset (computing)',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Line (geometry)',\n",
       "   'Computer science',\n",
       "   'Training (meteorology)',\n",
       "   'Gradient based algorithm',\n",
       "   'View Less'],\n",
       "  'citation_count': '731',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2154642048',\n",
       "   '2016589492',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '1881179843',\n",
       "   '2121553911',\n",
       "   '1984205520',\n",
       "   '2139273175',\n",
       "   '2047515372',\n",
       "   '2044422789']},\n",
       " {'id': '2132152975',\n",
       "  'title': 'Decoupled extended Kalman filter training of feedforward layered networks',\n",
       "  'abstract': 'Presents a training algorithm for feedforward layered networks based on a decoupled extended Kalman filter (DEKF). The authors present an artificial process noise extension to DEKF that increases its convergence rate and assists in the avoidance of local minima. Computationally efficient formulations for two particularly natural and useful cases of DEKF are given. Through a series of pattern classification and function approximation experiments, three members of DEKF are compared with one another and with standard backpropagation (SBP). These studies demonstrate that the judicious grouping of weights along with the use of artificial process noise in DEKF result in input-output mapping performance that is comparable to the global extended Kalman algorithm, and is often superior to SBP, while requiring significantly fewer presentations of training data than SBP and less overall training time than either of these procedures. >',\n",
       "  'date': '1991',\n",
       "  'authors': ['G.V. Puskorius', 'L.A. Feldkamp'],\n",
       "  'related_topics': ['Extended Kalman filter',\n",
       "   'Kalman filter',\n",
       "   'Backpropagation',\n",
       "   'Artificial neural network',\n",
       "   'Function approximation',\n",
       "   'Rate of convergence',\n",
       "   'Maxima and minima',\n",
       "   'Feed forward',\n",
       "   'Control theory',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '314',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2138484437',\n",
       "   '3147404844',\n",
       "   '2112462566',\n",
       "   '1761621746',\n",
       "   '2122568838',\n",
       "   '2077493113',\n",
       "   '107400462',\n",
       "   '2102496649']},\n",
       " {'id': '2112462566',\n",
       "  'title': 'Training Multilayer Perceptrons with the Extended Kalman Algorithm',\n",
       "  'abstract': 'A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Sharad Singhal', 'Lance Wu'],\n",
       "  'related_topics': ['Perceptron',\n",
       "   'Artificial neural network',\n",
       "   'Parameter identification problem',\n",
       "   'Nonlinear system',\n",
       "   'Artificial intelligence',\n",
       "   'Fraction (mathematics)',\n",
       "   'Computer science',\n",
       "   'Training (meteorology)',\n",
       "   'Kalman algorithm',\n",
       "   'View Less'],\n",
       "  'citation_count': '496',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2154642048',\n",
       "   '2173629880',\n",
       "   '304861154',\n",
       "   '2293807537',\n",
       "   '2105934661',\n",
       "   '1613359937',\n",
       "   '2051992922']},\n",
       " {'id': '2090248140',\n",
       "  'title': 'Generic constraints on underspecified target trajectories',\n",
       "  'abstract': 'Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning. >',\n",
       "  'date': '1989',\n",
       "  'authors': ['Jordan'],\n",
       "  'related_topics': ['Learning rule',\n",
       "   'Instance-based learning',\n",
       "   'Learning classifier system',\n",
       "   'Stability (learning theory)',\n",
       "   'Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Leabra',\n",
       "   'Online machine learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Competitive learning',\n",
       "   'Inductive transfer',\n",
       "   'Multi-task learning',\n",
       "   'Computational learning theory',\n",
       "   'Robot learning',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Motor learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '223',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2895674046',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '1967377907',\n",
       "   '1885639605',\n",
       "   '1981297107',\n",
       "   '2150367199',\n",
       "   '1892385946',\n",
       "   '1969166509',\n",
       "   '2046329526']},\n",
       " {'id': '1529008516',\n",
       "  'title': 'Supervised learning and systems with excess degrees of freedom',\n",
       "  'abstract': 'WHEN DISTINCT OUTPUTS OF AN ADAPTIVE SYSTEM HAVE EQUIVALENT EFFECTS ON THE ENVIRONMENT, THE PROBLEM OF FINDING APPROPRIATE ACTIONS GIVEN DESIRED RESULTS IS ILL-POSED. FOR SUPERVISED LEARNING ALGORITHMS, THE ILL-POSEDNESS OF SUCH \"INVERSE LEARNING PROBLEMS\" IMPLIES A CERTAIN FLEXIBILITY---DURING TRAINING, THERE ARE IN GENERAL MANY POSSIBLE TARGET VECTORS CORRESPONDING TO EACH INPUT VECTOR. TO ALLOW SUPERVISED LEARNING ALGORITHMS TO MAKE USE OF THIS FLEXIBILITY, THE CURRENT PAPER CONSIDERS HOW TO SPECIFY TARGETS BY SETS OF CONSTRAINTS, RATHER THAN AS PARTICULAR VECTORS. TWO CLASSES OF CONSTRAINTS ARE DISTINGUISHED---`CONFIGURATIONAL\\'\\' CONSTRAINTS, WHICH DEFINE REGIONS OF OUTPUT SPACE IN WHICH AN OUTPUT VECTOR MUST LIE, AND `TEMPORAL\\'\\' CONSTRAINTS, WHICH DEFINE RELATIONSHIPS BETWEEN OUTPUTS PRODUCED AT DIFFER- ENT POINTS IN TIME. LEARNING ALGORITHMS MINIMIZE A COST FUNCTION THAT CON- TAINS TERMS FOR BOTH KINDS OF CONSTRAINTS. THIS APPROACH TO INVERSE LEARN- ING IS ILLUSTRATED BY A ROBOTICS APPLICATION IN WHICH A NETWORK FINDS TRA- JECTORIES OF INVERSE KINEMATIC SOLUTIONS FOR MANIPULATORS WITH EXCESS DEGREES OF FREEDOM.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Michael I.'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Supervised learning',\n",
       "   'Degrees of freedom',\n",
       "   'Function (mathematics)',\n",
       "   'Adaptive system',\n",
       "   'Inverse',\n",
       "   'Kinematics',\n",
       "   'Robotics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '239',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1597286183',\n",
       "  'title': 'Neural computation of decisions in optimization problems',\n",
       "  'abstract': 'Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.',\n",
       "  'date': '1985',\n",
       "  'authors': ['J. J. Hopfield', 'D. W. Tank'],\n",
       "  'related_topics': ['Optimization problem',\n",
       "   'Models of neural computation',\n",
       "   'Hopfield network',\n",
       "   'Computation',\n",
       "   'Complex system',\n",
       "   'Cybernetics',\n",
       "   'Nonlinear system',\n",
       "   'Mathematical optimization',\n",
       "   'Decision theory',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,013',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2581275558',\n",
       "   '2011039300',\n",
       "   '2293063825',\n",
       "   '2177721432',\n",
       "   '1666015432',\n",
       "   '307896644',\n",
       "   '2042986967',\n",
       "   '2112325651',\n",
       "   '2032533296',\n",
       "   '1543738661']},\n",
       " {'id': '1507849272',\n",
       "  'title': 'A learning algorithm for Boltzmann machines',\n",
       "  'abstract': 'The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.',\n",
       "  'date': '1988',\n",
       "  'authors': ['David H. Ackley',\n",
       "   'Geoffrey E. Hinton',\n",
       "   'Terrence J. Sejnowski'],\n",
       "  'related_topics': ['Massively parallel',\n",
       "   'Learning rule',\n",
       "   'Constraint satisfaction',\n",
       "   'Domain (software engineering)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Encoding (memory)',\n",
       "   'Connection (vector bundle)',\n",
       "   'Theoretical computer science',\n",
       "   'Computation',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,783',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2581275558',\n",
       "   '1997063559',\n",
       "   '2293063825',\n",
       "   '2112325651',\n",
       "   '2056760934',\n",
       "   '2157629899',\n",
       "   '2098205603',\n",
       "   '1597474747',\n",
       "   '2414854470',\n",
       "   '807785616']},\n",
       " {'id': '1971129545',\n",
       "  'title': 'Generalization of backpropagation with application to a recurrent gas market model',\n",
       "  'abstract': 'Abstract   Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research.  This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Paul J. Werbos'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Catastrophic interference',\n",
       "   'Artificial neural network',\n",
       "   'Delta rule',\n",
       "   'Hopfield network',\n",
       "   'Generalization',\n",
       "   'Reinforcement learning',\n",
       "   'Least squares',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,013',\n",
       "  'reference_count': '20',\n",
       "  'references': ['22297218',\n",
       "   '2068484625',\n",
       "   '2176028050',\n",
       "   '1583833196',\n",
       "   '2010526455',\n",
       "   '3121926921',\n",
       "   '1969166509',\n",
       "   '3150413596',\n",
       "   '134309601',\n",
       "   '3022423118']},\n",
       " {'id': '3121926921',\n",
       "  'title': 'Beyond regression : new fools for prediction and analysis in the behavioral sciences',\n",
       "  'abstract': '',\n",
       "  'date': '1974',\n",
       "  'authors': ['P.'],\n",
       "  'related_topics': ['Regression',\n",
       "   'Behavioural sciences',\n",
       "   'Computer science',\n",
       "   'Cognitive psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,578',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2147800946',\n",
       "  'title': 'Backpropagation applied to handwritten zip code recognition',\n",
       "  'abstract': 'The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Y. LeCun',\n",
       "   'B. Boser',\n",
       "   'J. S. Denker',\n",
       "   'D. Henderson',\n",
       "   'R. E. Howard',\n",
       "   'W. Hubbard',\n",
       "   'L. D. Jackel'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Character (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Domain (software engineering)',\n",
       "   'Computer science',\n",
       "   'Image (mathematics)',\n",
       "   'Task (project management)',\n",
       "   'Artificial intelligence',\n",
       "   'Postal service',\n",
       "   'Zip code',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,702',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2154642048',\n",
       "   '2165758113',\n",
       "   '169539560',\n",
       "   '19621276',\n",
       "   '2101926813',\n",
       "   '56903235',\n",
       "   '2157475639',\n",
       "   '2606594511',\n",
       "   '1965770722',\n",
       "   '2116360511']},\n",
       " {'id': '2895674046',\n",
       "  'title': 'Adaptive Signal Processing',\n",
       "  'abstract': 'GENERAL INTRODUCTION. Adaptive Systems. The Adaptive Linear Combiner. THEORY OF ADAPTATION WITH STATIONARY SIGNALS. Properties of the Quadratic Performance Surface. Searching the Performance Surface. Gradient Estimation and Its Effects on Adaptation. ADAPTIVE ALGORITHMS AND STRUCTURES. The LMS Algorithm. The Z-Transform in Adaptive Signal Processing. Other Adaptive Algorithms and Structures. Adaptive Lattice Filters. APPLICATIONS. Adaptive Modeling and System Identification. Inverse Adaptive Modeling, Deconvolution, and Equalization. Adaptive Control Systems. Adaptive Interference Cancelling. Introduction to Adaptive Arrays and Adaptive Beamforming. Analysis of Adaptive Beamformers.',\n",
       "  'date': '1985',\n",
       "  'authors': ['Bernard Widrow', 'Samuel D. Stearns'],\n",
       "  'related_topics': ['Adaptive filter',\n",
       "   'Adaptive beamformer',\n",
       "   'Adaptive control',\n",
       "   'Adaptive system',\n",
       "   'Multidelay block frequency domain adaptive filter',\n",
       "   'Signal processing',\n",
       "   'Least mean squares filter',\n",
       "   'Linear system',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,334',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2798813531',\n",
       "  'title': 'Linear systems',\n",
       "  'abstract': '',\n",
       "  'date': '1980',\n",
       "  'authors': ['Thomas'],\n",
       "  'related_topics': ['Linear system',\n",
       "   'Computer science',\n",
       "   'Applied mathematics',\n",
       "   'Rational matrices',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,885',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2098398123',\n",
       "  'title': 'Non-linear system identification using neural networks',\n",
       "  'abstract': 'Multi-layered neural networks offer an exciting alternative for modelling complex non-linear systems. This paper investigates the identification of discrete-time nonlinear systems using neural networks with a single hidden layer. New parameter estimation algorithms are derived for the neural network model based on a prediction error formulation and the application to both simulated and real data is included to demonstrate the effectiveness of the neural network approach.',\n",
       "  'date': '1990',\n",
       "  'authors': ['S. Chen', 'S. A. Billings', 'Peter Grant'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Stochastic neural network',\n",
       "   'Cellular neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Nervous system network models',\n",
       "   'Artificial neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,317',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2103496339',\n",
       "   '1971735090',\n",
       "   '1540723801',\n",
       "   '1603277681',\n",
       "   '2102380305',\n",
       "   '1650765400',\n",
       "   '2125812768',\n",
       "   '1535689967']},\n",
       " {'id': '2176028050',\n",
       "  'title': 'Connectionist learning procedures',\n",
       "  'abstract': 'A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Connectionism',\n",
       "   'Task (computing)',\n",
       "   'Generalization',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Domain (software engineering)',\n",
       "   'Rate of convergence',\n",
       "   'Construct (python library)',\n",
       "   'Connection (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,901',\n",
       "  'reference_count': '85',\n",
       "  'references': ['1497256448',\n",
       "   '2581275558',\n",
       "   '2154642048',\n",
       "   '1498436455',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2293063825',\n",
       "   '2895674046',\n",
       "   '1597286183',\n",
       "   '22297218']},\n",
       " {'id': '1966812932',\n",
       "  'title': 'A Maximum Likelihood Approach to Continuous Speech Recognition',\n",
       "  'abstract': 'Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.',\n",
       "  'date': '1983',\n",
       "  'authors': ['Lalit R. Bahl', 'Frederick Jelinek', 'Robert L. Mercer'],\n",
       "  'related_topics': ['Speech processing',\n",
       "   'Sequential decoding',\n",
       "   'Decoding methods',\n",
       "   'Estimation theory',\n",
       "   'Statistical model',\n",
       "   'Speech production',\n",
       "   'Markov model',\n",
       "   'Natural language',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Maximum likelihood',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,959',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2142384583',\n",
       "   '1597533204',\n",
       "   '1575431606',\n",
       "   '2341171179',\n",
       "   '2163929346',\n",
       "   '2157477135',\n",
       "   '2029491572',\n",
       "   '2035227369',\n",
       "   '2137095888',\n",
       "   '1989226853']},\n",
       " {'id': '2101926813',\n",
       "  'title': 'Neocognitron: A Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position',\n",
       "  'abstract': \"A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.\",\n",
       "  'date': '1980',\n",
       "  'authors': ['Kunihiko Fukushima'],\n",
       "  'related_topics': ['Neocognitron',\n",
       "   'Form perception',\n",
       "   'Stimulus (physiology)',\n",
       "   'Artificial neural network',\n",
       "   'Unsupervised learning',\n",
       "   'Hypercomplex number',\n",
       "   'Gestalt psychology',\n",
       "   'Pattern recognition',\n",
       "   'Cascade',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,024',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2116360511',\n",
       "   '2322002063',\n",
       "   '2053120614',\n",
       "   '1588340522',\n",
       "   '1594551768',\n",
       "   '2010315761',\n",
       "   '2272360941',\n",
       "   '22889343',\n",
       "   '2324189819',\n",
       "   '2091546412']},\n",
       " {'id': '1991133427',\n",
       "  'title': 'Error bounds for convolutional codes and an asymptotically optimum decoding algorithm',\n",
       "  'abstract': 'The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.',\n",
       "  'date': '1967',\n",
       "  'authors': ['A. Viterbi'],\n",
       "  'related_topics': ['Sequential decoding',\n",
       "   'Serial concatenated convolutional codes',\n",
       "   'List decoding',\n",
       "   'Convolutional code',\n",
       "   'Concatenated error correction code',\n",
       "   'Linear code',\n",
       "   'Turbo code',\n",
       "   'Low-density parity-check code',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,436',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2034274945',\n",
       "   '1993944611',\n",
       "   '2087362480',\n",
       "   '2005530146',\n",
       "   '1976797517',\n",
       "   '1527268325',\n",
       "   '1527096151']},\n",
       " {'id': '2048330959',\n",
       "  'title': 'Cooperative computation of stereo disparity',\n",
       "  'abstract': 'Perhaps one of the most striking differences between a brain and today’s computers is the amount of “wiring.” In a digital computer the ratio of connections to components is about 3, whereas for the mammalian cortex it lies between 10 and 10,000 (1).',\n",
       "  'date': '1988',\n",
       "  'authors': ['D. Marr', 'T. Poggio'],\n",
       "  'related_topics': ['Computation',\n",
       "   'Information processing',\n",
       "   'Cortex (anatomy)',\n",
       "   'Correspondence problem',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Perception',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Combinatorial analysis',\n",
       "   'Digital computer',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,285',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2170716495',\n",
       "   '1997494543',\n",
       "   '2052810501',\n",
       "   '2087895317',\n",
       "   '2089840306',\n",
       "   '1975068880',\n",
       "   '1992476998',\n",
       "   '1981520343',\n",
       "   '2001963156',\n",
       "   '2090007926']},\n",
       " {'id': '2057175746',\n",
       "  'title': 'Shape matching and object recognition using shape contexts',\n",
       "  'abstract': 'We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set.',\n",
       "  'date': '2002',\n",
       "  'authors': ['S. Belongie', 'J. Malik', 'J. Puzicha'],\n",
       "  'related_topics': ['Shape analysis (digital geometry)',\n",
       "   'Heat kernel signature',\n",
       "   'Shape context',\n",
       "   'Similarity (geometry)',\n",
       "   'Correspondence problem',\n",
       "   'Feature extraction',\n",
       "   'GLOH',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,336',\n",
       "  'reference_count': '58',\n",
       "  'references': ['2310919327',\n",
       "   '2124386111',\n",
       "   '2119821739',\n",
       "   '2117812871',\n",
       "   '2138451337',\n",
       "   '2038952578',\n",
       "   '2124087378',\n",
       "   '2123977795',\n",
       "   '2101522199',\n",
       "   '2146766088']},\n",
       " {'id': '2159080219',\n",
       "  'title': 'Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference',\n",
       "  'abstract': 'From the Publisher:\\r\\nProbabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\\x97and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\\x97in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.\\r\\nProbabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Judea Pearl'],\n",
       "  'related_topics': ['Reasoning system',\n",
       "   'Intelligent decision support system',\n",
       "   'Probabilistic logic network',\n",
       "   'Probabilistic argumentation',\n",
       "   'Non-monotonic logic',\n",
       "   'Probabilistic logic',\n",
       "   'Uncertain inference',\n",
       "   'Decision support system',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'View Less'],\n",
       "  'citation_count': '24,375',\n",
       "  'reference_count': '235',\n",
       "  'references': ['2581275558',\n",
       "   '1997063559',\n",
       "   '1593793857',\n",
       "   '2797148637',\n",
       "   '2155322595',\n",
       "   '158727920',\n",
       "   '2138162238',\n",
       "   '2108309071',\n",
       "   '1986808060',\n",
       "   '2142901448']},\n",
       " {'id': '2156163116',\n",
       "  'title': 'Best practices for convolutional neural networks applied to visual document analysis',\n",
       "  'abstract': 'Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.',\n",
       "  'date': '2003',\n",
       "  'authors': ['P.Y. Simard', 'D. Steinkraus', 'J.C. Platt'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'MNIST database',\n",
       "   'Artificial neural network',\n",
       "   'Handwriting recognition',\n",
       "   'Set (abstract data type)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Support vector machine',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,757',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2310919327',\n",
       "   '1554663460',\n",
       "   '2159737176',\n",
       "   '2027197837',\n",
       "   '2068017609',\n",
       "   '2147345686',\n",
       "   '51975515',\n",
       "   '2166469100']},\n",
       " {'id': '2131686571',\n",
       "  'title': 'Fields of Experts: a framework for learning image priors',\n",
       "  'abstract': 'We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.',\n",
       "  'date': '2005',\n",
       "  'authors': ['S. Roth', 'M.J. Black'],\n",
       "  'related_topics': ['Markov random field',\n",
       "   'Approximate inference',\n",
       "   'Inpainting',\n",
       "   'Machine vision',\n",
       "   'Field (computer science)',\n",
       "   'Pixel',\n",
       "   'Markov process',\n",
       "   'Prior probability',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Iterative reconstruction',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,178',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2133665775',\n",
       "   '2116064496',\n",
       "   '2108384452',\n",
       "   '1997063559',\n",
       "   '2121927366',\n",
       "   '2113945798',\n",
       "   '2295936755',\n",
       "   '2116013899',\n",
       "   '2149760002',\n",
       "   '2105464873']},\n",
       " {'id': '2158778629',\n",
       "  'title': 'Toward automatic phenotyping of developing embryos from videos',\n",
       "  'abstract': 'We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Feng Ning',\n",
       "   'D. Delhomme',\n",
       "   'Y. LeCun',\n",
       "   'F. Piano',\n",
       "   'L. Bottou',\n",
       "   'P.E. Barbano'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Image processing',\n",
       "   'Contextual image classification',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Component (UML)',\n",
       "   'Set (abstract data type)',\n",
       "   'Computer science',\n",
       "   'Cytoplasm',\n",
       "   'Embryo',\n",
       "   'Cell wall',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '305',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2147880316',\n",
       "   '2104095591',\n",
       "   '1647075334',\n",
       "   '2134557905',\n",
       "   '2121927366',\n",
       "   '2119823327',\n",
       "   '1991848143',\n",
       "   '2157364932']},\n",
       " {'id': '2567948266',\n",
       "  'title': 'A view of the EM algorithm that justifies incremental, sparse, and other variants',\n",
       "  'abstract': 'The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Radford M. Neal', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Expectation–maximization algorithm',\n",
       "   'Conditional probability distribution',\n",
       "   'Function (mathematics)',\n",
       "   'Range (statistics)',\n",
       "   'Convergence (routing)',\n",
       "   'Standard algorithms',\n",
       "   'Distribution (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Energy (signal processing)',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,196',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2049633694',\n",
       "   '2117853077',\n",
       "   '2024476015',\n",
       "   '1580495158',\n",
       "   '1991278573',\n",
       "   '581152777']},\n",
       " {'id': '2159737176',\n",
       "  'title': 'Training Invariant Support Vector Machines',\n",
       "  'abstract': 'Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Dennis Decoste', 'Bernhard Schölkopf'],\n",
       "  'related_topics': ['Sequential minimal optimization',\n",
       "   'Structured support vector machine',\n",
       "   'Least squares support vector machine',\n",
       "   'MNIST database',\n",
       "   'Relevance vector machine',\n",
       "   'Support vector machine',\n",
       "   'Contextual image classification',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Invariant (physics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '771',\n",
       "  'reference_count': '38',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2310919327',\n",
       "   '2119821739',\n",
       "   '2140095548',\n",
       "   '1512098439',\n",
       "   '2087347434',\n",
       "   '1604938182',\n",
       "   '2147800946',\n",
       "   '2151040995']},\n",
       " {'id': '2124914669',\n",
       "  'title': 'Exponential Family Harmoniums with an Application to Information Retrieval',\n",
       "  'abstract': 'Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Max Welling', 'Michal Rosen-zvi', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Exponential random graph models',\n",
       "   'Divergence-from-randomness model',\n",
       "   'Graphical model',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Exponential family',\n",
       "   'Posterior probability',\n",
       "   'Statistical model',\n",
       "   'Document retrieval',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '570',\n",
       "  'reference_count': '14',\n",
       "  'references': ['1880262756',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1612003148',\n",
       "   '2140124448',\n",
       "   '1934021597',\n",
       "   '2138448681',\n",
       "   '145818128',\n",
       "   '2109720450',\n",
       "   '1813659000']},\n",
       " {'id': '2153663612',\n",
       "  'title': 'Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries',\n",
       "  'abstract': 'We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods',\n",
       "  'date': '2006',\n",
       "  'authors': ['M. Elad', 'M. Aharon'],\n",
       "  'related_topics': ['Non-local means',\n",
       "   'Video denoising',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image processing',\n",
       "   'K-SVD',\n",
       "   'Image quality',\n",
       "   'Sparse approximation',\n",
       "   'Matching pursuit',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,578',\n",
       "  'reference_count': '41',\n",
       "  'references': ['2160547390',\n",
       "   '2078204800',\n",
       "   '2146842127',\n",
       "   '2158940042',\n",
       "   '2151693816',\n",
       "   '2097323375',\n",
       "   '2113945798',\n",
       "   '2132680427',\n",
       "   '2079724595',\n",
       "   '2131686571']},\n",
       " {'id': '2613634265',\n",
       "  'title': 'Scaling learning algorithms towards AI',\n",
       "  'abstract': 'One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Yoshua Bengio', '', '', 'Yann'],\n",
       "  'related_topics': ['Kernel method',\n",
       "   'Kernel (statistics)',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Intelligent control',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Curse of dimensionality',\n",
       "   'Machine learning',\n",
       "   'Invariant (computer science)',\n",
       "   'Computer science',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,415',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2136922672',\n",
       "   '2148603752',\n",
       "   '2310919327',\n",
       "   '2119821739',\n",
       "   '2053186076',\n",
       "   '2116064496',\n",
       "   '2001141328',\n",
       "   '2110798204',\n",
       "   '2057175746',\n",
       "   '2140095548']},\n",
       " {'id': '1993845689',\n",
       "  'title': 'The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks',\n",
       "  'abstract': 'An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Geoffrey E. Hinton', 'Peter', 'Brendan J.', 'Radford M.'],\n",
       "  'related_topics': ['Helmholtz machine',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Connectionism',\n",
       "   'Stochastic process',\n",
       "   'Pattern recognition',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,275',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2740373864',\n",
       "   '2177040213',\n",
       "   '1533169541',\n",
       "   '2044875682',\n",
       "   '94647076']},\n",
       " {'id': '2109779438',\n",
       "  'title': 'The Cascade-Correlation Learning Architecture',\n",
       "  'abstract': 'Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Scott E. Fahlman', 'Christian Lebiere'],\n",
       "  'related_topics': ['Network topology',\n",
       "   'Network simulation',\n",
       "   'Artificial neural network',\n",
       "   'Topology (electrical circuits)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Detector',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,259',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2160208155',\n",
       "   '19621276',\n",
       "   '3121126077',\n",
       "   '2160699933',\n",
       "   '50076749',\n",
       "   '2127385318',\n",
       "   '2169163929',\n",
       "   '2167277568']},\n",
       " {'id': '2103626435',\n",
       "  'title': 'Practical Issues in Temporal Difference Learning',\n",
       "  'abstract': \"This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(λ) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(λ) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.\",\n",
       "  'date': '1992',\n",
       "  'authors': ['Gerald Tesauro'],\n",
       "  'related_topics': ['Temporal difference learning',\n",
       "   'Outcome (game theory)',\n",
       "   'Context (language use)',\n",
       "   'Artificial neural network',\n",
       "   'Connectionism',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Zero-knowledge proof',\n",
       "   'Perspective (graphical)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,601',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2137983211',\n",
       "   '3146803896',\n",
       "   '2100677568',\n",
       "   '2178806388',\n",
       "   '1583833196',\n",
       "   '2154952480',\n",
       "   '2159047538',\n",
       "   '1569296262']},\n",
       " {'id': '2125569215',\n",
       "  'title': 'The Curse of Highly Variable Functions for Local Kernel Machines',\n",
       "  'abstract': 'We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Yoshua Bengio', 'Olivier Delalleau', 'Nicolas L. Roux'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Kernel method',\n",
       "   'Kernel embedding of distributions',\n",
       "   'Radial basis function kernel',\n",
       "   'Unsupervised learning',\n",
       "   'Polynomial kernel',\n",
       "   'Curse of dimensionality',\n",
       "   'Tree kernel',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '222',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2119821739',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2140095548',\n",
       "   '2087347434',\n",
       "   '2154455818',\n",
       "   '2139823104',\n",
       "   '1604938182',\n",
       "   '3017143921',\n",
       "   '2160167256']},\n",
       " {'id': '2167967601',\n",
       "  'title': 'Convex Neural Networks',\n",
       "  'abstract': 'Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Nicolas L. Roux',\n",
       "   'Pascal Vincent',\n",
       "   'Olivier Delalleau',\n",
       "   'Patrice Marcotte'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convex optimization',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Competitive learning',\n",
       "   'Feedforward neural network',\n",
       "   'Time delay neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '189',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2135046866',\n",
       "   '3124955340',\n",
       "   '1678356000',\n",
       "   '1498436455',\n",
       "   '2151693816',\n",
       "   '2091886411',\n",
       "   '2504871398',\n",
       "   '2108263314',\n",
       "   '2109405055',\n",
       "   '2075887074']},\n",
       " {'id': '2153635508',\n",
       "  'title': 'LIBSVM: A library for support vector machines',\n",
       "  'abstract': 'LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Chih-Chung Chang', 'Chih-Jen Lin'],\n",
       "  'related_topics': ['Sequential minimal optimization',\n",
       "   'Structured support vector machine',\n",
       "   'Support vector machine',\n",
       "   'Relevance vector machine',\n",
       "   'Multiclass classification',\n",
       "   'Hinge loss',\n",
       "   'Radial basis function kernel',\n",
       "   'Graph kernel',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '43,932',\n",
       "  'reference_count': '52',\n",
       "  'references': ['2148603752',\n",
       "   '2119821739',\n",
       "   '2109943925',\n",
       "   '2172000360',\n",
       "   '1512098439',\n",
       "   '2104978738',\n",
       "   '1576520375',\n",
       "   '2087347434',\n",
       "   '2132870739',\n",
       "   '2124351082']},\n",
       " {'id': '1902027874',\n",
       "  'title': 'Learning the parts of objects by non-negative matrix factorization',\n",
       "  'abstract': 'Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Daniel D. Lee', 'H. Sebastian Seung', ''],\n",
       "  'related_topics': ['Non-negative matrix factorization',\n",
       "   'Matrix decomposition',\n",
       "   'Document-term matrix',\n",
       "   'Factorization',\n",
       "   'Representation (mathematics)',\n",
       "   'Sign (mathematics)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Vector quantization',\n",
       "   'Pattern recognition',\n",
       "   'Physics',\n",
       "   'Bioinformatics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,731',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2138451337',\n",
       "   '2108384452',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '2145889472',\n",
       "   '1983578042',\n",
       "   '1996355918',\n",
       "   '1993845689',\n",
       "   '2156406284',\n",
       "   '2180838288']},\n",
       " {'id': '2105464873',\n",
       "  'title': 'Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1 ?',\n",
       "  'abstract': 'The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and ban@ass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli. © 1997 Elsevier Science Ltd',\n",
       "  'date': '1997',\n",
       "  'authors': ['Bruno A. Olshausen', 'David J.'],\n",
       "  'related_topics': ['Basis function',\n",
       "   'Efficient coding hypothesis',\n",
       "   'Neural coding',\n",
       "   'K-SVD',\n",
       "   'Linear independence',\n",
       "   'Gabor wavelet',\n",
       "   'Function (mathematics)',\n",
       "   'Curse of dimensionality',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Communication',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,196',\n",
       "  'reference_count': '38',\n",
       "  'references': ['2132984323',\n",
       "   '2099741732',\n",
       "   '2108384452',\n",
       "   '2145889472',\n",
       "   '1536929369',\n",
       "   '2133069808',\n",
       "   '2107790757',\n",
       "   '2167034998',\n",
       "   '3022628558',\n",
       "   '2145012779']},\n",
       " {'id': '1802356529',\n",
       "  'title': 'Energy-based models for sparse overcomplete representations',\n",
       "  'abstract': 'We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Yee Whye Teh',\n",
       "   'Max Welling',\n",
       "   'Simon Osindero',\n",
       "   'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Independent component analysis',\n",
       "   'Independence (probability theory)',\n",
       "   'Conditional independence',\n",
       "   'Blind signal separation',\n",
       "   'Probability distribution',\n",
       "   'Density estimation',\n",
       "   'Free parameter',\n",
       "   'Contrast (statistics)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '206',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2116064496',\n",
       "   '2078204800',\n",
       "   '2154642048',\n",
       "   '1652505363',\n",
       "   '2099741732',\n",
       "   '2108384452',\n",
       "   '2151693816',\n",
       "   '2145889472',\n",
       "   '2146474141',\n",
       "   '2105464873']},\n",
       " {'id': '2075187489',\n",
       "  'title': 'The Cost of Cortical Computation',\n",
       "  'abstract': 'Electrophysiological recordings show that individual neurons in cortex are strongly activated when engaged in appropriate tasks, but they tell us little about how many neurons might be engaged by a task, which is important to know if we are to understand how cortex encodes information. For human cortex, I estimate the cost of individual spikes, then, from the known energy consumption of cortex, I establish how many neurons can be active concurrently. The cost of a single spike is high, and this severely limits, possibly to fewer than 1%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. The latter constraint explains the investment in local control of hemodynamics, exploited by functional magnetic resonance imaging, and the need for mechanisms of selective attention.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Peter Lennie'],\n",
       "  'related_topics': ['Functional magnetic resonance imaging',\n",
       "   'Cortex (anatomy)',\n",
       "   'Brain mapping',\n",
       "   'Electrophysiology',\n",
       "   'Task (project management)',\n",
       "   'Neuroscience',\n",
       "   'Energy consumption',\n",
       "   'Constraint (information theory)',\n",
       "   'Computation',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,155',\n",
       "  'reference_count': '37',\n",
       "  'references': ['2110208125',\n",
       "   '2042422091',\n",
       "   '1907121963',\n",
       "   '1976738367',\n",
       "   '2003739479',\n",
       "   '1603661052',\n",
       "   '1993303421',\n",
       "   '1991233288',\n",
       "   '2096519870',\n",
       "   '43284170']},\n",
       " {'id': '2102409316',\n",
       "  'title': 'Autoencoders, Minimum Description Length and Helmholtz Free Energy',\n",
       "  'abstract': 'An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Geoffrey E. Hinton', 'Richard S. Zemel'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Minimum description length',\n",
       "   'Upper and lower bounds',\n",
       "   'Code (cryptography)',\n",
       "   'Boltzmann distribution',\n",
       "   'Helmholtz free energy',\n",
       "   'Lyapunov function',\n",
       "   'Set (abstract data type)',\n",
       "   'Algorithm',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,045',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2176028050',\n",
       "   '2078626246',\n",
       "   '2063089147',\n",
       "   '1578739277',\n",
       "   '2110553242',\n",
       "   '2151907713']},\n",
       " {'id': '11828546',\n",
       "  'title': '4.7 – Statistical Modeling of Photographic Images',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': ['Eero P. Simoncelli'],\n",
       "  'related_topics': ['Statistical model',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '119',\n",
       "  'reference_count': '61',\n",
       "  'references': ['2132984323',\n",
       "   '2053691921',\n",
       "   '2113945798',\n",
       "   '2145889472',\n",
       "   '2105464873',\n",
       "   '2103504761',\n",
       "   '2134929491',\n",
       "   '2127006916',\n",
       "   '2137234026',\n",
       "   '2109863423']},\n",
       " {'id': '2144081223',\n",
       "  'title': 'Coot: model-building tools for molecular graphics.',\n",
       "  'abstract': \"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as `Coot'.\",\n",
       "  'date': '2004',\n",
       "  'authors': ['Paul Emsley', 'Kevin Cowtan'],\n",
       "  'related_topics': ['Computer graphics',\n",
       "   'Molecular graphics',\n",
       "   'Enzyme structure',\n",
       "   'Graphics',\n",
       "   'Oxidoreductase inhibitor',\n",
       "   'Model building',\n",
       "   'Software engineering',\n",
       "   'Computer science',\n",
       "   'Software',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Bioinformatics',\n",
       "   'View Less'],\n",
       "  'citation_count': '26,273',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2130479394',\n",
       "   '2013083986',\n",
       "   '2135839939',\n",
       "   '1986830449',\n",
       "   '1969222787']},\n",
       " {'id': '2111211467',\n",
       "  'title': 'New Algorithms and Methods to Estimate Maximum-Likelihood Phylogenies: Assessing the Performance of PhyML 3.0',\n",
       "  'abstract': 'PhyML is a phylogeny software based on the maximum-likelihood principle. Early PhyML versions used a fast algorithm performing nearest neighbor interchanges to improve a reasonable starting tree topology. Since the original publication (Guindon S., Gascuel O. 2003. A simple, fast and accurate algorithm to estimate large phylogenies by maximum likelihood. Syst. Biol. 52:696-704), PhyML has been widely used (>2500 citations in ISI Web of Science) because of its simplicity and a fair compromise between accuracy and speed. In the meantime, research around PhyML has continued, and this article describes the new algorithms and methods implemented in the program. First, we introduce a new algorithm to search the tree space with user-defined intensity using subtree pruning and regrafting topological moves. The parsimony criterion is used here to filter out the least promising topology modifications with respect to the likelihood function. The analysis of a large collection of real nucleotide and amino acid data sets of various sizes demonstrates the good performance of this method. Second, we describe a new test to assess the support of the data for internal branches of a phylogeny. This approach extends the recently proposed approximate likelihood-ratio test and relies on a nonparametric, Shimodaira-Hasegawa-like procedure. A detailed analysis of real alignments sheds light on the links between this new approach and the more classical nonparametric bootstrap method. Overall, our tests show that the last version (3.0) of PhyML is fast, accurate, stable, and ready to use. A Web server and binary files are available from http://www.atgc-montpellier.fr/phyml/.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Stéphane Guindon',\n",
       "   'Jean-François Dufayard',\n",
       "   'Vincent Lefort',\n",
       "   'Maria Anisimova',\n",
       "   'Wim Hordijk',\n",
       "   'Olivier Gascuel'],\n",
       "  'related_topics': ['Likelihood function',\n",
       "   'Tree (data structure)',\n",
       "   'Pruning (decision trees)',\n",
       "   'Algorithm',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Nonparametric statistics',\n",
       "   'Network topology',\n",
       "   'Data mining',\n",
       "   'Filter (signal processing)',\n",
       "   'Web server',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,540',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2146058063',\n",
       "   '2168696662',\n",
       "   '2103546861',\n",
       "   '2031611770',\n",
       "   '2127847431',\n",
       "   '2030966943',\n",
       "   '2103088017',\n",
       "   '2098448352',\n",
       "   '2105926960',\n",
       "   '2163627198']},\n",
       " {'id': '2470646526',\n",
       "  'title': 'SARS and MERS: recent insights into emerging coronaviruses',\n",
       "  'abstract': 'The emergence of Middle East respiratory syndrome coronavirus (MERS-CoV) in 2012 marked the second introduction of a highly pathogenic coronavirus into the human population in the twenty-first century. The continuing introductions of MERS-CoV from dromedary camels, the subsequent travel-related viral spread, the unprecedented nosocomial outbreaks and the high case-fatality rates highlight the need for prophylactic and therapeutic measures. Scientific advancements since the 2002-2003 severe acute respiratory syndrome coronavirus (SARS-CoV) pandemic allowed for rapid progress in our understanding of the epidemiology and pathogenesis of MERS-CoV and the development of therapeutics. In this Review, we detail our present understanding of the transmission and pathogenesis of SARS-CoV and MERS-CoV, and discuss the current state of development of measures to combat emerging coronaviruses.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Emmie de Wit',\n",
       "   'Neeltje van Doremalen',\n",
       "   'Darryl Falzarano',\n",
       "   'Vincent J. Munster'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Population',\n",
       "   'Viral pathogenesis',\n",
       "   'Pandemic',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Highly pathogenic',\n",
       "   'Viral spread',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,410',\n",
       "  'reference_count': '156',\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '1909499787',\n",
       "   '2138324310']},\n",
       " {'id': '2306794997',\n",
       "  'title': 'Epidemiology, Genetic Recombination, and Pathogenesis of Coronaviruses',\n",
       "  'abstract': 'Human coronaviruses (HCoVs) were first described in the 1960s for patients with the common cold. Since then, more HCoVs have been discovered, including those that cause severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), two pathogens that, upon infection, can cause fatal respiratory disease in humans. It was recently discovered that dromedary camels in Saudi Arabia harbor three different HCoV species, including a dominant MERS HCoV lineage that was responsible for the outbreaks in the Middle East and South Korea during 2015. In this review we aim to compare and contrast the different HCoVs with regard to epidemiology and pathogenesis, in addition to the virus evolution and recombination events which have, on occasion, resulted in outbreaks amongst humans.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Shuo Su',\n",
       "   'Gary Wong',\n",
       "   'Weifeng Shi',\n",
       "   'Jun Liu',\n",
       "   '',\n",
       "   'Alexander C.K. Lai',\n",
       "   'Jiyong Zhou',\n",
       "   'Wenjun Liu',\n",
       "   'Yuhai Bi',\n",
       "   'George F. Gao'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Common cold',\n",
       "   'Viral evolution',\n",
       "   'Pathogenesis',\n",
       "   'Epidemiology',\n",
       "   'Respiratory disease',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,894',\n",
       "  'reference_count': '94',\n",
       "  'references': ['2166867592',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '2138324310',\n",
       "   '2125251240',\n",
       "   '2160011624',\n",
       "   '2115555188',\n",
       "   '2134061616']},\n",
       " {'id': '1993577573',\n",
       "  'title': 'Isolation and characterization of a bat SARS-like coronavirus that uses the ACE2 receptor',\n",
       "  'abstract': 'The 2002-3 pandemic caused by severe acute respiratory syndrome coronavirus (SARS-CoV) was one of the most significant public health events in recent history. An ongoing outbreak of Middle East respiratory syndrome coronavirus suggests that this group of viruses remains a key threat and that their distribution is wider than previously recognized. Although bats have been suggested to be the natural reservoirs of both viruses, attempts to isolate the progenitor virus of SARS-CoV from bats have been unsuccessful. Diverse SARS-like coronaviruses (SL-CoVs) have now been reported from bats in China, Europe and Africa, but none is considered a direct progenitor of SARS-CoV because of their phylogenetic disparity from this virus and the inability of their spike proteins to use the SARS-CoV cellular receptor molecule, the human angiotensin converting enzyme II (ACE2). Here we report whole-genome sequences of two novel bat coronaviruses from Chinese horseshoe bats (family: Rhinolophidae) in Yunnan, China: RsSHC014 and Rs3367. These viruses are far more closely related to SARS-CoV than any previously identified bat coronaviruses, particularly in the receptor binding domain of the spike protein. Most importantly, we report the first recorded isolation of a live SL-CoV (bat SL-CoV-WIV1) from bat faecal samples in Vero E6 cells, which has typical coronavirus morphology, 99.9% sequence identity to Rs3367 and uses ACE2 from humans, civets and Chinese horseshoe bats for cell entry. Preliminary in vitro testing indicates that WIV1 also has a broad species tropism. Our results provide the strongest evidence to date that Chinese horseshoe bats are natural reservoirs of SARS-CoV, and that intermediate hosts may not be necessary for direct human infection by some bat SL-CoVs. They also highlight the importance of pathogen-discovery programs targeting high-risk wildlife groups in emerging disease hotspots as a strategy for pandemic preparedness.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Xing Yi Ge',\n",
       "   'Jia Lu Li',\n",
       "   'Xing Lou Yang',\n",
       "   'Aleksei A. Chmura',\n",
       "   'Guangjian Zhu',\n",
       "   'Jonathan H. Epstein',\n",
       "   'Jonna A Mazet',\n",
       "   'Ben Hu',\n",
       "   'Wei Zhang',\n",
       "   'Cheng Peng',\n",
       "   'Yu Ji Zhang',\n",
       "   'Chu Ming Luo',\n",
       "   'Bing Tan',\n",
       "   'Ning Wang',\n",
       "   'Yan Zhu',\n",
       "   'Gary Crameri',\n",
       "   'Shu Yi Zhang',\n",
       "   'Lin Fa Wang',\n",
       "   '',\n",
       "   'Peter Daszak',\n",
       "   'Zheng Li Shi'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Virus genetics',\n",
       "   'Alphacoronavirus',\n",
       "   'Virus',\n",
       "   'Outbreak',\n",
       "   'Tropism',\n",
       "   'Rhinolophus sinicus',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,339',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2166867592',\n",
       "   '2104548316',\n",
       "   '2119111857',\n",
       "   '1966238900',\n",
       "   '2103503670',\n",
       "   '2140338292',\n",
       "   '2141008678',\n",
       "   '2049975503',\n",
       "   '2101063972',\n",
       "   '1990059132']},\n",
       " {'id': '2775086803',\n",
       "  'title': 'Discovery of a rich gene pool of bat SARS-related coronaviruses provides new insights into the origin of SARS coronavirus.',\n",
       "  'abstract': 'A large number of SARS-related coronaviruses (SARSr-CoV) have been detected in horseshoe bats since 2005 in different areas of China. However, these bat SARSr-CoVs show sequence differences from SARS coronavirus (SARS-CoV) in different genes (S, ORF8, ORF3, etc) and are considered unlikely to represent the direct progenitor of SARS-CoV. Herein, we report the findings of our 5-year surveillance of SARSr-CoVs in a cave inhabited by multiple species of horseshoe bats in Yunnan Province, China. The full-length genomes of 11 newly discovered SARSr-CoV strains, together with our previous findings, reveals that the SARSr-CoVs circulating in this single location are highly diverse in the S gene, ORF3 and ORF8. Importantly, strains with high genetic similarity to SARS-CoV in the hypervariable N-terminal domain (NTD) and receptor-binding domain (RBD) of the S1 gene, the ORF3 and ORF8 region, respectively, were all discovered in this cave. In addition, we report the first discovery of bat SARSr-CoVs highly similar to human SARS-CoV in ORF3b and in the split ORF8a and 8b. Moreover, SARSr-CoV strains from this cave were more closely related to SARS-CoV in the non-structural protein genes ORF1a and 1b compared with those detected elsewhere. Recombination analysis shows evidence of frequent recombination events within the S gene and around the ORF8 between these SARSr-CoVs. We hypothesize that the direct progenitor of SARS-CoV may have originated after sequential recombination events between the precursors of these SARSr-CoVs. Cell entry studies demonstrated that three newly identified SARSr-CoVs with different S protein sequences are all able to use human ACE2 as the receptor, further exhibiting the close relationship between strains in this cave and SARS-CoV. This work provides new insights into the origin and evolution of SARS-CoV and highlights the necessity of preparedness for future emergence of SARS-like diseases.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Ben Hu',\n",
       "   'Lei Ping Zeng',\n",
       "   'Xing Lou Yang',\n",
       "   'Xing Yi Ge',\n",
       "   'Wei Zhang',\n",
       "   'Bei Li',\n",
       "   'Jia Zheng Xie',\n",
       "   'Xu Rui Shen',\n",
       "   'Yun Zhi Zhang',\n",
       "   'Ning Wang',\n",
       "   'Dong Sheng Luo',\n",
       "   'Xiao Shuang Zheng',\n",
       "   'Mei Niang Wang',\n",
       "   'Peter Daszak',\n",
       "   'Lin Fa Wang',\n",
       "   'Jie Cui',\n",
       "   'Zheng Li Shi'],\n",
       "  'related_topics': ['Gene pool',\n",
       "   'Sequence analysis',\n",
       "   'Genome',\n",
       "   'Cave',\n",
       "   'Gene',\n",
       "   'Genomics',\n",
       "   'Sequence alignment',\n",
       "   'Evolutionary biology',\n",
       "   'Polymerase chain reaction',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '596',\n",
       "  'reference_count': '43',\n",
       "  'references': ['2111211467',\n",
       "   '1993577573',\n",
       "   '2169198329',\n",
       "   '2195009776',\n",
       "   '2103503670',\n",
       "   '2298153446',\n",
       "   '2134061616',\n",
       "   '2141877163',\n",
       "   '2140338292',\n",
       "   '2141008678']},\n",
       " {'id': '2119111857',\n",
       "  'title': 'Dipeptidyl peptidase 4 is a functional receptor for the emerging human coronavirus-EMC',\n",
       "  'abstract': 'Most human coronaviruses cause mild upper respiratory tract disease but may be associated with more severe pulmonary disease in immunocompromised individuals. However, SARS coronavirus caused severe lower respiratory disease with nearly 10% mortality and evidence of systemic spread. Recently, another coronavirus (human coronavirus-Erasmus Medical Center (hCoV-EMC)) was identified in patients with severe and sometimes lethal lower respiratory tract infection. Viral genome analysis revealed close relatedness to coronaviruses found in bats. Here we identify dipeptidyl peptidase 4 (DPP4; also known as CD26) as a functional receptor for hCoV-EMC. DPP4 specifically co-purified with the receptor-binding S1 domain of the hCoV-EMC spike protein from lysates of susceptible Huh-7 cells. Antibodies directed against DPP4 inhibited hCoV-EMC infection of primary human bronchial epithelial cells and Huh-7 cells. Expression of human and bat (Pipistrellus pipistrellus) DPP4 in non-susceptible COS-7 cells enabled infection by hCoV-EMC. The use of the evolutionarily conserved DPP4 protein from different species as a functional receptor provides clues about the host range potential of hCoV-EMC. In addition, it will contribute critically to our understanding of the pathogenesis and epidemiology of this emerging human coronavirus, and may facilitate the development of intervention strategies.',\n",
       "  'date': '2013',\n",
       "  'authors': ['V. Stalin Raj',\n",
       "   'Huihui Mou',\n",
       "   'Saskia L. Smits',\n",
       "   'Dick H. W. Dekkers',\n",
       "   'Marcel A. Müller',\n",
       "   'Ronald Dijkman',\n",
       "   'Doreen Muth',\n",
       "   'Jeroen A. A. Demmers',\n",
       "   'Ali Zaki',\n",
       "   'Ron A. M. Fouchier',\n",
       "   'Volker Thiel',\n",
       "   '',\n",
       "   'Christian Drosten',\n",
       "   'Peter J. M. Rottier',\n",
       "   'Albert D. M. E. Osterhaus',\n",
       "   'Berend Jan Bosch',\n",
       "   'Bart L. Haagmans'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Dipeptidyl peptidase-4',\n",
       "   'Viral pathogenesis',\n",
       "   'Lower respiratory tract infection',\n",
       "   'Virus genetics',\n",
       "   'Respiratory disease',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,574',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2166867592',\n",
       "   '1966238900',\n",
       "   '2103503670',\n",
       "   '2113457186',\n",
       "   '2091671824',\n",
       "   '1690366459',\n",
       "   '1982533785',\n",
       "   '2126707939',\n",
       "   '1964982019',\n",
       "   '2101063972']},\n",
       " {'id': '2025170735',\n",
       "  'title': 'Coronavirus as a possible cause of severe acute respiratory syndrome',\n",
       "  'abstract': \"Summary  Background  An outbreak of severe acute respiratory syndrome (SARS) has been reported in Hong Kong. We investigated the viral cause and clinical presentation among 50 patients.  Methods  We analysed case notes and microbiological findings for 50 patients with SARS, representing more than five separate epidemiologically linked transmission clusters. We defined the clinical presentation and risk factors associated with severe disease and investigated the causal agents by chest radiography and laboratory testing of nasopharyngeal aspirates and sera samples. We compared the laboratory findings with those submitted for microbiological investigation of other diseases from patients whose identity was masked.  Findings  Patients' age ranged from 23 to 74 years. Fever, chills, myalgia, and cough were the most frequent complaints. When compared with chest radiographic changes, respiratory symptoms and auscultatory findings were disproportionally mild. Patients who were household contacts of other infected people and had older age, lymphopenia, and liver dysfunction were associated with severe disease. A virus belonging to the family  Coronaviridae  was isolated from two patients. By use of serological and reverse-transcriptase PCR specific for this virus, 45 of 50 patients with SARS, but no controls, had evidence of infection with this virus.  Interpretation  A coronavirus was isolated from patients with SARS that might be the primary agent associated with this disease. Serological and molecular tests specific for the virus permitted a definitive laboratory diagnosis to be made and allowed further investigation to define whether other cofactors play a part in disease progression.\",\n",
       "  'date': '2003',\n",
       "  'authors': ['Jsm Peiris',\n",
       "   'ST Lai',\n",
       "   'Llm Poon',\n",
       "   'Y Guan',\n",
       "   'Lyc Yam',\n",
       "   'W Lim',\n",
       "   'J Nicholls',\n",
       "   'Wks Yee',\n",
       "   'WW Yan',\n",
       "   'MT Cheung',\n",
       "   'Vcc Cheng',\n",
       "   'KH Chan',\n",
       "   'Dnc Tsang',\n",
       "   'Rwh Yung',\n",
       "   'TK Ng',\n",
       "   'KY Yuen'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Human coronavirus NL63',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus HKU1',\n",
       "   'Pneumonia',\n",
       "   'myalgia',\n",
       "   'Viral disease',\n",
       "   'Chills',\n",
       "   'Internal medicine',\n",
       "   'Immunology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,651',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2116682907',\n",
       "   '2122399224',\n",
       "   '2104730345',\n",
       "   '2141291230',\n",
       "   '2154664055',\n",
       "   '1970720481',\n",
       "   '576359727',\n",
       "   '2239493136',\n",
       "   '2081510963',\n",
       "   '2336133541']},\n",
       " {'id': '2129542667',\n",
       "  'title': 'Clinical progression and viral load in a community outbreak of coronavirus-associated SARS pneumonia: a prospective study.',\n",
       "  'abstract': 'Summary Background We investigated the temporal progression of the clinical, radiological, and virological changes in a community outbreak of severe acute respiratory syndrome (SARS). Methods We followed up 75 patients for 3 weeks managed with a standard treatment protocol of ribavirin and corticosteroids, and assessed the pattern of clinical disease, viral load, risk factors for poor clinical outcome, and the usefulness of virological diagnostic methods. Findings Fever and pneumonia initially improved but 64 (85%) patients developed recurrent fever after a mean of 8·9 (SD 3·1) days, 55 (73%) had watery diarrhoea after 7·5 (2·3) days, 60 (80%) had radiological worsening after 7·4 (2·2) days, and respiratory symptoms worsened in 34 (45%) after 8·6 (3·0) days. In 34 (45%) patients, improvement of initial pulmonary lesions was associated with appearance of new radiological lesions at other sites. Nine (12%) patients developed spontaneous pneumomediastinum and 15 (20%) developed acute respiratory distress syndrome (ARDS) in week 3. Quantitative reverse-transcriptase (RT) PCR of nasopharyngeal aspirates in 14 patients (four with ARDS) showed peak viral load at day 10, and at day 15 a load lower than at admission. Age and chronic hepatitis B virus infection treated with lamivudine were independent significant risk factors for progression to ARDS (p=0·001). SARS-associated coronavirus in faeces was seen on RT-PCR in 65 (97%) of 67 patients at day 14. The mean time to seroconversion was 20 days. Interpretation The consistent clinical progression, shifting radiological infiltrates, and an inverted V viral-load profile suggest that worsening in week 2 is unrelated to uncontrolled viral replication but may be related to immunopathological damage.',\n",
       "  'date': '2003',\n",
       "  'authors': ['J S M Peiris',\n",
       "   'C M Chu',\n",
       "   'V C C Cheng',\n",
       "   'K S Chan',\n",
       "   'I F N Hung',\n",
       "   'L L M Poon',\n",
       "   'K I Law',\n",
       "   'B S F Tang',\n",
       "   'T Y W Hon',\n",
       "   'C S Chan',\n",
       "   'K H Chan',\n",
       "   'J S C Ng',\n",
       "   'B J Zheng',\n",
       "   'W L Ng',\n",
       "   'R W M Lai',\n",
       "   'Y Guan',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Viral load',\n",
       "   'ARDS',\n",
       "   'Pneumonia',\n",
       "   'Respiratory disease',\n",
       "   'Ribavirin',\n",
       "   'Standard treatment',\n",
       "   'Coronavirus',\n",
       "   'Internal medicine',\n",
       "   'Immunology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,458',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2107978811',\n",
       "   '2161328469',\n",
       "   '2155583106',\n",
       "   '1675164605',\n",
       "   '2061759246']},\n",
       " {'id': '1703839189',\n",
       "  'title': 'Detection of a novel human coronavirus by real-time reverse-transcription polymerase chain reaction',\n",
       "  'abstract': 'We present two real-time reverse-transcription polymerase chain reaction assays for a novel human coronavirus (CoV), targeting regions upstream of the E gene (upE) or within open reading frame (ORF)1b, respectively. Sensitivity for upE is 3.4 copies per reaction (95% confidence interval (CI): 2.5-6.9 copies) or 291 copies/mL of sample. No cross-reactivity was observed with coronaviruses OC43, NL63, 229E, SARS-CoV, nor with 92 clinical specimens containing common human respiratory viruses. We recommend using upE for screening and ORF1b for confirmation.',\n",
       "  'date': '2012',\n",
       "  'authors': ['V M Corman',\n",
       "   'I Eckerle',\n",
       "   'T Bleicker',\n",
       "   'A Zaki',\n",
       "   'O Landt',\n",
       "   'M Eschbach-Bludau',\n",
       "   'S van Boheemen',\n",
       "   'R Gopal',\n",
       "   'M Ballhause',\n",
       "   'T M Bestebroer',\n",
       "   'D Muth',\n",
       "   'M A Müller',\n",
       "   'J F Drexler',\n",
       "   'M Zambon',\n",
       "   'A D Osterhaus',\n",
       "   'R M Fouchier',\n",
       "   'C Drosten'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Polymerase chain reaction',\n",
       "   'Respiratory virus',\n",
       "   'Open reading frame',\n",
       "   'Viral replication',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Virology',\n",
       "   'Molecular biology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '629',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2132260239',\n",
       "   '2129542667',\n",
       "   '2167080692',\n",
       "   '1593955729',\n",
       "   '2145810580',\n",
       "   '1975169783',\n",
       "   '2069961370',\n",
       "   '2105870155',\n",
       "   '2100516702',\n",
       "   '2161315652']},\n",
       " {'id': '2116586125',\n",
       "  'title': 'Characterization of a novel coronavirus associated with severe acute respiratory syndrome',\n",
       "  'abstract': 'In March 2003, a novel coronavirus (SARS-CoV) was discovered in association with cases of severe acute respiratory syndrome (SARS). The sequence of the complete genome of SARS-CoV was determined, and the initial characterization of the viral genome is presented in this report. The genome of SARS-CoV is 29,727 nucleotides in length and has 11 open reading frames, and its genome organization is similar to that of other coronaviruses. Phylogenetic analyses and sequence comparisons showed that SARS-CoV is not closely related to any of the previously characterized coronaviruses.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Paul A. Rota',\n",
       "   'M. Steven Oberste',\n",
       "   'Stephan S. Monroe',\n",
       "   'W. Allan Nix',\n",
       "   'Ray Campagnoli',\n",
       "   'Joseph P. Icenogle',\n",
       "   'Silvia Peñaranda',\n",
       "   'Bettina Bankamp',\n",
       "   'Kaija Maher',\n",
       "   'Min hsin Chen',\n",
       "   'Suxiong Tong',\n",
       "   'Azaibi Tamin',\n",
       "   'Luis Lowe',\n",
       "   'Michael Frace',\n",
       "   'Joseph L. DeRisi',\n",
       "   'Qi Chen',\n",
       "   'David Wang',\n",
       "   'Dean D. Erdman',\n",
       "   'Teresa C.T. Peret',\n",
       "   'Cara Burns',\n",
       "   'Thomas G. Ksiazek',\n",
       "   'Pierre E. Rollin',\n",
       "   'Anthony Sanchez',\n",
       "   'Stephanie Liffick',\n",
       "   'Brian Holloway',\n",
       "   'Josef Limor',\n",
       "   'Karen McCaustland',\n",
       "   'Mellissa Olsen-Rasmussen',\n",
       "   'Ron Fouchier',\n",
       "   'Stephan Günther',\n",
       "   'Albert D.H.E. Osterhaus',\n",
       "   'Christian Drosten',\n",
       "   'Mark A. Pallansch',\n",
       "   'Larry J. Anderson',\n",
       "   'William J. Bellini'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Human coronavirus NL63',\n",
       "   'Genome',\n",
       "   'Coronaviridae',\n",
       "   'Genomic organization',\n",
       "   'Nucleic acid sequence',\n",
       "   'Sequence analysis',\n",
       "   'Virology',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,054',\n",
       "  'reference_count': '13',\n",
       "  'references': []},\n",
       " {'id': '1987783718',\n",
       "  'title': 'Extracorporeal membrane oxygenation for 2009 Influenza A (H1N1) Acute Respiratory Distress Syndrome',\n",
       "  'abstract': 'CONTEXT The novel influenza A(H1N1) pandemic affected Australia and New Zealand during the 2009 southern hemisphere winter. It caused an epidemic of critical illness and some patients developed severe acute respiratory distress syndrome (ARDS) and were treated with extracorporeal membrane oxygenation (ECMO). OBJECTIVES To describe the characteristics of all patients with 2009 influenza A(H1N1)-associated ARDS treated with ECMO and to report incidence, resource utilization, and patient outcomes. DESIGN, SETTING, AND PATIENTS An observational study of all patients (n = 68) with 2009 influenza A(H1N1)-associated ARDS treated with ECMO in 15 intensive care units (ICUs) in Australia and New Zealand between June 1 and August 31, 2009. MAIN OUTCOME MEASURES Incidence, clinical features, degree of pulmonary dysfunction, technical characteristics, duration of ECMO, complications, and survival. RESULTS Sixty-eight patients with severe influenza-associated ARDS were treated with ECMO, of whom 61 had either confirmed 2009 influenza A(H1N1) (n = 53) or influenza A not subtyped (n = 8), representing an incidence rate of 2.6 ECMO cases per million population. An additional 133 patients with influenza A received mechanical ventilation but no ECMO in the same ICUs. The 68 patients who received ECMO had a median (interquartile range [IQR]) age of 34.4 (26.6-43.1) years and 34 patients (50%) were men. Before ECMO, patients had severe respiratory failure despite advanced mechanical ventilatory support with a median (IQR) Pao(2)/fraction of inspired oxygen (Fio(2)) ratio of 56 (48-63), positive end-expiratory pressure of 18 (15-20) cm H(2)O, and an acute lung injury score of 3.8 (3.5-4.0). The median (IQR) duration of ECMO support was 10 (7-15) days. At the time of reporting, 48 of the 68 patients (71%; 95% confidence interval [CI], 60%-82%) had survived to ICU discharge, of whom 32 had survived to hospital discharge and 16 remained as hospital inpatients. Fourteen patients (21%; 95% CI, 11%-30%) had died and 6 remained in the ICU, 2 of whom were still receiving ECMO. CONCLUSIONS During June to August 2009 in Australia and New Zealand, the ICUs at regional referral centers provided mechanical ventilation for many patients with 2009 influenza A(H1N1)-associated respiratory failure, one-third of whom received ECMO. These ECMO-treated patients were often young adults with severe hypoxemia and had a 21% mortality rate at the end of the study period.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Andrew Davies',\n",
       "   'Daryl Jones',\n",
       "   'Michael Bailey',\n",
       "   'John Beca',\n",
       "   'Rinaldo Bellomo',\n",
       "   'Nikki Blackwell',\n",
       "   'Paul Forrest',\n",
       "   'David Gattas',\n",
       "   'Emily Granger',\n",
       "   'Robert Herkes',\n",
       "   'Andrew Jackson',\n",
       "   'Shay McGuinness',\n",
       "   'Priya Nair',\n",
       "   'Vincent Pellegrino',\n",
       "   'Ville Yrjo Olavi Pettila',\n",
       "   'Brian Plunkett',\n",
       "   'Roger Pye',\n",
       "   'Paul Torzillo',\n",
       "   'Steven Webb',\n",
       "   'Michael Wilson',\n",
       "   'Marc Ziegenfuss'],\n",
       "  'related_topics': ['Extracorporeal membrane oxygenation',\n",
       "   'Intensive care',\n",
       "   'Fraction of inspired oxygen',\n",
       "   'Respiratory failure',\n",
       "   'Lung injury',\n",
       "   'ARDS',\n",
       "   'Population',\n",
       "   'Mechanical ventilation',\n",
       "   'Emergency medicine',\n",
       "   'Surgery',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,802',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2111412754',\n",
       "  'title': 'Identification of a new human coronavirus',\n",
       "  'abstract': 'Three human coronaviruses are known to exist: human coronavirus 229E (HCoV-229E), HCoV-OC43 and severe acute respiratory syndrome (SARS)-associated coronavirus (SARS-CoV). Here we report the identification of a fourth human coronavirus, HCoV-NL63, using a new method of virus discovery. The virus was isolated from a 7-month-old child suffering from bronchiolitis and conjunctivitis. The complete genome sequence indicates that this virus is not a recombinant, but rather a new group 1 coronavirus. The in vitro host cell range of HCoV-NL63 is notable because it replicates on tertiary monkey kidney cells and the monkey kidney LLC-MK2 cell line. The viral genome contains distinctive features, including a unique N-terminal fragment within the spike protein. Screening of clinical specimens from individuals suffering from respiratory illness identified seven additional HCoV-NL63-infected individuals, indicating that the virus was widely spread within the human population.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Lia van der Hoek',\n",
       "   'Krzysztof Pyrc',\n",
       "   'Maarten F Jebbink',\n",
       "   'Wilma Vermeulen-Oost',\n",
       "   'Ron J M Berkhout',\n",
       "   'Katja C Wolthers',\n",
       "   'Pauline M E Wertheim-van Dillen',\n",
       "   'Jos Kaandorp',\n",
       "   'Joke Spaargaren',\n",
       "   'Ben Berkhout'],\n",
       "  'related_topics': ['Human coronavirus OC43',\n",
       "   'Human coronavirus 229E',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus NL63',\n",
       "   'Human coronavirus HKU1',\n",
       "   'Virus',\n",
       "   'Population',\n",
       "   'Alphacoronavirus',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,819',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '2129542667',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2134061616',\n",
       "   '2166229810',\n",
       "   '2163400707',\n",
       "   '2159857626',\n",
       "   '2029293367']},\n",
       " {'id': '1963953102',\n",
       "  'title': 'Fingerprinting genomes using PCR with arbitrary primers',\n",
       "  'abstract': 'Simple and reproducible fingerprints of complex genomes can be generated using single arbitrarily chosen primers and the polymerase chain reaction (PCR). No prior sequence information is required. The method, arbitrarily primed PCR (AP-PCR), involves two cycles of low stringency amplification followed by PCR at higher stringency. We show that strains can be distinguished by comparing polymorphisms in genomic fingerprints. The generality of the method is demonstrated by application to twenty four strains from five species of Staphylococcus, eleven strains of Streptococcus pyogenes and three varieties of Oryza sativa (rice).',\n",
       "  'date': '1990',\n",
       "  'authors': ['John Welsh', 'Michael McClelland'],\n",
       "  'related_topics': ['RAPD',\n",
       "   'DNA profiling',\n",
       "   'Polymerase chain reaction',\n",
       "   'Genomic organization',\n",
       "   'Primer (molecular biology)',\n",
       "   'Genome',\n",
       "   'Molecular probe',\n",
       "   'Oryza sativa',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,259',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2170933940',\n",
       "  'title': 'Characterization and Complete Genome Sequence of a Novel Coronavirus, Coronavirus HKU1, from Patients with Pneumonia',\n",
       "  'abstract': 'Despite extensive laboratory investigations in patients with respiratory tract infections, no microbiological cause can be identified in a significant proportion of patients. In the past 3 years, several novel respiratory viruses, including human metapneumovirus, severe acute respiratory syndrome (SARS) coronavirus (SARSCoV), and human coronavirus NL63, were discovered. Here we report the discovery of another novel coronavirus, coronavirus HKU1 (CoV-HKU1), from a 71-year-old man with pneumonia who had just returned from Shenzhen, China. Quantitative reverse transcription-PCR showed that the amount of CoV-HKU1 RNA was 8.5 to 9.6 10 6 copies per ml in his nasopharyngeal aspirates (NPAs) during the first week of the illness and dropped progressively to undetectable levels in subsequent weeks. He developed increasing serum levels of specific antibodies against the recombinant nucleocapsid protein of CoV-HKU1, with immunoglobulin M (IgM) titers of 1:20, 1:40, and 1:80 and IgG titers of <1:1,000, 1:2,000, and 1:8,000 in the first, second and fourth weeks of the illness, respectively. Isolation of the virus by using various cell lines, mixed neuron-glia culture, and intracerebral inoculation of suckling mice was unsuccessful. The complete genome sequence of CoV-HKU1 is a 29,926-nucleotide, polyadenylated RNA, with GC content of 32%, the lowest among all known coronaviruses with available genome sequence. Phylogenetic analysis reveals that CoV-HKU1 is a new group 2 coronavirus. Screening of 400 NPAs, negative for SARS-CoV, from patients with respiratory illness during the SARS period identified the presence of CoV-HKU1 RNA in an additional specimen, with a viral load of 1.13 10 6 copies per ml, from a 35-year-old woman with pneumonia. Our data support the existence of a novel group 2 coronavirus associated with pneumonia in humans.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Patrick C. Y. Woo',\n",
       "   'Susanna K. P. Lau',\n",
       "   'Chung-ming Chu',\n",
       "   'Kwok-hung Chan',\n",
       "   'Hoi-wah Tsoi',\n",
       "   'Yi Huang',\n",
       "   'Beatrice H. L. Wong',\n",
       "   'Rosana W. S. Poon',\n",
       "   'James J. Cai',\n",
       "   'Wei-kwang Luk',\n",
       "   'Leo L. M. Poon',\n",
       "   'Samson S. Y. Wong',\n",
       "   'Yi Guan',\n",
       "   'J. S. Malik Peiris',\n",
       "   'Kwok-yung Yuen'],\n",
       "  'related_topics': ['Human coronavirus OC43',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus HKU1',\n",
       "   'Human coronavirus NL63',\n",
       "   'Human metapneumovirus',\n",
       "   'Respiratory tract infections',\n",
       "   'Pneumonia',\n",
       "   'Coronaviridae',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,469',\n",
       "  'reference_count': '52',\n",
       "  'references': []},\n",
       " {'id': '2126707939',\n",
       "  'title': 'Severe acute respiratory syndrome',\n",
       "  'abstract': \"Severe acute respiratory syndrome (SARS) was caused by a previously unrecognized animal coronavirus that exploited opportunities provided by 'wet markets' in southern China to adapt to become a virus readily transmissible between humans. Hospitals and international travel proved to be 'amplifiers' that permitted a local outbreak to achieve global dimensions. In this review we will discuss the substantial scientific progress that has been made towards understanding the virus-SARS coronavirus (SARS-CoV)-and the disease. We will also highlight the progress that has been made towards developing vaccines and therapies The concerted and coordinated response that contained SARS is a triumph for global public health and provides a new paradigm for the detection and control of future emerging infectious disease threats.\",\n",
       "  'date': '2004',\n",
       "  'authors': ['J S M Peiris', 'Y Guan', 'K Y Yuen'],\n",
       "  'related_topics': ['Emerging infectious disease',\n",
       "   'Coronavirus',\n",
       "   'Global health',\n",
       "   'Disease',\n",
       "   'Outbreak',\n",
       "   'Public health',\n",
       "   'Vaccination',\n",
       "   'Scientific progress',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Immunology',\n",
       "   'View Less'],\n",
       "  'citation_count': '993',\n",
       "  'reference_count': '146',\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2129542667',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2147166346',\n",
       "   '2116586125',\n",
       "   '1966238900']},\n",
       " {'id': '2107277218',\n",
       "  'title': 'ANALYSIS OF RELATIVE GENE EXPRESSION DATA USING REAL-TIME QUANTITATIVE PCR AND THE 2(-DELTA DELTA C(T)) METHOD',\n",
       "  'abstract': 'The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2(-Delta Delta C(T)) method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2(-Delta Delta C(T)) method. In addition, we present the derivation and applications of two variations of the 2(-Delta Delta C(T)) method that may be useful in the analysis of real-time, quantitative PCR data.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Kenneth J. Livak', 'Thomas D. Schmittgen'],\n",
       "  'related_topics': ['Cell wall organization',\n",
       "   'Protein kinase B signaling',\n",
       "   'Lupeol synthase',\n",
       "   'Endosperm cellularization',\n",
       "   'Cell redox homeostasis',\n",
       "   'Cell wall modification',\n",
       "   'Female sex determination',\n",
       "   'Floral organ abscission',\n",
       "   'Molecular biology',\n",
       "   'Biology',\n",
       "   'Bioinformatics',\n",
       "   'View Less'],\n",
       "  'citation_count': '135,494',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2164578725',\n",
       "   '2109970232',\n",
       "   '2128088040',\n",
       "   '2145879504',\n",
       "   '1983241347',\n",
       "   '2123325948',\n",
       "   '2134343377',\n",
       "   '1756433044',\n",
       "   '1566892773',\n",
       "   '2069943574']},\n",
       " {'id': '2565805236',\n",
       "  'title': 'Middle East Respiratory Syndrome Coronavirus (MERS-CoV)',\n",
       "  'abstract': '',\n",
       "  'date': '2016',\n",
       "  'authors': ['Geethamma'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'citation_count': '632',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2791599184',\n",
       "  'title': 'Treatment of Middle East Respiratory Syndrome with a combination of lopinavir-ritonavir and interferon-β1b (MIRACLE trial): study protocol for a randomized controlled trial.',\n",
       "  'abstract': 'It had been more than 5\\xa0years since the first case of Middle East Respiratory Syndrome coronavirus infection (MERS-CoV) was recorded, but no specific treatment has been investigated in randomized clinical trials. Results from in vitro and animal studies suggest that a combination of lopinavir/ritonavir and interferon-β1b (IFN-β1b) may be effective against MERS-CoV. The aim of this study is to investigate the efficacy of treatment with a combination of lopinavir/ritonavir and recombinant IFN-β1b provided with standard supportive care, compared to treatment with placebo provided with standard supportive care in patients with laboratory-confirmed MERS requiring hospital admission. The protocol is prepared in accordance with the SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) guidelines. Hospitalized adult patients with laboratory-confirmed MERS will be enrolled in this recursive, two-stage, group sequential, multicenter, placebo-controlled, double-blind randomized controlled trial. The trial is initially designed to include 2 two-stage components. The first two-stage component is designed to adjust sample size and determine futility stopping, but not efficacy stopping. The second two-stage component is designed to determine efficacy stopping and possibly readjustment of sample size. The primary outcome is 90-day mortality. This will be the first randomized controlled trial of a potential treatment for MERS. The study is sponsored by King Abdullah International Medical Research Center, Riyadh, Saudi Arabia. Enrollment for this study began in November 2016, and has enrolled thirteen patients as of Jan 24-2018. ClinicalTrials.gov, ID: NCT02845843. Registered on 27 July 2016.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Yaseen M. Arabi',\n",
       "   '',\n",
       "   '',\n",
       "   'Adel Alothman',\n",
       "   '',\n",
       "   'Hanan H. Balkhy',\n",
       "   '',\n",
       "   'Abdulaziz Al-Dawood',\n",
       "   '',\n",
       "   'Sameera AlJohani',\n",
       "   '',\n",
       "   'Shmeylan Al Harbi',\n",
       "   '',\n",
       "   'Suleiman Kojan',\n",
       "   '',\n",
       "   'Majed Al Jeraisy',\n",
       "   '',\n",
       "   'Ahmad M. Deeb',\n",
       "   '',\n",
       "   'Abdullah M. Assiri',\n",
       "   'Fahad Al-Hameed',\n",
       "   '',\n",
       "   'Asim AlSaedi',\n",
       "   '',\n",
       "   'Yasser Mandourah',\n",
       "   'Ghaleb A. Almekhlafi',\n",
       "   'Nisreen Murad Sherbeeni',\n",
       "   'Fatehi Elnour Elzein',\n",
       "   'Javed Memon',\n",
       "   'Yusri Taha',\n",
       "   'Abdullah Almotairi',\n",
       "   'Khalid A. Maghrabi',\n",
       "   'Ismael Qushmaq',\n",
       "   'Ali Al Bshabshe',\n",
       "   'Ayman Kharaba',\n",
       "   'Sarah Shalhoub',\n",
       "   'Jesna Jose',\n",
       "   'Robert A. Fowler',\n",
       "   '',\n",
       "   'Frederick G. Hayden',\n",
       "   'Mohamed A. Hussein'],\n",
       "  'related_topics': ['Lopinavir/ritonavir',\n",
       "   'Randomized controlled trial',\n",
       "   'Lopinavir',\n",
       "   'Clinical trial',\n",
       "   'Ritonavir',\n",
       "   'Placebo',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '267',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2166867592',\n",
       "   '2145758369',\n",
       "   '2139937737',\n",
       "   '2591646177',\n",
       "   '1703839189',\n",
       "   '2034462612',\n",
       "   '1977050884',\n",
       "   '2586093485',\n",
       "   '2150120685',\n",
       "   '2345375456']},\n",
       " {'id': '2255243349',\n",
       "  'title': 'Coronaviruses - drug discovery and therapeutic options.',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which are caused by coronaviruses, have attracted substantial attention owing to their high mortality rates and potential to cause epidemics. Yuen and colleagues discuss progress with treatment options for these syndromes, including virus- and host-targeted drugs, and the challenges that need to be overcome in their further development. In humans, infections with the human coronavirus (HCoV) strains HCoV-229E, HCoV-OC43, HCoV-NL63 and HCoV-HKU1 usually result in mild, self-limiting upper respiratory tract infections, such as the common cold. By contrast, the CoVs responsible for severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which were discovered in Hong Kong, China, in 2003, and in Saudi Arabia in 2012, respectively, have received global attention over the past 12 years owing to their ability to cause community and health-care-associated outbreaks of severe infections in human populations. These two viruses pose major challenges to clinical management because there are no specific antiviral drugs available. In this Review, we summarize the epidemiology, virology, clinical features and current treatment strategies of SARS and MERS, and discuss the discovery and development of new virus-based and host-based therapeutic options for CoV infections.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Alimuddin Zumla',\n",
       "   'Jasper F. W. Chan',\n",
       "   'Esam I. Azhar',\n",
       "   'David S. C. Hui',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Respiratory tract infections',\n",
       "   'Common cold',\n",
       "   'Outbreak',\n",
       "   'Virus',\n",
       "   'Epidemiology',\n",
       "   'Virology',\n",
       "   'Drug discovery',\n",
       "   'Biology',\n",
       "   'Human coronavirus',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,197',\n",
       "  'reference_count': '299',\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '2138324310']},\n",
       " {'id': '2292021561',\n",
       "  'title': 'Therapeutic efficacy of the small molecule GS-5734 against Ebola virus in rhesus monkeys',\n",
       "  'abstract': 'The most recent Ebola virus outbreak in West Africa, which was unprecedented in the number of cases and fatalities, geographic distribution, and number of nations affected, highlights the need for safe, effective, and readily available antiviral agents for treatment and prevention of acute Ebola virus (EBOV) disease (EVD) or sequelae. No antiviral therapeutics have yet received regulatory approval or demonstrated clinical efficacy. Here we report the discovery of a novel small molecule GS-5734, a monophosphoramidate prodrug of an adenosine analogue, with antiviral activity against EBOV. GS-5734 exhibits antiviral activity against multiple variants of EBOV and other filoviruses in cell-based assays. The pharmacologically active nucleoside triphosphate (NTP) is efficiently formed in multiple human cell types incubated with GS-5734 in vitro, and the NTP acts as an alternative substrate and RNA-chain terminator in primer-extension assays using a surrogate respiratory syncytial virus RNA polymerase. Intravenous administration of GS-5734 to nonhuman primates resulted in persistent NTP levels in peripheral blood mononuclear cells (half-life, 14\\u2009h) and distribution to sanctuary sites for viral replication including testes, eyes, and brain. In a rhesus monkey model of EVD, once-daily intravenous administration of 10\\u2009mg\\u2009kg(-1) GS-5734 for 12 days resulted in profound suppression of EBOV replication and protected 100% of EBOV-infected animals against lethal disease, ameliorating clinical disease signs and pathophysiological markers, even when treatments were initiated three days after virus exposure when systemic viral RNA was detected in two out of six treated animals. These results show the first substantive post-exposure protection by a small-molecule antiviral compound against EBOV in nonhuman primates. The broad-spectrum antiviral activity of GS-5734 in vitro against other pathogenic RNA viruses, including filoviruses, arenaviruses, and coronaviruses, suggests the potential for wider medical use. GS-5734 is amenable to large-scale manufacturing, and clinical studies investigating the drug safety and pharmacokinetics are ongoing.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Travis K. Warren',\n",
       "   'Robert',\n",
       "   'Michael K. Lo',\n",
       "   'Adrian S.',\n",
       "   'Richard L.',\n",
       "   'Veronica Soloveva',\n",
       "   'Dustin',\n",
       "   'Michel',\n",
       "   'Roy',\n",
       "   'Hon C.',\n",
       "   'Nate',\n",
       "   'Robert',\n",
       "   'Jay Wells',\n",
       "   'Kelly S. Stuthman',\n",
       "   'Sean A. Van Tongeren',\n",
       "   'Nicole L. Garza',\n",
       "   'Ginger Donnelly',\n",
       "   'Amy C. Shurtleff',\n",
       "   'Cary J. Retterer',\n",
       "   'Dima Gharaibeh',\n",
       "   'Rouzbeh Zamani',\n",
       "   'Tara Kenny',\n",
       "   'Brett P. Eaton',\n",
       "   'Elizabeth Grimes',\n",
       "   'Lisa S. Welch',\n",
       "   'Laura Gomba',\n",
       "   'Catherine L. Wilhelmsen',\n",
       "   'Donald K. Nichols',\n",
       "   'Jonathan E. Nuss',\n",
       "   'Elyse R. Nagle',\n",
       "   'Jeffrey R. Kugelman',\n",
       "   'Gustavo Palacios',\n",
       "   'Edward',\n",
       "   'Sean',\n",
       "   'Ernest',\n",
       "   'Michael O.',\n",
       "   'Lijun',\n",
       "   'Willard',\n",
       "   'Bruce',\n",
       "   'Queenie',\n",
       "   'Kwon',\n",
       "   'Lydia',\n",
       "   'Darius',\n",
       "   'Yeojin',\n",
       "   'Kirsten M.',\n",
       "   'Iva',\n",
       "   'Joy Y.',\n",
       "   'Ona',\n",
       "   'Yili',\n",
       "   'Pamela Wong'],\n",
       "  'related_topics': ['Ebola virus',\n",
       "   'Virus',\n",
       "   'Viral pathogenesis',\n",
       "   'Viral replication',\n",
       "   'Ebolavirus',\n",
       "   'Drug development',\n",
       "   'Virology',\n",
       "   'Peripheral blood mononuclear cell',\n",
       "   'RNA',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '920',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2094993149',\n",
       "   '1971292277',\n",
       "   '2314681741',\n",
       "   '2326558924',\n",
       "   '277041599',\n",
       "   '2072202424',\n",
       "   '2117671399',\n",
       "   '2030160453',\n",
       "   '2138192885',\n",
       "   '1975876487']},\n",
       " {'id': '2290466312',\n",
       "  'title': 'A simple practice guide for dose conversion between animals and human.',\n",
       "  'abstract': 'Understanding the concept of extrapolation of dose between species is important for pharmaceutical researchers when initiating new animal or human experiments. Interspecies allometric scaling for dose conversion from animal to human studies is one of the most controversial areas in clinical pharmacology. Allometric approach considers the differences in body surface area, which is associated with animal weight while extrapolating the doses of therapeutic agents among the species. This review provides basic information about translation of doses between species and estimation of starting dose for clinical trials using allometric scaling. The method of calculation of injection volume for parenteral formulation based on human equivalent dose is also briefed.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Anroop B Nair', 'Shery Jacob'],\n",
       "  'related_topics': ['Clinical pharmacology',\n",
       "   'Body surface area',\n",
       "   'Clinical trial',\n",
       "   'Pharmacology',\n",
       "   'Biology',\n",
       "   'Dose conversion',\n",
       "   'Human studies',\n",
       "   'Injection volume',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,862',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2024938615',\n",
       "   '2100521244',\n",
       "   '2142366069',\n",
       "   '2065093669',\n",
       "   '2080335269',\n",
       "   '2034194552',\n",
       "   '2160483062',\n",
       "   '2057666951']},\n",
       " {'id': '2793022939',\n",
       "  'title': 'Coronavirus Susceptibility to the Antiviral Remdesivir (GS-5734) Is Mediated by the Viral Polymerase and the Proofreading Exoribonuclease',\n",
       "  'abstract': 'Emerging coronaviruses (CoVs) cause severe disease in humans, but no approved therapeutics are available. The CoV nsp14 exoribonuclease (ExoN) has complicated development of antiviral nucleosides due to its proofreading activity. We recently reported that the nucleoside analogue GS-5734 (remdesivir) potently inhibits human and zoonotic CoVs in vitro and in a severe acute respiratory syndrome coronavirus (SARS-CoV) mouse model. However, studies with GS-5734 have not reported resistance associated with GS-5734, nor do we understand the action of GS-5734 in wild-type (WT) proofreading CoVs. Here, we show that GS-5734 inhibits murine hepatitis virus (MHV) with similar 50% effective concentration values (EC50) as SARS-CoV and Middle East respiratory syndrome coronavirus (MERS-CoV). Passage of WT MHV in the presence of the GS-5734 parent nucleoside selected two mutations in the nsp12 polymerase at residues conserved across all CoVs that conferred up to 5.6-fold resistance to GS-5734, as determined by EC50 The resistant viruses were unable to compete with WT in direct coinfection passage in the absence of GS-5734. Introduction of the MHV resistance mutations into SARS-CoV resulted in the same in vitro resistance phenotype and attenuated SARS-CoV pathogenesis in a mouse model. Finally, we demonstrate that an MHV mutant lacking ExoN proofreading was significantly more sensitive to GS-5734. Combined, the results indicate that GS-5734 interferes with the nsp12 polymerase even in the setting of intact ExoN proofreading activity and that resistance can be overcome with increased, nontoxic concentrations of GS-5734, further supporting the development of GS-5734 as a broad-spectrum therapeutic to protect against contemporary and emerging CoVs.IMPORTANCE Coronaviruses (CoVs) cause severe human infections, but there are no approved antivirals to treat these infections. Development of nucleoside-based therapeutics for CoV infections has been hampered by the presence of a proofreading exoribonuclease. Here, we expand the known efficacy of the nucleotide prodrug remdesivir (GS-5734) to include a group β-2a CoV. Further, GS-5734 potently inhibits CoVs with intact proofreading. Following selection with the GS-5734 parent nucleoside, 2 amino acid substitutions in the nsp12 polymerase at residues that are identical across CoVs provide low-level resistance to GS-5734. The resistance mutations decrease viral fitness of MHV in vitro and attenuate pathogenesis in a SARS-CoV animal model of infection. Together, these studies define the target of GS-5734 activity and demonstrate that resistance is difficult to select, only partial, and impairs fitness and virulence of MHV and SARS-CoV, supporting further development of GS-5734 as a potential effective pan-CoV antiviral.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Maria L. Agostini',\n",
       "   'Erica L. Andres',\n",
       "   'Amy C Sims',\n",
       "   'Rachel Lauren Graham',\n",
       "   'Timothy Patrick Sheahan',\n",
       "   'Xiaotao Lu',\n",
       "   'Everett Clinton Smith',\n",
       "   '',\n",
       "   'James Brett Case',\n",
       "   'Joy Y. Feng',\n",
       "   'Robert Jordan',\n",
       "   'Adrian S. Ray',\n",
       "   'Tomas Cihlar',\n",
       "   'Dustin Siegel',\n",
       "   'Richard L. Mackman',\n",
       "   'Michael O. Clarke',\n",
       "   'Ralph S Baric',\n",
       "   'Mark R. Denison'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Proofreading',\n",
       "   'Nucleoside analogue',\n",
       "   'Polymerase',\n",
       "   'Viral replication',\n",
       "   'Virus',\n",
       "   'Mutation',\n",
       "   'Exoribonuclease',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '767',\n",
       "  'reference_count': '58',\n",
       "  'references': ['2166867592',\n",
       "   '2104548316',\n",
       "   '2725497285',\n",
       "   '2195009776',\n",
       "   '311927316',\n",
       "   '2255243349',\n",
       "   '2292021561',\n",
       "   '2115555188',\n",
       "   '2298153446',\n",
       "   '2586093485']},\n",
       " {'id': '2100820722',\n",
       "  'title': 'Identification of Severe Acute Respiratory Syndrome in Canada',\n",
       "  'abstract': 'background Severe acute respiratory syndrome (SARS) is a condition of unknown cause that has recently been recognized in patients in Asia, North America, and Europe. This report summarizes the initial epidemiologic findings, clinical description, and diagnostic findings that followed the identification of SARS in Canada. methods SARS was first identified in Canada in early March 2003. We collected epidemiologic, clinical, and diagnostic data from each of the first 10 cases prospectively as they were identified. Specimens from all cases were sent to local, provincial, national, and international laboratories for studies to identify an etiologic agent. results The patients ranged from 24 to 78 years old; 60 percent were men. Transmission occurred only after close contact. The most common presenting symptoms were fever (in 100 percent of cases) and malaise (in 70 percent), followed by nonproductive cough (in 100 percent) and dyspnea (in 80 percent) associated with infiltrates on chest radiography (in 100 percent). Lymphopenia (in 89 percent of those for whom data were available), elevated lactate dehydrogenase levels (in 80 percent), elevated aspartate aminotransferase levels (in 78 percent), and elevated creatinine kinase levels (in 56 percent) were common. Empirical therapy most commonly included antibiotics, oseltamivir, and intravenous ribavirin. Mechanical ventilation was required in five patients. Three patients died, and five have had clinical improvement. The results of laboratory investigations were negative or not clinically significant except for the amplification of human metapneumovirus from respiratory specimens from five of nine patients and the isolation and amplification of a novel coronavirus from five of nine patients. In four cases both pathogens were isolated. conclusions SARS is a condition associated with substantial morbidity and mortality. It appears to be of viral origin, with patterns suggesting droplet or contact transmission. The role of human metapneumovirus, a novel coronavirus, or both requires further investigation.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Susan M. Poutanen',\n",
       "   '',\n",
       "   'Donald E. Low',\n",
       "   'Bonnie Henry',\n",
       "   'Sandy Finkelstein',\n",
       "   'David Rose',\n",
       "   'Karen Green',\n",
       "   'Raymond Tellier',\n",
       "   '',\n",
       "   'Ryan Draker',\n",
       "   'Dena Adachi',\n",
       "   'Melissa Ayers',\n",
       "   'Adrienne K. Chan',\n",
       "   'Danuta M. Skowronski',\n",
       "   'Irving Salit',\n",
       "   'Andrew E. Simor',\n",
       "   'Arthur S. Slutsky',\n",
       "   'Patrick W. Doyle',\n",
       "   'Mel Krajden',\n",
       "   'Martin Petric',\n",
       "   'Robert C. Brunham',\n",
       "   'Allison J. McGeer'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Human metapneumovirus',\n",
       "   'Epidemiology',\n",
       "   'Respiratory disease',\n",
       "   'Coronavirus',\n",
       "   'Contact tracing',\n",
       "   'Ribavirin',\n",
       "   'Oseltamivir',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Surgery',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,769',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2597070792',\n",
       "   '2161328469',\n",
       "   '2170881661',\n",
       "   '2093852073',\n",
       "   '2463755683',\n",
       "   '2152552492',\n",
       "   '2135259291',\n",
       "   '2136166622',\n",
       "   '2097665403',\n",
       "   '1898899939']},\n",
       " {'id': '2125251240',\n",
       "  'title': 'A cluster of cases of severe acute respiratory syndrome in Hong Kong.',\n",
       "  'abstract': 'Background Information on the clinical features of the severe acute respiratory syndrome (SARS) will be of value to physicians caring for patients suspected of having this disorder. Methods We abstracted data on the clinical presentation and course of disease in 10 epidemiologically linked Chinese patients (5 men and 5 women 38 to 72 years old) in whom SARS was diagnosed between February 22, 2003, and March 22, 2003, at our hospitals in Hong Kong, China. Results Exposure between the source patient and subsequent patients ranged from minimal to that between patient and health care provider. The incubation period ranged from 2 to 11 days. All patients presented with fever (temperature, >38°C for over 24 hours), and most presented with rigor, dry cough, dyspnea, malaise, headache, and hypoxemia. Physical examination of the chest revealed crackles and percussion dullness. Lymphopenia was observed in nine patients, and most patients had mildly elevated aminotransferase levels but normal serum creatinine levels...',\n",
       "  'date': '2003',\n",
       "  'authors': ['Kenneth W. Tsang',\n",
       "   'Pak L.',\n",
       "   'Gaik C.',\n",
       "   'Wilson K.',\n",
       "   'Teresa',\n",
       "   'Moira',\n",
       "   'Wah K.',\n",
       "   'Wing H.',\n",
       "   'Loretta Y.',\n",
       "   'Thomas M.',\n",
       "   'Poon C.',\n",
       "   'Bing',\n",
       "   'Mary S.',\n",
       "   'Jane',\n",
       "   'Kwok Y.',\n",
       "   'Kar N.'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Crackles',\n",
       "   'Physical examination',\n",
       "   'Hypoxemia',\n",
       "   'Respiratory disease',\n",
       "   'Malaise',\n",
       "   'Pediatrics',\n",
       "   'Pharmacotherapy',\n",
       "   'Medicine',\n",
       "   'Creatinine',\n",
       "   'Surgery',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,587',\n",
       "  'reference_count': '4',\n",
       "  'references': ['1607298558', '1982444609', '2041775285', '2132293969']},\n",
       " {'id': '2107922358',\n",
       "  'title': 'Rapid detection and quantification of RNA of Ebola and Marburg viruses, Lassa virus, Crimean-Congo hemorrhagic fever virus, Rift Valley fever virus, dengue virus, and yellow fever virus by real-time reverse transcription-PCR.',\n",
       "  'abstract': 'Viral hemorrhagic fevers (VHFs) are acute infections with high case fatality rates. Important VHF agents are Ebola and Marburg viruses (MBGV/EBOV), Lassa virus (LASV), Crimean-Congo hemorrhagic fever virus (CCHFV), Rift Valley fever virus (RVFV), dengue virus (DENV), and yellow fever virus (YFV). VHFs are clinically difficult to diagnose and to distinguish; a rapid and reliable laboratory diagnosis is required in suspected cases. We have established six one-step, real-time reverse transcription-PCR assays for these pathogens based on the Superscript reverse transcriptase-Platinum Taq polymerase enzyme mixture. Novel primers and/or 5′-nuclease detection probes were designed for RVFV, DENV, YFV, and CCHFV by using the latest DNA database entries. PCR products were detected in real time on a LightCycler instrument by using 5′-nuclease technology (RVFV, DENV, and YFV) or SybrGreen dye intercalation (MBGV/EBOV, LASV, and CCHFV). The inhibitory effect of SybrGreen on reverse transcription was overcome by initial immobilization of the dye in the reaction capillaries. Universal cycling conditions for SybrGreen and 5′-nuclease probe detection were established. Thus, up to three assays could be performed in parallel, facilitating rapid testing for several pathogens. All assays were thoroughly optimized and validated in terms of analytical sensitivity by using in vitro-transcribed RNA. The ≥95% detection limits as determined by probit regression analysis ranged from 1,545 to 2,835 viral genome equivalents/ml of serum (8.6 to 16 RNA copies per assay). The suitability of the assays was exemplified by detection and quantification of viral RNA in serum samples of VHF patients.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Christian Drosten',\n",
       "   'Stephan Göttig',\n",
       "   'Stefan Schilling',\n",
       "   'Marcel Asper',\n",
       "   'Marcus Panning',\n",
       "   'Herbert Schmitz',\n",
       "   'Stephan Günther'],\n",
       "  'related_topics': ['Lassa virus',\n",
       "   'Phlebovirus',\n",
       "   'Dengue virus',\n",
       "   'Ebola virus',\n",
       "   'Flavivirus',\n",
       "   'Arenavirus',\n",
       "   'Marburgvirus',\n",
       "   'Crimean Congo hemorrhagic fever virus',\n",
       "   'Virology',\n",
       "   'Microbiology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '786',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2047480444',\n",
       "   '1994091239',\n",
       "   '2070721758',\n",
       "   '2131589770',\n",
       "   '2149579937',\n",
       "   '2137089963',\n",
       "   '2163760194',\n",
       "   '2134971582',\n",
       "   '2171308211',\n",
       "   '2798078005']},\n",
       " {'id': '2127062009',\n",
       "  'title': 'Viruses and Bacteria in the Etiology of the Common Cold',\n",
       "  'abstract': 'Two hundred young adults with common colds were studied during a 10-month period. Virus culture, antigen detection, PCR, and serology with paired samples were used to identify the infection. Viral etiology was established for 138 of the 200 patients (69%). Rhinoviruses were detected in 105 patients, coronavirus OC43 or 229E infection was detected in 17, influenza A or B virus was detected in 12, and single infections with parainfluenza virus, respiratory syncytial virus, adenovirus, and enterovirus were found in 14 patients. Evidence for bacterial infection was found in seven patients. Four patients had a rise in antibodies against Chlamydia pneumoniae, one had a rise in antibodies against Haemophilus influenzae, one had a rise in antibodies against Streptococcus pneumoniae, and one had immunoglobulin M antibodies against Mycoplasma pneumoniae. The results show that although approximately 50% of episodes of the common cold were caused by rhinoviruses, the etiology can vary depending on the epidemiological situation with regard to circulating viruses. Bacterial infections were rare, supporting the concept that the common cold is almost exclusively a viral disease.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Mika J. Mäkelä',\n",
       "   'Tuomo Puhakka',\n",
       "   'Olli Ruuskanen',\n",
       "   'Maija Leinonen',\n",
       "   'Pekka Saikku',\n",
       "   'Marko Kimpimäki',\n",
       "   'Soile Blomqvist',\n",
       "   'Timo Hyypiä',\n",
       "   'Pertti Arstila'],\n",
       "  'related_topics': ['Viral culture',\n",
       "   'Serology',\n",
       "   'Rhinovirus',\n",
       "   'Coronavirus',\n",
       "   'Mycoplasma pneumoniae',\n",
       "   'Common cold',\n",
       "   'Virus',\n",
       "   'Enterovirus',\n",
       "   'Virology',\n",
       "   'Microbiology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '886',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2055750915',\n",
       "   '2337555053',\n",
       "   '2098388207',\n",
       "   '163073849',\n",
       "   '2032842024',\n",
       "   '1856165804',\n",
       "   '2018812376',\n",
       "   '2146133178',\n",
       "   '1830634530',\n",
       "   '1699035432']},\n",
       " {'id': '2084994773',\n",
       "  'title': 'Phylogenetic analysis of a highly conserved region of the polymerase gene from 11 coronaviruses and development of a consensus polymerase chain reaction assay.',\n",
       "  'abstract': 'Viruses in the genus Coronavirus are currently placed in three groups based on antigenic cross-reactivity and sequence analysis of structural protein genes. Consensus polymerase chain reaction (PCR) primers were used to obtain cDNA, then cloned and sequenced a highly conserved 922 nucleotide region in open reading frame (ORF) 1b of the polymerase (pol) gene from eight coronaviruses. These sequences were compared with published sequences for three additional coronaviruses. In this comparison, it was found that nucleotide substitution frequencies (per 100 nucleotides) varied from 46.40 to 50.13 when viruses were compared among the traditional coronavirus groups and, with one exception (the human coronavirus (HCV) 229E), varied from 2.54 to 15.89 when compared within these groups. (The substitution frequency for 229E, as compared to other members of the same group, varied from 35.37 to 35.72.) Phylogenetic analysis of these pol gene sequences resulted in groupings which correspond closely with the previously described groupings, including recent data which places the two avian coronaviruses--infectious bronchitis virus (IBV) of chickens and turkey coronavirus (TCV)--in the same group [Guy, J.S., Barnes, H.J., Smith L.G., Breslin, J., 1997. Avian Dis. 41:583-590]. A single pair of degenerate primers was identified which amplify a 251 bp region from coronaviruses of all three groups using the same reaction conditions. This consensus PCR assay for the genus Coronavirus may be useful in identifying as yet unknown coronaviruses.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Charles B. Stephensen',\n",
       "   'Donald B. Casebolt',\n",
       "   'Nupur N. Gangopadhyay'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Consensus PCR',\n",
       "   'Turkey coronavirus',\n",
       "   'Sequence analysis',\n",
       "   'Polymerase Gene',\n",
       "   'Conserved sequence',\n",
       "   'Polymerase',\n",
       "   'Sequence alignment',\n",
       "   'Genetics',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '230',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2134812217',\n",
       "   '2009310436',\n",
       "   '132455992',\n",
       "   '2156596665',\n",
       "   '1582561043',\n",
       "   '2149495938',\n",
       "   '2087363345',\n",
       "   '2329318335',\n",
       "   '3011200155',\n",
       "   '1994193749']},\n",
       " {'id': '2149579937',\n",
       "  'title': 'Imported Lassa fever in Germany: molecular characterization of a new Lassa virus strain.',\n",
       "  'abstract': 'We describe the isolation and characterization of a new Lassa virus strain imported into Germany by a traveler who had visited Ghana, Cote D\\'Ivoire, and Burkina Faso. This strain, designated \"AV,\" originated from a region in West Africa where Lassa fever has not been reported. Viral S RNA isolated from the patient\\'s serum was amplified and sequenced. A long-range reverse transcription polymerase chain reaction allowed amplification of the full-length (3.4 kb) S RNA. The coding sequences of strain AV differed from those of all known Lassa prototype strains (Josiah, Nigeria, and LP) by approximately 20%, mainly at third codon positions. Phylogenetically, strain AV appears to be most closely related to strain Josiah from Sierra Leone. Lassa viruses comprise a group of genetically highly diverse strains, which has implications for vaccine development. The new method for full-length S RNA amplification may facilitate identification and molecular analysis of new arenaviruses or arenavirus strains.',\n",
       "  'date': '2000',\n",
       "  'authors': ['S Günther', 'P', 'T', 'O', 'M', 'A', 'T', 'J ter', 'H'],\n",
       "  'related_topics': ['Lassa fever',\n",
       "   'Lassa virus',\n",
       "   'Arenavirus',\n",
       "   'Sierra leone',\n",
       "   'Strain (biology)',\n",
       "   'RNA',\n",
       "   'Virology',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Vero cell',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '211',\n",
       "  'reference_count': '40',\n",
       "  'references': ['2154128645',\n",
       "   '1967150940',\n",
       "   '2044967254',\n",
       "   '2171229191',\n",
       "   '2163443142',\n",
       "   '1845818816',\n",
       "   '2326037208',\n",
       "   '2109638242',\n",
       "   '1941129807',\n",
       "   '1509033271']},\n",
       " {'id': '2090060897',\n",
       "  'title': 'Evaluation of concurrent shedding of bovine coronavirus via the respiratory tract and enteric route in feedlot cattle.',\n",
       "  'abstract': 'Objective—To assess the relationship between shedding of bovine coronavirus (BCV) via the respiratory tract and enteric routes and the association with weight gain in feedlot cattle. Animals—56 crossbred steers. Procedures—Paired fecal samples and nasal swab specimens were obtained and were tested for BCV, using antigen-capture ELISA. Paired serum samples obtained were tested for antibodies to BCV, using antibody-detection ELISA. Information was collected on weight gain, clinical signs, and treatments for enteric and respiratory tract disease during the study period. Results—Number of samples positive for bovine respiratory coronavirus (BRCV) or bovine enteric coro navirus (BECV) was 37/224 (17%) and 48/223 (22%), respectively. Some cattle (25/46, 45%) shed BECV and BRCV. There were 25/29 (86%) cattle positive for BECV that shed BRCV, but only 1/27 (4%) cattle negative to BECV shed BRCV. Twenty-seven of 48 (56%) paired nasal swab specimens and fecal samples positive for BECV were positive for BRCV. In con...',\n",
       "  'date': '2001',\n",
       "  'authors': ['Kyoung Oh Cho',\n",
       "   'Armando E. Hoet',\n",
       "   'Steven C. Loerch',\n",
       "   'Thomas E. Wittum',\n",
       "   'Linda J. Saif'],\n",
       "  'related_topics': ['Bovine coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Feces',\n",
       "   'Nasal Swab',\n",
       "   'Respiratory tract',\n",
       "   'Respiratory system',\n",
       "   'Antibody',\n",
       "   'Weight gain',\n",
       "   'Veterinary medicine',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '104',\n",
       "  'reference_count': '31',\n",
       "  'references': ['50597173',\n",
       "   '2072381230',\n",
       "   '2409643934',\n",
       "   '1987051173',\n",
       "   '2045765248',\n",
       "   '1963591796',\n",
       "   '2406151794',\n",
       "   '2032346601',\n",
       "   '1979854169',\n",
       "   '2302897180']},\n",
       " {'id': '2004869546',\n",
       "  'title': 'TTV a common virus, but pathogenic?',\n",
       "  'abstract': '',\n",
       "  'date': '1998',\n",
       "  'authors': ['Yvonne Cossart'],\n",
       "  'related_topics': ['Virus', 'Virology', 'Medicine'],\n",
       "  'citation_count': '60',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2038264706',\n",
       "   '2089551619',\n",
       "   '2016137045',\n",
       "   '2328399749',\n",
       "   '2022277835',\n",
       "   '2028973331']},\n",
       " {'id': '2030133843',\n",
       "  'title': 'HGV: hepatitis G virus or harmless G virus?',\n",
       "  'abstract': 'The discovery of the hepatitis G virus (HGV) has given hepatologists a new lease on life. Just when they were becoming frustrated with the slow rate of progress in unravell ing the pathobiological consequences of hepatitis B and C virus infections, along comes another candidate virus. HGV, a single-stranded ribonucleic acid (RNA) virus that belongs to the Flaviviridae family, has a global distribution. The virus is present in 1-2% of blood donors in the USA, a frequency higher than that of either HCV or hepatitis B virus (HBV) (Alter). Even more striking is the seroprevalence of 15\"2% reported in West African residents (JMed Virol 1996; 50: 97). HGVexis t s in a chronic carrier state. The virus is transmitted parenterally and is often present in patients who have received multiple transfusions or who are on haemodialysis (N Engl J Med 1996; 334: 1485), and in intravenous drug users. There is preliminary evidence for perinatal transmission (Lancet 1996; 347: 615). HGV RNA sequences have been identified in serum from patients with non-A-E acute and chronic hepatitis and cirrhosis. Impressive data comes from Brescia, Italy, where 35% of patients with acute hepatitis and 39% of those with chronic hepatitis were positive for HGV RNA (Fiordalisi). Among blood donors the virus is more common in those with raised serum aminotransferase concentrations (3\"9%) than in those with normal concentrations (0\"8%) (ff Med Virol 1996; 50: 97). These findings imply that HGV is a human pathogen, but is it? Other information is more consistent with HGV being an innocent passenger. The great majority of individuals who become HGV-RNA positive after blood transfusion have normal serum aminotronsperase concentrations and neither they, nor those found positive for HGV RNA in other circumstances, develop liver disease during prolonged follow-up (Alter). Moreover, when serum enzyme concentrations are raised they seldom accord with levels of viraemia. HGV and HCV are often, and HGV and HBV less often, found together in serum. In those coinfected with HGV and HCV, aminotransterases run parallel to HCV rather than HGV, and the presence of the latter seems to have no effect on outcome. HGV usually accounts for only a minority of cases of acute non-A-E hepatitis, and there is no evidence yet of progression over time to chronic hepatitis, cirrhosis, or hepatocellular',\n",
       "  'date': '1996',\n",
       "  'authors': ['Michael C Kew', 'Chris Kassianides'],\n",
       "  'related_topics': ['Hepatitis B',\n",
       "   'Hepatitis B virus',\n",
       "   'Hepatitis',\n",
       "   'Flaviviridae',\n",
       "   'Liver disease',\n",
       "   'Virus',\n",
       "   'Cirrhosis',\n",
       "   'Blood transfusion',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '20',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2083266836',\n",
       "   '2313004219',\n",
       "   '2023962288',\n",
       "   '2155517838',\n",
       "   '2075432722',\n",
       "   '1984200234']},\n",
       " {'id': '1967300023',\n",
       "  'title': 'Acute Kidney Injury Network: Report of an Initiative to Improve Outcomes in Acute Kidney Injury',\n",
       "  'abstract': \"Acute kidney injury (AKI) is a complex disorder for which currently there is no accepted definition. Having a uniform standard for diagnosing and classifying AKI would enhance our ability to manage these patients. Future clinical and translational research in AKI will require collaborative networks of investigators drawn from various disciplines, dissemination of information via multidisciplinary joint conferences and publications, and improved translation of knowledge from pre-clinical research. We describe an initiative to develop uniform standards for defining and classifying AKI and to establish a forum for multidisciplinary interaction to improve care for patients with or at risk for AKI. Members representing key societies in critical care and nephrology along with additional experts in adult and pediatric AKI participated in a two day conference in Amsterdam, The Netherlands, in September 2005 and were assigned to one of three workgroups. Each group's discussions formed the basis for draft recommendations that were later refined and improved during discussion with the larger group. Dissenting opinions were also noted. The final draft recommendations were circulated to all participants and subsequently agreed upon as the consensus recommendations for this report. Participating societies endorsed the recommendations and agreed to help disseminate the results. The term AKI is proposed to represent the entire spectrum of acute renal failure. Diagnostic criteria for AKI are proposed based on acute alterations in serum creatinine or urine output. A staging system for AKI which reflects quantitative changes in serum creatinine and urine output has been developed. We describe the formation of a multidisciplinary collaborative network focused on AKI. We have proposed uniform standards for diagnosing and classifying AKI which will need to be validated in future studies. The Acute Kidney Injury Network offers a mechanism for proceeding with efforts to improve patient outcomes.\",\n",
       "  'date': '2007',\n",
       "  'authors': ['Ravindra L Mehta',\n",
       "   'John A Kellum',\n",
       "   'Sudhir V Shah',\n",
       "   'Bruce A Molitoris',\n",
       "   'Claudio Ronco',\n",
       "   'David G Warnock',\n",
       "   'Adeera Levin'],\n",
       "  'related_topics': ['Renal angina',\n",
       "   'Acute kidney injury',\n",
       "   'Translational research',\n",
       "   'Intensive care medicine',\n",
       "   'Nephrology',\n",
       "   'MEDLINE',\n",
       "   'Multidisciplinary approach',\n",
       "   'Medicine',\n",
       "   'Collaborative network',\n",
       "   'Future studies',\n",
       "   'Internal medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,411',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2139937737',\n",
       "   '2149687213',\n",
       "   '2131419242',\n",
       "   '1489794536',\n",
       "   '2133482423',\n",
       "   '2116909658',\n",
       "   '2043132544',\n",
       "   '2069722312',\n",
       "   '2009995285',\n",
       "   '2073859061']},\n",
       " {'id': '2131419242',\n",
       "  'title': 'Acute Kidney Injury, Mortality, Length of Stay, and Costs in Hospitalized Patients',\n",
       "  'abstract': 'The marginal effects of acute kidney injury on in-hospital mortality, length of stay (LOS), and costs have not been well described. A consecutive sample of 19,982 adults who were admitted to an urban academic medical center, including 9210 who had two or more serum creatinine (SCr) determinations, was evaluated. The presence and degree of acute kidney injury were assessed using absolute and relative increases from baseline to peak SCr concentration during hospitalization. Large increases in SCr concentration were relatively rare (e.g., >or=2.0 mg/dl in 105 [1%] patients), whereas more modest increases in SCr were common (e.g., >or=0.5 mg/dl in 1237 [13%] patients). Modest changes in SCr were significantly associated with mortality, LOS, and costs, even after adjustment for age, gender, admission International Classification of Diseases, Ninth Revision, Clinical Modification diagnosis, severity of illness (diagnosis-related group weight), and chronic kidney disease. For example, an increase in SCr >or=0.5 mg/dl was associated with a 6.5-fold (95% confidence interval 5.0 to 8.5) increase in the odds of death, a 3.5-d increase in LOS, and nearly 7500 dollars in excess hospital costs. Acute kidney injury is associated with significantly increased mortality, LOS, and costs across a broad spectrum of conditions. Moreover, outcomes are related directly to the severity of acute kidney injury, whether characterized by nominal or percentage changes in serum creatinine.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Glenn M. Chertow',\n",
       "   'Elisabeth',\n",
       "   'Melissa',\n",
       "   'Joseph V.',\n",
       "   'David W.'],\n",
       "  'related_topics': ['Kidney disease',\n",
       "   'Acute kidney injury',\n",
       "   'Nephrology',\n",
       "   'Severity of illness',\n",
       "   'Renal angina',\n",
       "   'Creatinine',\n",
       "   'Confidence interval',\n",
       "   'Epidemiology',\n",
       "   'Internal medicine',\n",
       "   'Surgery',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,403',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2157825442',\n",
       "   '1992332433',\n",
       "   '2133482423',\n",
       "   '1996020381',\n",
       "   '1550111394',\n",
       "   '2072075701',\n",
       "   '2029723446',\n",
       "   '2153767046',\n",
       "   '2154145185',\n",
       "   '2023644538']},\n",
       " {'id': '2143432233',\n",
       "  'title': 'A comparison of albumin and saline for fluid resuscitation in the intensive care unit',\n",
       "  'abstract': 'background It remains uncertain whether the choice of resuscitation fluid for patients in intensive care units (ICUs) affects survival. We conducted a multicenter, randomized, double-blind trial to compare the effect of fluid resuscitation with albumin or saline on mortality in a heterogeneous population of patients in the ICU. methods We randomly assigned patients who had been admitted to the ICU to receive either 4 percent albumin or normal saline for intravascular-fluid resuscitation during the next 28 days. The primary outcome measure was death from any cause during the 28-day period after randomization. results Of the 6997 patients who underwent randomization, 3497 were assigned to receive albumin and 3500 to receive saline; the two groups had similar baseline characteristics. There were 726 deaths in the albumin group, as compared with 729 deaths in the saline group (relative risk of death, 0.99; 95 percent confidence interval, 0.91 to 1.09; P=0.87). The proportion of patients with new single-organ and multiple-organ failure was similar in the two groups (P=0.85). There were no significant differences between the groups in the mean (±SD) numbers of days spent in the ICU (6.5±6.6 in the albumin group and 6.2±6.2 in the saline group, P=0.44), days spent in the hospital (15.3±9.6 and 15.6±9.6, respectively; P = 0.30), days of mechanical ventilation (4.5±6.1 and 4.3±5.7, respectively; P = 0.74), or days of renal-replacement therapy (0.5±2.3 and 0.4±2.0, respectively; P = 0.41). conclusions In patients in the ICU, use of either 4 percent albumin or normal saline for fluid resuscitation results in similar outcomes at 28 days.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Rinaldo Bellomo', 'Julie French', 'John'],\n",
       "  'related_topics': ['Resuscitation',\n",
       "   'Intensive care',\n",
       "   'Intensive care unit',\n",
       "   'Saline',\n",
       "   'Randomization',\n",
       "   'Mechanical ventilation',\n",
       "   'Randomized controlled trial',\n",
       "   'Relative risk',\n",
       "   'Anesthesia',\n",
       "   'Surgery',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,138',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2768146862',\n",
       "   '2107978811',\n",
       "   '127034668',\n",
       "   '2161328469',\n",
       "   '1991864206',\n",
       "   '2599527603',\n",
       "   '1554783366',\n",
       "   '2112577081',\n",
       "   '1603121691',\n",
       "   '2046852559']},\n",
       " {'id': '2117958746',\n",
       "  'title': 'Intensity of renal support in critically ill patients with acute kidney injury',\n",
       "  'abstract': 'We randomly assigned critically ill patients with acute kidney injury and failure of at least one nonrenal organ or sepsis to receive intensive or less intensive renal-replacement therapy. The primary end point was death from any cause by day 60. In both study groups, hemodynamically stable patients underwent intermittent hemodialysis, and hemodynamically unstable patients underwent continuous venovenous hemodiafiltration or sustained low-efficiency dialysis. Patients receiving the intensive treatment strategy underwent intermittent hemodialysis and sustained low-efficiency dialysis six times per week and continuous venovenous hemodiafiltration at 35 ml per kilogram of body weight per hour; for patients receiving the less-intensive treatment strategy, the corresponding treatments were provided thrice weekly and at 20 ml per kilogram per hour. Results Baseline characteristics of the 1124 patients in the two groups were similar. The rate of death from any cause by day 60 was 53.6% with intensive therapy and 51.5% with less-intensive therapy (odds ratio, 1.09; 95% confidence interval, 0.86 to 1.40; P = 0.47). There was no significant difference between the two groups in the duration of renalreplacement therapy or the rate of recovery of kidney function or nonrenal organ failure. Hypotension during intermittent dialysis occurred in more patients randomly assigned to receive intensive therapy, although the frequency of hemodialysis sessions complicated by hypotension was similar in the two groups. Conclusions Intensive renal support in critically ill patients with acute kidney injury did not decrease mortality, improve recovery of kidney function, or reduce the rate of nonrenal organ failure as compared with less-intensive therapy involving a defined dose of intermittent hemodialysis three times per week and continuous renal-replacement therapy at 20 ml per kilogram per hour. (ClinicalTrials.gov number, NCT00076219.)',\n",
       "  'date': '2008',\n",
       "  'authors': ['Paul M. Palevsky',\n",
       "   'Jane Hongyuan',\n",
       "   'Theresa Z.',\n",
       "   'Glenn M.',\n",
       "   'Susan T.',\n",
       "   'Devasmita',\n",
       "   'Kevin',\n",
       "   'John A.',\n",
       "   'Emil',\n",
       "   'Roland M.H.',\n",
       "   'Mark W.',\n",
       "   'Kathleen M.',\n",
       "   'B. Taylor',\n",
       "   'Anitha',\n",
       "   'Suzanne',\n",
       "   'Robert A.',\n",
       "   'Peter',\n",
       "   'E.',\n",
       "   'R.',\n",
       "   'W.',\n",
       "   'U.',\n",
       "   'K.',\n",
       "   'A.',\n",
       "   'N.',\n",
       "   'J.',\n",
       "   'P.',\n",
       "   'D.',\n",
       "   'D.',\n",
       "   'L.',\n",
       "   'J.',\n",
       "   'J.',\n",
       "   'M.',\n",
       "   'R.',\n",
       "   'A.',\n",
       "   'A.',\n",
       "   'G.',\n",
       "   'V.',\n",
       "   'G.',\n",
       "   'R.',\n",
       "   'M.',\n",
       "   'K.',\n",
       "   'Q.',\n",
       "   'M.',\n",
       "   'M.',\n",
       "   'M.',\n",
       "   'K.',\n",
       "   'T.',\n",
       "   'E.',\n",
       "   'J.',\n",
       "   'A. Felsenfeld'],\n",
       "  'related_topics': ['Dialysis',\n",
       "   'Acute kidney injury',\n",
       "   'Hemodialysis',\n",
       "   'Renal function',\n",
       "   'Sepsis',\n",
       "   'Randomized controlled trial',\n",
       "   'Odds ratio',\n",
       "   'Surgery',\n",
       "   'Confidence interval',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,029',\n",
       "  'reference_count': '29',\n",
       "  'references': ['1898928487',\n",
       "   '2149687213',\n",
       "   '1996020381',\n",
       "   '2148973700',\n",
       "   '2043132544',\n",
       "   '2069722312',\n",
       "   '1980228387',\n",
       "   '1997278766',\n",
       "   '2107538404',\n",
       "   '1988629947']},\n",
       " {'id': '1531106656',\n",
       "  'title': 'Intensity of continuous renal-replacement therapy in critically ill patients.',\n",
       "  'abstract': 'Background    The optimal intensity of continuous renal-replacement therapy remains unclear. We conducted a multicenter, randomized trial to compare the effect of this therapy, delivered at two different levels of intensity, on 90-day mortality among critically ill patients with acute kidney injury.    Methods    We randomly assigned critically ill adults with acute kidney injury to continuous renal-replacement therapy in the form of postdilution continuous venovenous hemodiafiltration with an effluent flow of either 40 ml per kilogram of body weight per hour (higher intensity) or 25 ml per kilogram per hour (lower intensity). The primary outcome measure was death within 90 days after randomization.    Results    Of the 1508 enrolled patients, 747 were randomly assigned to higher-intensity therapy, and 761 to lower-intensity therapy with continuous venovenous hemodiafiltration. Data on primary outcomes were available for 1464 patients (97.1%): 721 in the higher-intensity group and 743 in the lower-intensity group. The two study groups had similar baseline characteristics and received the study treatment for an average of 6.3 and 5.9 days, respectively (P = 0.35). At 90 days after randomization, 322 deaths had occurred in the higher-intensity group and 332 deaths in the lower-intensity group, for a mortality of 44.7% in each group (odds ratio, 1.00; 95% confidence interval [CI], 0.81 to 1.23; P = 0.99). At 90 days, 6.8% of survivors in the higher-intensity group (27 of 399), as compared with 4.4% of survivors in the lower-intensity group (18 of 411), were still receiving renal-replacement therapy (odds ratio, 1.59; 95% CI, 0.86 to 2.92; P = 0.14). Hypophosphatemia was more common in the higher-intensity group than in the lower-intensity group (65% vs. 54%, P Conclusions    In critically ill patients with acute kidney injury, treatment with higher-intensity continuous renal-replacement therapy did not reduce mortality at 90 days. (ClinicalTrials.gov number, NCT00221013.)',\n",
       "  'date': '2009',\n",
       "  'authors': ['Rinaldo Bellomo',\n",
       "   'Alan Cass',\n",
       "   'Louise Cole',\n",
       "   'Simon Finfer',\n",
       "   'Martin Gallagher',\n",
       "   'Serigne Lo',\n",
       "   'Colin McArthur',\n",
       "   'Shay McGuinness',\n",
       "   'John Myburgh',\n",
       "   'Robyn Norton',\n",
       "   'Carlos Scheinkestel',\n",
       "   'Steve Su'],\n",
       "  'related_topics': ['Renal replacement therapy',\n",
       "   'Acute kidney injury',\n",
       "   'Randomized controlled trial',\n",
       "   'Randomization',\n",
       "   'Odds ratio',\n",
       "   'Prospective cohort study',\n",
       "   'Confidence interval',\n",
       "   'Internal medicine',\n",
       "   'Surgery',\n",
       "   'Hypophosphatemia',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,395',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2149687213',\n",
       "   '2098931866',\n",
       "   '1489794536',\n",
       "   '2117958746',\n",
       "   '2116909658',\n",
       "   '2148973700',\n",
       "   '1970593590',\n",
       "   '2156916779',\n",
       "   '2159674113',\n",
       "   '2085529382']},\n",
       " {'id': '2157775267',\n",
       "  'title': 'Intensive insulin therapy and mortality among critically ill patients: a meta-analysis including NICE-SUGAR study data',\n",
       "  'abstract': 'Background:  Hyperglycemia is associated with increased mortality in critically ill patients. Randomized trials of intensive insulin therapy have reported inconsistent effects on mortality and increased rates of severe hypoglycemia. We conducted a meta-analysis to update the totality of evidence regarding the influence of intensive insulin therapy compared with conventional insulin therapy on mortality and severe hypoglycemia in the intensive care unit (ICU).   Methods:  We conducted searches of electronic databases, abstracts from scientific conferences and bibliographies of relevant articles. We included published randomized controlled trials conducted in the ICU that directly compared intensive insulin therapy with conventional glucose management and that documented mortality. We included in our meta-analysis the data from the recent NICE-SUGAR (Normoglycemia in Intensive Care Evaluation — Survival Using Glucose Algorithm Regulation) study.   Results:  We included 26 trials involving a total of 13 567 patients in our meta-analysis. Among the 26 trials that reported mortality, the pooled relative risk (RR) of death with intensive insulin therapy compared with conventional therapy was 0.93 (95% confidence interval [CI] 0.83–1.04). Among the 14 trials that reported hypoglycemia, the pooled RR with intensive insulin therapy was 6.0 (95% CI 4.5–8.0). The ICU setting was a contributing factor, with patients in surgical ICUs appearing to benefit from intensive insulin therapy (RR 0.63, 95% CI 0.44–0.91); patients in the other ICU settings did not (medical ICU: RR 1.0, 95% CI 0.78–1.28; mixed ICU: RR 0.99, 95% CI 0.86–1.12). The different targets of intensive insulin therapy (glucose level ≤ 6.1 mmol/L v. ≤ 8.3 mmol/L) did not influence either mortality or risk of hypoglycemia.   Interpretation:  Intensive insulin therapy significantly increased the risk of hypoglycemia and conferred no overall mortality benefit among critically ill patients. However, this therapy may be beneficial to patients admitted to a surgical ICU.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Donald E.G. Griesdale',\n",
       "   'Russell J. de',\n",
       "   'Rob M. van',\n",
       "   'Daren K.',\n",
       "   'Deborah J.',\n",
       "   'Atul',\n",
       "   'Rupinder',\n",
       "   'William R.',\n",
       "   'Dean R.',\n",
       "   'Simon',\n",
       "   'Daniel'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Intensive care unit',\n",
       "   'Hypoglycemia',\n",
       "   'Insulin',\n",
       "   'Relative risk',\n",
       "   'Randomized controlled trial',\n",
       "   'Cause of death',\n",
       "   'Meta-analysis',\n",
       "   'Internal medicine',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,338',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2125435699',\n",
       "   '2118858814',\n",
       "   '1980717583',\n",
       "   '1986215651',\n",
       "   '2145053281',\n",
       "   '2107328434',\n",
       "   '2115285670',\n",
       "   '2148706741',\n",
       "   '2081040380',\n",
       "   '2007884458']},\n",
       " {'id': '2028701043',\n",
       "  'title': 'Long-term Risk of Mortality and Other Adverse Outcomes After Acute Kidney Injury: A Systematic Review and Meta-analysis',\n",
       "  'abstract': 'Background  Acute kidney injury (AKI) is common in hospitalized patients. The impact of AKI on long-term outcomes is controversial.  Study Design  Systematic review and meta-analysis.  Setting & Participants  Persons with AKI.  Selection Criteria for Studies  MEDLINE and EMBASE databases were searched from 1985 through October 2007. Original studies describing outcomes of AKI for patients who survived hospital discharge were included. Studies were excluded from review when participants were followed up for less than 6 months.  Predictor  AKI, defined as acute changes in serum creatinine level or acute need for renal replacement therapy.  Outcomes  Chronic kidney disease (CKD), cardiovascular disease, and mortality.  Results  48 studies that contained a total of 47,017 participants were reviewed; 15 studies reported long-term data for patients without AKI. The incidence rate of mortality was 8.9 deaths/100 person-years in survivors of AKI and 4.3 deaths/100 patient-years in survivors without AKI (rate ratio [RR], 2.59; 95% confidence interval, 1.97 to 3.42). AKI was associated independently with mortality risk in 6 of 6 studies that performed multivariate adjustment (adjusted RR, 1.6 to 3.9) and with myocardial infarction in 2 of 2 studies (RR, 2.05; 95% confidence interval, 1.61 to 2.61). The incidence rate of CKD after an episode of AKI was 7.8 events/100 patient-years, and the rate of end-stage renal disease was 4.9 events/100 patient-years.  Limitations  The relative risk for CKD and end-stage renal disease after AKI was unattainable because of lack of follow-up of appropriate controls without AKI.  Conclusions  The development of AKI, defined as acute changes in serum creatinine level, characterizes hospitalized patients at increased risk of long-term adverse outcomes.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Steven G. Coca',\n",
       "   'Bushra Yusuf',\n",
       "   '',\n",
       "   'Michael G. Shlipak',\n",
       "   '',\n",
       "   'Amit X. Garg',\n",
       "   'Chirag R. Parikh',\n",
       "   ''],\n",
       "  'related_topics': ['Kidney disease',\n",
       "   'Renal replacement therapy',\n",
       "   'Acute kidney injury',\n",
       "   'Risk factor',\n",
       "   'Rate ratio',\n",
       "   'Nephrology',\n",
       "   'Relative risk',\n",
       "   'Epidemiology',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '999',\n",
       "  'reference_count': '77',\n",
       "  'references': ['2125435699',\n",
       "   '2536590171',\n",
       "   '1979423827',\n",
       "   '75245760',\n",
       "   '2107328434',\n",
       "   '2131419242',\n",
       "   '2133482423',\n",
       "   '2153104532',\n",
       "   '2122814783',\n",
       "   '2111704803']},\n",
       " {'id': '2111704803',\n",
       "  'title': 'Incidence and Outcomes in Acute Kidney Injury: A Comprehensive Population-Based Study',\n",
       "  'abstract': 'Epidemiological studies of acute kidney injury (AKI) and acute-on-chronic renal failure (ACRF) are surprisingly sparse and confounded by differences in definition. Reported incidences vary, with few studies being population-based. Given this and our aging population, the incidence of AKI may be much higher than currently thought. We tested the hypothesis that the incidence is higher by including all patients with AKI (in a geographical population base of 523,390) regardless of whether they required renal replacement therapy irrespective of the hospital setting in which they were treated. We also tested the hypothesis that the Risk, Injury, Failure, Loss, and End-Stage Kidney (RIFLE) classification predicts outcomes. We identified all patients with serum creatinine concentrations ≥150 μmol/L (male) or ≥130μmol/L (female) over a 6-mo period in 2003. Clinical outcomes were obtained from each patient9s case records. The incidences of AKI and ACRF were 1811 and 336 per million population, respectively. Median age was 76 yr for AKI and 80.5 yr for ACRF. Sepsis was a precipitating factor in 47% of patients. The RIFLE classification was useful for predicting full recovery of renal function ( P   P   P   P  = 0.035). RIFLE did not predict mortality at 90 d or 6 mo. Thus the incidence of AKI is much higher than previously thought, with implications for service planning and providing information to colleagues about methods to prevent deterioration of renal function. The RIFLE classification is useful for identifying patients at greatest risk of adverse short-term outcomes.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Tariq Ali',\n",
       "   'Izhar',\n",
       "   'William',\n",
       "   'Gordon',\n",
       "   'John Andrew',\n",
       "   'William Cairns Stewart',\n",
       "   'Alison Murray'],\n",
       "  'related_topics': ['Renal replacement therapy',\n",
       "   'Rifle',\n",
       "   'Acute kidney injury',\n",
       "   'Kidney disease',\n",
       "   'Population',\n",
       "   'Nephrology',\n",
       "   'Renal function',\n",
       "   'Incidence (epidemiology)',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,037',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2487377689',\n",
       "   '2139937737',\n",
       "   '2149687213',\n",
       "   '2069722312',\n",
       "   '2155939210',\n",
       "   '2036544704',\n",
       "   '2139529386',\n",
       "   '2171344771',\n",
       "   '2095573004',\n",
       "   '2150098992']},\n",
       " {'id': '2135163018',\n",
       "  'title': 'Chronic Dialysis and Death Among Survivors of Acute Kidney Injury Requiring Dialysis',\n",
       "  'abstract': 'Context Severe acute kidney injury among hospitalized patients often necessitates initiation of short-term dialysis. Little is known about the long-term outcome of those who survive to hospital discharge. Objective To assess the risk of chronic dialysis and all-cause mortality in individuals who experience an episode of acute kidney injury requiring dialysis. Design, Setting, and Participants We conducted a population-based cohort study of all adult patients in Ontario, Canada, with acute kidney injury who required in-hospital dialysis and survived free of dialysis for at least 30 days after discharge between July 1, 1996, and December 31, 2006. These individuals were matched with patients without acute kidney injury or dialysis during their index hospitalization. Matching was by age plus or minus 5 years, sex, history of chronic kidney disease, receipt of mechanical ventilation during the index hospitalization, and a propensity score for developing acute kidney injury requiring dialysis. Patients were followed up until March 31, 2007. Main Outcome Measures The primary end point was the need for chronic dialysis and the secondary outcome was all-cause mortality. Results We identified 3769 adults with acute kidney injury requiring in-hospital dialysis and 13\\xa0598 matched controls. The mean age was 62 years and median follow-up was 3 years. The incidence rate of chronic dialysis was 2.63 per 100 person-years among individuals with acute kidney injury requiring dialysis, and 0.91 per 100 person-years among control participants (adjusted hazard ratio, 3.23; 95% confidence interval, 2.70-3.86). All-cause mortality rates were 10.10 and 10.85 per 100 person-years, respectively (adjusted hazard ratio, 0.95; 95% confidence interval, 0.89-1.02). Conclusions Acute kidney injury necessitating in-hospital dialysis was associated with an increased risk of chronic dialysis but not all-cause mortality.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Ron Wald',\n",
       "   'Robert R.',\n",
       "   'Jin',\n",
       "   'Ping',\n",
       "   'Damon C.',\n",
       "   'Muhammad M.',\n",
       "   'Joel G.'],\n",
       "  'related_topics': ['Dialysis',\n",
       "   'Kidney disease',\n",
       "   'Acute kidney injury',\n",
       "   'Hemodialysis',\n",
       "   'Mortality rate',\n",
       "   'Hazard ratio',\n",
       "   'Population',\n",
       "   'Retrospective cohort study',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Surgery',\n",
       "   'View Less'],\n",
       "  'citation_count': '698',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2000445173',\n",
       "   '2149687213',\n",
       "   '2131419242',\n",
       "   '2052806549',\n",
       "   '2117958746',\n",
       "   '2111945961',\n",
       "   '2163900716',\n",
       "   '2043132544',\n",
       "   '2069722312',\n",
       "   '1988629947']},\n",
       " {'id': '2042074736',\n",
       "  'title': 'Acute Renal Failure After Coronary Intervention: Incidence, Risk Factors, and Relationship to Mortality',\n",
       "  'abstract': 'Abstract  PURPOSE: This study set out to define the incidence, predictors, and mortality related to acute renal failure (ARF) and acute renal failure requiring dialysis (ARFD) after coronary intervention.  PATIENTS AND METHODS: Derivation-validation set methods were used in 1,826 consecutive patients undergoing coronary intervention with evaluation of baseline creatinine clearance (CrCl), diabetic status, contrast exposure, postprocedure creatinine, ARF, ARFD, in-hospital mortality, and long-term survival (derivation set). Multiple logistic regression was used to derive the prior probability of ARFD in a second set of 1,869 consecutive patients (validation set).  RESULTS: The incidence of ARF and ARFD was 144.6/1,000 and 7.7/1,000 cases respectively. The cutoff dose of contrast below which there was no ARFD was 100 mL. No patient with a CrCl > 47 mL/min developed ARFD. These thresholds were confirmed in the validation set. Multivariate analysis found CrCl [odds ratio (OR) = 0.83, 95% confidence interval (CI) 0.77 to 0.89,  P   P  = 0.01), and contrast dose (OR = 1.008, 95% CI 1.002 to 1.013,  P  = 0.01) to be independent predictors of ARFD. Patients in the validation set who underwent dialysis had a predicted prior probability of ARFD of between 0.07 and 0.73. The in-hospital mortality for those who developed ARFD was 35.7% and the 2-year survival was 18.8%.  CONCLUSION: The occurrence of ARFD after coronary intervention is rare (',\n",
       "  'date': '1997',\n",
       "  'authors': ['Peter A McCullough',\n",
       "   'Robert Wolyn',\n",
       "   'Leslie L Rocher',\n",
       "   'Robert N Levin',\n",
       "   'William W O’Neill'],\n",
       "  'related_topics': ['Renal function',\n",
       "   'Odds ratio',\n",
       "   'Dialysis',\n",
       "   'Confidence interval',\n",
       "   'Contrast-induced nephropathy',\n",
       "   'Kidney disease',\n",
       "   'Predictive value of tests',\n",
       "   'Creatinine',\n",
       "   'Internal medicine',\n",
       "   'Surgery',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,194',\n",
       "  'reference_count': '52',\n",
       "  'references': ['1973948212',\n",
       "   '1550111394',\n",
       "   '2029723446',\n",
       "   '2071273488',\n",
       "   '2330776976',\n",
       "   '1996698106',\n",
       "   '2018224788',\n",
       "   '2056699935',\n",
       "   '2316461329',\n",
       "   '2073085411']},\n",
       " {'id': '2106882534',\n",
       "  'title': 'CLUSTAL W: IMPROVING THE SENSITIVITY OF PROGRESSIVE MULTIPLE SEQUENCE ALIGNMENT THROUGH SEQUENCE WEIGHTING, POSITION-SPECIFIC GAP PENALTIES AND WEIGHT MATRIX CHOICE',\n",
       "  'abstract': 'The sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. Firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. Secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. Thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. Fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. These modifications are incorporated into a new program, CLUSTAL W which is freely available.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Julie D. Thompson', 'Desmond G. Higgins', 'Toby J. Gibson'],\n",
       "  'related_topics': ['Gap penalty',\n",
       "   'Multiple sequence alignment',\n",
       "   'Structural alignment',\n",
       "   'Substitution matrix',\n",
       "   'MUSCLE',\n",
       "   'Alignment-free sequence analysis',\n",
       "   'BLOSUM',\n",
       "   'Sequence logo',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '67,894',\n",
       "  'reference_count': '34',\n",
       "  'references': ['2097706568',\n",
       "   '2015292449',\n",
       "   '2009310436',\n",
       "   '2143210482',\n",
       "   '2065461553',\n",
       "   '1998300401',\n",
       "   '2008708467',\n",
       "   '2045391589',\n",
       "   '2149208773',\n",
       "   '2102122585']},\n",
       " {'id': '2463755683',\n",
       "  'title': 'Update: Outbreak of severe acute respiratory syndrome - Worldwide, 2003',\n",
       "  'abstract': '',\n",
       "  'date': '2003',\n",
       "  'authors': ['T.',\n",
       "   'L.',\n",
       "   'M.',\n",
       "   'J.-S.',\n",
       "   'Y.-C.',\n",
       "   'I.-H.',\n",
       "   'K.-T.',\n",
       "   'K.-H.',\n",
       "   'T.-J.',\n",
       "   'H.-T.',\n",
       "   'S.-J.',\n",
       "   'S.',\n",
       "   'P.',\n",
       "   'K.',\n",
       "   'A.'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Respiratory system',\n",
       "   'Medicine',\n",
       "   'Emergency medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '339',\n",
       "  'reference_count': '1',\n",
       "  'references': ['2089784797']},\n",
       " {'id': '2398786667',\n",
       "  'title': 'Microbial Threats to Health: Emergence, Detection, and Response',\n",
       "  'abstract': 'Infectious diseases are a global hazard that puts every nation and every person at risk. The recent SARS outbreak is a prime example. Knowing neither geographic nor political borders, often arriving silently and lethally, microbial pathogens constitute a grave threat to the health of humans. Indeed, a majority of countries recently identified the spread of infectious disease as the greatest global problem they confront. Throughout history, humans have struggled to control both the causes and consequences of infectious diseases and we will continue to do so into the foreseeable future.Following up on a high-profile 1992 report from the Institute of Medicine, Microbial Threats to Health examines the current state of knowledge and policy pertaining to emerging and re-emerging infectious diseases from around the globe. It examines the spectrum of microbial threats, factors in disease emergence, and the ultimate capacity of the United States to meet the challenges posed by microbial threats to human health. From the impact of war or technology on disease emergence to the development of enhanced disease surveillance and vaccine strategies, Microbial Threats to Health contains valuable information for researchers, students, health care providers, policymakers, public health officials. and the interested public.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Mark S.', 'Margaret A.', 'Joshua'],\n",
       "  'related_topics': ['Public health',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Disease surveillance',\n",
       "   'Health care',\n",
       "   'Hazard',\n",
       "   'Outbreak',\n",
       "   'Disease',\n",
       "   'Economic growth',\n",
       "   'Politics',\n",
       "   'Political science',\n",
       "   'View Less'],\n",
       "  'citation_count': '649',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2127949919',\n",
       "  'title': 'Nipah Virus: A Recently Emergent Deadly Paramyxovirus',\n",
       "  'abstract': 'A paramyxovirus virus termed Nipah virus has been identified as the etiologic agent of an outbreak of severe encephalitis in people with close contact exposure to pigs in Malaysia and Singapore. The outbreak was first noted in late September 1998 and by mid-June 1999, more than 265 encephalitis cases, including 105 deaths, had been reported in Malaysia, and 11 cases of encephalitis or respiratory illness with one death had been reported in Singapore. Electron microscopic, serologic, and genetic studies indicate that this virus belongs to the family Paramyxoviridae and is most closely related to the recently discovered Hendra virus. We suggest that these two viruses are representative of a new genus within the family Paramyxoviridae. Like Hendra virus, Nipah virus is unusual among the paramyxoviruses in its ability to infect and cause potentially fatal disease in a number of host species, including humans.',\n",
       "  'date': '2000',\n",
       "  'authors': ['K. B. Chua',\n",
       "   'W. J. Bellini',\n",
       "   'P. A. Rota',\n",
       "   'B. H. Harcourt',\n",
       "   'A. Tamin',\n",
       "   'S. K. Lam',\n",
       "   'T. G. Ksiazek',\n",
       "   'P. E. Rollin',\n",
       "   'S. R. Zaki',\n",
       "   'W.-J. Shieh',\n",
       "   'C. S. Goldsmith',\n",
       "   'D. J. Gubler',\n",
       "   'J. T. Roehrig',\n",
       "   'B. Eaton',\n",
       "   'A. R. Gould',\n",
       "   'J. Olson',\n",
       "   'P. Daniels',\n",
       "   'A. E. Ling',\n",
       "   'C. J. Peters',\n",
       "   'L. J. Anderson',\n",
       "   'B. W. J. Mahy'],\n",
       "  'related_topics': ['Hendra Virus',\n",
       "   'Tioman virus',\n",
       "   'Henipavirus',\n",
       "   'Veterinary virology',\n",
       "   'Australian bat lyssavirus',\n",
       "   'Menangle virus',\n",
       "   'Virus',\n",
       "   'Encephalitis',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,314',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2094644220',\n",
       "   '1981329413',\n",
       "   '1983097458',\n",
       "   '2130324980',\n",
       "   '2040567164',\n",
       "   '2028665290',\n",
       "   '1699123896',\n",
       "   '2029131064',\n",
       "   '2147898590',\n",
       "   '367527820']},\n",
       " {'id': '1576737979',\n",
       "  'title': 'Microarray-based detection and genotyping of viral pathogens',\n",
       "  'abstract': 'The detection of viral pathogens is of critical importance in biology, medicine, and agriculture. Unfortunately, existing techniques to screen for a broad spectrum of viruses suffer from severe limitations. To facilitate the comprehensive and unbiased analysis of viral prevalence in a given biological setting, we have developed a genomic strategy for highly parallel viral screening. The cornerstone of this approach is a long oligonucleotide (70-mer) DNA microarray capable of simultaneously detecting hundreds of viruses. Using virally infected cell cultures, we were able to efficiently detect and identify many diverse viruses. Related viral serotypes could be distinguished by the unique pattern of hybridization generated by each virus. Furthermore, by selecting microarray elements derived from highly conserved regions within viral families, individual viruses that were not explicitly represented on the microarray were still detected, raising the possibility that this approach could be used for virus discovery. Finally, by using a random PCR amplification strategy in conjunction with the microarray, we were able to detect multiple viruses in human respiratory specimens without the use of sequence-specific or degenerate primers. This method is versatile and greatly expands the spectrum of detectable viruses in a single assay while simultaneously providing the capability to discriminate among viral subtypes.',\n",
       "  'date': '2002',\n",
       "  'authors': ['David Wang',\n",
       "   'Laurent Coscoy',\n",
       "   'Maxine Zylberberg',\n",
       "   'Pedro C. Avila',\n",
       "   'Homer A. Boushey',\n",
       "   'Don Ganem',\n",
       "   'Joseph L. DeRisi'],\n",
       "  'related_topics': ['DNA microarray',\n",
       "   'Genotyping',\n",
       "   'Virus',\n",
       "   'Genome',\n",
       "   'Polymerase chain reaction',\n",
       "   'Microarray',\n",
       "   'Genotype',\n",
       "   'Human genetics',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '927',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2055043387',\n",
       "   '1502936039',\n",
       "   '1549647993',\n",
       "   '2137089963',\n",
       "   '2028318125',\n",
       "   '2099369211',\n",
       "   '2169391021',\n",
       "   '2033380151',\n",
       "   '2037142940',\n",
       "   '2133247102']},\n",
       " {'id': '2128788856',\n",
       "  'title': 'Human Metapneumovirus Infections in Young and Elderly Adults',\n",
       "  'abstract': 'Human metapneumovirus virus (hMPV) is a newly discovered respiratory pathogen with limited epidemiological data available. Cohorts of young and older adults were prospectively evaluated for hMPV infection during 2 winter seasons. Patients hospitalized for cardiopulmonary conditions during that period were also studied. Overall, 44 (4.5%) of 984 illnesses were associated with hMPV infection, and 9 (4.1%) of 217 asymptomatic subjects were infected. There was a significant difference in rates of hMPV illnesses between years 1 and 2 (7/452 [1.5%] vs. 37/532 [7.0%]; P<.0001). In the second year, 11% of hospitalized patients had evidence of hMPV infection. Infections occurred in all age groups but were most common among young adults. Frail elderly people with hMPV infection frequently sought medical attention. In conclusion, hMPV infection occurs in adults of all ages and may account for a significant portion of persons hospitalized with respiratory infections during some years.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Ann R Falsey',\n",
       "   'Dean Erdman',\n",
       "   'Larry J Anderson',\n",
       "   'Edward E Walsh'],\n",
       "  'related_topics': ['Human metapneumovirus',\n",
       "   'Asymptomatic',\n",
       "   'Young adult',\n",
       "   'Epidemiology',\n",
       "   'Metapneumovirus',\n",
       "   'Viral disease',\n",
       "   'Pediatrics',\n",
       "   'Paramyxoviridae Infections',\n",
       "   'Respiratory disease',\n",
       "   'Medicine',\n",
       "   'Immunology',\n",
       "   'View Less'],\n",
       "  'citation_count': '699',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2170881661',\n",
       "   '2152552492',\n",
       "   '2048618475',\n",
       "   '1589490904',\n",
       "   '2110198546',\n",
       "   '2165127900',\n",
       "   '2076770315',\n",
       "   '2079378048',\n",
       "   '2068094897',\n",
       "   '2065996927']},\n",
       " {'id': '2076620790',\n",
       "  'title': 'A morbillivirus that caused fatal disease in horses and humans',\n",
       "  'abstract': 'A morbillivirus has been isolated and added to an increasing list of emerging viral diseases. This virus caused an outbreak of fatal respiratory disease in horses and humans. Genetic analyses show it to be only distantly related to the classic morbilliviruses rinderpest, measles, and canine distemper. When seen by electron microscopy, viruses had 10- and 18-nanometer surface projections that gave them a \"double-fringed\" appearance. The virus induced syncytia that developed in the endothelium of blood vessels, particularly the lungs.',\n",
       "  'date': '1995',\n",
       "  'authors': ['K Murray', 'P', 'P', 'A', 'A', 'L', 'H', 'L', 'L', 'B'],\n",
       "  'related_topics': ['Morbillivirus',\n",
       "   'Rinderpest virus',\n",
       "   'Phocine distemper virus',\n",
       "   'Paramyxoviridae',\n",
       "   'Canine distemper',\n",
       "   'Rinderpest',\n",
       "   'Hendra Virus',\n",
       "   'Menangle virus',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '798',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2034148010',\n",
       "   '2123427059',\n",
       "   '1972759043',\n",
       "   '2020526182',\n",
       "   '2002849005',\n",
       "   '2078406556',\n",
       "   '1582863396',\n",
       "   '2148205933',\n",
       "   '1997387663',\n",
       "   '2106255335']},\n",
       " {'id': '2123324969',\n",
       "  'title': 'Guidelines for the management of adults with community-acquired pneumonia. Diagnosis, assessment of severity, antimicrobial therapy, and prevention.',\n",
       "  'abstract': '',\n",
       "  'date': '2001',\n",
       "  'authors': ['M. S.',\n",
       "   'L. A.',\n",
       "   'A.',\n",
       "   'J. B.',\n",
       "   'W. A.',\n",
       "   'G. D.',\n",
       "   'N.',\n",
       "   'T.',\n",
       "   'M. J.',\n",
       "   'P. A.',\n",
       "   'F.',\n",
       "   'T. J.',\n",
       "   'J. F.',\n",
       "   'J.',\n",
       "   'G. A.',\n",
       "   'A.',\n",
       "   'R.',\n",
       "   'V. L.'],\n",
       "  'related_topics': ['Pneumonia severity index',\n",
       "   'Pneumonia',\n",
       "   'CURB-65',\n",
       "   'Community-acquired pneumonia',\n",
       "   'MEDLINE',\n",
       "   'Antimicrobial',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Viral therapy',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,482',\n",
       "  'reference_count': '143',\n",
       "  'references': ['2320270386',\n",
       "   '2130141864',\n",
       "   '2004554957',\n",
       "   '2115071132',\n",
       "   '2336287850',\n",
       "   '2035760184',\n",
       "   '2158075347',\n",
       "   '1988729025',\n",
       "   '2339696769',\n",
       "   '2027162482']},\n",
       " {'id': '2130141864',\n",
       "  'title': 'Practice Guidelines for the Management of Community-Acquired Pneumonia in Adults',\n",
       "  'abstract': \"John G. Bartlett,1 Scott F Dowell,2 Lionel A. Mandell,6 Thomas M. File, Jr.,3 Daniel M. Musher,4 and Michael J. Fine5 'Johns Hopkins University School of Medicine, Baltimore, Maryland, 2Centers for Disease Control and Prevention, Atlanta, Georgia, 3Northeastern Ohio Universities College of Medicine, Cleveland, Ohio, 4Baylor College of Medicine and Veterans Affairs Medical Center, Houston, Texas, and 5University of Pittsburgh, Pennsylvania, USA; and 6McMaster University, Toronto, Canada\",\n",
       "  'date': '2000',\n",
       "  'authors': ['John G Bartlett',\n",
       "   'Scott F Dowell',\n",
       "   'Lionel A Mandell',\n",
       "   'Thomas M File',\n",
       "   'Daniel M Musher',\n",
       "   'Michael J Fine'],\n",
       "  'related_topics': ['Antibacterial agent',\n",
       "   'Veterans Affairs',\n",
       "   'Atlanta',\n",
       "   'Epidemiology',\n",
       "   'Library science',\n",
       "   'Community-acquired pneumonia',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine',\n",
       "   'Disease control',\n",
       "   'Lung disease',\n",
       "   'Medical treatment',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,220',\n",
       "  'reference_count': '205',\n",
       "  'references': ['1833207062',\n",
       "   '2320270386',\n",
       "   '2109779439',\n",
       "   '2103828083',\n",
       "   '3140139051',\n",
       "   '2202941334',\n",
       "   '2409448230',\n",
       "   '2336287850',\n",
       "   '2037712857',\n",
       "   '2112302527']},\n",
       " {'id': '1991467275',\n",
       "  'title': 'Bronchiolitis Obliterans Organizing Pneumonia',\n",
       "  'abstract': 'Bronchiolar disorders can be divided into 2 general categories: (1) airway disorders (cellular bronchiolitis and obliterative bronchiolitis) and (2) parenchymal disorders (respiratory bronchiolitis-interstitial lung disease, which occurs in smokers and is treatable with smoking cessation or corticosteroid therapy, and bronchiolitis obliterans organizing pneumonia, an inflammatory lung disease simultaneously involving the terminal bronchioles and alveoli). This article reviews the clinical findings and therapeutic management of bronchiolitis obliterans organizing pneumonia.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Gary R. Epler'],\n",
       "  'related_topics': ['Bronchiolitis obliterans organizing pneumonia',\n",
       "   'Bronchiolitis',\n",
       "   'Respiratory system',\n",
       "   'Airway',\n",
       "   'Smoking cessation',\n",
       "   'Pathology',\n",
       "   'Parenchyma',\n",
       "   'Medicine',\n",
       "   'Lung disease',\n",
       "   'Terminal Bronchioles',\n",
       "   'View Less'],\n",
       "  'citation_count': '338',\n",
       "  'reference_count': '82',\n",
       "  'references': ['2626588662',\n",
       "   '3025576394',\n",
       "   '1981681632',\n",
       "   '2030046096',\n",
       "   '2006351935',\n",
       "   '1971694872',\n",
       "   '2076477487',\n",
       "   '1976530706',\n",
       "   '2331116046',\n",
       "   '2037346751']},\n",
       " {'id': '1982444609',\n",
       "  'title': 'Bronchiolitis obliterans organizing pneumonia: CT features in 14 patients.',\n",
       "  'abstract': 'Bronchiolitis obliterans organizing pneumonia is a disease characterized by the presence of granulation tissue within small airways and the presence of areas of organizing pneumonia. We retrospectively reviewed the chest radiographs, CT scans, and biopsy specimens in 14 consecutive patients with proved bronchiolitis obliterans organizing pneumonia. Six patients were immunocompromised because of leukemia or bone-marrow transplantation. In all patients, 10-mm collimation CT scans were available. In 11 of the 14 patients, select 1.5-mm scans were obtained. The CT findings included patchy unilateral (n = 1) or bilateral air-space consolidation (n = 9), small nodular opacities (n = 7), irregular linear opacities (n = 2), bronchial wall thickening and dilatation (n = 6), and small pleural effusions (n = 4). All patients had areas of air-space consolidation, small nodules, or both. A predominantly subpleural distribution of the air-space consolidation was apparent on the radiographs of two patients and on CT scans of six. Pathologically, the nodules and the consolidation represented different degrees of inflammation in bronchioles, alveolar ducts, and alveoli. Although most of the findings were apparent on the radiographs, the CT scans depicted the anatomic distribution and extent of bronchiolitis obliterans organizing pneumonia more accurately than did the plain chest radiographs.',\n",
       "  'date': '1990',\n",
       "  'authors': ['N L Müller', 'C A', 'R R'],\n",
       "  'related_topics': ['Bronchiolitis obliterans organizing pneumonia',\n",
       "   'Bronchiolitis obliterans',\n",
       "   'Pneumonia',\n",
       "   'Transplantation',\n",
       "   'Lung',\n",
       "   'Respiratory disease',\n",
       "   'Thorax',\n",
       "   'Biopsy',\n",
       "   'Radiology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '272',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2107053896',\n",
       "  'title': 'Hospital Outbreak of Middle East Respiratory Syndrome Coronavirus',\n",
       "  'abstract': 'Background In September 2012, the World Health Organization reported the first cases of pneumonia caused by the novel Middle East respiratory syndrome coronavirus (MERS-CoV). We describe a cluster of health care–acquired MERS-CoV infections. Methods Medical records were reviewed for clinical and demographic information and determination of potential contacts and exposures. Case patients and contacts were interviewed. The incubation period and serial interval (the time between the successive onset of symptoms in a chain of transmission) were estimated. Viral RNA was sequenced. Results Between April 1 and May 23, 2013, a total of 23 cases of MERS-CoV infection were reported in the eastern province of Saudi Arabia. Symptoms included fever in 20 patients (87%), cough in 20 (87%), shortness of breath in 11 (48%), and gastrointestinal symptoms in 8 (35%); 20 patients (87%) presented with abnormal chest radiographs. As of June 12, a total of 15 patients (65%) had died, 6 (26%) had recovered, and 2 (9%) remained ...',\n",
       "  'date': '2013',\n",
       "  'authors': ['Abdullah Assiri',\n",
       "   'Allison McGeer',\n",
       "   'Trish M. Perl',\n",
       "   'Connie S. Price',\n",
       "   'Abdullah A. Al Rabeeah',\n",
       "   'Derek A.T. Cummings',\n",
       "   'Zaki N. Alabdullatif',\n",
       "   'Maher Assad',\n",
       "   'Abdulmohsen Almulhim',\n",
       "   'Hatem Makhdoom',\n",
       "   'Hossam Madani',\n",
       "   'Rafat Alhakeem',\n",
       "   'Jaffar A. Al-Tawfiq',\n",
       "   'Matthew Cotten',\n",
       "   'Simon J. Watson',\n",
       "   'Paul Kellam',\n",
       "   '',\n",
       "   'Alimuddin I. Zumla',\n",
       "   '',\n",
       "   'Ziad A. Memish'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Coronavirus',\n",
       "   'Incubation period',\n",
       "   'Serial interval',\n",
       "   'Outbreak',\n",
       "   'Disease cluster',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Immunology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,265',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2100820722',\n",
       "   '1703839189',\n",
       "   '2147166346',\n",
       "   '2116586125',\n",
       "   '2045002682',\n",
       "   '2113457186',\n",
       "   '2058144955',\n",
       "   '2111412754']},\n",
       " {'id': '2112147913',\n",
       "  'title': 'Middle East respiratory syndrome coronavirus (MERS-CoV): announcement of the Coronavirus Study Group.',\n",
       "  'abstract': 'During the summer of 2012, in Jeddah, Saudi Arabia, a hitherto unknown coronavirus (CoV) was isolated from the sputum of a patient with acute pneumonia and renal failure ([1][1], [2][2]). The isolate was provisionally called human coronavirus Erasmus Medical Center (EMC) ([3][3]). Shortly thereafter',\n",
       "  'date': '2013',\n",
       "  'authors': ['R. J. de Groot',\n",
       "   'S. C. Baker',\n",
       "   'R. S. Baric',\n",
       "   'C. S. Brown',\n",
       "   'C. Drosten',\n",
       "   'L. Enjuanes',\n",
       "   'R. A. M. Fouchier',\n",
       "   'M. Galiano',\n",
       "   'A. E. Gorbalenya',\n",
       "   'Z. A. Memish',\n",
       "   'S. Perlman',\n",
       "   'L. L. M. Poon',\n",
       "   'E. J. Snijder',\n",
       "   'G. M. Stephens',\n",
       "   'P. C. Y. Woo',\n",
       "   'A. M. Zaki',\n",
       "   'M. Zambon',\n",
       "   'J. Ziebuhr'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Sputum',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Acute pneumonia',\n",
       "   'Human coronavirus',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,149',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2166867592',\n",
       "   '2113457186',\n",
       "   '1690366459',\n",
       "   '2066347985',\n",
       "   '2105558355',\n",
       "   '1035662337',\n",
       "   '2102229939',\n",
       "   '2060720058',\n",
       "   '2079206979']},\n",
       " {'id': '2045002682',\n",
       "  'title': 'Family Cluster of Middle East Respiratory Syndrome Coronavirus Infections',\n",
       "  'abstract': 'A human coronavirus, called the Middle East respiratory syndrome coronavirus (MERS-CoV), was first identified in September 2012 in samples obtained from a Saudi Arabian businessman who died from acute respiratory failure. Since then, 49 cases of infections caused by MERS-CoV (previously called a novel coronavirus) with 26 deaths have been reported to date. In this report, we describe a family case cluster of MERS-CoV infection, including the clinical presentation, treatment outcomes, and household relationships of three young men who became ill with MERS-CoV infection after the hospitalization of an elderly male relative, who died of the disease. Twenty-four other family members living in the same household and 124 attending staff members at the hospitals did not become ill. MERS-CoV infection may cause a spectrum of clinical illness. Although an animal reservoir is suspected, none has been discovered. Meanwhile, global concern rests on the ability of MERS-CoV to cause major illness in close contacts of patients.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Ziad A. Memish',\n",
       "   'Alimuddin I. Zumla',\n",
       "   'Rafat F. Al-Hakeem',\n",
       "   'Abdullah A. Al-Rabeeah',\n",
       "   'Gwen M. Stephens'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Disease cluster',\n",
       "   'Disease',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'Immunology',\n",
       "   'Family cluster',\n",
       "   'Human coronavirus',\n",
       "   'Treatment outcome',\n",
       "   'View Less'],\n",
       "  'citation_count': '519',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2166867592',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '2158773042',\n",
       "   '2130450914',\n",
       "   '2088479029',\n",
       "   '297155885']},\n",
       " {'id': '1852588318',\n",
       "  'title': 'Assays for laboratory confirmation of novel human coronavirus (hCoV-EMC) infections.',\n",
       "  'abstract': 'We present a rigorously validated and highly sensitive confirmatory real-time RT-PCR assay (1A assay) that can be used in combination with the previously reported upE assay. Two additional RT-PCR assays for sequencing are described, targeting the RdRp gene (RdRpSeq assay) and N gene (NSeq assay), where an insertion/deletion polymorphism might exist among different hCoV-EMC strains. Finally, a simplified and biologically safe protocol for detection of antibody response by immunofluorescence microscopy was developed using convalescent patient serum.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Victor Corman',\n",
       "   'Marcel Müller',\n",
       "   'U. Costabel',\n",
       "   'J. Timm',\n",
       "   'Tabea Binger',\n",
       "   'Bernhard Meyer',\n",
       "   'P. Kreher',\n",
       "   'Erik Lattwein',\n",
       "   'Monika Eschbach-Bludau',\n",
       "   'A. Nitsche',\n",
       "   'T. Bleicker',\n",
       "   'O. Landt',\n",
       "   'Brunhilde Schweiger',\n",
       "   'Jan-Felix Drexler',\n",
       "   'Albert Osterhaus',\n",
       "   'Bart Haagmans',\n",
       "   'U. Dittmer',\n",
       "   'F. Bonin',\n",
       "   'Thorsten Wolff',\n",
       "   'Christian Drosten'],\n",
       "  'related_topics': ['Sequence analysis',\n",
       "   'Virology',\n",
       "   'Gene',\n",
       "   'Molecular biology',\n",
       "   'Polymorphism (computer science)',\n",
       "   'DNA',\n",
       "   'Biology',\n",
       "   'Antibody response',\n",
       "   'Highly sensitive',\n",
       "   'Human coronavirus',\n",
       "   'Immunofluorescence Microscopy',\n",
       "   'View Less'],\n",
       "  'citation_count': '413',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2166867592',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '1690366459',\n",
       "   '2167080692',\n",
       "   '2082755732',\n",
       "   '1593955729',\n",
       "   '2145810580',\n",
       "   '2122612816',\n",
       "   '2136039989']},\n",
       " {'id': '2163627712',\n",
       "  'title': 'Clinical features and short-term outcomes of 144 patients with SARS in the greater Toronto area.',\n",
       "  'abstract': 'ContextSevere acute respiratory syndrome (SARS) is an emerging infectious disease\\nthat first manifested in humans in China in November 2002 and has subsequently\\nspread worldwide.ObjectivesTo describe the clinical characteristics and short-term outcomes of\\nSARS in the first large group of patients in North America; to describe how\\nthese patients were treated and the variables associated with poor outcome.Design, Setting, and PatientsRetrospective case series involving 144 adult patients admitted to 10\\nacademic and community hospitals in the greater Toronto, Ontario, area between\\nMarch 7 and April 10, 2003, with a diagnosis of suspected or probable SARS.\\nPatients were included if they had fever, a known exposure to SARS, and respiratory\\nsymptoms or infiltrates observed on chest radiograph. Patients were excluded\\nif an alternative diagnosis was determined.Main Outcome MeasuresLocation of exposure to SARS; features of the history, physical examination,\\nand laboratory tests at admission to the hospital; and 21-day outcomes such\\nas death or intensive care unit (ICU) admission with or without mechanical\\nventilation.ResultsOf the 144 patients, 111 (77%) were exposed to SARS in the hospital\\nsetting. Features of the clinical examination most commonly found in these\\npatients at admission were self-reported fever (99%), documented elevated\\ntemperature (85%), nonproductive cough (69%), myalgia (49%), and dyspnea (42%).\\nCommon laboratory features included elevated lactate dehydrogenase (87%),\\nhypocalcemia (60%), and lymphopenia (54%). Only 2% of patients had rhinorrhea.\\nA total of 126 patients (88%) were treated with ribavirin, although its use\\nwas associated with significant toxicity, including hemolysis (in 76%) and\\ndecrease in hemoglobin of 2 g/dL (in 49%). Twenty-nine patients (20%) were\\nadmitted to the ICU with or without mechanical ventilation, and 8 patients\\ndied (21-day mortality, 6.5%; 95% confidence interval [CI], 1.9%-11.8%). Multivariable\\nanalysis showed that the presence of diabetes (relative risk [RR], 3.1; 95%\\nCI, 1.4-7.2; P = .01) or other comorbid conditions\\n(RR, 2.5; 95% CI, 1.1-5.8; P = .03) were independently\\nassociated with poor outcome (death, ICU admission, or mechanical ventilation).ConclusionsThe majority of cases in the SARS outbreak in the greater Toronto area\\nwere related to hospital exposure. In the event that contact history becomes\\nunreliable, several features of the clinical presentation will be useful in\\nraising the suspicion of SARS. Although SARS is associated with significant\\nmorbidity and mortality, especially in patients with diabetes or other comorbid\\nconditions, the vast majority (93.5%) of patients in our cohort survived.Published online May 6, 2003 (doi:10.1001/jama.289.21.JOC30885).',\n",
       "  'date': '2003',\n",
       "  'authors': ['Christopher M. Booth',\n",
       "   'Larissa M. Matukas',\n",
       "   'George A. Tomlinson',\n",
       "   'Anita R. Rachlis',\n",
       "   'David B. Rose',\n",
       "   'Hy A. Dwosh',\n",
       "   'Sharon L. Walmsley',\n",
       "   'Tony Mazzulli',\n",
       "   '',\n",
       "   'Monica Avendano',\n",
       "   'Peter Derkach',\n",
       "   'Issa E. Ephtimios',\n",
       "   'Ian Kitai',\n",
       "   'Barbara D. Mederski',\n",
       "   'Steven B. Shadowitz',\n",
       "   'Wayne L. Gold',\n",
       "   '',\n",
       "   'Laura A. Hawryluck',\n",
       "   'Elizabeth Rea',\n",
       "   'Jordan S. Chenkin',\n",
       "   'David W. Cescon',\n",
       "   'Susan M. Poutanen',\n",
       "   '',\n",
       "   '',\n",
       "   'Allan S. Detsky'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Intensive care unit',\n",
       "   'Relative risk',\n",
       "   'Retrospective cohort study',\n",
       "   'Risk factor',\n",
       "   'Physical examination',\n",
       "   'myalgia',\n",
       "   'Mechanical ventilation',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Surgery',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,608',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2104548316',\n",
       "   '2131262274',\n",
       "   '2115709314',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2136883754',\n",
       "   '2463755683',\n",
       "   '1966714873',\n",
       "   '2136166622',\n",
       "   '2158075347']},\n",
       " {'id': '2140143765',\n",
       "  'title': 'Clinical features and virological analysis of a case of Middle East respiratory syndrome coronavirus infection',\n",
       "  'abstract': 'Summary Background The Middle East respiratory syndrome coronavirus (MERS-CoV) is an emerging virus involved in cases and case clusters of severe acute respiratory infection in the Arabian Peninsula, T unisia, Morocco, France, Italy, Germany, and the UK. We provide a full description of a fatal case of MERS-CoV infection and associated phylogenetic analyses. Methods We report data for a patient who was admitted to the Klinikum Schwabing (Munich, Germany) for severe acute respiratory infection. We did diagnostic RT -PCR and indirect immunofl uorescence. From time of diagnosis, respiratory, faecal, and urine samples were obtained for virus quantifi cation. We constructed a maximum likelihood tree of the fi ve available complete MERS-CoV genomes.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Christian Drosten',\n",
       "   'Michael Seilmaier',\n",
       "   'Victor M. Corman',\n",
       "   'Wulf Hartmann',\n",
       "   'Gregor Scheible',\n",
       "   'Stefan Sack',\n",
       "   'Wolfgang Guggemos',\n",
       "   'Rene Kallies',\n",
       "   'Doreen Muth',\n",
       "   'Sandra Junglen',\n",
       "   'Marcel A. Müller',\n",
       "   'Walter Haas',\n",
       "   'Hana Guberina',\n",
       "   'Tim Röhnisch',\n",
       "   'Monika Schmid-Wendtner',\n",
       "   'Souhaib Aldabbagh',\n",
       "   'Ulf Dittmer',\n",
       "   'Hermann Gold',\n",
       "   'Petra Graf',\n",
       "   'Frank Bonin',\n",
       "   'Andrew Rambaut',\n",
       "   '',\n",
       "   'Clemens Martin Wendtner'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Viral load',\n",
       "   'Respiratory system',\n",
       "   'Virus',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Fatal outcome',\n",
       "   'Maximum likelihood tree',\n",
       "   'Severe acute respiratory infection',\n",
       "   'View Less'],\n",
       "  'citation_count': '440',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2166867592',\n",
       "   '2103546861',\n",
       "   '2132260239',\n",
       "   '1703839189',\n",
       "   '2119111857',\n",
       "   '2112147913',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2119775949',\n",
       "   '1690366459']},\n",
       " {'id': '2119775949',\n",
       "  'title': 'Clinical features and viral diagnosis of two cases of infection with Middle East Respiratory Syndrome coronavirus: a report of nosocomial transmission',\n",
       "  'abstract': \"Summary  Background  Human infection with a novel coronavirus named Middle East Respiratory Syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia and the Middle East in September, 2012, with 44 laboratory-confirmed cases as of May 23, 2013. We report detailed clinical and virological data for two related cases of MERS-CoV disease, after nosocomial transmission of the virus from one patient to another in a French hospital.  Methods  Patient 1 visited Dubai in April, 2013; patient 2 lives in France and did not travel abroad. Both patients had underlying immunosuppressive disorders. We tested specimens from the upper (nasopharyngeal swabs) or the lower (bronchoalveolar lavage, sputum) respiratory tract and whole blood, plasma, and serum specimens for MERS-CoV by real-time RT-PCR targeting the upE and Orf1A genes of MERS-CoV.  Findings  Initial clinical presentation included fever, chills, and myalgia in both patients, and for patient 1, diarrhoea. Respiratory symptoms rapidly became predominant with acute respiratory failure leading to mechanical ventilation and extracorporeal membrane oxygenation (ECMO). Both patients developed acute renal failure. MERS-CoV was detected in lower respiratory tract specimens with high viral load (eg, cycle threshold [Ct] values of 22·9 for upE and 24 for Orf1a for a bronchoalveolar lavage sample from patient 1; Ct values of 22·5 for upE and 23·9 for Orf1a for an induced sputum sample from patient 2), whereas nasopharyngeal specimens were weakly positive or inconclusive. The two patients shared the same room for 3 days. The incubation period was estimated at 9–12 days for the second case. No secondary transmission was documented in hospital staff despite the absence of specific protective measures before the diagnosis of MERS-CoV was suspected. Patient 1 died on May 28, due to refractory multiple organ failure.  Interpretation  Patients with respiratory symptoms returning from the Middle East or exposed to a confirmed case should be isolated and investigated for MERS-CoV with lower respiratory tract sample analysis and an assumed incubation period of 12 days. Immunosuppression should also be taken into account as a risk factor.  Funding  French Institute for Public Health Surveillance, ANR grant Labex Integrative Biology of Emerging Infectious Diseases, and the European Community's Seventh Framework Programme projects EMPERIE and PREDEMICS.\",\n",
       "  'date': '2013',\n",
       "  'authors': ['Benoit Guery',\n",
       "   'Julien Poissy',\n",
       "   'Loubna El Mansouf',\n",
       "   'Caroline Séjourné',\n",
       "   'Nicolas Ettahar',\n",
       "   'Xavier Lemaire',\n",
       "   'Fanny Vuotto',\n",
       "   'Anne Goffard',\n",
       "   'Sylvie Behillil',\n",
       "   '',\n",
       "   '',\n",
       "   'Vincent Enouf',\n",
       "   '',\n",
       "   '',\n",
       "   'Valérie Caro',\n",
       "   'Alexandra Mailles',\n",
       "   'Didier Che',\n",
       "   'Jean Claude Manuguerra',\n",
       "   'Daniel Mathieu',\n",
       "   'Arnaud Fontanet',\n",
       "   '',\n",
       "   'Sylvie Van Der Werf',\n",
       "   '',\n",
       "   ''],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Sputum',\n",
       "   'Coronavirus',\n",
       "   'Chills',\n",
       "   'Extracorporeal membrane oxygenation',\n",
       "   'Viral load',\n",
       "   'Respiratory tract',\n",
       "   'Bronchoalveolar lavage',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '476',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2166867592',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '2112147913',\n",
       "   '1852588318',\n",
       "   '2113457186',\n",
       "   '2163627712',\n",
       "   '2046153984',\n",
       "   '1690366459',\n",
       "   '1998725525']},\n",
       " {'id': '2195009776',\n",
       "  'title': 'A SARS-like cluster of circulating bat coronaviruses shows potential for human emergence',\n",
       "  'abstract': 'The emergence of severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome (MERS)-CoV underscores the threat of cross-species transmission events leading to outbreaks in humans. Here we examine the disease potential of a SARS-like virus, SHC014-CoV, which is currently circulating in Chinese horseshoe bat populations. Using the SARS-CoV reverse genetics system, we generated and characterized a chimeric virus expressing the spike of bat coronavirus SHC014 in a mouse-adapted SARS-CoV backbone. The results indicate that group 2b viruses encoding the SHC014 spike in a wild-type backbone can efficiently use multiple orthologs of the SARS receptor human angiotensin converting enzyme II (ACE2), replicate efficiently in primary human airway cells and achieve in vitro titers equivalent to epidemic strains of SARS-CoV. Additionally, in vivo experiments demonstrate replication of the chimeric virus in mouse lung with notable pathogenesis. Evaluation of available SARS-based immune-therapeutic and prophylactic modalities revealed poor efficacy; both monoclonal antibody and vaccine approaches failed to neutralize and protect from infection with CoVs using the novel spike protein. On the basis of these findings, we synthetically re-derived an infectious full-length SHC014 recombinant virus and demonstrate robust viral replication both in vitro and in vivo. Our work suggests a potential risk of SARS-CoV re-emergence from viruses currently circulating in bat populations.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Vineet D. Menachery',\n",
       "   'Boyd L. Yount',\n",
       "   'Kari Debbink',\n",
       "   'Sudhakar Agnihothram',\n",
       "   'Lisa E. Gralinski',\n",
       "   'Jessica A. Plante',\n",
       "   'Rachel L. Graham',\n",
       "   'Trevor Scobey',\n",
       "   'Xing Yi Ge',\n",
       "   'Eric F. Donaldson',\n",
       "   'Scott H. Randell',\n",
       "   'Antonio Lanzavecchia',\n",
       "   'Wayne A. Marasco',\n",
       "   'Zhengli Li Shi',\n",
       "   'Ralph S. Baric'],\n",
       "  'related_topics': ['Virus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Viral replication',\n",
       "   'Recombinant virus',\n",
       "   'Antibody',\n",
       "   'Reverse genetics',\n",
       "   'Monoclonal antibody',\n",
       "   'Virology',\n",
       "   'Horseshoe bat',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '628',\n",
       "  'reference_count': '25',\n",
       "  'references': ['1993577573',\n",
       "   '2094993149',\n",
       "   '2126707939',\n",
       "   '2092969802',\n",
       "   '2152528032',\n",
       "   '1995367098',\n",
       "   '2143230291',\n",
       "   '2074618762',\n",
       "   '1963580683',\n",
       "   '2098037373']},\n",
       " {'id': '2115555188',\n",
       "  'title': 'Middle East Respiratory Syndrome Coronavirus: Another Zoonotic Betacoronavirus Causing SARS-Like Disease',\n",
       "  'abstract': 'SUMMARY  The source of the severe acute respiratory syndrome (SARS) epidemic was traced to wildlife market civets and ultimately to bats. Subsequent hunting for novel coronaviruses (CoVs) led to the discovery of two additional human and over 40 animal CoVs, including the prototype lineage C betacoronaviruses, Tylonycteris bat CoV HKU4 and Pipistrellus bat CoV HKU5; these are phylogenetically closely related to the Middle East respiratory syndrome (MERS) CoV, which has affected more than 1,000 patients with over 35% fatality since its emergence in 2012. All primary cases of MERS are epidemiologically linked to the Middle East. Some of these patients had contacted camels which shed virus and/or had positive serology. Most secondary cases are related to health care-associated clusters. The disease is especially severe in elderly men with comorbidities. Clinical severity may be related to MERS-CoV9s ability to infect a broad range of cells with DPP4 expression, evade the host innate immune response, and induce cytokine dysregulation. Reverse transcription-PCR on respiratory and/or extrapulmonary specimens rapidly establishes diagnosis. Supportive treatment with extracorporeal membrane oxygenation and dialysis is often required in patients with organ failure. Antivirals with potent in vitro activities include neutralizing monoclonal antibodies, antiviral peptides, interferons, mycophenolic acid, and lopinavir. They should be evaluated in suitable animal models before clinical trials. Developing an effective camel MERS-CoV vaccine and implementing appropriate infection control measures may control the continuing epidemic.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Jasper F. W. Chan',\n",
       "   'Susanna K. P. Lau',\n",
       "   'Kelvin K. W. To',\n",
       "   'Vincent C. C. Cheng',\n",
       "   'Patrick C. Y. Woo',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Virus',\n",
       "   'Disease',\n",
       "   'Innate immune system',\n",
       "   'Infection control',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Respiratory system',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '719',\n",
       "  'reference_count': '351',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '1703839189',\n",
       "   '2565805236',\n",
       "   '2119111857',\n",
       "   '2112147913']},\n",
       " {'id': '2298153446',\n",
       "  'title': 'SARS-like WIV1-CoV poised for human emergence',\n",
       "  'abstract': 'Outbreaks from zoonotic sources represent a threat to both human disease as well as the global economy. Despite a wealth of metagenomics studies, methods to leverage these datasets to identify future threats are underdeveloped. In this study, we describe an approach that combines existing metagenomics data with reverse genetics to engineer reagents to evaluate emergence and pathogenic potential of circulating zoonotic viruses. Focusing on the severe acute respiratory syndrome (SARS)-like viruses, the results indicate that the WIV1-coronavirus (CoV) cluster has the ability to directly infect and may undergo limited transmission in human populations. However, in vivo attenuation suggests additional adaptation is required for epidemic disease. Importantly, available SARS monoclonal antibodies offered success in limiting viral infection absent from available vaccine approaches. Together, the data highlight the utility of a platform to identify and prioritize prepandemic strains harbored in animal reservoirs and document the threat posed by WIV1-CoV for emergence in human populations.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Vineet D. Menachery',\n",
       "   'Boyd L. Yount',\n",
       "   'Amy C Sims',\n",
       "   'Kari Debbink',\n",
       "   'Sudhakar S. Agnihothram',\n",
       "   'Lisa E. Gralinski',\n",
       "   'Rachel Lauren Graham',\n",
       "   'Trevor Scobey',\n",
       "   'Jessica A. Plante',\n",
       "   'Scott R. Royal',\n",
       "   'Jesica Swanstrom',\n",
       "   'Timothy Patrick Sheahan',\n",
       "   'Raymond J Pickles',\n",
       "   '',\n",
       "   'Davide Corti',\n",
       "   'Scott H Randell',\n",
       "   'Antonio Lanzavecchia',\n",
       "   'Wayne A. Marasco',\n",
       "   'Ralph S Baric',\n",
       "   ''],\n",
       "  'related_topics': ['Transmission (medicine)',\n",
       "   'Metagenomics',\n",
       "   'Outbreak',\n",
       "   'Disease cluster',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Epidemic disease',\n",
       "   'Human disease',\n",
       "   'Limiting',\n",
       "   'Viral infection',\n",
       "   'View Less'],\n",
       "  'citation_count': '271',\n",
       "  'reference_count': '27',\n",
       "  'references': ['1993577573',\n",
       "   '2094993149',\n",
       "   '2126707939',\n",
       "   '2092969802',\n",
       "   '2101176145',\n",
       "   '2152528032',\n",
       "   '1998383538',\n",
       "   '1995367098',\n",
       "   '2143230291',\n",
       "   '1963580683']},\n",
       " {'id': '2525468044',\n",
       "  'title': 'Viral Load Kinetics of MERS Coronavirus Infection.',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus continues to circulate in the Middle East. During a recent outbreak in Korea, changes in MERS coronavirus viral load were determined during the course of illness in 17 patients.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Myoung Don Oh',\n",
       "   'Wan Beom Park',\n",
       "   'Pyoeng Gyun Choe',\n",
       "   'Su Jin Choi',\n",
       "   'Jong I.I. Kim',\n",
       "   'Jeesoo Chae',\n",
       "   'Sung Sup Park',\n",
       "   'Eui Chong Kim',\n",
       "   'Hong Sang Oh',\n",
       "   'Eun Jung Kim',\n",
       "   'Eun Young Nam',\n",
       "   'Sun Hee Na',\n",
       "   'Dong Ki Kim',\n",
       "   'Sang Min Lee',\n",
       "   'Kyoung Ho Song',\n",
       "   'Ji Hwan Bang',\n",
       "   'Eu Suk Kim',\n",
       "   'Hong Bin Kim',\n",
       "   'Sang Won Park',\n",
       "   'Nam Joong Kim'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Viral load',\n",
       "   'Outbreak',\n",
       "   'Virology',\n",
       "   'Severity of illness',\n",
       "   'Medicine',\n",
       "   'Coronavirus Infections',\n",
       "   'Course of illness',\n",
       "   'View Less'],\n",
       "  'citation_count': '175',\n",
       "  'reference_count': '4',\n",
       "  'references': ['2126707939', '2103118479', '2405185968', '2134527559']},\n",
       " {'id': '1945961678',\n",
       "  'title': 'Treatment With Lopinavir/Ritonavir or Interferon-β1b Improves Outcome of MERS-CoV Infection in a Nonhuman Primate Model of Common Marmoset',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus (MERS-CoV) causes severe disease in human with an overall case-fatality rate of >35%. Effective antivirals are crucial for improving the clinical outcome of MERS. Although a number of repurposed drugs, convalescent-phase plasma, antiviral peptides, and neutralizing antibodies exhibit anti-MERS-CoV activity in vitro, most are not readily available or have not been evaluated in nonhuman primates. We assessed 3 repurposed drugs with potent in vitro anti-MERS-CoV activity (mycophenolate mofetil [MMF], lopinavir/ritonavir, and interferon-β1b) in common marmosets with severe disease resembling MERS in humans. The lopinavir/ritonavir-treated and interferon-β1b-treated animals had better outcome than the untreated animals, with improved clinical (mean clinical scores ↓50.9%-95.0% and ↓weight loss than the untreated animals), radiological (minimal pulmonary infiltrates), and pathological (mild bronchointerstitial pneumonia) findings, and lower mean viral loads in necropsied lung (↓0.59-1.06 log10 copies/glyceraldehyde 3-phosphate dehydrogenase [GAPDH]; P < .050) and extrapulmonary (↓0.11-1.29 log10 copies/GAPDH; P < .050 in kidney) tissues. In contrast, all MMF-treated animals developed severe and/or fatal disease with higher mean viral loads (↑0.15-0.54 log10 copies/GAPDH) than the untreated animals. The mortality rate at 36 hours postinoculation was 67% (untreated and MMF-treated) versus 0-33% (lopinavir/ritonavir-treated and interferon-β1b-treated). Lopinavir/ritonavir and interferon-β1b alone or in combination should be evaluated in clinical trials. MMF alone may worsen MERS and should not be used.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Jasper Fuk Woo Chan',\n",
       "   'Yanfeng Yao',\n",
       "   'Man Lung Yeung',\n",
       "   'Wei Deng',\n",
       "   'Linlin Bao',\n",
       "   'Lilong Jia',\n",
       "   'Fengdi Li',\n",
       "   'Chong Xiao',\n",
       "   'Hong Gao',\n",
       "   'Pin Yu',\n",
       "   'Jian Piao Cai',\n",
       "   'Hin Chu',\n",
       "   'Jie Zhou',\n",
       "   'Honglin Chen',\n",
       "   'Chuan Qin',\n",
       "   'Kwok Yung Yuen'],\n",
       "  'related_topics': ['Lopinavir/ritonavir',\n",
       "   'Lopinavir',\n",
       "   'Ritonavir',\n",
       "   'Viral load',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Immunology',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '573',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2166867592',\n",
       "   '2006434809',\n",
       "   '2115555188',\n",
       "   '2034462612',\n",
       "   '1977050884',\n",
       "   '2150120685',\n",
       "   '1947409115',\n",
       "   '2017248106',\n",
       "   '2078121682',\n",
       "   '2096238447']},\n",
       " {'id': '2099941783',\n",
       "  'title': 'Presence of Middle East respiratory syndrome coronavirus antibodies in Saudi Arabia: a nationwide, cross-sectional, serological study',\n",
       "  'abstract': \"Summary  Background  Scientific evidence suggests that dromedary camels are the intermediary host for the Middle East respiratory syndrome coronavirus (MERS-CoV). However, the actual number of infections in people who have had contact with camels is unknown and most index patients cannot recall any such contact. We aimed to do a nationwide serosurvey in Saudi Arabia to establish the prevalence of MERS-CoV antibodies, both in the general population and in populations of individuals who have maximum exposure to camels.  Methods  In the cross-sectional serosurvey, we tested human serum samples obtained from healthy individuals older than 15 years who attended primary health-care centres or participated in a national burden-of-disease study in all 13 provinces of Saudi Arabia. Additionally, we tested serum samples from shepherds and abattoir workers with occupational exposure to camels. Samples were screened by recombinant ELISA and MERS-CoV seropositivity was confirmed by recombinant immunofluorescence and plaque reduction neutralisation tests. We used two-tailed Mann Whitney  U  exact tests, χ 2 , and Fisher's exact tests to analyse the data.  Findings  Between Dec 1, 2012, and Dec 1, 2013, we obtained individual serum samples from 10\\u2008009 individuals. Anti-MERS-CoV antibodies were confirmed in 15 (0·15%; 95% CI 0·09–0·24) of 10\\u2008009 people in six of the 13 provinces. The mean age of seropositive individuals was significantly younger than that of patients with reported, laboratory-confirmed, primary Middle Eastern respiratory syndrome (43·5 years [SD 17·3]  vs  53·8 years [17·5]; p=0·008). Men had a higher antibody prevalence than did women (11 [0·25%] of 4341  vs  two [0·05%] of 4378; p=0·028) and antibody prevalence was significantly higher in central versus coastal provinces (14 [0·26%] of 5479  vs  one [0·02%] of 4529; p=0·003). Compared with the general population, seroprevalence of MERS-CoV antibodies was significantly increased by 15 times in shepherds (two [2·3%] of 87, p=0·0004) and by 23 times in slaughterhouse workers (five [3·6%] of 140; p  Interpretation  Seroprevalence of MERS-CoV antibodies was significantly higher in camel-exposed individuals than in the general population. By simple multiplication, a projected 44\\u2008951 (95% CI 26\\u2008971–71\\u2008922) individuals older than 15 years might be seropositive for MERS-CoV in Saudi Arabia. These individuals might be the source of infection for patients with confirmed MERS who had no previous exposure to camels.  Funding  European Union, German Centre for Infection Research, Federal Ministry of Education and Research, German Research Council, and Ministry of Health of Saudi Arabia.\",\n",
       "  'date': '2015',\n",
       "  'authors': ['Marcel A. Müller',\n",
       "   'Benjamin Meyer',\n",
       "   'Victor M. Corman',\n",
       "   'Malak Al-Masri',\n",
       "   'Abdulhafeez Turkestani',\n",
       "   'Daniel Ritz',\n",
       "   'Andrea Sieberg',\n",
       "   'Souhaib Aldabbagh',\n",
       "   'Berend J. Bosch',\n",
       "   'Erik Lattwein',\n",
       "   'Raafat F. Alhakeem',\n",
       "   'Abdullah M. Assiri',\n",
       "   'Ali M. Albarrak',\n",
       "   'Ali M. Al-Shangiti',\n",
       "   'Jaffar A. Al-Tawfiq',\n",
       "   '',\n",
       "   'Paul Wikramaratna',\n",
       "   'Abdullah A. Alrabeeah',\n",
       "   'Christian Drosten',\n",
       "   'Ziad A. Memish'],\n",
       "  'related_topics': ['Seroprevalence',\n",
       "   'European union',\n",
       "   'Population',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Serology',\n",
       "   'Cross-sectional study',\n",
       "   'Seroepidemiologic Studies',\n",
       "   'Demography',\n",
       "   'Antibody',\n",
       "   'Veterinary medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '268',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2107053896',\n",
       "   '2119111857',\n",
       "   '2160011624',\n",
       "   '1852588318',\n",
       "   '2145441153',\n",
       "   '2128886090',\n",
       "   '2119837294',\n",
       "   '2108756402',\n",
       "   '1968393246',\n",
       "   '2130227690']},\n",
       " {'id': '1909499787',\n",
       "  'title': 'MERS, SARS, and Ebola: The Role of Super-Spreaders in Infectious Disease.',\n",
       "  'abstract': 'Super-spreading occurs when a single patient infects a disproportionate number of contacts. The 2015 MERS-CoV, 2003 SARS-CoV, and to a lesser extent 2014-15 Ebola virus outbreaks were driven by super-spreaders. We summarize documented super-spreading in these outbreaks, explore contributing factors, and suggest studies to better understand super-spreading.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Gary Wong',\n",
       "   'Wenjun Liu',\n",
       "   'Yingxia Liu',\n",
       "   'Boping Zhou',\n",
       "   'Yuhai Bi',\n",
       "   'George F. Gao'],\n",
       "  'related_topics': ['Ebola virus',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Outbreak',\n",
       "   'Virology',\n",
       "   'Bioinformatics',\n",
       "   'Biology',\n",
       "   'Disease transmission',\n",
       "   'Single patient',\n",
       "   'View Less'],\n",
       "  'citation_count': '279',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2115102869',\n",
       "   '1975375203',\n",
       "   '2096145431',\n",
       "   '2227495319',\n",
       "   '1994871753',\n",
       "   '1979807576',\n",
       "   '1942680103',\n",
       "   '2024845268',\n",
       "   '2135540513',\n",
       "   '2089797630']},\n",
       " {'id': '3027518954',\n",
       "  'title': 'Pathogen genomics in public health',\n",
       "  'abstract': 'Summary Rapid advances in DNA sequencing technology (“next-generation sequencing”) have inspired optimism about the potential of human genomics for “precision medicine.” Meanwhile, pathogen genomic...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Gregory L. Armstrong',\n",
       "   'Duncan R. MacCannell',\n",
       "   'Jill Taylor',\n",
       "   'Heather A. Carleton',\n",
       "   'Elizabeth B. Neuhaus',\n",
       "   'Richard S. Bradbury',\n",
       "   'James E. Posey',\n",
       "   'Marta Gwinn'],\n",
       "  'related_topics': ['Genomics',\n",
       "   'DNA sequencing',\n",
       "   'Pathogen',\n",
       "   'Public health',\n",
       "   'Computational biology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Human genomics',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '80',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2792024998',\n",
       "  'title': 'From “A”IV to “Z”IKV: Attacks from Emerging and Re-emerging Pathogens',\n",
       "  'abstract': '100 years after the infamous “Spanish flu” pandemic, the 2017–2018 flu season has been severe, with numerous infections worldwide. In between, there have been continuous, relentless attacks from (re-)emerging viruses. To fully understand viral pathogenesis and develop effective medical countermeasures, we must strengthen current surveillance and basic research efforts.',\n",
       "  'date': '2018',\n",
       "  'authors': ['George F. Gao', ''],\n",
       "  'related_topics': ['Pandemic',\n",
       "   'Flu season',\n",
       "   'Viral pathogenesis',\n",
       "   'Influenza A virus',\n",
       "   'Intensive care medicine',\n",
       "   'Biology',\n",
       "   'Basic research',\n",
       "   'View Less'],\n",
       "  'citation_count': '165',\n",
       "  'reference_count': '10',\n",
       "  'references': ['1975375203',\n",
       "   '2116682907',\n",
       "   '2788045019',\n",
       "   '1783641736',\n",
       "   '1942680103',\n",
       "   '1967283148',\n",
       "   '2793181185',\n",
       "   '2081635462',\n",
       "   '2473338860',\n",
       "   '2782496877']},\n",
       " {'id': '2955025503',\n",
       "  'title': 'Viral and Bacterial Etiology of Acute Febrile Respiratory Syndrome among Patients in Qinghai, China',\n",
       "  'abstract': 'Objective This study was conducted to investigate the viral and bacterial etiology and epidemiology of patients with acute febrile respiratory syndrome (AFRS) in Qinghai using a commercial routine multiplex-ligation-nucleic acid amplification test (NAT)-based assay.   Methods A total of 445 nasopharyngeal swabs specimens from patients with AFRS were analyzed using the RespiFinderSmart22kit (PathoFinder BV, Netherlands) and the LightCycler 480 real-time PCR system.   Results Among the 225 (225/445, 51%) positive specimens, 329 positive pathogens were detected, including 298 (90.58%) viruses and 31 (9%) bacteria. The most commonly detected pathogens were infiuenza virus (IFV; 37.39%; 123/329), adenovirus (AdV; 17.02%; 56/329), human coronaviruses (HCoVs; 10.94%; 36/329), rhinovirus/enterovirus (RV/EV; 10.03%; 33/329), parainfiuenza viruses (PIVs; 8.51%; 28/329), and Mycoplasma pneumoniae (M. pneu; 8.51%; 28/329), respectively. Among the co-infected cases (17.53%; 78/445), IFV/AdV and IFV/M. pneu were the most common co-infections. Most of the respiratory viruses were detected in summer and fall.   Conclusion In our study, IFV-A was the most common respiratory pathogen among 22 detected pathogens, followed by AdV, HCoV, RV/EV, PIV, and M. pneu. Bacteria appeared less frequently than viruses, and co-infection was the most common phenomenon among viral pathogens. Pathogens were distributed among different age groups and respiratory viruses were generally active in July, September, and November. Enhanced surveillance and early detection can be useful in the diagnosis, treatment, and prevention of AFRS, as well as for guiding the development of appropriate public health strategies.',\n",
       "  'date': '2019',\n",
       "  'authors': ['Gao Shan Liu',\n",
       "   'Hong Li',\n",
       "   'Sheng Cang Zhao',\n",
       "   'Rou Jian Lu',\n",
       "   'Pei Hua Niu',\n",
       "   'Wen Jie Tan'],\n",
       "  'related_topics': ['Enterovirus',\n",
       "   'Rhinovirus',\n",
       "   'Mycoplasma pneumoniae',\n",
       "   'Virus',\n",
       "   'Respiratory system',\n",
       "   'Virology',\n",
       "   'Epidemiology',\n",
       "   'Bacteria',\n",
       "   'Medicine',\n",
       "   'Bacterial etiology',\n",
       "   'View Less'],\n",
       "  'citation_count': '9',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2078917493',\n",
       "   '2287076968',\n",
       "   '2120839730',\n",
       "   '2412526156',\n",
       "   '2081835188',\n",
       "   '2083870139',\n",
       "   '2131338055',\n",
       "   '2097213674',\n",
       "   '2016253387',\n",
       "   '2159773954']},\n",
       " {'id': '2257005270',\n",
       "  'title': 'Coronaviruses and the human airway: a universal system for virus-host interaction studies.',\n",
       "  'abstract': 'Human coronaviruses (HCoVs) are large RNA viruses that infect the human respiratory tract. The emergence of both Severe Acute Respiratory Syndrome and Middle East Respiratory syndrome CoVs as well as the yearly circulation of four common CoVs highlights the importance of elucidating the different mechanisms employed by these viruses to evade the host immune response, determine their tropism and identify antiviral compounds. Various animal models have been established to investigate HCoV infection, including mice and non-human primates. To establish a link between the research conducted in animal models and humans, an organotypic human airway culture system, that recapitulates the human airway epithelium, has been developed. Currently, different cell culture systems are available to recapitulate the human airways, including the Air-Liquid Interface (ALI) human airway epithelium (HAE) model. Tracheobronchial HAE cultures recapitulate the primary entry point of human respiratory viruses while the alveolar model allows for elucidation of mechanisms involved in viral infection and pathogenesis in the alveoli. These organotypic human airway cultures represent a universal platform to study respiratory virus-host interaction by offering more detailed insights compared to cell lines. Additionally, the epidemic potential of this virus family highlights the need for both vaccines and antivirals. No commercial vaccine is available but various effective antivirals have been identified, some with potential for human treatment. These morphological airway cultures are also well suited for the identification of antivirals, evaluation of compound toxicity and viral inhibition.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Hulda Run Jonsdottir', '', 'Ronald Dijkman', ''],\n",
       "  'related_topics': ['Tissue tropism',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Respiratory epithelium',\n",
       "   'Respiratory tract',\n",
       "   'Virus',\n",
       "   'Tropism',\n",
       "   'Immune system',\n",
       "   'Immunity',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '72',\n",
       "  'reference_count': '110',\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '1993577573',\n",
       "   '2119111857',\n",
       "   '2116586125',\n",
       "   '2195009776',\n",
       "   '311927316',\n",
       "   '2167384912',\n",
       "   '2111412754',\n",
       "   '2170933940']},\n",
       " {'id': '3000834295',\n",
       "  'title': 'Coronavirus Infections-More Than Just the Common Cold.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Catharine I. Paules', 'Hilary D. Marston', 'Anthony S. Fauci'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Common cold',\n",
       "   'Betacoronavirus',\n",
       "   'Medicine',\n",
       "   'Virology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,432',\n",
       "  'reference_count': '12',\n",
       "  'references': ['3000413850',\n",
       "   '2470646526',\n",
       "   '2909194930',\n",
       "   '3027659922',\n",
       "   '3027264380',\n",
       "   '2718090702',\n",
       "   '1993435091',\n",
       "   '2792208289',\n",
       "   '3030422584',\n",
       "   '3023275846']},\n",
       " {'id': '3003951199',\n",
       "  'title': 'Importation and Human-to-Human Transmission of a Novel Coronavirus in Vietnam.',\n",
       "  'abstract': 'Human-to-Human Coronavirus Transmission in Vietnam The authors describe transmission of 2019-nCoV from a father, who had flown with his wife from Wuhan to Hanoi, to the son, who met his father and ...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Lan T. Phan',\n",
       "   'Thuong V. Nguyen',\n",
       "   'Quang C. Luong',\n",
       "   'Thinh V. Nguyen',\n",
       "   'Hieu T. Nguyen',\n",
       "   'Hung Q. Le',\n",
       "   'Thuc T. Nguyen',\n",
       "   'Thang M. Cao',\n",
       "   'Quang D. Pham'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Transmission (mechanics)',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Lung pathology',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral transmission',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,046',\n",
       "  'reference_count': '1',\n",
       "  'references': ['3001897055']},\n",
       " {'id': '2999409984',\n",
       "  'title': 'The continuing 2019-nCoV epidemic threat of novel coronaviruses to global health - The latest 2019 novel coronavirus outbreak in Wuhan, China.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['David S. Hui',\n",
       "   'Esam Ei Azhar',\n",
       "   'Tariq A. Madani',\n",
       "   'Francine Ntoumi',\n",
       "   'Richard Kock',\n",
       "   'Osman Dar',\n",
       "   'Giuseppe Ippolito',\n",
       "   'Timothy D. Mchugh',\n",
       "   'Ziad A. Memish',\n",
       "   'Christian Drosten',\n",
       "   'Alimuddin Zumla',\n",
       "   'Eskild Petersen'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Global health',\n",
       "   'Medicine',\n",
       "   'China',\n",
       "   'Environmental health',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,895',\n",
       "  'reference_count': '10',\n",
       "  'references': ['3027264380',\n",
       "   '2981657433',\n",
       "   '2981752008',\n",
       "   '3000092258',\n",
       "   '3029903408',\n",
       "   '2442480670',\n",
       "   '3031559976',\n",
       "   '3023268903',\n",
       "   '3029135544',\n",
       "   '2414957595']},\n",
       " {'id': '2999318660',\n",
       "  'title': 'Outbreak of pneumonia of unknown etiology in Wuhan, China: The mystery and the miracle.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Hongzhou Lu', 'Charles W. Stratton', 'Yi Wei Tang'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Outbreak',\n",
       "   'Etiology',\n",
       "   'Medicine',\n",
       "   'Miracle',\n",
       "   'Virology',\n",
       "   'China',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,100',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2470646526',\n",
       "   '2132260239',\n",
       "   '2255243349',\n",
       "   '2766931063',\n",
       "   '2103503670',\n",
       "   '2134061616',\n",
       "   '1997954607',\n",
       "   '2158887145',\n",
       "   '2121494157']},\n",
       " {'id': '1803784511',\n",
       "  'title': 'Acute respiratory distress syndrome: the Berlin Definition.',\n",
       "  'abstract': 'The acute respiratory distress syndrome (ARDS) was defined in 1994 by the American-European Consensus Conference (AECC); since then, issues regarding the reliability and validity of this definition have emerged. Using a consensus process, a panel of experts convened in 2011 (an initiative of the European Society of Intensive Care Medicine endorsed by the American Thoracic Society and the Society of Critical Care Medicine) developed the Berlin Definition, focusing on feasibility, reliability, validity, and objective evaluation of its performance. A draft definition proposed 3 mutually exclusive categories of ARDS based on degree of hypoxemia: mild (200 mm Hg < PaO2/FIO2 ≤ 300 mm Hg), moderate (100 mm Hg < PaO2/FIO2 ≤ 200 mm Hg), and severe (PaO2/FIO2 ≤ 100 mm Hg) and 4 ancillary variables for severe ARDS: radiographic severity, respiratory system compliance (≤40 mL/cm H2O), positive end-expiratory pressure (≥10 cm H2O), and corrected expired volume per minute (≥10 L/min). The draft Berlin Definition was empirically evaluated using patient-level meta-analysis of 4188 patients with ARDS from 4 multicenter clinical data sets and 269 patients with ARDS from 3 single-center data sets containing physiologic information. The 4 ancillary variables did not contribute to the predictive validity of severe ARDS for mortality and were removed from the definition. Using the Berlin Definition, stages of mild, moderate, and severe ARDS were associated with increased mortality (27%; 95% CI, 24%-30%; 32%; 95% CI, 29%-34%; and 45%; 95% CI, 42%-48%, respectively; P < .001) and increased median duration of mechanical ventilation in survivors (5 days; interquartile [IQR], 2-11; 7 days; IQR, 4-14; and 9 days; IQR, 5-17, respectively; P < .001). Compared with the AECC definition, the final Berlin Definition had better predictive validity for mortality, with an area under the receiver operating curve of 0.577 (95% CI, 0.561-0.593) vs 0.536 (95% CI, 0.520-0.553; P < .001). This updated and revised Berlin Definition for ARDS addresses a number of the limitations of the AECC definition. The approach of combining consensus discussions with empirical evaluation may serve as a model to create more accurate, evidence-based, critical illness syndrome definitions and to better inform clinical care, research, and health services planning.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Ards Definition Task Force',\n",
       "   'V Marco Ranieri',\n",
       "   'Gordon D Rubenfeld',\n",
       "   'B Taylor Thompson',\n",
       "   'Niall D Ferguson',\n",
       "   'Ellen Caldwell',\n",
       "   'Eddy Fan',\n",
       "   'Luigi Camporota',\n",
       "   '',\n",
       "   'Arthur S Slutsky'],\n",
       "  'related_topics': ['ARDS',\n",
       "   'Prone ventilation',\n",
       "   'Interquartile range',\n",
       "   'Severity of illness',\n",
       "   'Mechanical ventilation',\n",
       "   'Hypoxemia',\n",
       "   'Epidemiology',\n",
       "   'Airway pressure release ventilation',\n",
       "   'Emergency medicine',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,998',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2161328469',\n",
       "   '2068854215',\n",
       "   '2328176404',\n",
       "   '2326364273',\n",
       "   '1823772832',\n",
       "   '1979469936',\n",
       "   '2113752525',\n",
       "   '2070070465',\n",
       "   '2168829312',\n",
       "   '2074935456']},\n",
       " {'id': '2999364275',\n",
       "  'title': 'Evolution of the novel coronavirus from the ongoing Wuhan outbreak and modeling of its spike protein for risk of human transmission',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Xintian Xu',\n",
       "   'Ping Chen',\n",
       "   'Jingfang Wang',\n",
       "   'Jiannan Feng',\n",
       "   'Hui Zhou',\n",
       "   'Xuan Li',\n",
       "   'Wu Zhong',\n",
       "   'Pei Hao'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Betacoronavirus',\n",
       "   'Transmission (mechanics)',\n",
       "   'Viral Epidemiology',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Spike Protein',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,500',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2605343262',\n",
       "   '2775086803',\n",
       "   '2119111857',\n",
       "   '2404280981',\n",
       "   '2060809301',\n",
       "   '1982533785',\n",
       "   '3021832855',\n",
       "   '2126080553',\n",
       "   '3000376083']},\n",
       " {'id': '2909194930',\n",
       "  'title': 'From SARS to MERS, Thrusting Coronaviruses into the Spotlight',\n",
       "  'abstract': 'Coronaviruses (CoVs) have formerly been regarded as relatively harmless respiratory pathogens to humans. However, two outbreaks of severe respiratory tract infection, caused by the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV), as a result of zoonotic CoVs crossing the species barrier, caused high pathogenicity and mortality rates in human populations. This brought CoVs global attention and highlighted the importance of controlling infectious pathogens at international borders. In this review, we focus on our current understanding of the epidemiology, pathogenesis, prevention, and treatment of SARS-CoV and MERS-CoV, as well as provides details on the pivotal structure and function of the spike proteins (S proteins) on the surface of each of these viruses. For building up more suitable animal models, we compare the current animal models recapitulating pathogenesis and summarize the potential role of host receptors contributing to diverse host affinity in various species. We outline the research still needed to fully elucidate the pathogenic mechanism of these viruses, to construct reproducible animal models, and ultimately develop countermeasures to conquer not only SARS-CoV and MERS-CoV, but also these emerging coronaviral diseases.',\n",
       "  'date': '2019',\n",
       "  'authors': ['Zhiqi Song',\n",
       "   'Yanfeng Xu',\n",
       "   'Linlin Bao',\n",
       "   'Ling Zhang',\n",
       "   'Pin Yu',\n",
       "   'Yajin Qu',\n",
       "   'Hua Zhu',\n",
       "   'Wenjie Zhao',\n",
       "   'Yunlin Han',\n",
       "   'Chuan Qin'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Outbreak',\n",
       "   'Mechanism (biology)',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Pathogenicity',\n",
       "   'Respiratory pathogens',\n",
       "   'Severe acute respiratory syndrome coronavirus',\n",
       "   'Structure and function',\n",
       "   'View Less'],\n",
       "  'citation_count': '784',\n",
       "  'reference_count': '209',\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '2470646526',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2107053896',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '1993577573',\n",
       "   '2138324310']},\n",
       " {'id': '2991899552',\n",
       "  'title': 'Clinical Features Predicting Mortality Risk in Patients With Viral Pneumonia: The MuLBSTA Score.',\n",
       "  'abstract': 'Objective The aim of this study was to further clarify clinical characteristics and predict mortality risk among patients with viral pneumonia. Methods A total of 528 patients with viral pneumonia at RuiJin hospital in Shanghai from May 2015 to May 2019 were recruited. Multiplex real-time RT-PCR was used to detect respiratory viruses. Demographic information, comorbidities, routine laboratory examinations, immunological indexes, etiological detections, radiological images and treatment were collected on admission. Results 76 (14.4%) patients died within 90 days in hospital. A predictive MuLBSTA score was calculated on the basis of a multivariate logistic regression model in order to predict mortality with a weighted score that included multilobular infiltrates (OR = 5.20, 95% CI 1.41-12.52, p = 0.010; 5 points), lymphocyte ≤ 0.8∗109/L (OR = 4.53, 95% CI 2.55-8.05, p < 0.001; 4 points), bacterial coinfection (OR = 3.71, 95% CI 2.11-6.51, p < 0.001; 4 points), acute-smoker (OR = 3.19, 95% CI 1.34-6.26, p = 0.001; 3 points), quit-smoker (OR = 2.18, 95% CI 0.99-4.82, p = 0.054; 2 points), hypertension (OR = 2.39, 95% CI 1.55-4.26, p = 0.003; 2 points) and age ≥60 years (OR = 2.14, 95% CI 1.04-4.39, p = 0.038; 2 points). 12 points was used as a cut-off value for mortality risk stratification. This model showed sensitivity of 0.776, specificity of 0.778 and a better predictive ability than CURB-65 (AUROC = 0.773 vs. 0.717, p < 0.001). Conclusion Here, we designed an easy-to-use clinically predictive tool for assessing 90-day mortality risk of viral pneumonia. It can accurately stratify hospitalized patients with viral pneumonia into relevant risk categories and could provide guidance to make further clinical decisions.',\n",
       "  'date': '2019',\n",
       "  'authors': ['Lingxi Guo',\n",
       "   'Dong Wei',\n",
       "   'Xinxin Zhang',\n",
       "   'Yurong Wu',\n",
       "   'Qingyun Li',\n",
       "   'Min Zhou',\n",
       "   'Jieming Qu'],\n",
       "  'related_topics': ['Viral pneumonia',\n",
       "   'Etiology',\n",
       "   'Internal medicine',\n",
       "   'Coinfection',\n",
       "   'Medicine',\n",
       "   'Hospitalized patients',\n",
       "   'In patient',\n",
       "   'Multivariate logistic regression model',\n",
       "   'Routine laboratory',\n",
       "   'Weighted score',\n",
       "   'View Less'],\n",
       "  'citation_count': '275',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2133979383',\n",
       "   '2065974896',\n",
       "   '2159340685',\n",
       "   '2091139031',\n",
       "   '2103645914',\n",
       "   '2948483377',\n",
       "   '2021046603',\n",
       "   '2155020492',\n",
       "   '1975461687',\n",
       "   '2900850632']},\n",
       " {'id': '3002533507',\n",
       "  'title': 'A Novel Coronavirus Emerging in China - Key Questions for Impact Assessment.',\n",
       "  'abstract': 'A Novel Coronavirus Emerging in China A novel coronavirus, designated as 2019-nCoV, emerged in Wuhan, China, at the end of 2019. Although many details of the emergence of this virus remain unknown,...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Vincent J. Munster',\n",
       "   'Marion Koopmans',\n",
       "   'Neeltje van Doremalen',\n",
       "   'Debby van Riel',\n",
       "   'Emmie de Wit'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'China',\n",
       "   'Impact assessment',\n",
       "   'Economic growth',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,081',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3002715510',\n",
       "  'title': 'Another Decade, Another Coronavirus.',\n",
       "  'abstract': 'For the third time in as many decades, a zoonotic coronavirus has crossed species to infect human populations. This virus, provisionally called 2019-nCoV, was first identified in Wuhan, China, in p...',\n",
       "  'date': '2020',\n",
       "  'authors': ['Stanley Perlman'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Virus',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '736',\n",
       "  'reference_count': '8',\n",
       "  'references': ['3001897055',\n",
       "   '2801339009',\n",
       "   '2126707939',\n",
       "   '2156273941',\n",
       "   '2217313808',\n",
       "   '2538584349',\n",
       "   '96734778',\n",
       "   '2002481497']},\n",
       " {'id': '3001971765',\n",
       "  'title': 'Real-time tentative assessment of the epidemiological characteristics of novel coronavirus infections in Wuhan, China, as at 22 January 2020.',\n",
       "  'abstract': 'A novel coronavirus (2019-nCoV) causing severe acute respiratory disease emerged recently in Wuhan, China. Information on reported cases strongly indicates human-to-human spread, and the most recent information is increasingly indicative of sustained human-to-human transmission. While the overall severity profile among cases may change as more mild cases are identified, we estimate a risk of fatality among hospitalised cases at 14% (95% confidence interval: 3.9–32%).',\n",
       "  'date': '2020',\n",
       "  'authors': ['Peng Wu',\n",
       "   'Xinxin Hao',\n",
       "   'Eric H Y Lau',\n",
       "   'Jessica Y Wong',\n",
       "   'Kathy S M Leung',\n",
       "   'Joseph T Wu',\n",
       "   'Benjamin J Cowling',\n",
       "   'Gabriel M Leung'],\n",
       "  'related_topics': ['Communicable disease',\n",
       "   'Coronavirus',\n",
       "   'Epidemiology',\n",
       "   'Risk assessment',\n",
       "   'Confidence interval',\n",
       "   'Transmission (medicine)',\n",
       "   'Pediatrics',\n",
       "   'Public health',\n",
       "   'Medicine',\n",
       "   'China',\n",
       "   'View Less'],\n",
       "  'citation_count': '399',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2306794997',\n",
       "   '1909499787',\n",
       "   '2918873120',\n",
       "   '2801339009',\n",
       "   '2227495319',\n",
       "   '2534644646',\n",
       "   '1042757214',\n",
       "   '2156614913',\n",
       "   '2109088393',\n",
       "   '2127974353']},\n",
       " {'id': '2147166346',\n",
       "  'title': 'Transmission Dynamics and Control of Severe Acute Respiratory Syndrome',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) is a recently described illness of humans that has spread widely over the past 6 months. With the use of detailed epidemiologic data from Singapore and epidemic curves from other settings, we estimated the reproductive number for SARS in the absence of interventions and in the presence of control efforts. We estimate that a single infectious case of SARS will infect about three secondary cases in a population that has not yet instituted control measures. Public-health efforts to reduce transmission are expected to have a substantial impact on reducing the size of the epidemic.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Marc Lipsitch',\n",
       "   'Ted Cohen',\n",
       "   'Ben Cooper',\n",
       "   'James M. Robins',\n",
       "   'Stefan Ma',\n",
       "   'Lyn James',\n",
       "   'Gowri Gopalakrishna',\n",
       "   'Suok Kai Chew',\n",
       "   'Chorh Chuan Tan',\n",
       "   'Matthew H. Samore',\n",
       "   'David Fisman',\n",
       "   'Megan Murray'],\n",
       "  'related_topics': ['Population',\n",
       "   'Serial interval',\n",
       "   'Transmission (mechanics)',\n",
       "   'Epidemiology',\n",
       "   'Contact tracing',\n",
       "   'Intensive care medicine',\n",
       "   'Viral disease',\n",
       "   'Respiratory disease',\n",
       "   'Psychological intervention',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,553',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '1606697907',\n",
       "   '2011756067',\n",
       "   '2318510691',\n",
       "   '1965399019',\n",
       "   '1979065938']},\n",
       " {'id': '2149508011',\n",
       "  'title': 'Evidence for camel-to-human transmission of MERS coronavirus',\n",
       "  'abstract': 'We describe the isolation and sequencing of Middle East respiratory syndrome coronavirus (MERS-CoV) obtained from a dromedary camel and from a patient who died of laboratory-confirmed MERS-CoV infection after close contact with camels that had rhinorrhea. Nasal swabs collected from the patient and from one of his nine camels were positive for MERS-CoV RNA. In addition, MERS-CoV was isolated from the patient and the camel. The full genome sequences of the two isolates were identical. Serologic data indicated that MERS-CoV was circulating in the camels but not in the patient before the human infection occurred. These data suggest that this fatal case of human MERS-CoV infection was transmitted through close contact with an infected camel.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Esam I. Azhar',\n",
       "   'Sherif A. El-Kafrawy',\n",
       "   'Suha A. Farraj',\n",
       "   'Ahmed M. Hassan',\n",
       "   'Muneera S. Al-Saeed',\n",
       "   'Anwar M. Hashem',\n",
       "   'Tariq A. Madani'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Serology',\n",
       "   'Transmission (medicine)',\n",
       "   'rhinorrhea',\n",
       "   'Isolation (health care)',\n",
       "   'Virology',\n",
       "   'Nasal Swab',\n",
       "   'Biology',\n",
       "   'Close contact',\n",
       "   'Dromedary camel',\n",
       "   'View Less'],\n",
       "  'citation_count': '768',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2160011624',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2113457186',\n",
       "   '2145441153',\n",
       "   '2119775949',\n",
       "   '1690366459',\n",
       "   '2049975503']},\n",
       " {'id': '2103503670',\n",
       "  'title': 'Bats are natural reservoirs of SARS-like coronaviruses.',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) emerged in 2002 to 2003 in southern China. The origin of its etiological agent, the SARS coronavirus (SARS-CoV), remains elusive. Here we report that species of bats are a natural host of coronaviruses closely related to those responsible for the SARS outbreak. These viruses, termed SARS-like coronaviruses (SL-CoVs), display greater genetic variation than SARS-CoV isolated from humans or from civets. The human and civet isolates of SARS-CoV nestle phylogenetically within the spectrum of SL-CoVs, indicating that the virus responsible for the SARS outbreak was a member of this coronavirus group.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Wendong Li',\n",
       "   'Zhengli',\n",
       "   'Meng',\n",
       "   'Wuze',\n",
       "   'Craig',\n",
       "   'Jonathan H.',\n",
       "   'Hanzhong',\n",
       "   'Gary',\n",
       "   'Zhihong',\n",
       "   'Huajun',\n",
       "   'Jianhong',\n",
       "   'Jennifer',\n",
       "   'Hume',\n",
       "   'Peter',\n",
       "   'Bryan T.',\n",
       "   'Shuyi',\n",
       "   'Lin-Fa'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Alphacoronavirus',\n",
       "   'Disease reservoir',\n",
       "   'Coronaviridae',\n",
       "   'Outbreak',\n",
       "   'Rhinolophus',\n",
       "   'Civet',\n",
       "   'Nidovirales',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,278',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2134061616',\n",
       "   '1990059132',\n",
       "   '2127949919',\n",
       "   '96734778',\n",
       "   '2076620790',\n",
       "   '2042499956']},\n",
       " {'id': '2807736175',\n",
       "  'title': 'Saliva as a diagnostic specimen for testing respiratory virus by a point-of-care molecular assay: a diagnostic validity study.',\n",
       "  'abstract': 'Abstract  Objectives  Automated point-of-care molecular assays have greatly shortened the turnaround time of respiratory virus testing. One of the major bottlenecks now lies at the specimen collection step, especially in a busy clinical setting. Saliva is a convenient specimen type that can be provided easily by adult patients. This study assessed the diagnostic validity, specimen collection time and cost associated with the use of saliva.  Methods  This was a prospective diagnostic validity study comparing the detection rate of respiratory viruses between saliva and nasopharyngeal aspirate (NPA) among adult hospitalized patients using Xpert® Xpress Flu/RSV. The cost and time associated with the collection of saliva and nasopharyngeal specimens were also estimated.  Results  Between July and October 2017, 214 patients were recruited. The overall agreement between saliva and NPA was 93.3% (196/210, κ 0.851, 95% CI 0.776–0.926). There was no significant difference in the detection rate of respiratory viruses between saliva and NPA (32.9% (69/210) versus 35.7% (75/210); p 0.146). The overall sensitivity and specificity were 90.8% (81.9%–96.2%) and 100% (97.3%–100%), respectively, for saliva, and were 96.1% (88.9%–99.2%) and 98.5% (94.7%–99.8%), respectively, for NPA. The time and cost associated with the collection of saliva were 2.26-fold and 2.59-fold lower, respectively, than those of NPA.  Conclusions  Saliva specimens have high sensitivity and specificity in the detection of respiratory viruses by an automated multiplex Clinical Laboratory Improvement Amendments-waived point-of-care molecular assay when compared with those of NPA. The use of saliva also reduces the time and cost associated with specimen collection.',\n",
       "  'date': '2019',\n",
       "  'authors': ['K.K.W. To',\n",
       "   'C.C.Y. Yip',\n",
       "   '',\n",
       "   'C.Y.W. Lai',\n",
       "   '',\n",
       "   'C.K.H. Wong',\n",
       "   'D.T.Y. Ho',\n",
       "   'P.K.P. Pang',\n",
       "   'A.C.K. Ng',\n",
       "   'K.-H. Leung',\n",
       "   'R.W.S. Poon',\n",
       "   '',\n",
       "   'K.-H. Chan',\n",
       "   'V.C.C. Cheng',\n",
       "   '',\n",
       "   'I.F.N. Hung',\n",
       "   '',\n",
       "   'K.-Y. Yuen'],\n",
       "  'related_topics': ['Specimen collection',\n",
       "   'Respiratory virus',\n",
       "   'Saliva',\n",
       "   'Nasopharyngeal aspirate',\n",
       "   'Point-of-care testing',\n",
       "   'Multiplex',\n",
       "   'Point of care',\n",
       "   'Respiratory system',\n",
       "   'Gastroenterology',\n",
       "   'Medicine',\n",
       "   'Internal medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '113',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2000714505',\n",
       "   '2164777277',\n",
       "   '2102263282',\n",
       "   '2103338644',\n",
       "   '2752904261',\n",
       "   '2605374894',\n",
       "   '2101172433',\n",
       "   '2623181050',\n",
       "   '2752140764',\n",
       "   '2548288333']},\n",
       " {'id': '2105637133',\n",
       "  'title': 'Discovery of Seven Novel Mammalian and Avian Coronaviruses in the Genus Deltacoronavirus Supports Bat Coronaviruses as the Gene Source of Alphacoronavirus and Betacoronavirus and Avian Coronaviruses as the Gene Source of Gammacoronavirus and Deltacoronavirus',\n",
       "  'abstract': 'Recently, we reported the discovery of three novel coronaviruses, bulbul coronavirus HKU11, thrush coronavirus HKU12, and munia coronavirus HKU13, which were identified as representatives of a novel genus, Deltacoronavirus, in the subfamily Coronavirinae. In this territory-wide molecular epidemiology study involving 3,137 mammals and 3,298 birds, we discovered seven additional novel deltacoronaviruses in pigs and birds, which we named porcine coronavirus HKU15, white-eye coronavirus HKU16, sparrow coronavirus HKU17, magpie robin coronavirus HKU18, night heron coronavirus HKU19, wigeon coronavirus HKU20, and common moorhen coronavirus HKU21. Complete genome sequencing and comparative genome analysis showed that the avian and mammalian deltacoronaviruses have similar genome characteristics and structures. They all have relatively small genomes (25.421 to 26.674 kb), the smallest among all coronaviruses. They all have a single papain-like protease domain in the nsp3 gene; an accessory gene, NS6 open reading frame (ORF), located between the M and N genes; and a variable number of accessory genes (up to four) downstream of the N gene. Moreover, they all have the same putative transcription regulatory sequence of ACACCA. Molecular clock analysis showed that the most recent common ancestor of all coronaviruses was estimated at approximately 8100 BC, and those of Alphacoronavirus, Betacoronavirus, Gammacoronavirus, and Deltacoronavirus were at approximately 2400 BC, 3300 BC, 2800 BC, and 3000 BC, respectively. From our studies, it appears that bats and birds, the warm blooded flying vertebrates, are ideal hosts for the coronavirus gene source, bats for Alphacoronavirus and Betacoronavirus and birds for Gammacoronavirus and Deltacoronavirus, to fuel coronavirus evolution and dissemination.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Patrick C. Y. Woo',\n",
       "   'Susanna K. P. Lau',\n",
       "   'Carol S. F. Lam',\n",
       "   'Candy C. Y. Lau',\n",
       "   'Alan K. L. Tsang',\n",
       "   'John H. N. Lau',\n",
       "   'Ru Bai',\n",
       "   'Jade L. L. Teng',\n",
       "   'Chris C. C. Tsang',\n",
       "   'Ming Wang',\n",
       "   'Bo-Jian Zheng',\n",
       "   'Kwok-Hung Chan',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Coronavirinae',\n",
       "   'Bulbul coronavirus HKU11',\n",
       "   'Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Alphacoronavirus',\n",
       "   'Deltacoronavirus',\n",
       "   'Gammacoronavirus',\n",
       "   'Gene',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,086',\n",
       "  'reference_count': '55',\n",
       "  'references': ['2141885858',\n",
       "   '2025170735',\n",
       "   '2110835349',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2103503670',\n",
       "   '2134061616',\n",
       "   '2056584399',\n",
       "   '2111412754',\n",
       "   '2140338292']},\n",
       " {'id': '2889758689',\n",
       "  'title': 'Genomic characterization and infectivity of a novel SARS-like coronavirus in Chinese bats',\n",
       "  'abstract': 'SARS coronavirus (SARS-CoV), the causative agent of the large SARS outbreak in 2003, originated in bats. Many SARS-like coronaviruses (SL-CoVs) have been detected in bats, particularly those that reside in China, Europe, and Africa. To further understand the evolutionary relationship between SARS-CoV and its reservoirs, 334 bats were collected from Zhoushan city, Zhejiang province, China, between 2015 and 2017. PCR amplification of the conserved coronaviral protein RdRp detected coronaviruses in 26.65% of bats belonging to this region, and this number was influenced by seasonal changes. Full genomic analyses of the two new SL-CoVs from Zhoushan (ZXC21 and ZC45) showed that their genomes were 29,732 nucleotides (nt) and 29,802 nt in length, respectively, with 13 open reading frames (ORFs). These results revealed 81% shared nucleotide identity with human/civet SARS CoVs, which was more distant than that observed previously for bat SL-CoVs in China. Importantly, using pathogenic tests, we found that the virus can reproduce and cause disease in suckling rats, and further studies showed that the virus-like particles can be observed in the brains of suckling rats by electron microscopy. Thus, this study increased our understanding of the genetic diversity of the SL-CoVs carried by bats and also provided a new perspective to study the possibility of cross-species transmission of SL-CoVs using suckling rats as an animal model.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Dan Hu',\n",
       "   'Changqiang Zhu',\n",
       "   'Lele Ai',\n",
       "   'Ting He',\n",
       "   'Yi Wang',\n",
       "   'Fuqiang Ye',\n",
       "   'Lu Yang',\n",
       "   'Chenxi Ding',\n",
       "   'Xuhui Zhu',\n",
       "   'Ruicheng Lv',\n",
       "   'Jin Zhu',\n",
       "   'Bachar Hassan',\n",
       "   'Youjun Feng',\n",
       "   'Weilong Tan',\n",
       "   'Changjun Wang'],\n",
       "  'related_topics': ['ORFS',\n",
       "   'Infectivity',\n",
       "   'Phylogenetics',\n",
       "   'Civet',\n",
       "   'Outbreak',\n",
       "   'Virus',\n",
       "   'Polymerase chain reaction',\n",
       "   'Virulence',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '196',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2311203695',\n",
       "   '2104548316',\n",
       "   '1993577573',\n",
       "   '2775086803',\n",
       "   '2169198329',\n",
       "   '2298153446',\n",
       "   '2046153984',\n",
       "   '2141877163',\n",
       "   '2140338292',\n",
       "   '2141008678']},\n",
       " {'id': '2769543984',\n",
       "  'title': 'Human intestinal tract serves as an alternative infection route for Middle East respiratory syndrome coronavirus',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus (MERS-CoV) has caused human respiratory infections with a high case fatality rate since 2012. However, the mode of virus transmission is not well understood. The findings of epidemiological and virological studies prompted us to hypothesize that the human gastrointestinal tract could serve as an alternative route to acquire MERS-CoV infection. We demonstrated that human primary intestinal epithelial cells, small intestine explants, and intestinal organoids were highly susceptible to MERS-CoV and can sustain robust viral replication. We also identified the evidence of enteric MERS-CoV infection in the stool specimen of a clinical patient. MERS-CoV was considerably resistant to fed-state gastrointestinal fluids but less tolerant to highly acidic fasted-state gastric fluid. In polarized Caco-2 cells cultured in Transwell inserts, apical MERS-CoV inoculation was more effective in establishing infection than basolateral inoculation. Notably, direct intragastric inoculation of MERS-CoV caused a lethal infection in human DPP4 transgenic mice. Histological examination revealed MERS-CoV enteric infection in all inoculated mice, as shown by the presence of virus-positive cells, progressive inflammation, and epithelial degeneration in small intestines, which were exaggerated in the mice pretreated with the proton pump inhibitor pantoprazole. With the progression of the enteric infection, inflammation, virus-positive cells, and live viruses emerged in the lung tissues, indicating the development of sequential respiratory infection. Taken together, these data suggest that the human intestinal tract may serve as an alternative infection route for MERS-CoV.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Jie Zhou',\n",
       "   'Cun Li',\n",
       "   'Guangyu Zhao',\n",
       "   'Hin Chu',\n",
       "   'Dong Wang',\n",
       "   'Helen Hoi-Ning Yan',\n",
       "   'Vincent Kwok-Man Poon',\n",
       "   'Lei Wen',\n",
       "   'Bosco Ho-Yin Wong',\n",
       "   'Xiaoyu Zhao',\n",
       "   'Man Chun Chiu',\n",
       "   'Dong Yang',\n",
       "   'Yixin Wang',\n",
       "   'Rex K. H. Au-Yeung',\n",
       "   'Ivy Hau-Yee Chan',\n",
       "   'Shihui Sun',\n",
       "   'Jasper Fuk-Woo Chan',\n",
       "   'Kelvin Kai-Wang To',\n",
       "   'Ziad Ahmed Memish',\n",
       "   '',\n",
       "   'Victor M. Corman',\n",
       "   'Christian Drosten',\n",
       "   'Ivan Fan-Ngai Hung',\n",
       "   'Yusen Zhou',\n",
       "   'Suet Yi Leung',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Respiratory infection',\n",
       "   'Human gastrointestinal tract',\n",
       "   'Small intestine',\n",
       "   'Lung',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Respiratory system',\n",
       "   'Inflammation',\n",
       "   'Viral replication',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '272',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '2045002682',\n",
       "   '2115555188',\n",
       "   '2002513358',\n",
       "   '2144410942',\n",
       "   '1757215199',\n",
       "   '1947409115']},\n",
       " {'id': '2140338292',\n",
       "  'title': 'Severe acute respiratory syndrome coronavirus-like virus in Chinese horseshoe bats',\n",
       "  'abstract': 'Although the finding of severe acute respiratory syndrome coronavirus (SARS-CoV) in caged palm civets from live animal markets in China has provided evidence for interspecies transmission in the genesis of the SARS epidemic, subsequent studies suggested that the civet may have served only as an amplification host for SARS-CoV. In a surveillance study for CoV in noncaged animals from the wild areas of the Hong Kong Special Administration Region, we identified a CoV closely related to SARS-CoV (bat-SARS-CoV) from 23 (39%) of 59 anal swabs of wild Chinese horseshoe bats (Rhinolophus sinicus) by using RT-PCR. Sequencing and analysis of three bat-SARS-CoV genomes from samples collected at different dates showed that bat-SARS-CoV is closely related to SARS-CoV from humans and civets. Phylogenetic analysis showed that bat-SARS-CoV formed a distinct cluster with SARS-CoV as group 2b CoV, distantly related to known group 2 CoV. Most differences between the bat-SARS-CoV and SARS-CoV genomes were observed in the spike genes, ORF 3 and ORF 8, which are the regions where most variations also were observed between human and civet SARS-CoV genomes. In addition, the presence of a 29-bp insertion in ORF 8 of bat-SARS-CoV genome, not in most human SARS-CoV genomes, suggests that it has a common ancestor with civet SARS-CoV. Antibody against recombinant bat-SARS-CoV nucleocapsid protein was detected in 84% of Chinese horseshoe bats by using an enzyme immunoassay. Neutralizing antibody to human SARS-CoV also was detected in bats with lower viral loads. Precautions should be exercised in the handling of these animals.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Susanna K. P. Lau',\n",
       "   'Patrick C. Y.',\n",
       "   'Kenneth S. M.',\n",
       "   'Yi',\n",
       "   'Hoi-Wah',\n",
       "   'Beatrice H. L.',\n",
       "   'Samson S. Y.',\n",
       "   'Suet-Yi',\n",
       "   'Kwok-Hung',\n",
       "   'Kwok-Yung'],\n",
       "  'related_topics': ['Civet',\n",
       "   'Alphacoronavirus',\n",
       "   'Rhinolophus sinicus',\n",
       "   'Phylogenetics',\n",
       "   'Genome',\n",
       "   'Virus',\n",
       "   'Phylogenetic tree',\n",
       "   'Sequence analysis',\n",
       "   'Virology',\n",
       "   'Genetics',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,727',\n",
       "  'reference_count': '40',\n",
       "  'references': ['2141885858',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2116586125',\n",
       "   '1966238900',\n",
       "   '2169198329',\n",
       "   '2171091522',\n",
       "   '2134061616',\n",
       "   '2111412754']},\n",
       " {'id': '3004280078',\n",
       "  'title': 'A pneumonia outbreak associated with a new coronavirus of probable bat origin',\n",
       "  'abstract': 'Since the outbreak of severe acute respiratory syndrome (SARS) 18\\xa0years ago, a large number of SARS-related coronaviruses (SARSr-CoVs) have been discovered in their natural reservoir host, bats1-4. Previous studies have shown that some bat SARSr-CoVs have the potential to infect humans5-7. Here we report the identification and characterization of a new coronavirus (2019-nCoV), which caused an epidemic of acute respiratory syndrome in humans in Wuhan, China. The epidemic, which started on 12 December 2019, had caused 2,794 laboratory-confirmed infections including 80 deaths by 26 January 2020. Full-length genome sequences were obtained from five patients at an early stage of the outbreak. The sequences are almost identical and share 79.6% sequence identity to SARS-CoV. Furthermore, we show that 2019-nCoV is 96% identical at the whole-genome level to a bat coronavirus. Pairwise protein sequence analysis of seven conserved non-structural proteins domains show that this virus belongs to the species of SARSr-CoV. In addition, 2019-nCoV virus isolated from the bronchoalveolar lavage fluid of a critically ill patient could be neutralized by sera from several patients. Notably, we confirmed that 2019-nCoV uses the same cell entry receptor-angiotensin converting enzyme II (ACE2)-as SARS-CoV.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Peng Zhou',\n",
       "   'Xing Lou Yang',\n",
       "   'Xian Guang Wang',\n",
       "   'Ben Hu',\n",
       "   'Lei Zhang',\n",
       "   'Wei Zhang',\n",
       "   'Hao Rui Si',\n",
       "   'Yan Zhu',\n",
       "   'Bei Li',\n",
       "   'Chao Lin Huang',\n",
       "   'Hui Dong Chen',\n",
       "   'Jing Chen',\n",
       "   'Yun Luo',\n",
       "   'Hua Guo',\n",
       "   'Ren Di Jiang',\n",
       "   'Mei Qin Liu',\n",
       "   'Ying Chen',\n",
       "   'Xu Rui Shen',\n",
       "   'Xi Wang',\n",
       "   'Xiao Shuang Zheng',\n",
       "   'Kai Zhao',\n",
       "   'Quan Jiao Chen',\n",
       "   'Fei Deng',\n",
       "   'Lin Lin Liu',\n",
       "   'Bing Yan',\n",
       "   'Fa Xian Zhan',\n",
       "   'Yan Yi Wang',\n",
       "   'Geng Fu Xiao',\n",
       "   'Zheng Li Shi'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Outbreak',\n",
       "   'Virus',\n",
       "   'Natural reservoir',\n",
       "   'Pneumonia',\n",
       "   'Virology',\n",
       "   'Bronchoalveolar lavage',\n",
       "   'Antibody',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,423',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '2132260239',\n",
       "   '1993577573',\n",
       "   '2775086803',\n",
       "   '1966238900',\n",
       "   '2195009776',\n",
       "   '2918873120',\n",
       "   '2103503670',\n",
       "   '2298153446']},\n",
       " {'id': '2103441770',\n",
       "  'title': 'Fast and accurate short read alignment with Burrows–Wheeler transform',\n",
       "  'abstract': 'Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.\\r\\n\\r\\nResults: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ~10–20× faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package.\\r\\n\\r\\nAvailability: http://maq.sourceforge.net \\r\\n\\r\\nContact: [email\\xa0protected]',\n",
       "  'date': '2009',\n",
       "  'authors': ['Heng Li', 'Richard Durbin'],\n",
       "  'related_topics': ['Hybrid genome assembly',\n",
       "   'Sequence assembly',\n",
       "   '2 base encoding',\n",
       "   'Peak calling',\n",
       "   'DNA sequencing theory',\n",
       "   'Paired-end tag',\n",
       "   'Burrows–Wheeler transform',\n",
       "   'FASTQ format',\n",
       "   'Variant Call Format',\n",
       "   'Deep sequencing',\n",
       "   'Reference genome',\n",
       "   'ABI Solid Sequencing',\n",
       "   'Minion',\n",
       "   'Nanopore sequencing',\n",
       "   'Shotgun sequencing',\n",
       "   'Massive parallel sequencing',\n",
       "   'ChIP-exo',\n",
       "   'Algorithm',\n",
       "   'Sequence alignment',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Cancer genome sequencing',\n",
       "   'Genomics',\n",
       "   'Sequence analysis',\n",
       "   'DNA sequencing',\n",
       "   'MALBAC',\n",
       "   'Illumina dye sequencing',\n",
       "   'Structural variation',\n",
       "   'Genomic Structural Variation',\n",
       "   'Exome sequencing',\n",
       "   'DNase-Seq',\n",
       "   'Human genome',\n",
       "   'Genome',\n",
       "   'INDEL Mutation',\n",
       "   'Variome',\n",
       "   'ATAC-seq',\n",
       "   'PAR-CLIP',\n",
       "   'Indel',\n",
       "   'Population genomics',\n",
       "   'Exome',\n",
       "   'Ancient DNA',\n",
       "   'Selective sweep',\n",
       "   'PRDM9',\n",
       "   'Kataegis',\n",
       "   'Candidate Gene Identification',\n",
       "   'DNA',\n",
       "   'POLD1',\n",
       "   'Mutation Accumulation',\n",
       "   'Genome resequencing',\n",
       "   'Genotyping by sequencing',\n",
       "   'Structural variant',\n",
       "   'View Less'],\n",
       "  'citation_count': '29,912',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2108234281',\n",
       "   '2158714788',\n",
       "   '2124985265',\n",
       "   '2112113834',\n",
       "   '2136145671',\n",
       "   '2139760555',\n",
       "   '2015292449',\n",
       "   '2132341951',\n",
       "   '2055666215',\n",
       "   '2142619120']},\n",
       " {'id': '2141052558',\n",
       "  'title': 'RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies.',\n",
       "  'abstract': 'Motivation: Phylogenies are increasingly used in all fields of medical and biological research. Moreover, because of the next-generation sequencing revolution, datasets used for conducting phylogenetic analyses grow at an unprecedented pace. RAxML (Randomized Axelerated Maximum Likelihood) is a popular program for phylogenetic analyses of large datasets under maximum likelihood. Since the last RAxML paper in 2006, it has been continuously maintained and extended to accommodate the increasingly growing input datasets and to serve the needs of the user community.\\r\\n\\r\\nResults: I present some of the most notable new features and extensions of RAxML, such as a substantial extension of substitution models and supported data types, the introduction of SSE3, AVX and AVX2 vector intrinsics, techniques for reducing the memory requirements of the code and a plethora of operations for conducting post-analyses on sets of trees. In addition, an up-to-date 50-page user manual covering all new RAxML options is available.\\r\\n\\r\\nAvailability and implementation: The code is available under GNU GPL at https://github.com/stamatak/standard-RAxML.\\r\\n\\r\\nContact: gro.sti-h@sikatamats.sordnaxela\\r\\n\\r\\nSupplementary information: Supplementary data are available at Bioinformatics online.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Alexandros Stamatakis'],\n",
       "  'related_topics': ['Intrinsics',\n",
       "   'Data type',\n",
       "   'Supermatrix',\n",
       "   'SSE3',\n",
       "   'Phylogenomics',\n",
       "   'Data mining',\n",
       "   'Code (cryptography)',\n",
       "   'Phylogenetic tree',\n",
       "   'Software',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '18,528',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2111211467',\n",
       "   '2168696662',\n",
       "   '2127847431',\n",
       "   '1794270752',\n",
       "   '2012220164',\n",
       "   '2068187483',\n",
       "   '2151736966',\n",
       "   '2100030044',\n",
       "   '2122082385',\n",
       "   '2156921764']},\n",
       " {'id': '2804822363',\n",
       "  'title': 'SWISS-MODEL: homology modelling of protein structures and complexes.',\n",
       "  'abstract': 'Homology modelling has matured into an important technique in structural biology, significantly contributing to narrowing the gap between known protein sequences and experimentally determined structures. Fully automated workflows and servers simplify and streamline the homology modelling process, also allowing users without a specific computational expertise to generate reliable protein models and have easy access to modelling results, their visualization and interpretation. Here, we present an update to the SWISS-MODEL server, which pioneered the field of automated modelling 25 years ago and been continuously further developed. Recently, its functionality has been extended to the modelling of homo- and heteromeric complexes. Starting from the amino acid sequences of the interacting proteins, both the stoichiometry and the overall structure of the complex are inferred by homology modelling. Other major improvements include the implementation of a new modelling engine, ProMod3 and the introduction a new local model quality estimation method, QMEANDisCo. SWISS-MODEL is freely available at https://swissmodel.expasy.org.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Andrew Waterhouse',\n",
       "   '',\n",
       "   'Martino Bertoni',\n",
       "   '',\n",
       "   'Stefan Bienert',\n",
       "   '',\n",
       "   'Gabriel Studer',\n",
       "   '',\n",
       "   'Gerardo Tauriello',\n",
       "   '',\n",
       "   'Rafal Gumienny',\n",
       "   '',\n",
       "   'Florian T Heer',\n",
       "   '',\n",
       "   'Tjaart A P de Beer',\n",
       "   '',\n",
       "   'Christine Rempfer',\n",
       "   '',\n",
       "   'Lorenza Bordoli',\n",
       "   '',\n",
       "   'Rosalba Lepore',\n",
       "   '',\n",
       "   'Torsten Schwede',\n",
       "   ''],\n",
       "  'related_topics': ['Structural biology',\n",
       "   'Server',\n",
       "   'Visualization',\n",
       "   'Protein structure',\n",
       "   'Workflow',\n",
       "   'Theoretical computer science',\n",
       "   'Software',\n",
       "   'Homology (biology)',\n",
       "   'Biology',\n",
       "   'Protein model',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,408',\n",
       "  'reference_count': '69',\n",
       "  'references': ['2158714788',\n",
       "   '2142678478',\n",
       "   '2149525061',\n",
       "   '2152301430',\n",
       "   '2154139219',\n",
       "   '2015642465',\n",
       "   '2060809301',\n",
       "   '2065283382',\n",
       "   '2051210555',\n",
       "   '2159614853']},\n",
       " {'id': '2991491848',\n",
       "  'title': 'A Randomized, Controlled Trial of Ebola Virus Disease Therapeutics.',\n",
       "  'abstract': 'Abstract Background Although several experimental therapeutics for Ebola virus disease (EVD) have been developed, the safety and efficacy of the most promising therapies need to be assessed in the ...',\n",
       "  'date': '2019',\n",
       "  'authors': ['Mulangu',\n",
       "   'Dodd Le',\n",
       "   'Davey',\n",
       "   'Tshiani Mbaya O',\n",
       "   'Proschan M',\n",
       "   'Mukadi D',\n",
       "   'Lusakibanza Manzo M',\n",
       "   'Nzolo D',\n",
       "   'Tshomba Oloma',\n",
       "   'Ibanda',\n",
       "   'Ali R',\n",
       "   'Coulibaly',\n",
       "   'Levine',\n",
       "   'Grais',\n",
       "   'Diaz',\n",
       "   'Lane',\n",
       "   'Muyembe-Tamfum Jj',\n",
       "   'Sivahera',\n",
       "   'Camara',\n",
       "   'Kojan',\n",
       "   'Walker',\n",
       "   'Dighero-Kemp',\n",
       "   'Cao',\n",
       "   'Mukumbayi',\n",
       "   'Mbala-Kingebeni',\n",
       "   'Ahuka',\n",
       "   'Albert',\n",
       "   'Bonnett',\n",
       "   'Crozier',\n",
       "   'Duvenhage',\n",
       "   'Proffitt',\n",
       "   'Teitelbaum',\n",
       "   'Moench',\n",
       "   'Aboulhab',\n",
       "   'Barrett',\n",
       "   'Cahill',\n",
       "   'Cone',\n",
       "   'Eckes',\n",
       "   'Hensley',\n",
       "   'Herpin',\n",
       "   'Higgs',\n",
       "   'Ledgerwood',\n",
       "   'Pierson',\n",
       "   'Smolskis',\n",
       "   'Sow',\n",
       "   'Tierney',\n",
       "   'Sivapalasingam',\n",
       "   'Holman',\n",
       "   'Gettinger',\n",
       "   'Vallée D'],\n",
       "  'related_topics': ['Ebola virus',\n",
       "   'Randomized controlled trial',\n",
       "   'MEDLINE',\n",
       "   'Disease',\n",
       "   'Young adult',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Extramural',\n",
       "   'Multicenter study',\n",
       "   'View Less'],\n",
       "  'citation_count': '889',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2605343262',\n",
       "  'title': 'GISAID: Global initiative on sharing all influenza data - from vision to reality.',\n",
       "  'abstract': '',\n",
       "  'date': '2017',\n",
       "  'authors': ['Yuelong Shu', 'John McCauley'],\n",
       "  'related_topics': ['Public health informatics',\n",
       "   'Data sharing',\n",
       "   'Global health',\n",
       "   'Data collection',\n",
       "   'Information Dissemination',\n",
       "   'MEDLINE',\n",
       "   'Pandemic',\n",
       "   'Knowledge management',\n",
       "   'Influenza A virus subtype H5N1',\n",
       "   'Business',\n",
       "   'View Less'],\n",
       "  'citation_count': '932',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2106173155',\n",
       "   '2259815689',\n",
       "   '2587970647',\n",
       "   '2063651055',\n",
       "   '2222043208',\n",
       "   '2532120756',\n",
       "   '2557499142',\n",
       "   '2104424333',\n",
       "   '2033459705',\n",
       "   '2096039130']},\n",
       " {'id': '3001388158',\n",
       "  'title': 'China coronavirus: Six questions scientists are asking',\n",
       "  'abstract': 'Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.  Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ewen', 'David'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'China',\n",
       "   'Geography',\n",
       "   'Genealogy',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '64',\n",
       "  'reference_count': '2',\n",
       "  'references': ['3002539152', '3000771439']},\n",
       " {'id': '3004397688',\n",
       "  'title': 'Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak.',\n",
       "  'abstract': 'Abstract   Backgrounds  An ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city in China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and other countries. We present estimates of the basic reproduction number, R0, of 2019-nCoV in the early phase of the outbreak.    Methods  Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (γ), we estimated R0 by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI.    Findings  The early outbreak data largely follows the exponential growth. We estimated that the mean R0 ranges from 2.24 (95%CI: 1.96–2.55) to 3.58 (95%CI: 2.89–4.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of R0.    Conclusion  The mean estimate of R0 for the 2019-nCoV ranges from 2.24 to 3.58, and is significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Shi Zhao',\n",
       "   'Qianyin Lin',\n",
       "   'Jinjun Ran',\n",
       "   'Salihu S Musa',\n",
       "   'Guangpu Yang',\n",
       "   'Weiming Wang',\n",
       "   'Yijun Lou',\n",
       "   'Daozhou Gao',\n",
       "   'Lin Yang',\n",
       "   'Daihai He',\n",
       "   'Maggie H Wang'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Serial interval',\n",
       "   'Basic reproduction number',\n",
       "   'Coronavirus',\n",
       "   'Mainland China',\n",
       "   'Estimation',\n",
       "   'Pandemic',\n",
       "   'Exponential growth',\n",
       "   'Demography',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,413',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2107053896',\n",
       "   '3002764620',\n",
       "   '3004026249',\n",
       "   '2999612210',\n",
       "   '2147166346',\n",
       "   '3026046290',\n",
       "   '1990049863',\n",
       "   '2117002055',\n",
       "   '3002747665',\n",
       "   '2102187991']},\n",
       " {'id': '3002764620',\n",
       "  'title': 'Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions',\n",
       "  'abstract': 'Since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-nCoV) in Wuhan, China, has increased rapidly, with cases arising across China and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%CI, 2.39-4.13); 58-76% of transmissions must be prevented to stop increasing; Wuhan case ascertainment of 5.0% (3.6-7.4); 21022 (11090-33490) total infections in Wuhan 1 to 22 January.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Read Jm', 'Bridgen', 'Cummings Da', 'Ho A', 'Jewell Cp'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Basic reproduction number',\n",
       "   'Transmission (mechanics)',\n",
       "   'Epidemiology',\n",
       "   'Estimation',\n",
       "   'Environmental health',\n",
       "   'Biology',\n",
       "   'Case ascertainment',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '671',\n",
       "  'reference_count': '15',\n",
       "  'references': ['3001118548',\n",
       "   '2582743722',\n",
       "   '3002539152',\n",
       "   '3017468735',\n",
       "   '2999612210',\n",
       "   '2147166346',\n",
       "   '3002533591',\n",
       "   '3026046290',\n",
       "   '1998725525',\n",
       "   '3001343166']},\n",
       " {'id': '3002533591',\n",
       "  'title': 'Transmission Dynamics of 2019 Novel Coronavirus (2019-nCoV)',\n",
       "  'abstract': 'Background: Since December 29, 2019, pneumonia infection with 2019-nCoV has rapidly spread out from Wuhan, HubeProvince, China to most others provinces and ot',\n",
       "  'date': '2020',\n",
       "  'authors': ['Tao Liu',\n",
       "   'Jianxiong Hu',\n",
       "   'Min Kang',\n",
       "   'Lifeng Lin',\n",
       "   'Haojie Zhong',\n",
       "   'Jianpeng Xiao',\n",
       "   'Guanhao He',\n",
       "   'Tie Song',\n",
       "   'Qiong Huang',\n",
       "   'Zuhua Rong',\n",
       "   'Aiping Deng',\n",
       "   'Weilin Zeng',\n",
       "   'Xiaohua Tan',\n",
       "   'Siqing Zeng',\n",
       "   'Zhihua Zhu',\n",
       "   'Jiansen Li',\n",
       "   'Donghua Wan',\n",
       "   'Jing Lu',\n",
       "   'Huihong Deng',\n",
       "   'Jianfeng He',\n",
       "   'Wenjun Ma'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Transmission (mechanics)',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '313',\n",
       "  'reference_count': '23',\n",
       "  'references': ['3001118548',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3003573988',\n",
       "   '3001465255',\n",
       "   '3004397688',\n",
       "   '2147166346',\n",
       "   '2117002055',\n",
       "   '2140763962']},\n",
       " {'id': '1815575713',\n",
       "  'title': 'Transmission characteristics of MERS and SARS in the healthcare setting: a comparative study',\n",
       "  'abstract': 'The Middle East respiratory syndrome (MERS) coronavirus has caused recurrent outbreaks in the Arabian Peninsula since 2012. Although MERS has low overall human-to-human transmission potential, there is occasional amplification in the healthcare setting, a pattern reminiscent of the dynamics of the severe acute respiratory syndrome (SARS) outbreaks in 2003. Here we provide a head-to-head comparison of exposure patterns and transmission dynamics of large hospital clusters of MERS and SARS, including the most recent South Korean outbreak of MERS in 2015.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Gerardo Chowell',\n",
       "   '',\n",
       "   'Fatima Abdirizak',\n",
       "   'Sunmi Lee',\n",
       "   'Jonggul Lee',\n",
       "   'Eunok Jung',\n",
       "   'Hiroshi Nishiura',\n",
       "   'Cécile Viboud'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'Coronavirus Infections',\n",
       "   'Cross infection',\n",
       "   'Transmission potential',\n",
       "   'View Less'],\n",
       "  'citation_count': '411',\n",
       "  'reference_count': '45',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2006434809',\n",
       "   '2138324310',\n",
       "   '2147166346',\n",
       "   '1990049863',\n",
       "   '2069251911',\n",
       "   '2096145431',\n",
       "   '1968393246',\n",
       "   '2130227690']},\n",
       " {'id': '2069251911',\n",
       "  'title': 'Superspreading and the effect of individual variation on disease emergence',\n",
       "  'abstract': \"Population-level analyses often use average quantities to describe heterogeneous systems, particularly when variation does not arise from identifiable groups. A prominent example, central to our current understanding of epidemic spread, is the basic reproductive number, R(0), which is defined as the mean number of infections caused by an infected individual in a susceptible population. Population estimates of R(0) can obscure considerable individual variation in infectiousness, as highlighted during the global emergence of severe acute respiratory syndrome (SARS) by numerous 'superspreading events' in which certain individuals infected unusually large numbers of secondary cases. For diseases transmitted by non-sexual direct contacts, such as SARS or smallpox, individual variation is difficult to measure empirically, and thus its importance for outbreak dynamics has been unclear. Here we present an integrated theoretical and statistical analysis of the influence of individual variation in infectiousness on disease emergence. Using contact tracing data from eight directly transmitted diseases, we show that the distribution of individual infectiousness around R(0) is often highly skewed. Model predictions accounting for this variation differ sharply from average-based approaches, with disease extinction more likely and outbreaks rarer but more explosive. Using these models, we explore implications for outbreak control, showing that individual-specific control measures outperform population-wide measures. Moreover, the dramatic improvements achieved through targeted control policies emphasize the need to identify predictive correlates of higher infectiousness. Our findings indicate that superspreading is a normal feature of disease spread, and to frame ongoing discussion we propose a rigorous definition for superspreading events and a method to predict their frequency.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['J. O. Lloyd-Smith',\n",
       "   'S. J. Schreiber',\n",
       "   'P. E. Kopp',\n",
       "   'W. M. Getz'],\n",
       "  'related_topics': ['Susceptible individual',\n",
       "   'Super-spreader',\n",
       "   'Basic reproduction number',\n",
       "   'Outbreak',\n",
       "   'Disease',\n",
       "   'Contact tracing',\n",
       "   'Variation (linguistics)',\n",
       "   'Extinction',\n",
       "   'Demography',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,977',\n",
       "  'reference_count': '96',\n",
       "  'references': ['1995945562',\n",
       "   '2009435671',\n",
       "   '2131262274',\n",
       "   '2147166346',\n",
       "   '1606697907',\n",
       "   '2146272590',\n",
       "   '1965499304',\n",
       "   '2096145431',\n",
       "   '2104595316',\n",
       "   '2463755683']},\n",
       " {'id': '2096145431',\n",
       "  'title': 'Transmission dynamics of the etiological agent of SARS in Hong Kong: impact of public health interventions.',\n",
       "  'abstract': 'We present an analysis of the first 10 weeks of the severe acute respiratory syndrome (SARS) epidemic in Hong Kong. The epidemic to date has been characterized by two large clusters-initiated by two separate \"super-spread\" events (SSEs)-and by ongoing community transmission. By fitting a stochastic model to data on 1512 cases, including these clusters, we show that the etiological agent of SARS is moderately transmissible. Excluding SSEs, we estimate that 2.7 secondary infections were generated per case on average at the start of the epidemic, with a substantial contribution from hospital transmission. Transmission rates fell during the epidemic, primarily as a result of reductions in population contact rates and improved hospital infection control, but also because of more rapid hospital attendance by symptomatic individuals. As a result, the epidemic is now in decline, although continued vigilance is necessary for this to be maintained. Restrictions on longer range population movement are shown to be a potentially useful additional control measure in some contexts. We estimate that most currently infected persons are now hospitalized, which highlights the importance of control of nosocomial transmission.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Steven Riley',\n",
       "   'Christophe Fraser',\n",
       "   'Christl A. Donnelly',\n",
       "   'Azra C. Ghani',\n",
       "   'Laith J. Abu-Raddad',\n",
       "   'Anthony J. Hedley',\n",
       "   'Gabriel M. Leung',\n",
       "   'Lai Ming Ho',\n",
       "   'Tai Hing Lam',\n",
       "   'Thuan Q. Thach',\n",
       "   'Patsy Chau',\n",
       "   'King Pan Chan',\n",
       "   'Su Vui Lo',\n",
       "   'Pak Yin Leung',\n",
       "   'Thomas Tsang',\n",
       "   'William Ho',\n",
       "   'Koon Hung Lee',\n",
       "   'Edith M.C. Lau',\n",
       "   'Neil M. Ferguson',\n",
       "   'Roy M. Anderson'],\n",
       "  'related_topics': ['Population',\n",
       "   'Secondary infection',\n",
       "   'Global health',\n",
       "   'Transmission (mechanics)',\n",
       "   'Infection control',\n",
       "   'Contact tracing',\n",
       "   'Epidemiology',\n",
       "   'Attendance',\n",
       "   'Environmental health',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,316',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2169198329',\n",
       "   '1606697907',\n",
       "   '2104595316',\n",
       "   '2124853344']},\n",
       " {'id': '2104595316',\n",
       "  'title': 'Mathematical Epidemiology of Infectious Diseases: Model Building, Analysis and Interpretation',\n",
       "  'abstract': 'Provides systematic coverage of the mathematical theory of modelling epidemics in populations, with a clear and coherent discussion of the issues, concepts and phenomena. Mathematical modelling of epidemics is a vast and important area of study and this book helps the reader to translate, model, analyse and interpret, with numerous applications, examples and exercises to aid understanding.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Odo', 'J. A. P'],\n",
       "  'related_topics': ['Mathematical theory',\n",
       "   'Mathematical modelling of infectious disease',\n",
       "   'Model building',\n",
       "   'Interpretation (philosophy)',\n",
       "   'Management science',\n",
       "   'Mathematical model',\n",
       "   'Next-generation matrix',\n",
       "   'Area studies',\n",
       "   'Operations research',\n",
       "   'Medicine',\n",
       "   'Basic Reproduction Ratio',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,610',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1968393246',\n",
       "  'title': 'Middle East respiratory syndrome coronavirus: quantification of the extent of the epidemic, surveillance biases, and transmissibility',\n",
       "  'abstract': 'Summary  Background  The novel Middle East respiratory syndrome coronavirus (MERS-CoV) had, as of Aug 8, 2013, caused 111 virologically confirmed or probable human cases of infection worldwide. We analysed epidemiological and genetic data to assess the extent of human infection, the performance of case detection, and the transmission potential of MERS-CoV with and without control measures.  Methods  We assembled a comprehensive database of all confirmed and probable cases from public sources and estimated the incubation period and generation time from case cluster data. Using data of numbers of visitors to the Middle East and their duration of stay, we estimated the number of symptomatic cases in the Middle East. We did independent analyses, looking at the growth in incident clusters, the growth in viral population, the reproduction number of cluster index cases, and cluster sizes to characterise the dynamical properties of the epidemic and the transmission scenario.  Findings  The estimated number of symptomatic cases up to Aug 8, 2013, is 940 (95% CI 290–2200), indicating that at least 62% of human symptomatic cases have not been detected. We find that the case-fatality ratio of primary cases detected via routine surveillance (74%; 95% CI 49–91) is biased upwards because of detection bias; the case-fatality ratio of secondary cases was 20% (7–42). Detection of milder cases (or clinical management) seemed to have improved in recent months. Analysis of human clusters indicated that chains of transmission were not self-sustaining when infection control was implemented, but that  R  in the absence of controls was in the range 0·8–1·3. Three independent data sources provide evidence that  R  cannot be much above 1, with an upper bound of 1·2–1·5.  Interpretation  By showing that a slowly growing epidemic is underway either in human beings or in an animal reservoir, quantification of uncertainty in transmissibility estimates, and provision of the first estimates of the scale of the epidemic and extent of case detection biases, we provide valuable information for more informed risk assessment.  Funding  Medical Research Council, Bill & Melinda Gates Foundation, EU FP7, and National Institute of General Medical Sciences.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Simon Cauchemez',\n",
       "   'Christophe Fraser',\n",
       "   'Maria D Van Kerkhove',\n",
       "   'Christl A Donnelly',\n",
       "   'Steven Riley',\n",
       "   'Andrew Rambaut',\n",
       "   'Vincent Enouf',\n",
       "   'Sylvie van der Werf',\n",
       "   'Neil M Ferguson'],\n",
       "  'related_topics': ['Population',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Basic reproduction number',\n",
       "   'Epidemiology',\n",
       "   'Risk assessment',\n",
       "   'Respiratory tract infections',\n",
       "   'Coronavirus',\n",
       "   'Survival analysis',\n",
       "   'Demography',\n",
       "   'Medicine',\n",
       "   'Immunology',\n",
       "   'View Less'],\n",
       "  'citation_count': '342',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2157725602',\n",
       "   '2147166346',\n",
       "   '2160011624',\n",
       "   '2045002682',\n",
       "   '2113457186',\n",
       "   '2130227690',\n",
       "   '2140763962',\n",
       "   '2102187991']},\n",
       " {'id': '2158899491',\n",
       "  'title': 'Natural Language Processing (Almost) from Scratch',\n",
       "  'abstract': 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Ronan',\n",
       "   'Jason Weston',\n",
       "   'Léon',\n",
       "   'Michael',\n",
       "   'Koray Kavukcuoglu',\n",
       "   'Pavel Kuksa'],\n",
       "  'related_topics': ['Named-entity recognition',\n",
       "   'Chunking (psychology)',\n",
       "   'Semantic role labeling',\n",
       "   'Tag system',\n",
       "   'Task (computing)',\n",
       "   'Machine learning',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Basis (linear algebra)',\n",
       "   'Scratch',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,413',\n",
       "  'reference_count': '95',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2147880316',\n",
       "   '2125838338',\n",
       "   '2110798204',\n",
       "   '2158139315',\n",
       "   '2159080219',\n",
       "   '2098162425',\n",
       "   '2150102617',\n",
       "   '2296073425']},\n",
       " {'id': '2110158442',\n",
       "  'title': 'Contour Detection and Hierarchical Image Segmentation',\n",
       "  'abstract': 'This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.',\n",
       "  'date': '2011',\n",
       "  'authors': ['P Arbeláez', 'M Maire', 'C Fowlkes', 'J Malik'],\n",
       "  'related_topics': ['Scale-space segmentation',\n",
       "   'Segmentation-based object categorization',\n",
       "   'Image segmentation',\n",
       "   'Active contour model',\n",
       "   'Object detection',\n",
       "   'Edge detection',\n",
       "   'Segmentation',\n",
       "   'Image processing',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,633',\n",
       "  'reference_count': '76',\n",
       "  'references': ['2121947440',\n",
       "   '2067191022',\n",
       "   '2116040950',\n",
       "   '2124351162',\n",
       "   '1999478155',\n",
       "   '2145023731',\n",
       "   '1578099820',\n",
       "   '2169551590',\n",
       "   '2121927366',\n",
       "   '1528789833']},\n",
       " {'id': '1999478155',\n",
       "  'title': 'Efficient Graph-Based Image Segmentation',\n",
       "  'abstract': 'This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Pedro F. Felzenszwalb', 'Daniel P. Huttenlocher'],\n",
       "  'related_topics': ['Segmentation-based object categorization',\n",
       "   'Image segmentation',\n",
       "   'Range segmentation',\n",
       "   'Connected-component labeling',\n",
       "   'Scale-space segmentation',\n",
       "   'Graph (abstract data type)',\n",
       "   'Minimum spanning tree-based segmentation',\n",
       "   'Image texture',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,269',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2121947440',\n",
       "   '2752885492',\n",
       "   '1971784203',\n",
       "   '1964443764',\n",
       "   '2160167256',\n",
       "   '2137560895',\n",
       "   '2167077256',\n",
       "   '2132603077',\n",
       "   '1640070940',\n",
       "   '2109562068']},\n",
       " {'id': '2143516773',\n",
       "  'title': 'Fast approximate energy minimization via graph cuts',\n",
       "  'abstract': 'Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Y. Boykov', 'O. Veksler', 'R. Zabih'],\n",
       "  'related_topics': ['Graph cuts in computer vision',\n",
       "   'Approximation algorithm',\n",
       "   'Minimum cut',\n",
       "   'Simulated annealing',\n",
       "   'Cut',\n",
       "   'Standard algorithms',\n",
       "   'Energy minimization',\n",
       "   'Computational complexity theory',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,524',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2121947440',\n",
       "   '2104095591',\n",
       "   '1997063559',\n",
       "   '1977545325',\n",
       "   '2113137767',\n",
       "   '2620619910',\n",
       "   '2121781154',\n",
       "   '2913192828',\n",
       "   '1554544485',\n",
       "   '1649464328']},\n",
       " {'id': '2169551590',\n",
       "  'title': 'Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images',\n",
       "  'abstract': 'In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Y.Y. Boykov', 'M.-P. Jolly'],\n",
       "  'related_topics': ['Scale-space segmentation',\n",
       "   'Segmentation-based object categorization',\n",
       "   'Image segmentation',\n",
       "   'Minimum spanning tree-based segmentation',\n",
       "   'Region growing',\n",
       "   'Image texture',\n",
       "   'Connected-component labeling',\n",
       "   'GrabCut',\n",
       "   'Graph cuts in computer vision',\n",
       "   'Segmentation',\n",
       "   'Simple interactive object extraction',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,221',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2121947440',\n",
       "   '2104095591',\n",
       "   '2113137767',\n",
       "   '1991113069',\n",
       "   '1564419782',\n",
       "   '2098152234',\n",
       "   '2086921140',\n",
       "   '2096139825',\n",
       "   '1987983010',\n",
       "   '2132603077']},\n",
       " {'id': '2031342017',\n",
       "  'title': 'Unbiased look at dataset bias',\n",
       "  'abstract': 'Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Antonio Torralba', 'Alexei A. Efros'],\n",
       "  'related_topics': ['Automatic identification and data capture',\n",
       "   'Pascal (programming language)',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,652',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2108598243',\n",
       "   '2161969291',\n",
       "   '2031489346',\n",
       "   '2110764733',\n",
       "   '1576445103',\n",
       "   '2145607950',\n",
       "   '2217896605',\n",
       "   '2120419212',\n",
       "   '1566135517',\n",
       "   '2166049352']},\n",
       " {'id': '2913932916',\n",
       "  'title': 'Semantic hashing',\n",
       "  'abstract': \"We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ''semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.\",\n",
       "  'date': '2009',\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Feature hashing',\n",
       "   'Latent semantic analysis',\n",
       "   'Locality-sensitive hashing',\n",
       "   'Graphical model',\n",
       "   'Set (abstract data type)',\n",
       "   'Memory address',\n",
       "   'Unsupervised learning',\n",
       "   'Information retrieval',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,096',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2136922672',\n",
       "   '1880262756',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '2150102617',\n",
       "   '2162006472',\n",
       "   '2038276547',\n",
       "   '2157364932',\n",
       "   '1978394996']},\n",
       " {'id': '2103359087',\n",
       "  'title': 'Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine',\n",
       "  'abstract': 'Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.',\n",
       "  'date': '2010',\n",
       "  'authors': ['George Dahl',\n",
       "   \"Marc'aurelio Ranzato\",\n",
       "   'Abdel-rahman Mohamed',\n",
       "   'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'TIMIT',\n",
       "   'Word error rate',\n",
       "   'Covariance',\n",
       "   'Conditional independence',\n",
       "   'Conditional probability distribution',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '400',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2136922672',\n",
       "   '1498436455',\n",
       "   '1567512734',\n",
       "   '137106866',\n",
       "   '1983334819',\n",
       "   '2161000554',\n",
       "   '2161893161',\n",
       "   '2083380015',\n",
       "   '2131700150',\n",
       "   '2110871230']},\n",
       " {'id': '2033819227',\n",
       "  'title': 'Multiple view geometry in computer vision',\n",
       "  'abstract': 'From the Publisher:\\r\\nA basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Richard Hartley', 'Andrew Zisserman'],\n",
       "  'related_topics': ['Structure from motion',\n",
       "   'Epipolar geometry',\n",
       "   'Computer graphics',\n",
       "   'Trifocal tensor',\n",
       "   'RANSAC',\n",
       "   'Bundle adjustment',\n",
       "   'Visual odometry',\n",
       "   'Eight-point algorithm',\n",
       "   'Computer science',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '28,629',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2124386111',\n",
       "  'title': 'Object recognition from local scale-invariant features',\n",
       "  'abstract': 'An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.',\n",
       "  'date': '1999',\n",
       "  'authors': ['D.G. Lowe'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Haar-like features',\n",
       "   'Scale space',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Implicit Shape Model',\n",
       "   'Feature extraction',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '21,469',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2914885528',\n",
       "   '2124087378',\n",
       "   '2123977795',\n",
       "   '2011891945',\n",
       "   '22745672',\n",
       "   '2096077837',\n",
       "   '2096600681',\n",
       "   '2131806657',\n",
       "   '2042243448',\n",
       "   '1553558465']},\n",
       " {'id': '2154422044',\n",
       "  'title': 'Object class recognition by unsupervised scale-invariant learning',\n",
       "  'abstract': 'We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).',\n",
       "  'date': '2003',\n",
       "  'authors': ['R. Fergus', 'P. Perona', 'A. Zisserman'],\n",
       "  'related_topics': ['Object model',\n",
       "   'Implicit Shape Model',\n",
       "   'Constellation model',\n",
       "   'Feature extraction',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Caltech 101',\n",
       "   'Contextual image classification',\n",
       "   'Naive Bayes classifier',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,999',\n",
       "  'reference_count': '19',\n",
       "  'references': []},\n",
       " {'id': '2012778485',\n",
       "  'title': 'Invariant Features from Interest Point Groups',\n",
       "  'abstract': \"This paper approaches the problem of ¯nding correspondences between images in which there are large changes in viewpoint, scale and illumi- nation. Recent work has shown that scale-space `interest points' may be found with good repeatability in spite of such changes. Further- more, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descrip- tors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process. In this work we introduce a family of features which use groups of interest points to form geometrically invariant descriptors of image regions. Feature descriptors are formed by resampling the image rel- ative to canonical frames de¯ned by the points. In addition to robust matching, a key advantage of this approach is that each match implies a hypothesis of the local 2D (projective) transformation. This allows us to immediately reject most of the false matches using a Hough trans- form. We reject remaining outliers using RANSAC and the epipolar constraint. Results show that dense feature matching can be achieved in a few seconds of computation on 1GHz Pentium III machines.\",\n",
       "  'date': '2002',\n",
       "  'authors': ['Matthew Brown', 'David G. Lowe'],\n",
       "  'related_topics': ['RANSAC',\n",
       "   'Epipolar geometry',\n",
       "   'Invariant (mathematics)',\n",
       "   'Discriminative model',\n",
       "   'Pattern recognition',\n",
       "   'Outlier',\n",
       "   'Resampling',\n",
       "   'Computer vision',\n",
       "   'Computation',\n",
       "   'Pentium',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,139',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2124386111',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2165497495',\n",
       "   '2103504761',\n",
       "   '1505641881',\n",
       "   '2011891945',\n",
       "   '22745672',\n",
       "   '2005433550']},\n",
       " {'id': '2124404372',\n",
       "  'title': 'Robust wide-baseline stereo from maximally stable extremal regions',\n",
       "  'abstract': 'Abstract   The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied.  A new set of image elements that are put into correspondence, the so called  extremal regions , is introduced. Extremal regions possess highly desirable properties: the set is closed under (1) continuous (and thus projective) transformation of image coordinates and (2) monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely invariant stable subset of extremal regions, the maximally stable extremal regions (MSER).  A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences.  The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5×), illumination conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Jiri Matas', 'Ondrej Chum', 'Martin Urban', 'Tomás Pajdla'],\n",
       "  'related_topics': ['Maximally stable extremal regions',\n",
       "   'Epipolar geometry',\n",
       "   'Harris affine region detector',\n",
       "   'Invariant (mathematics)',\n",
       "   'Hessian affine region detector',\n",
       "   'Similarity measure',\n",
       "   'Robustness (computer science)',\n",
       "   'Principal curvature-based region detector',\n",
       "   'Algorithm',\n",
       "   'Topology',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,034',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2165497495',\n",
       "   '2124260943',\n",
       "   '1541642243',\n",
       "   '2132332894',\n",
       "   '2143753158']},\n",
       " {'id': '1676552347',\n",
       "  'title': 'An Affine Invariant Interest Point Detector',\n",
       "  'abstract': 'This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.',\n",
       "  'date': '2002',\n",
       "  'authors': ['K. Mikolajczyk', 'C. Schmid'],\n",
       "  'related_topics': ['Affine shape adaptation',\n",
       "   'Harris affine region detector',\n",
       "   'Affine transformation',\n",
       "   'Affine combination',\n",
       "   'Affine coordinate system',\n",
       "   'Affine hull',\n",
       "   'Hessian affine region detector',\n",
       "   'Affine group',\n",
       "   'Topology',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,120',\n",
       "  'reference_count': '17',\n",
       "  'references': []},\n",
       " {'id': '2124087378',\n",
       "  'title': 'Local grayvalue invariants for image retrieval',\n",
       "  'abstract': 'This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.',\n",
       "  'date': '1997',\n",
       "  'authors': ['C. Schmid', 'R. Mohr'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Search engine indexing',\n",
       "   'Computer vision',\n",
       "   'Robustness (computer science)',\n",
       "   'Autocorrelation',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Image matching',\n",
       "   'Voting algorithm',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,330',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2914885528',\n",
       "   '2111308925',\n",
       "   '2098693229',\n",
       "   '2123977795',\n",
       "   '2095757522',\n",
       "   '2011891945',\n",
       "   '2109863423',\n",
       "   '2112328181',\n",
       "   '2022735534',\n",
       "   '2160835070']},\n",
       " {'id': '2111308925',\n",
       "  'title': 'A COMBINED CORNER AND EDGE DETECTOR',\n",
       "  'abstract': 'The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Christopher G.', 'Mike'],\n",
       "  'related_topics': ['Motion analysis',\n",
       "   'Corner detection',\n",
       "   'Interest point detection',\n",
       "   'Alvey',\n",
       "   'Principal curvature-based region detector',\n",
       "   'Harris affine region detector',\n",
       "   'GLOH',\n",
       "   'Hessian affine region detector',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,157',\n",
       "  'reference_count': '6',\n",
       "  'references': ['1639227073',\n",
       "   '1756736144',\n",
       "   '2063599328',\n",
       "   '2048192053',\n",
       "   '2039106392',\n",
       "   '2997169974']},\n",
       " {'id': '2165497495',\n",
       "  'title': 'Reliable feature matching across widely separated views',\n",
       "  'abstract': 'We present a robust method for automatically matching features in images corresponding to the same physical point on an object seen from two arbitrary viewpoints. Unlike conventional stereo matching approaches we assume no prior knowledge about the relative camera positions and orientations. In fact in our application this is the information we wish to determine from the image feature matches. Features are detected in two or more images and characterised using affine texture invariants. The problem of window effects is explicitly addressed by our method-our feature characterisation is invariant to linear transformations of the image data including rotation, stretch and skew. The feature matching process is optimised for a structure-from-motion application where we wish to ignore unreliable matches at the expense of reducing the number of feature matches.',\n",
       "  'date': '2000',\n",
       "  'authors': ['A. Baumberg'],\n",
       "  'related_topics': ['Feature extraction',\n",
       "   'Affine transformation',\n",
       "   'Hessian affine region detector',\n",
       "   'Motion estimation',\n",
       "   'Invariant (mathematics)',\n",
       "   'Skew',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Electrical capacitance tomography',\n",
       "   'Linear map',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '913',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2124386111',\n",
       "   '2130103520',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2085261163',\n",
       "   '2112328181',\n",
       "   '3022352042',\n",
       "   '1970269179',\n",
       "   '2143753158',\n",
       "   '1549739843']},\n",
       " {'id': '1949116567',\n",
       "  'title': 'Unsupervised Learning of Models for Recognition',\n",
       "  'abstract': 'We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Markus Weber', 'Max Welling', 'Pietro Perona', ''],\n",
       "  'related_topics': ['Constellation model',\n",
       "   'Unsupervised learning',\n",
       "   'Cluster analysis',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Contextual image classification',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Focus (optics)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '947',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2049633694',\n",
       "   '3017143921',\n",
       "   '1564419782',\n",
       "   '2095757522',\n",
       "   '1958762911',\n",
       "   '2124722975',\n",
       "   '2117138270',\n",
       "   '2125791971',\n",
       "   '2029727948',\n",
       "   '1628541567']},\n",
       " {'id': '2177274842',\n",
       "  'title': 'A performance evaluation of local descriptors',\n",
       "  'abstract': 'In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.',\n",
       "  'date': '2005',\n",
       "  'authors': ['K. Mikolajczyk', 'C. Schmid'],\n",
       "  'related_topics': ['Principal curvature-based region detector',\n",
       "   'Hessian affine region detector',\n",
       "   'GLOH',\n",
       "   'Interest point detection',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Shape context',\n",
       "   'Contextual image classification',\n",
       "   'Implicit Shape Model',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '20,352',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2151103935',\n",
       "   '2033819227',\n",
       "   '2124386111',\n",
       "   '2163352848',\n",
       "   '2131846894',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2145023731',\n",
       "   '1980911747',\n",
       "   '2145072179']},\n",
       " {'id': '2131846894',\n",
       "  'title': 'Video Google: a text retrieval approach to object matching in videos',\n",
       "  'abstract': 'We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Sivic', 'Zisserman'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Video copy detection',\n",
       "   'Inverted index',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Vector quantization',\n",
       "   'Visual dictionary',\n",
       "   'Caltech 101',\n",
       "   'Computer vision',\n",
       "   'Pattern recognition',\n",
       "   'Invariant (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,789',\n",
       "  'reference_count': '19',\n",
       "  'references': ['3013264884',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '1660390307',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2165497495',\n",
       "   '2160484851',\n",
       "   '1541642243']},\n",
       " {'id': '1980911747',\n",
       "  'title': 'A Comparison of Affine Region Detectors',\n",
       "  'abstract': \"The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris (Mikolajczyk and Schmid, 2002; Schaffalitzky and Zisserman, 2002) and Hessian points (Mikolajczyk and Schmid, 2002), a detector of `maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of `salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression.\\r\\n\\r\\nThe objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['K. Mikolajczyk',\n",
       "   'T. Tuytelaars',\n",
       "   'C. Schmid',\n",
       "   'A. Zisserman',\n",
       "   'J. Matas',\n",
       "   'F. Schaffalitzky',\n",
       "   'T. Kadir',\n",
       "   'L. Van Gool'],\n",
       "  'related_topics': ['Hessian affine region detector',\n",
       "   'Harris affine region detector',\n",
       "   'Kadir–Brady saliency detector',\n",
       "   'Affine shape adaptation',\n",
       "   'Affine transformation',\n",
       "   'Principal curvature-based region detector',\n",
       "   'Maximally stable extremal regions',\n",
       "   'Detector',\n",
       "   'Algorithm',\n",
       "   'Geometry',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,232',\n",
       "  'reference_count': '49',\n",
       "  'references': ['2151103935',\n",
       "   '2033819227',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2131846894',\n",
       "   '2154422044',\n",
       "   '2145023731',\n",
       "   '1625255723',\n",
       "   '2172188317',\n",
       "   '2124404372']},\n",
       " {'id': '2104978738',\n",
       "  'title': 'The pyramid match kernel: discriminative classification with sets of image features',\n",
       "  'abstract': 'Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches',\n",
       "  'date': '2005',\n",
       "  'authors': ['K. Grauman', 'T. Darrell'],\n",
       "  'related_topics': ['Variable kernel density estimation',\n",
       "   'Radial basis function kernel',\n",
       "   'String kernel',\n",
       "   'Tree kernel',\n",
       "   'Kernel method',\n",
       "   'Kernel embedding of distributions',\n",
       "   'Polynomial kernel',\n",
       "   'Kernel (statistics)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,993',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2151103935',\n",
       "   '2153635508',\n",
       "   '3145128584',\n",
       "   '2148603752',\n",
       "   '1563088657',\n",
       "   '2131846894',\n",
       "   '2057175746',\n",
       "   '1510073064',\n",
       "   '2145072179',\n",
       "   '2914885528']},\n",
       " {'id': '2172188317',\n",
       "  'title': 'Scale & Affine Invariant Interest Point Detectors',\n",
       "  'abstract': 'In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix.\\r\\n\\r\\nOur scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point.\\r\\n\\r\\nWe present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching resultss the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Krystian Mikolajczyk', 'Cordelia Schmid'],\n",
       "  'related_topics': ['Harris affine region detector',\n",
       "   'Affine shape adaptation',\n",
       "   'Hessian affine region detector',\n",
       "   'Affine transformation',\n",
       "   'Affine combination',\n",
       "   'Affine hull',\n",
       "   'Affine coordinate system',\n",
       "   'Affine group',\n",
       "   'Algorithm',\n",
       "   'Topology',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,428',\n",
       "  'reference_count': '43',\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '2012778485',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2111308925',\n",
       "   '2165497495']},\n",
       " {'id': '2147717514',\n",
       "  'title': 'Approximate nearest neighbors: towards removing the curse of dimensionality',\n",
       "  'abstract': \"We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in R d , the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authors' STOC'98 and FOCS'01 papers. It unifies, generalizes and simplifies the results from those papers.\",\n",
       "  'date': '1998',\n",
       "  'authors': ['Piotr Indyk', 'Rajeev Motwani'],\n",
       "  'related_topics': ['Nearest neighbor search',\n",
       "   'Best bin first',\n",
       "   'Ball tree',\n",
       "   'Fixed-radius near neighbors',\n",
       "   'Cover tree',\n",
       "   'Minimum spanning tree',\n",
       "   'Curse of dimensionality',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,012',\n",
       "  'reference_count': '66',\n",
       "  'references': ['2752885492',\n",
       "   '2147152072',\n",
       "   '1634005169',\n",
       "   '2295428206',\n",
       "   '1956559956',\n",
       "   '2160066518',\n",
       "   '1502916507',\n",
       "   '3017143921',\n",
       "   '2427881153',\n",
       "   '2152565070']},\n",
       " {'id': '2162006472',\n",
       "  'title': 'Locality-sensitive hashing scheme based on p-stable distributions',\n",
       "  'abstract': 'We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p',\n",
       "  'date': '2004',\n",
       "  'authors': ['Mayur Datar',\n",
       "   'Nicole Immorlica',\n",
       "   'Piotr Indyk',\n",
       "   'Vahab S. Mirrokni'],\n",
       "  'related_topics': ['Locality-sensitive hashing',\n",
       "   'Dynamic perfect hashing',\n",
       "   'Universal hashing',\n",
       "   'Nearest neighbor search',\n",
       "   'K-independent hashing',\n",
       "   'Hopscotch hashing',\n",
       "   'Feature hashing',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,252',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2147717514',\n",
       "   '1502916507',\n",
       "   '1541459201',\n",
       "   '2520931985',\n",
       "   '2165533158',\n",
       "   '2045533739',\n",
       "   '2169351022',\n",
       "   '2109034006',\n",
       "   '2048779798',\n",
       "   '1595303882']},\n",
       " {'id': '2138451337',\n",
       "  'title': 'Eigenfaces for recognition',\n",
       "  'abstract': 'We have developed a near-real-time computer system that can locate and track a subject\\'s head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Matthew Turk', 'Alex Pentland'],\n",
       "  'related_topics': ['Eigenface',\n",
       "   'Three-dimensional face recognition',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Face detection',\n",
       "   'Face hallucination',\n",
       "   'Facial recognition system',\n",
       "   'Face perception',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Psychology',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '20,172',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2135463994',\n",
       "   '2125848778',\n",
       "   '2130259898',\n",
       "   '2055712799',\n",
       "   '2125999363',\n",
       "   '1509703770',\n",
       "   '1526492552',\n",
       "   '1507699566',\n",
       "   '2032361618',\n",
       "   '1986450498']},\n",
       " {'id': '2107034620',\n",
       "  'title': 'A Bayesian hierarchical model for learning natural scene categories',\n",
       "  'abstract': 'We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.',\n",
       "  'date': '2005',\n",
       "  'authors': ['L. Fei-Fei', 'P. Perona'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Categorization',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Caltech 101',\n",
       "   'Theme (narrative)',\n",
       "   'Visual dictionary',\n",
       "   'Dynamic topic model',\n",
       "   'LabelMe',\n",
       "   'Contextual image classification',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,720',\n",
       "  'reference_count': '19',\n",
       "  'references': ['1880262756',\n",
       "   '2124386111',\n",
       "   '2045656233',\n",
       "   '1566135517',\n",
       "   '1484228140',\n",
       "   '2127006916',\n",
       "   '2171188998',\n",
       "   '1699734612',\n",
       "   '2104924585',\n",
       "   '2094414211']},\n",
       " {'id': '2156598602',\n",
       "  'title': 'Photo tourism: exploring photo collections in 3D',\n",
       "  'abstract': 'We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Noah Snavely', 'Steven M. Seitz', 'Richard Szeliski'],\n",
       "  'related_topics': ['Digital photo frame',\n",
       "   'Rendering (computer graphics)',\n",
       "   'Rephotography',\n",
       "   'Computer graphics (images)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Tourism',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,019',\n",
       "  'reference_count': '43',\n",
       "  'references': ['2151103935',\n",
       "   '3029645440',\n",
       "   '2033819227',\n",
       "   '2131846894',\n",
       "   '2110764733',\n",
       "   '1980911747',\n",
       "   '2141282920',\n",
       "   '2119781527',\n",
       "   '2063366997',\n",
       "   '2085261163']},\n",
       " {'id': '3097096317',\n",
       "  'title': 'Robust Real-Time Face Detection',\n",
       "  'abstract': 'This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Paul Viola', 'Michael J. Jones'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Face detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Haar-like features',\n",
       "   'AdaBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'Cascading classifiers',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,088',\n",
       "  'reference_count': '21',\n",
       "  'references': []},\n",
       " {'id': '2121647436',\n",
       "  'title': 'Eigenfaces vs. Fisherfaces: recognition using class specific linear projection',\n",
       "  'abstract': 'We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher\\'s linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases.',\n",
       "  'date': '1997',\n",
       "  'authors': ['P.N. Belhumeur', 'J.P. Hespanha', 'D.J. Kriegman'],\n",
       "  'related_topics': ['Eigenface',\n",
       "   'Three-dimensional face recognition',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Face hallucination',\n",
       "   'Facial recognition system',\n",
       "   'Multilinear subspace learning',\n",
       "   'Linear subspace',\n",
       "   'Linear discriminant analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '16,889',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2138451337',\n",
       "   '2098693229',\n",
       "   '2123977795',\n",
       "   '2115689562',\n",
       "   '3017143921',\n",
       "   '2098947662',\n",
       "   '2113341759',\n",
       "   '2740373864',\n",
       "   '2130259898',\n",
       "   '2159173611']},\n",
       " {'id': '2033419168',\n",
       "  'title': 'The FERET evaluation methodology for face-recognition algorithms',\n",
       "  'abstract': 'Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1,199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to 1) assess the state of the art, 2) identify future areas of research, and 3) measure algorithm performance.',\n",
       "  'date': '2000',\n",
       "  'authors': ['P.J. Phillips', 'Hyeonjoon Moon', 'S.A. Rizvi', 'P.J. Rauss'],\n",
       "  'related_topics': ['FERET database',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Facial recognition system',\n",
       "   'Biometrics',\n",
       "   'Pattern recognition',\n",
       "   'Data mining',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'FERET',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,285',\n",
       "  'reference_count': '12',\n",
       "  'references': []},\n",
       " {'id': '2123921160',\n",
       "  'title': 'From few to many: illumination cone models for face recognition under variable lighting and pose',\n",
       "  'abstract': 'We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions.',\n",
       "  'date': '2001',\n",
       "  'authors': ['A.S. Georghiades', 'P.N. Belhumeur', 'D.J. Kriegman'],\n",
       "  'related_topics': ['Illumination problem',\n",
       "   'Image-based modeling and rendering',\n",
       "   'Generative model',\n",
       "   'Face (geometry)',\n",
       "   'Standard test image',\n",
       "   'Facial recognition system',\n",
       "   'Convex cone',\n",
       "   'Iterative reconstruction',\n",
       "   'Lambertian reflectance',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,440',\n",
       "  'reference_count': '82',\n",
       "  'references': ['2138451337',\n",
       "   '2217896605',\n",
       "   '2121647436',\n",
       "   '2152826865',\n",
       "   '2033419168',\n",
       "   '2159686933',\n",
       "   '2123977795',\n",
       "   '2115689562',\n",
       "   '2120954940',\n",
       "   '2098947662']},\n",
       " {'id': '2137659841',\n",
       "  'title': 'Overview of the face recognition grand challenge',\n",
       "  'abstract': 'Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.',\n",
       "  'date': '2005',\n",
       "  'authors': ['P.J. Phillips',\n",
       "   'P.J. Flynn',\n",
       "   'T. Scruggs',\n",
       "   'K.W. Bowyer',\n",
       "   'Jin Chang',\n",
       "   'K. Hoffman',\n",
       "   'J. Marques',\n",
       "   'Jaesik Min',\n",
       "   'W. Worek'],\n",
       "  'related_topics': ['Face Recognition Grand Challenge',\n",
       "   'Face detection',\n",
       "   'Facial recognition system',\n",
       "   'Word error rate',\n",
       "   'Biometrics',\n",
       "   'Machine learning',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Baseline (configuration management)',\n",
       "   'NIST',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,766',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2138451337',\n",
       "   '2102773363',\n",
       "   '2143542740',\n",
       "   '2137385871',\n",
       "   '2120838001',\n",
       "   '1555969862',\n",
       "   '138943044']},\n",
       " {'id': '2098693229',\n",
       "  'title': 'Face recognition using eigenfaces',\n",
       "  'abstract': \"An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner. >\",\n",
       "  'date': '1991',\n",
       "  'authors': ['M.A. Turk', 'A.P. Pentland'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Eigenface',\n",
       "   'Face detection',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Feature vector',\n",
       "   'Unsupervised learning',\n",
       "   'Set (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Face space',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,467',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2138451337',\n",
       "   '2125848778',\n",
       "   '2130259898',\n",
       "   '2055712799',\n",
       "   '2125999363',\n",
       "   '1507699566',\n",
       "   '1998186877',\n",
       "   '2169718527']},\n",
       " {'id': '2125310925',\n",
       "  'title': 'Recovering Surface Layout from an Image',\n",
       "  'abstract': 'Humans have an amazing ability to instantly grasp the overall 3D structure of a scene--ground orientation, relative positions of major landmarks, etc.--even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \"surface layout\" of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.\\r\\n\\r\\nIn this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Derek Hoiem', 'Alexei A. Efros', 'Martial Hebert'],\n",
       "  'related_topics': ['Orientation (computer vision)',\n",
       "   'Image processing',\n",
       "   'GRASP',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'View synthesis',\n",
       "   'Color image',\n",
       "   'Object detection',\n",
       "   'Perspective (graphical)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '872',\n",
       "  'reference_count': '53',\n",
       "  'references': ['2033819227',\n",
       "   '2147880316',\n",
       "   '2121947440',\n",
       "   '1999478155',\n",
       "   '2143516773',\n",
       "   '1566135517',\n",
       "   '2024046085',\n",
       "   '2209124607',\n",
       "   '2032210760',\n",
       "   '1484228140']},\n",
       " {'id': '2994340921',\n",
       "  'title': 'The AR face databasae',\n",
       "  'abstract': '',\n",
       "  'date': '1998',\n",
       "  'authors': ['A. M.'],\n",
       "  'related_topics': ['Face (sociological concept)',\n",
       "   'Computer science',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,771',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2006793117',\n",
       "  'title': 'The CMU pose, illumination, and expression database',\n",
       "  'abstract': 'In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.',\n",
       "  'date': '2003',\n",
       "  'authors': ['T. Sim', 'S. Baker', 'M. Bsat'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Expression (mathematics)',\n",
       "   'Computer vision',\n",
       "   'Computer graphics (images)',\n",
       "   'Computer science',\n",
       "   'Database',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,520',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2155759509',\n",
       "   '2118774738',\n",
       "   '2102760078',\n",
       "   '2120420721',\n",
       "   '2110822444',\n",
       "   '2121114545',\n",
       "   '2106143125',\n",
       "   '2141503314',\n",
       "   '2144855601']},\n",
       " {'id': '2111993661',\n",
       "  'title': 'Image retrieval: Ideas, influences, and trends of the new age',\n",
       "  'abstract': 'We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Ritendra Datta', 'Dhiraj Joshi', 'Jia Li', 'James Z. Wang'],\n",
       "  'related_topics': ['Content-based image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Image retrieval',\n",
       "   'Emerging technologies',\n",
       "   'Process (engineering)',\n",
       "   'Adaptation (computer science)',\n",
       "   'Information retrieval',\n",
       "   'Documentation',\n",
       "   'Computer science',\n",
       "   'Annotation',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,570',\n",
       "  'reference_count': '282',\n",
       "  'references': ['1480376833',\n",
       "   '2177274842',\n",
       "   '2121947440',\n",
       "   '2067191022',\n",
       "   '2119479037',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '1989702938',\n",
       "   '2130660124',\n",
       "   '1579271636']},\n",
       " {'id': '1666447063',\n",
       "  'title': 'Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary',\n",
       "  'abstract': 'We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.',\n",
       "  'date': '2002',\n",
       "  'authors': ['P. Duygulu',\n",
       "   'Kobus Barnard',\n",
       "   'J. F. G. de Freitas',\n",
       "   'David A. Forsyth'],\n",
       "  'related_topics': ['Vocabulary',\n",
       "   'Lexicon',\n",
       "   'Machine translation',\n",
       "   'Test set',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Noun',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,198',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2121947440',\n",
       "   '1574901103',\n",
       "   '1508960934',\n",
       "   '2006969979',\n",
       "   '1579838312',\n",
       "   '1934863104',\n",
       "   '2129765547',\n",
       "   '2293605478',\n",
       "   '1540386283',\n",
       "   '1585814348']},\n",
       " {'id': '1934863104',\n",
       "  'title': 'Learning the semantics of words and pictures',\n",
       "  'abstract': 'We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition.',\n",
       "  'date': '2001',\n",
       "  'authors': ['K. Barnard', 'D. Forsyth'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Unsupervised learning',\n",
       "   'Feature (computer vision)',\n",
       "   'Human–computer information retrieval',\n",
       "   'Semantics',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Information retrieval',\n",
       "   'Statistical model',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '768',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2049633694',\n",
       "   '2160066518',\n",
       "   '2135705692',\n",
       "   '2125101937',\n",
       "   '2062270497',\n",
       "   '2155099190',\n",
       "   '1587328194',\n",
       "   '2099251025',\n",
       "   '2011549082',\n",
       "   '2117086609']},\n",
       " {'id': '2166770390',\n",
       "  'title': 'Object Detection Using the Statistics of Parts',\n",
       "  'abstract': 'In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively.\\r\\n\\r\\nEach classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated class-conditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation—labeling the input as non-object—or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Henry Schneiderman', 'Takeo Kanade'],\n",
       "  'related_topics': ['Margin classifier',\n",
       "   'Quadratic classifier',\n",
       "   'Object detection',\n",
       "   'Classifier (UML)',\n",
       "   'AdaBoost',\n",
       "   'Face detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Likelihood-ratio test',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '498',\n",
       "  'reference_count': '31',\n",
       "  'references': []},\n",
       " {'id': '1587328194',\n",
       "  'title': 'Finding Naked People',\n",
       "  'abstract': 'This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image. No manual intervention is required. The approach combines color and texture properties to obtain an effective mask for skin regions. The skin mask is shown to be effective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture. The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn from a wide collection of sources.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Margaret M. Fleck', 'David A. Forsyth', 'Chris Bregler'],\n",
       "  'related_topics': ['Object model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Texture (music)',\n",
       "   'Computer vision',\n",
       "   'Test set',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Image (mathematics)',\n",
       "   'Range (mathematics)',\n",
       "   'Computer science',\n",
       "   'World Wide Web',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '681',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2145023731',\n",
       "   '2123977795',\n",
       "   '2093191240',\n",
       "   '2008297189',\n",
       "   '2068272887',\n",
       "   '2053197265',\n",
       "   '2102475035',\n",
       "   '1530454533',\n",
       "   '2037732452',\n",
       "   '2069266228']},\n",
       " {'id': '2293605478',\n",
       "  'title': 'Clustering art',\n",
       "  'abstract': 'We extend a recently developed method (K. Barnard and D. Forsyth, 2001) for learning the semantics of image databases using text and pictures. We incorporate statistical natural language processing in order to deal with free text. We demonstrate the current system on a difficult dataset, namely 10000 images of work from the Fine Arts Museum of San Francisco. The images include line drawings, paintings, and pictures of sculpture and ceramics. Many of the images have associated free text which varies greatly from physical description to interpretation and mood. We use WordNet to provide semantic grouping information and to help disambiguate word senses, as well as emphasize the hierarchical nature of semantic relationships. This allows us to impose a natural structure on the image collection that reflects semantics to a considerable degree. Our method produces a joint probability distribution for words and picture elements. We demonstrate that this distribution can be used: (a) to provide illustrations for given captions, and (b) to generate words for images outside the training set. Results from this annotation process yield a quantitative study of our method. Finally, the annotation process can be seen as a form of object recognizer that has been learned through a partially supervised process.',\n",
       "  'date': '2001',\n",
       "  'authors': ['K. Barnard', 'P. Duygulu', 'D. Forsyth'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Semantics',\n",
       "   'Natural language',\n",
       "   'Computational linguistics',\n",
       "   'Cluster analysis',\n",
       "   'Object (computer science)',\n",
       "   'Annotation',\n",
       "   'Natural language processing',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Computer science',\n",
       "   'Text mining',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '227',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2121947440',\n",
       "   '2049633694',\n",
       "   '2102381086',\n",
       "   '1934863104',\n",
       "   '2081687495',\n",
       "   '2117086609',\n",
       "   '2166447979',\n",
       "   '2004690028',\n",
       "   '1972812142',\n",
       "   '1504061712']},\n",
       " {'id': '2970081408',\n",
       "  'title': 'Language Change',\n",
       "  'abstract': '',\n",
       "  'date': '2004',\n",
       "  'authors': ['Adrian'],\n",
       "  'related_topics': ['Language change', 'Linguistics', 'Psychology'],\n",
       "  'citation_count': '285',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2055225264',\n",
       "  'title': 'PicASHOW: pictorial authority search by hyperlinks on the web.',\n",
       "  'abstract': 'We describe PicASHOW, a fully automated WWW image retrieval system that is based on several link-structure analyzing algorithms. Our basic premise is that a page p displays (or links to) an image when the author of p considers the image to be of value to the viewers of the page. We thus extend some well known link-based WWW page retrieval schemes to the context of image retrieval. PicASHOW’s analysis of the link structure enables it to retrieve relevant images even when those are stored in files with meaningless names. The same analysis also allows it to identify image containers and image hubs. We define these as Web pages that are rich in relevant images, or from which many images are readily accessible. PicASHOW requires no image analysis whatsoever and no creation of taxonomies for preclassification of the Web’s images. It can be implemented by standard WWW search engines with reasonable overhead, in terms of both computations and storage, and with no change to user query formats. It can thus be used to easily add image retrieving capabilities to standard search engines. Our results demonstrate that PicASHOW, while relying almost exclusively on link analysis, compares well with dedicated WWW image retrieval systems. We conclude that link analysis, a proven effective technique for Web page search, can improve the performance of Web image retrieval, as well as extend its definition to include the retrieval of image hubs and containers.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Ronny', 'Aya'],\n",
       "  'related_topics': ['Image retrieval',\n",
       "   'Static web page',\n",
       "   'Web page',\n",
       "   'Web search engine',\n",
       "   'Printer-friendly',\n",
       "   'Web search query',\n",
       "   'Web crawler',\n",
       "   'Hyperlink',\n",
       "   'Information retrieval',\n",
       "   'World Wide Web',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '122',\n",
       "  'reference_count': '18',\n",
       "  'references': ['3013264884',\n",
       "   '2138621811',\n",
       "   '2006119904',\n",
       "   '2008297189',\n",
       "   '2079672501',\n",
       "   '1987777228',\n",
       "   '2089199911',\n",
       "   '2099251025',\n",
       "   '2117086609',\n",
       "   '2140350208']},\n",
       " {'id': '2050457084',\n",
       "  'title': 'Categories, Photographs & Predicaments: Exploratory Research on Representing Pictures for Access',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': [\"Brian C. O'Connor\", \"Mary Keeney O'Connor\"],\n",
       "  'related_topics': ['Exploratory research',\n",
       "   'Applied psychology',\n",
       "   'Psychology',\n",
       "   'Multimedia',\n",
       "   'View Less'],\n",
       "  'citation_count': '5',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '181417509',\n",
       "  'title': 'Storage and Retrieval of Feature Data for a Very Large Online Image Collection.',\n",
       "  'abstract': '',\n",
       "  'date': '1996',\n",
       "  'authors': ['T. K.', 'Lucien A.', 'Dwayne'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image texture',\n",
       "   'Feature data',\n",
       "   'Image processing',\n",
       "   'Digital image processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '84',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2612148268',\n",
       "  'title': 'Categories, photographs & predicaments : Exploratory research on representing pictures for access : Theory and practice in the organization of images and other visuo-spatial data for retrieval',\n",
       "  'abstract': '',\n",
       "  'date': '1999',\n",
       "  'authors': ['B. C.', 'M. K.'],\n",
       "  'related_topics': ['Exploratory research',\n",
       "   'Spatial analysis',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2217896605',\n",
       "  'title': 'Neural network-based face detection',\n",
       "  'abstract': 'We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.',\n",
       "  'date': '1998',\n",
       "  'authors': ['H.A. Rowley', 'S. Baluja', 'T. Kanade'],\n",
       "  'related_topics': ['Face detection',\n",
       "   'Object-class detection',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Artificial neural network',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Pixel',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,472',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2139212933',\n",
       "   '2981264952',\n",
       "   '2133671888',\n",
       "   '2124351082',\n",
       "   '2147800946',\n",
       "   '2098947662',\n",
       "   '1997011019',\n",
       "   '2173629880',\n",
       "   '2042371054',\n",
       "   '2159173611']},\n",
       " {'id': '2045656233',\n",
       "  'title': 'Bayesian Data Analysis',\n",
       "  'abstract': 'FUNDAMENTALS OF BAYESIAN INFERENCE Probability and Inference Single-Parameter Models Introduction to Multiparameter Models Asymptotics and Connections to Non-Bayesian Approaches Hierarchical Models FUNDAMENTALS OF BAYESIAN DATA ANALYSIS Model Checking Evaluating, Comparing, and Expanding Models Modeling Accounting for Data Collection Decision Analysis ADVANCED COMPUTATION Introduction to Bayesian Computation Basics of Markov Chain Simulation Computationally Efficient Markov Chain Simulation Modal and Distributional Approximations REGRESSION MODELS Introduction to Regression Models Hierarchical Linear Models Generalized Linear Models Models for Robust Inference Models for Missing Data NONLINEAR AND NONPARAMETRIC MODELS Parametric Nonlinear Models Basic Function Models Gaussian Process Models Finite Mixture Models Dirichlet Process Models APPENDICES A: Standard Probability Distributions B: Outline of Proofs of Asymptotic Theorems C: Computation in R and Stan Bibliographic Notes and Exercises appear at the end of each chapter.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Andrew',\n",
       "   'John B.',\n",
       "   'Hal S.',\n",
       "   'David B.',\n",
       "   'Aki',\n",
       "   'Donald B. Rubin'],\n",
       "  'related_topics': ['Variable-order Bayesian network',\n",
       "   'Bayesian statistics',\n",
       "   'Dynamic Bayesian network',\n",
       "   'Bayesian inference',\n",
       "   'Dirichlet process',\n",
       "   'Probabilistic programming language',\n",
       "   'Markov chain',\n",
       "   'Bayesian probability',\n",
       "   'Applied mathematics',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '30,244',\n",
       "  'reference_count': '4',\n",
       "  'references': ['2159080219', '2156273867', '2126163471', '1533179050']},\n",
       " {'id': '2130416410',\n",
       "  'title': 'Markov Chain Monte Carlo in Practice',\n",
       "  'abstract': 'INTRODUCING MARKOV CHAIN MONTE CARLO Introduction The Problem Markov Chain Monte Carlo Implementation Discussion HEPATITIS B: A CASE STUDY IN MCMC METHODS Introduction Hepatitis B Immunization Modelling Fitting a Model Using Gibbs Sampling Model Elaboration Conclusion MARKOV CHAIN CONCEPTS RELATED TO SAMPLING ALGORITHMS Markov Chains Rates of Convergence Estimation The Gibbs Sampler and Metropolis-Hastings Algorithm INTRODUCTION TO GENERAL STATE-SPACE MARKOV CHAIN THEORY Introduction Notation and Definitions Irreducibility, Recurrence, and Convergence Harris Recurrence Mixing Rates and Central Limit Theorems Regeneration Discussion FULL CONDITIONAL DISTRIBUTIONS Introduction Deriving Full Conditional Distributions Sampling from Full Conditional Distributions Discussion STRATEGIES FOR IMPROVING MCMC Introduction Reparameterization Random and Adaptive Direction Sampling Modifying the Stationary Distribution Methods Based on Continuous-Time Processes Discussion IMPLEMENTING MCMC Introduction Determining the Number of Iterations Software and Implementation Output Analysis Generic Metropolis Algorithms Discussion INFERENCE AND MONITORING CONVERGENCE Difficulties in Inference from Markov Chain Simulation The Risk of Undiagnosed Slow Convergence Multiple Sequences and Overdispersed Starting Points Monitoring Convergence Using Simulation Output Output Analysis for Inference Output Analysis for Improving Efficiency MODEL DETERMINATION USING SAMPLING-BASED METHODS Introduction Classical Approaches The Bayesian Perspective and the Bayes Factor Alternative Predictive Distributions How to Use Predictive Distributions Computational Issues An Example Discussion HYPOTHESIS TESTING AND MODEL SELECTION Introduction Uses of Bayes Factors Marginal Likelihood Estimation by Importance Sampling Marginal Likelihood Estimation Using Maximum Likelihood Application: How Many Components in a Mixture? Discussion Appendix: S-PLUS Code for the Laplace-Metropolis Estimator MODEL CHECKING AND MODEL IMPROVEMENT Introduction Model Checking Using Posterior Predictive Simulation Model Improvement via Expansion Example: Hierarchical Mixture Modelling of Reaction Times STOCHASTIC SEARCH VARIABLE SELECTION Introduction A Hierarchical Bayesian Model for Variable Selection Searching the Posterior by Gibbs Sampling Extensions Constructing Stock Portfolios With SSVS Discussion BAYESIAN MODEL COMPARISON VIA JUMP DIFFUSIONS Introduction Model Choice Jump-Diffusion Sampling Mixture Deconvolution Object Recognition Variable Selection Change-Point Identification Conclusions ESTIMATION AND OPTIMIZATION OF FUNCTIONS Non-Bayesian Applications of MCMC Monte Carlo Optimization Monte Carlo Likelihood Analysis Normalizing-Constant Families Missing Data Decision Theory Which Sampling Distribution? Importance Sampling Discussion STOCHASTIC EM: METHOD AND APPLICATION Introduction The EM Algorithm The Stochastic EM Algorithm Examples GENERALIZED LINEAR MIXED MODELS Introduction Generalized Linear Models (GLMs) Bayesian Estimation of GLMs Gibbs Sampling for GLMs Generalized Linear Mixed Models (GLMMs) Specification of Random-Effect Distributions Hyperpriors and the Estimation of Hyperparameters Some Examples Discussion HIERARCHICAL LONGITUDINAL MODELLING Introduction Clinical Background Model Detail and MCMC Implementation Results Summary and Discussion MEDICAL MONITORING Introduction Modelling Medical Monitoring Computing Posterior Distributions Forecasting Model Criticism Illustrative Application Discussion MCMC FOR NONLINEAR HIERARCHICAL MODELS Introduction Implementing MCMC Comparison of Strategies A Case Study from Pharmacokinetics-Pharmacodynamics Extensions and Discussion BAYESIAN MAPPING OF DISEASE Introduction Hypotheses and Notation Maximum Likelihood Estimation of Relative Risks Hierarchical Bayesian Model of Relative Risks Empirical Bayes Estimation of Relative Risks Fully Bayesian Estimation of Relative Risks Discussion MCMC IN IMAGE ANALYSIS Introduction The Relevance of MCMC to Image Analysis Image Models at Different Levels Methodological Innovations in MCMC Stimulated by Imaging Discussion MEASUREMENT ERROR Introduction Conditional-Independence Modelling Illustrative examples Discussion GIBBS SAMPLING METHODS IN GENETICS Introduction Standard Methods in Genetics Gibbs Sampling Approaches MCMC Maximum Likelihood Application to a Family Study of Breast Cancer Conclusions MIXTURES OF DISTRIBUTIONS: INFERENCE AND ESTIMATION Introduction The Missing Data Structure Gibbs Sampling Implementation Convergence of the Algorithm Testing for Mixtures Infinite Mixtures and Other Extensions AN ARCHAEOLOGICAL EXAMPLE: RADIOCARBON DATING Introduction Background to Radiocarbon Dating Archaeological Problems and Questions Illustrative Examples Discussion Index',\n",
       "  'date': '1997',\n",
       "  'authors': ['W.R. Gilks', 'S.', 'David'],\n",
       "  'related_topics': ['Gibbs sampling',\n",
       "   'Slice sampling',\n",
       "   'Metropolis–Hastings algorithm',\n",
       "   'Markov chain Monte Carlo',\n",
       "   'Marginal likelihood',\n",
       "   'Bayes factor',\n",
       "   'Importance sampling',\n",
       "   'Bayesian inference',\n",
       "   'Mathematical optimization',\n",
       "   'Econometrics',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,193',\n",
       "  'reference_count': '2',\n",
       "  'references': ['2108207895', '2163738067']},\n",
       " {'id': '2030536784',\n",
       "  'title': 'Pictorial Structures for Object Recognition',\n",
       "  'abstract': 'In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Pedro F. Felzenszwalb', 'Daniel P. Huttenlocher'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Object model',\n",
       "   'Method',\n",
       "   'Visual appearance',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Object (computer science)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Articulated body pose estimation',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,810',\n",
       "  'reference_count': '43',\n",
       "  'references': ['3145128584',\n",
       "   '2138451337',\n",
       "   '2752885492',\n",
       "   '2143516773',\n",
       "   '2159080219',\n",
       "   '1560013842',\n",
       "   '1997063559',\n",
       "   '301824129',\n",
       "   '2123977795',\n",
       "   '2085261163']},\n",
       " {'id': '2147880316',\n",
       "  'title': 'Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data',\n",
       "  'abstract': 'We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.',\n",
       "  'date': '2001',\n",
       "  'authors': ['John D. Lafferty', 'Andrew', 'Fernando C. N.'],\n",
       "  'related_topics': ['Variable-order Markov model',\n",
       "   'Maximum-entropy Markov model',\n",
       "   'Graphical model',\n",
       "   'Conditional random field',\n",
       "   'Markov model',\n",
       "   'Markov property',\n",
       "   'Sequence labeling',\n",
       "   'Conditional entropy',\n",
       "   'Random field',\n",
       "   'Bayesian network',\n",
       "   'Discriminative model',\n",
       "   'Hidden Markov model',\n",
       "   'Structured prediction',\n",
       "   'Hidden semi-Markov model',\n",
       "   'Probabilistic logic',\n",
       "   'Probabilistic relevance model',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,754',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2310919327',\n",
       "   '3124955340',\n",
       "   '1574901103',\n",
       "   '2009570821',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '1773803948',\n",
       "   '2160842254',\n",
       "   '3021452258',\n",
       "   '2117400858']},\n",
       " {'id': '2124351162',\n",
       "  'title': '\"GrabCut\": interactive foreground extraction using iterated graph cuts',\n",
       "  'abstract': 'The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Carsten Rother', 'Vladimir Kolmogorov', 'Andrew Blake'],\n",
       "  'related_topics': ['GrabCut',\n",
       "   'Image segmentation',\n",
       "   'Simple interactive object extraction',\n",
       "   'Graph cuts in computer vision',\n",
       "   'Image editing',\n",
       "   'Cut',\n",
       "   'Iterative method',\n",
       "   'Segmentation',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,950',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2104095591',\n",
       "   '2169551590',\n",
       "   '2049633694',\n",
       "   '2101309634',\n",
       "   '2134820502',\n",
       "   '2077786999',\n",
       "   '2103917701',\n",
       "   '2740373864',\n",
       "   '2103334940',\n",
       "   '1785730614']},\n",
       " {'id': '2024046085',\n",
       "  'title': 'Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors)',\n",
       "  'abstract': 'Boosting is one of the most important recent developments in\\n\\t\\t\\t classification methodology. Boosting works by sequentially applying a\\n\\t\\t\\t classification algorithm to reweighted versions of the training data and then\\n\\t\\t\\t taking a weighted majority vote of the sequence of classifiers thus produced.\\n\\t\\t\\t For many classification algorithms, this simple strategy results in dramatic\\n\\t\\t\\t improvements in performance. We show that this seemingly mysterious phenomenon\\n\\t\\t\\t can be understood in terms of well-known statistical principles, namely\\n\\t\\t\\t additive modeling and maximum likelihood. For the two-class problem, boosting\\n\\t\\t\\t can be viewed as an approximation to additive modeling on the logistic scale\\n\\t\\t\\t using maximum Bernoulli likelihood as a criterion. We develop more direct\\n\\t\\t\\t approximations and show that they exhibit nearly identical results to boosting.\\n\\t\\t\\t Direct multiclass generalizations based on multinomial likelihood are derived\\n\\t\\t\\t that exhibit performance comparable to other recently proposed multiclass\\n\\t\\t\\t generalizations of boosting in most situations, and far superior in some. We\\n\\t\\t\\t suggest a minor modification to boosting that can reduce computation, often by\\n\\t\\t\\t factors of 10 to 50. Finally, we apply these insights to produce an alternative\\n\\t\\t\\t formulation of boosting decision trees. This approach, based on best-first\\n\\t\\t\\t truncated tree induction, often leads to better performance, and can provide\\n\\t\\t\\t interpretable descriptions of the aggregate decision rule. It is also much\\n\\t\\t\\t faster computationally, making it more suitable to large-scale data mining\\n\\t\\t\\t applications.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Jerome', 'Trevor', 'Robert'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'Gradient boosting',\n",
       "   'Boosting (machine learning)',\n",
       "   'LPBoost',\n",
       "   'LogitBoost',\n",
       "   'AdaBoost',\n",
       "   'Statistical classification',\n",
       "   'Decision rule',\n",
       "   'Machine learning',\n",
       "   'Econometrics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,451',\n",
       "  'reference_count': '6',\n",
       "  'references': ['1678356000',\n",
       "   '2102201073',\n",
       "   '1540007258',\n",
       "   '2099968818',\n",
       "   '1881647329',\n",
       "   '2141518341']},\n",
       " {'id': '2168002178',\n",
       "  'title': 'Shape matching and object recognition using low distortion correspondences',\n",
       "  'abstract': \"We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['A.C. Berg', 'T.L. Berg', 'J. Malik'],\n",
       "  'related_topics': ['Caltech 101',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Thin plate spline',\n",
       "   'Facial recognition system',\n",
       "   'Face detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Quadratic programming',\n",
       "   'Spline (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,117',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2151103935',\n",
       "   '3097096317',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2154422044',\n",
       "   '2124087378',\n",
       "   '2119823327',\n",
       "   '2155511848',\n",
       "   '2101522199',\n",
       "   '2295106276']},\n",
       " {'id': '1484228140',\n",
       "  'title': 'Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons',\n",
       "  'abstract': 'We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions.\\r\\n\\r\\nGiven a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Thomas Leung', 'Jitendra Malik'],\n",
       "  'related_topics': ['Texton',\n",
       "   'Visual appearance',\n",
       "   'Texture synthesis',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Texture (geology)',\n",
       "   'Cluster analysis',\n",
       "   'Basis (linear algebra)',\n",
       "   'Normal',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,043',\n",
       "  'reference_count': '44',\n",
       "  'references': ['2170120409',\n",
       "   '2117812871',\n",
       "   '2138451337',\n",
       "   '2130416410',\n",
       "   '1997063559',\n",
       "   '1634005169',\n",
       "   '2116013899',\n",
       "   '1481646516',\n",
       "   '2123977795',\n",
       "   '3017143921']},\n",
       " {'id': '3035018050',\n",
       "  'title': 'Early Detection and Assessment of Covid-19',\n",
       "  'abstract': \"Background: Since the Covid-19 global pandemic emerged, developing countries have been facing multiple challenges over its diagnosis. We aimed to establish a relationship between the signs and symptoms of COVID-19 for early detection and assessment to reduce the transmission rate of SARS-Cov-2. Methods: We collected published data on the clinical features of Covid-19 retrospectively and categorized them into physical and blood biomarkers. Common features were assigned scores by the Borg scoring method with slight modifications and were incorporated into a newly-developed Hashmi-Asif Covid-19 assessment Chart. Correlations between signs and symptoms with the development of Covid-19 was assessed by Pearson correlation and Spearman Correlation coefficient (rho). Linear regression analysis was employed to assess the highest correlating features. The frequency of signs and symptoms in developing Covid-19 was assessed through Chi-square test two tailed with Cramer's V strength. Changes in signs and symptoms were incorporated into a chart that consisted of four tiers representing disease stages. Results: Data from 10,172 Covid-19 laboratory confirmed cases showed a correlation with Fever in 43.9% (P = 0.000) cases, cough 54.08% and dry mucus 25.68% equally significant (P = 0.000), Hyperemic pharyngeal mucus membrane 17.92% (P = 0.005), leukopenia 28.11% (P = 0.000), lymphopenia 64.35% (P = 0.000), thrombopenia 35.49% (P = 0.000), elevated Alanine aminotransferase 50.02% (P = 0.000), and Aspartate aminotransferase 34.49% (P = 0.000). The chart exhibited a maximum scoring of 39. Normal tier scoring was ≤ 12/39, mild state scoring was 13-22/39, and star values scoring was ≥7/15; this latter category on the chart means Covid-19 is progressing and quarantine should be adopted. Moderate stage scored 23-33 and severe scored 34-39 in the chart. Conclusion: The Hashmi-Asif Covid-19 Chart is significant in assessing subclinical and clinical stages of Covid-19 to reduce the transmission rate.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Hafiz Abdul Sattar Hashmi', 'Hafiz Muhammad Asif'],\n",
       "  'related_topics': ['Chart',\n",
       "   'Subclinical infection',\n",
       "   \"Spearman's rank correlation coefficient\",\n",
       "   'Linear regression',\n",
       "   'Internal medicine',\n",
       "   'Pearson product-moment correlation coefficient',\n",
       "   'Stage (cooking)',\n",
       "   'Leukopenia',\n",
       "   'Correlation',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '8',\n",
       "  'reference_count': '39',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3003668884',\n",
       "   '3003465021',\n",
       "   '3009912996',\n",
       "   '3008696669',\n",
       "   '3008818676',\n",
       "   '3016535995',\n",
       "   '3012747666']},\n",
       " {'id': '3037451072',\n",
       "  'title': 'Analysis of Risk Perceptions and Related Factors Concerning COVID-19 Epidemic in Chongqing, China.',\n",
       "  'abstract': 'To assess perceptions of risk and related factors concerning COVID-19 epidemic among residents in Chongqing city, China. With convenience sampling, a web questionnaire survey was conducted among 476 residents living in Chongqing on February 13rd to 14th in 2020, when citizens just started to get back to work. Residents’ estimated perceived risks were (4.63\\u2009±\\u20090.57), (4.19\\u2009±\\u20090.76), (3.23\\u2009±\\u20090.91) and (2.29\\u2009±\\u20090.96) for the infectivity, pathogenicity, lethality and self-rated\\xa0infection possibility of COVID-19, respectively. Females (OR\\u2009=\\u20094.234), people with income\\u2009≥\\u20092000 yuan (2000–4999 yuan: OR\\u2009=\\u20095.052, 5000–9999 yuan: OR\\u2009=\\u20094.301,\\u2009≥\\u200910,000 yuan: OR\\u2009=\\u200923.459), the married status (OR\\u2009=\\u20091.811), the divorced status, widows or widowers (OR\\u2009=\\u20093.038), people living with families including children (OR\\u2009=\\u20095.085) or chronic patients (OR\\u2009=\\u20092.423) had a higher perceived risk level, as well as people who used free media websites (OR\\u2009=\\u20091.756), community workers (OR\\u2009=\\u20094.064) or community information platforms (OR\\u2009=\\u20092.235) as main media information sources. The perceived risk increased by 4.9% for every one-year increase of age. People who used WeChat contacts (OR\\u2009=\\u20090.196) as the main media information source, reported a lower perceived risk. Residents reported a high level of risk perception towards COVID-19 in Chongqing and it was impacted by the population demographic characteristics. Media information sources, including community information platforms and community workers may cause the increase of public risk perceptions.',\n",
       "  'date': '2021',\n",
       "  'authors': ['Shan He', 'Siyu Chen', 'Lingna Kong', 'Weiwei Liu'],\n",
       "  'related_topics': ['Population',\n",
       "   'Risk perception',\n",
       "   'Questionnaire',\n",
       "   'Environmental health',\n",
       "   'China',\n",
       "   'Information source',\n",
       "   'Psychology',\n",
       "   'Perception',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Related factors',\n",
       "   'View Less'],\n",
       "  'citation_count': '15',\n",
       "  'reference_count': '15',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3003668884',\n",
       "   '3001465255',\n",
       "   '3008818676',\n",
       "   '2792024998',\n",
       "   '3016902371',\n",
       "   '3022459584',\n",
       "   '2091069417',\n",
       "   '3134923022']},\n",
       " {'id': '3034593359',\n",
       "  'title': 'Epidemiological and Clinical Characteristics of Cases During the Early Phase of COVID-19 Pandemic: A Systematic Review and Meta-Analysis',\n",
       "  'abstract': 'Background: On 29th December 2019, a cluster of cases displaying the symptoms of a \"pneumonia of unknown cause\" was identified in Wuhan, Hubei province of China. This systematic review and meta-analysis aims to review the epidemiological and clinical characteristics of COVID-19 cases in the early phase of the COVID-19 pandemic. Methods: The search strategy involved peer-reviewed studies published between 1st January and 11th February 2020 in Pubmed, Google scholar and China Knowledge Resource Integrated database. Publications identified were screened for their title and abstracts according to the eligibility criteria, and further shortlisted by full-text screening. Three independent reviewers extracted data from these studies, and studies were assessed for potential risk of bias. Studies comprising non-overlapping patient populations, were included for qualitative and quantitative synthesis of results. Pooled prevalence with 95% confidence intervals were calculated for patient characteristics. Results: A total of 29 publications were selected after full-text review. This comprised of 18 case reports, three case series and eight cross-sectional studies on patients admitted from mid-December of 2019 to early February of 2020. A total of 533 adult patients with pooled median age of 56 (95% CI: 49-57) and a pooled prevalence of male of 60% (95% CI: 52-68%) were admitted to hospital at a pooled median of 7 days (95% CI: 7-7) post-onset of symptoms. The most common symptoms at admission were fever, cough and fatigue, with a pooled prevalence of 90% (95% CI: 81-97%), 58% (95% CI: 47-68%), and 50% (95% CI: 29-71%), respectively. Myalgia, shortness of breath, headache, diarrhea and sore throat were less common with pooled prevalence of 27% (95% CI: 20-36%), 25% (95% CI: 15-35%), 10% (95% CI: 7-13%), 8% (95% CI: 5-13%), and 7% (95% CI: 1-15%), respectively. ICU patients had a higher proportion of shortness of breath at presentation, as well as pre-existing hypertension, cardiovascular disease and COPD, compared to non-ICU patients in 2 studies (n = 179). Conclusion: This study highlights the key epidemiological and clinical features of COVID-19 cases during the early phase of the COVID-19 pandemic.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Jiayun Koh',\n",
       "   'Shimoni Urvish Shah',\n",
       "   'Pearleen Ee Yong Chua',\n",
       "   'Hao Gui',\n",
       "   'Junxiong Pang'],\n",
       "  'related_topics': ['Epidemiology',\n",
       "   'Meta-analysis',\n",
       "   'Sore throat',\n",
       "   'Confidence interval',\n",
       "   'COPD',\n",
       "   'Internal medicine',\n",
       "   'myalgia',\n",
       "   'Disease cluster',\n",
       "   'Pneumonia',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '22',\n",
       "  'reference_count': '66',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3008028633',\n",
       "   '3002539152',\n",
       "   '3001195213',\n",
       "   '3003465021']},\n",
       " {'id': '3033301213',\n",
       "  'title': 'Does Early Childhood Vaccination Protect Against COVID-19?',\n",
       "  'abstract': 'The coronavirus disease 2019 (COVID-19) is an on-going pandemic caused by the SARS-coronavirus-2 (SARS-CoV-2) which targets the respiratory system of humans. The published data show that children, unlike adults, are less susceptible to contracting the disease. This article aims at understanding why children constitute a minor group among hospitalized COVID-19 patients. Here, we hypothesize that the measles, mumps, and rubella (MMR) vaccine could provide a broad neutralizing antibody against numbers of diseases, including COVID-19. Our hypothesis is based on the 30 amino acid sequence homology between the SARS-CoV-2 Spike (S) glycoprotein (PDB: 6VSB) of both the measles virus fusion (F1) glycoprotein (PDB: 5YXW_B) and the rubella virus envelope (E1) glycoprotein (PDB: 4ADG_A). Computational analysis of the homologous region detected the sequence as antigenic epitopes in both measles and rubella. Therefore, we believe that humoral immunity, created through the MMR vaccination, provides children with advantageous protection against COVID-19 as well, however, an experimental analysis is required.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Karzan R Sidiq',\n",
       "   'Dana Khdr Sabir',\n",
       "   'Shakhawan M Ali',\n",
       "   'Rimantas Kodzius'],\n",
       "  'related_topics': ['Rubella',\n",
       "   'Rubella virus',\n",
       "   'Measles virus',\n",
       "   'Vaccination',\n",
       "   'Measles',\n",
       "   'Immunization',\n",
       "   'Virus',\n",
       "   'Neutralizing antibody',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '16',\n",
       "  'reference_count': '29',\n",
       "  'references': ['3004318991',\n",
       "   '3003217347',\n",
       "   '3008818676',\n",
       "   '3007643904',\n",
       "   '3011242477',\n",
       "   '3010819577',\n",
       "   '3012310845',\n",
       "   '3004348779',\n",
       "   '3035011439',\n",
       "   '3004896487']},\n",
       " {'id': '3021916232',\n",
       "  'title': 'Knowledge, awareness and practice of health care professionals amid sars-cov-2, corona virus disease outbreak',\n",
       "  'abstract': 'Objective: To assess the knowledge, awareness and practice level of health care workers towards Corona Virus disease - 2019 (COVID-19).\\r\\nMethods: A cross sectional study was conducted by administering a well-structured questionnaire comprising of three sections including knowledge, attitude and practice amongst health care professionals in various hospitals and clinics, over a duration of two months ‘Feb-March’ 2020. The data from 810 participants were collected manually as well as through online survey registered on www.surveys.google.com, using a validated questionnaire. The questionnaire comprised of three sections assessing knowledge, awareness and practice of participants. The\\xa0descriptive analysis was carried out for demographics and dependent variables with statistical program for social sciences. Spearman test was used to detect any relationship between the health care professional response with respect to their gender and level of education. A p-value of < 0.05 was considered statistically significant.\\r\\nResults: More than half (57.2%) of the health care professionals were working in a hospital setting. Fifty two percent of health care professionals had awareness and 72% were practicing adequate measures to combat COVID-19. The majority (81.9%) believed that the sign and symptoms are similar to a common flu and the main strata of population that could be affected by COVID-19 are elderly (79%). Seventy three percent of participants did not attend any lecture, workshop or seminar on COVID-19 for awareness purpose. Sixty seven percent of health care professionals were practicing universal precaution for infection control and 57.4% were using sodium hypochlorite as a surface disinfectant in dental surgeries. There was no significant relationship (p > 0.05) between the health care professionals’ responses with gender and their education level.\\r\\nConclusion: The study suggests that the vast majority of the health care professionals have adequate knowledge and awareness related to COVID-19. However some aspects of practice of health care professionals were found to be deficient including, following CDC guidelines during patient care, acquiring verified knowledge related to COVID-19, disinfection protocol and the use of N-95 mask. Mandatory Continued professional development programs including lectures and workshops on COVID-19 for all health care professionals are the need of the hour, to manage the pandemic and limiting the morbidity and mortality related to it.\\r\\ndoi: https://doi.org/10.12669/pjms.36.COVID19-S4.2704\\r\\nHow to cite this:Ahmed N, Shakoor M, Vohra F, Abduljabbar T, Mariam Q, Rehman MA. Knowledge, Awareness and Practice of Health care Professionals amid SARS-CoV-2, Corona Virus Disease Outbreak. Pak J Med Sci. 2020;36(COVID19-S4):COVID19-S49-S56.\\xa0\\xa0doi: https://doi.org/10.12669/pjms.36.COVID19-S4.2704\\r\\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Naseer Ahmed',\n",
       "   'Maria Shakoor',\n",
       "   'Fahim Vohra',\n",
       "   'Tariq Abduljabbar',\n",
       "   'Quratulain Mariam',\n",
       "   'Mariam Abdul Rehman'],\n",
       "  'related_topics': ['Health care',\n",
       "   'Population',\n",
       "   'Universal precautions',\n",
       "   'Infection control',\n",
       "   'Professional development',\n",
       "   'Family medicine',\n",
       "   'Cross-sectional study',\n",
       "   'Pandemic',\n",
       "   'Test (assessment)',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '15',\n",
       "  'reference_count': '22',\n",
       "  'references': ['3008827533',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3008696669',\n",
       "   '3008818676',\n",
       "   '3006645647',\n",
       "   '3011176674',\n",
       "   '3009607814',\n",
       "   '3011802905',\n",
       "   '3005798348']},\n",
       " {'id': '3031029566',\n",
       "  'title': 'Identification of RT-PCR-Negative Asymptomatic COVID-19 Patients via Serological Testing',\n",
       "  'abstract': 'Asymptomatic individuals with coronavirus disease (COVID-19) have been identified via nucleic acid testing for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2); however, the epidemiologic characteristics and viral shedding pattern of asymptomatic patients remain largely unknown. In this study, serological testing was applied when identifying nine asymptomatic cases of COVID-19 who showed persistent negative RT-PCR test results for SARS-CoV-2 nucleic acid and no symptoms of COVID-19. Two asymptomatic cases were presumed to be index patients who had cleared the virus when their close contacts developed symptoms of COVID-19. Three of the asymptomatic cases were local individuals who spontaneously recovered before their presumed index patients developed symptoms of COVID-19. This report presents the epidemiologic and clinical characteristics of asymptomatic individuals with SARS-CoV-2 infection that were undetected on RT-PCR tests in previous epidemiologic investigations probably due to the transient viral shedding duration.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Jinru Wu',\n",
       "   'Xinyi Liu',\n",
       "   'Dan Zhou',\n",
       "   'Guangqian Qiu',\n",
       "   'Miao Dai',\n",
       "   'Qingting Yang',\n",
       "   'Zhonghui Pan',\n",
       "   'Ning Zhou',\n",
       "   'Pa Wu'],\n",
       "  'related_topics': ['Asymptomatic',\n",
       "   'Viral shedding',\n",
       "   'Coronavirus',\n",
       "   'Serology',\n",
       "   'Disease',\n",
       "   'Virus',\n",
       "   'Immunology',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'View Less'],\n",
       "  'citation_count': '6',\n",
       "  'reference_count': '19',\n",
       "  'references': ['3002539152',\n",
       "   '3006961006',\n",
       "   '3008696669',\n",
       "   '3008818676',\n",
       "   '3015571324',\n",
       "   '3008874180',\n",
       "   '3010781325',\n",
       "   '3035011439',\n",
       "   '3015792206',\n",
       "   '3012188173']},\n",
       " {'id': '3037552531',\n",
       "  'title': 'Analysis of clinical features and early warning signs in patients with severe COVID-19: A retrospective cohort study.',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) was first identified in Wuhan, China, in December 2019. Although previous studies have described the clinical aspects of COVID-19, few studies have focused on the early detection of severe COVID-19. Therefore, this study aimed to identify the predictors of severe COVID-19 and to compare clinical features between patients with severe COVID-19 and those with less severe COVID-19. Patients admitted to designated hospital in the Henan Province of China who were either discharged or died prior to February 15, 2020 were enrolled retrospectively. Additionally, patients who underwent at least one of the following treatments were assigned to the severe group: continuous renal replacement therapy, high-flow oxygen absorption, noninvasive and invasive mechanical ventilation, or extracorporeal membrane oxygenation. The remaining patients were assigned to the non-severe group. Demographic information, initial symptoms, and first visit examination results were collected from the electronic medical records and compared between the groups. Multivariate logistic regression analysis was performed to determine the predictors of severe COVID-19. A receiver operating characteristic curve was used to identify a threshold for each predictor. Altogether,104 patients were enrolled in our study with 30 and 74 patients in the severe and non-severe groups, respectively. Multivariate logistic analysis indicated that patients aged ≥63 years (odds ratio = 41.0; 95% CI: 2.8, 592.4), with an absolute lymphocyte value of ≤1.02×109/L (odds ratio = 6.1; 95% CI = 1.5, 25.2) and a C-reactive protein level of ≥65.08mg/L (odds ratio = 8.9; 95% CI = 1.0, 74.2) were at a higher risk of severe illness. Thus, our results could be helpful in the early detection of patients at risk for severe illness, enabling the implementation of effective interventions and likely lowering the morbidity of COVID-19 patients.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Xinkui Liu',\n",
       "   'Xinpei Yue',\n",
       "   'Furong Liu',\n",
       "   'Le Wei',\n",
       "   'Yuntian Chu',\n",
       "   'Honghong Bao',\n",
       "   'Yichao Dong',\n",
       "   'Wenjie Cheng',\n",
       "   'Linpeng Yang'],\n",
       "  'related_topics': ['Odds ratio',\n",
       "   'Retrospective cohort study',\n",
       "   'Predictive value of tests',\n",
       "   'Multivariate analysis',\n",
       "   'Renal replacement therapy',\n",
       "   'Logistic regression',\n",
       "   'Extracorporeal membrane oxygenation',\n",
       "   'Medical record',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '4',\n",
       "  'reference_count': '23',\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3009885589',\n",
       "   '3004280078',\n",
       "   '3008818676',\n",
       "   '3007814559',\n",
       "   '3009976289',\n",
       "   '2131262274',\n",
       "   '3009859788']},\n",
       " {'id': '3037851904',\n",
       "  'title': 'Could urinary ACE2 protein level help identify individuals susceptible to SARS-CoV-2 infection and complication?',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Xiaotian Ni',\n",
       "   '',\n",
       "   'Changqing Sun',\n",
       "   'Yaping Tian',\n",
       "   'Yanjie Huang',\n",
       "   'Tongqing Gong',\n",
       "   'Lan Song',\n",
       "   'Xing Yang',\n",
       "   'Kai Li',\n",
       "   'Nairen Zheng',\n",
       "   'Jianping Wang',\n",
       "   'Hongxing Wu',\n",
       "   'Ruoxian Zhang',\n",
       "   'Yi Wang',\n",
       "   'Guangshun Wang',\n",
       "   'Jun Qin',\n",
       "   ''],\n",
       "  'related_topics': ['Complication',\n",
       "   'Virology',\n",
       "   'Urinary system',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Protein level',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3008827533',\n",
       "   '3004280078',\n",
       "   '3008818676',\n",
       "   '2601317354',\n",
       "   '2946740876']},\n",
       " {'id': '2102634410',\n",
       "  'title': 'Fleischner Society: Glossary of Terms for Thoracic Imaging',\n",
       "  'abstract': 'Members of the Fleischner Society compiled a glossary of terms for thoracic imaging that replaces previous glossaries published in 1984 and 1996 for thoracic radiography and computed tomography (CT), respectively. The need to update the previous versions came from the recognition that new words have emerged, others have become obsolete, and the meaning of some terms has changed. Brief descriptions of some diseases are included, and pictorial examples (chest radiographs and CT scans) are provided for the majority of terms.',\n",
       "  'date': '2008',\n",
       "  'authors': ['David M Hansell',\n",
       "   'Alexander A',\n",
       "   'Heber',\n",
       "   'Theresa C',\n",
       "   'Nestor L',\n",
       "   'Jacques'],\n",
       "  'related_topics': ['Glossary',\n",
       "   'Radiography',\n",
       "   'Thorax',\n",
       "   'Medical physics',\n",
       "   'Medicine',\n",
       "   'Computed tomography',\n",
       "   'Cystic lung disease',\n",
       "   'Thoracic Radiography',\n",
       "   'Thoracic imaging',\n",
       "   'X ray computed',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,981',\n",
       "  'reference_count': '134',\n",
       "  'references': ['2416914730',\n",
       "   '2626588662',\n",
       "   '2017898137',\n",
       "   '1924766221',\n",
       "   '2121350896',\n",
       "   '2041775285',\n",
       "   '2107051779',\n",
       "   '2157678780',\n",
       "   '2114857071',\n",
       "   '2155323443']},\n",
       " {'id': '2800783955',\n",
       "  'title': 'Radiographic and CT Features of Viral Pneumonia.',\n",
       "  'abstract': 'Viruses are the most common causes of respiratory infection. The imaging findings of viral pneumonia are diverse and overlap with those of other nonviral infectious and inflammatory conditions. However, identification of the underlying viral pathogens may not always be easy. There are a number of indicators for identifying viral pathogens on the basis of imaging patterns, which are associated with the pathogenesis of viral infections. Viruses in the same viral family share a similar pathogenesis of pneumonia, and the imaging patterns have distinguishable characteristics. Although not all cases manifest with typical patterns, most typical imaging patterns of viral pneumonia can be classified according to viral families. Although a definite diagnosis cannot be achieved on the basis of imaging features alone, recognition of viral pneumonia patterns may aid in differentiating viral pathogens, thus reducing the use of antibiotics. Recently, new viruses associated with recent outbreaks including human metapneumovirus, severe acute respiratory syndrome coronavirus, and Middle East respiratory syndrome coronavirus have been discovered. The imaging findings of these emerging pathogens have been described in a few recent studies. This review focuses on the radiographic and computed tomographic patterns of viral pneumonia caused by different pathogens, including new pathogens. Clinical characteristics that could affect imaging, such as patient age and immune status, seasonal variation and community outbreaks, and pathogenesis, are also discussed. The first goal of this review is to indicate that there are imaging features that should raise the possibility of viral infections. Second, to help radiologists differentiate viral infections, viruses in the same viridae that have similar pathogenesis and can have similar imaging characteristics are shown. By considering both the clinical and radiologic characteristics, radiologists can suggest the diagnosis of viral pneumonia. ©RSNA, 2018.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Hyun Jung Koo',\n",
       "   'Soyeoun Lim',\n",
       "   'Jooae Choe',\n",
       "   'Sang Ho Choi',\n",
       "   'Heungsup Sung',\n",
       "   'Kyung Hyun Do'],\n",
       "  'related_topics': ['Viral pneumonia',\n",
       "   'Pneumonia (non-human)',\n",
       "   'Respiratory infection',\n",
       "   'Human metapneumovirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Immunology',\n",
       "   'Pathogenesis',\n",
       "   'Outbreak',\n",
       "   'Antibiotics',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '349',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2112136274',\n",
       "  'title': 'Middle East Respiratory Syndrome Coronavirus (MERS-CoV) Infection: Chest CT Findings',\n",
       "  'abstract': 'OBJECTIVE. The purpose of this study was to describe the chest CT findings in seven patients with Middle East respiratory syndrome coronavirus (MERS-CoV) infection. CONCLUSION. The most common CT finding in hospitalized patients with MERS-CoV infection is that of bilateral predominantly subpleural and basilar airspace changes, with more extensive ground-glass opacities than consolidation. The subpleural and peribronchovascular predilection of the abnormalities is suggestive of an organizing pneumonia pattern.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Amr M. Ajlan', 'Rayan A.', 'Lamia Ghazi', 'Ahmed', 'Tariq A.'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Internal medicine',\n",
       "   'Pathology',\n",
       "   'Medicine',\n",
       "   'Chest ct',\n",
       "   'Ct findings',\n",
       "   'Hospitalized patients',\n",
       "   'Infection chest',\n",
       "   'Organizing pneumonia',\n",
       "   'View Less'],\n",
       "  'citation_count': '256',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2006434809',\n",
       "   '2149661971',\n",
       "   '1703839189',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2109520345',\n",
       "   '2119837294',\n",
       "   '2049975503']},\n",
       " {'id': '2056155046',\n",
       "  'title': 'Severe Acute Respiratory Syndrome: Temporal Lung Changes at Thin-Section CT in 30 Patients',\n",
       "  'abstract': 'PURPOSE: To evaluate lung abnormalities on serial thin-section computed tomographic (CT) scans in patients with severe acute respiratory syndrome (SARS) during acute and convalescent periods. MATERIALS AND METHODS: Serial thin-section CT scans in 30 patients (17 men, aged 42.5 years ± 12.2 [SD]) with SARS were reviewed by two radiologists together for predominant patterns of lung abnormalities: ground-glass opacities, ground-glass opacities with superimposed linear opacities, consolidation, reticular pattern, and mixed pattern (consolidation, ground-glass opacities, and reticular pattern). Scans were classified according to duration in weeks after symptom onset. Longitudinal changes of specific abnormalities were documented in 17 patients with serial scans obtained during 3 weeks. Each lung was divided into three zones; each zone was evaluated for percentage of lung involvement. Summation of scores from all six lung zones provided overall CT score (maximal CT score, 24). RESULTS: Median CT scores increase...',\n",
       "  'date': '2004',\n",
       "  'authors': ['Gaik C. Ooi',\n",
       "   'Pek L.',\n",
       "   'Nestor L.',\n",
       "   'Wai C.',\n",
       "   'Lin J.',\n",
       "   'James C. M.',\n",
       "   'Bing',\n",
       "   'Savvas',\n",
       "   'Kenneth W. T.'],\n",
       "  'related_topics': ['Zones of the lung',\n",
       "   'Respiratory disease',\n",
       "   'Lung',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Pulmonary fibrosis',\n",
       "   'Radiology',\n",
       "   'Respiratory system',\n",
       "   'Tomography',\n",
       "   'Surgery',\n",
       "   'Spontaneous remission',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '262',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2025170735',\n",
       "   '2131262274',\n",
       "   '2125251240',\n",
       "   '1971054351',\n",
       "   '1976741900',\n",
       "   '2151996610',\n",
       "   '2119467724',\n",
       "   '2099918622',\n",
       "   '2098293966',\n",
       "   '2097858003']},\n",
       " {'id': '2279340859',\n",
       "  'title': 'Middle East Respiratory Syndrome-Coronavirus Infection: A Case Report of Serial Computed Tomographic Findings in a Young Male Patient.',\n",
       "  'abstract': 'Radiologic findings of Middle East respiratory syndrome (MERS), a novel coronavirus infection, have been rarely reported. We report a 30-year-old male presented with fever, abdominal pain, and diarrhea, who was diagnosed with MERS. A chest computed tomographic scan revealed rapidly developed multifocal nodular consolidations with ground-glass opacity halo and mixed consolidation, mainly in the dependent and peripheral areas. After treatment, follow-up imaging showed that these abnormalities markedly decreased but fibrotic changes developed.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Won Jin Choi', 'Ki Nam Lee', 'Eun Ju Kang', 'Hyuck Lee'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Abdominal pain',\n",
       "   'Diarrhea',\n",
       "   'Radiology',\n",
       "   'Radiography',\n",
       "   'Pathology',\n",
       "   'Medicine',\n",
       "   'Computed tomographic',\n",
       "   'Young male',\n",
       "   'View Less'],\n",
       "  'citation_count': '25',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2166867592',\n",
       "   '2006434809',\n",
       "   '1703839189',\n",
       "   '2112136274',\n",
       "   '2119837294',\n",
       "   '2103118479',\n",
       "   '2111742711']},\n",
       " {'id': '2080286891',\n",
       "  'title': 'Severe acute respiratory syndrome: radiographic appearances and pattern of progression in 138 patients',\n",
       "  'abstract': 'PURPOSE: To retrospectively evaluate the radiographic appearances and pattern of progression of severe acute respiratory syndrome (SARS). MATERIALS AND METHODS: Chest radiographs obtained at clinical presentation and during treatment in 138 patients with confirmed SARS (66 men, 72 women; mean age, 39 years; age range, 20–83 years) were assessed. Radiographic appearances of pulmonary parenchymal abnormality, distribution, and extent of involvement on initial chest radiographs were documented. Recognizable patterns of radiographic progression were determined by comparing the overall mean percentage of lung involvement for each patient on serial radiographs. RESULTS: Initial chest radiographs were abnormal in 108 of 138 (78.3%) patients and showed air-space opacity. Lower lung zone (70 of 108, 64.8%) and right lung (82 of 108, 75.9%) were more commonly involved. In most patients, peripheral lung involvement was more common (81 of 108, 75.0%). Unifocal involvement (59 of 108, 54.6%) was more common than multi...',\n",
       "  'date': '2003',\n",
       "  'authors': ['K. T. Wong',\n",
       "   'Gregory E.',\n",
       "   'David S. C.',\n",
       "   'Nelson',\n",
       "   'Edmund H. Y.',\n",
       "   'Alan',\n",
       "   'C. B.',\n",
       "   'Timothy',\n",
       "   'Peter Cameron',\n",
       "   'Sydney S. C.',\n",
       "   'Joseph J. Y.',\n",
       "   'Anil T.'],\n",
       "  'related_topics': ['Respiratory disease',\n",
       "   'Lung',\n",
       "   'Pneumonia',\n",
       "   'Radiology',\n",
       "   'Retrospective cohort study',\n",
       "   'Radiography',\n",
       "   'Surgery',\n",
       "   'Abnormality',\n",
       "   'Respiratory system',\n",
       "   'Medicine',\n",
       "   'Mean age',\n",
       "   'View Less'],\n",
       "  'citation_count': '387',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2131262274',\n",
       "   '2151996610',\n",
       "   '2119467724',\n",
       "   '2129716802',\n",
       "   '2321891473']},\n",
       " {'id': '2162899218',\n",
       "  'title': 'Radiologic pattern of disease in patients with severe acute respiratory syndrome: the Toronto experience.',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) is a transmissible febrile respiratory illness caused by a recently discovered coronavirus. Various patterns of disease progression may be observed that have different implications for the prognosis in those affected by SARS. The appearance of the lungs on chest radiographs of patients with this condition may be normal or may include focal airspace opacity or multifocal or diffuse opacities. Thoracic computed tomography (CT) is more sensitive in depicting SARS than is conventional chest radiography, and CT images obtained in patients with normal chest radiographs may show extensive disease and airspace consolidation. However, because the radiologic appearance of SARS is not distinct from that of other diseases that cause lower respiratory tract infection, early identification of SARS will depend in part on the prompt recognition of clusters of cases of febrile respiratory tract illness. To aid in the differential diagnosis and management of SARS, radiologists must ...',\n",
       "  'date': '2004',\n",
       "  'authors': ['Narinder S. Paul',\n",
       "   'Heidi Roberts',\n",
       "   'Jagdish Butany',\n",
       "   'Tae Bong Chung',\n",
       "   'Wayne Gold',\n",
       "   '',\n",
       "   'Sangeeta Mehta',\n",
       "   'Eli Konen',\n",
       "   'Anuradha Rao',\n",
       "   'Yves Provost',\n",
       "   'Harry H. Hong',\n",
       "   'Leon Zelovitsky',\n",
       "   'Gordon L. Weisbrod'],\n",
       "  'related_topics': ['Lower respiratory tract infection',\n",
       "   'Differential diagnosis',\n",
       "   'Coronavirus',\n",
       "   'Respiratory tract',\n",
       "   'Disease',\n",
       "   'Respiratory system',\n",
       "   'Radiography',\n",
       "   'Radiology',\n",
       "   'Pathology',\n",
       "   'Medicine',\n",
       "   'In patient',\n",
       "   'View Less'],\n",
       "  'citation_count': '90',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2163627712',\n",
       "   '2080286891',\n",
       "   '2124413369',\n",
       "   '2155100049',\n",
       "   '2123101845',\n",
       "   '2049695691']},\n",
       " {'id': '3004802901',\n",
       "  'title': 'CT Manifestations of Two Cases of 2019 Novel Coronavirus (2019-nCoV) Pneumonia.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yicheng Fang',\n",
       "   'Huangqi Zhang',\n",
       "   'Yunyu Xu',\n",
       "   'Jicheng Xie',\n",
       "   'Peipei Pang',\n",
       "   'Wenbin Ji'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Medicine',\n",
       "   'Pathology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease progression',\n",
       "   'Lung pathology',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed',\n",
       "   'X ray computed',\n",
       "   'View Less'],\n",
       "  'citation_count': '189',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2092969802',\n",
       "  'title': 'A decade after SARS: strategies for controlling emerging coronaviruses',\n",
       "  'abstract': 'Two novel coronaviruses have emerged in humans in the twenty-first century: severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV), both of which cause acute respiratory distress syndrome (ARDS) and are associated with high mortality rates. There are no clinically approved vaccines or antiviral drugs available for either of these infections; thus, the development of effective therapeutic and preventive strategies that can be readily applied to new emergent strains is a research priority. In this Review, we describe the emergence and identification of novel human coronaviruses over the past 10 years, discuss their key biological features, including tropism and receptor use, and summarize approaches for developing broadly effective vaccines.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Rachel L. Graham', 'Eric F. Donaldson', 'Ralph S. Baric'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'ARDS',\n",
       "   'Tissue tropism',\n",
       "   'Viral Epidemiology',\n",
       "   'Tropism',\n",
       "   'Intensive care medicine',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'Acute respiratory distress',\n",
       "   'High mortality',\n",
       "   'Severe acute respiratory syndrome coronavirus',\n",
       "   'View Less'],\n",
       "  'citation_count': '582',\n",
       "  'reference_count': '153',\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '1993577573',\n",
       "   '2119111857',\n",
       "   '1966238900',\n",
       "   '2160011624']},\n",
       " {'id': '3005272159',\n",
       "  'title': 'Chest CT Findings in 2019 Novel Coronavirus (2019-nCoV) Infections from Wuhan, China: Key Points for the Radiologist.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Jeffrey P Kanne'],\n",
       "  'related_topics': ['Medicine',\n",
       "   'Radiology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease progression',\n",
       "   'Lung pathology',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed',\n",
       "   'View Less'],\n",
       "  'citation_count': '487',\n",
       "  'reference_count': '15',\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '2903899730',\n",
       "   '2306794997',\n",
       "   '2103118479',\n",
       "   '2111742711',\n",
       "   '2056155046',\n",
       "   '3135910874',\n",
       "   '3024264813',\n",
       "   '3009749892']},\n",
       " {'id': '3001465255',\n",
       "  'title': 'A novel coronavirus outbreak of global health concern.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Chen Wang',\n",
       "   'Peter W Horby',\n",
       "   'Frederick G Hayden',\n",
       "   'George F Gao'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Outbreak',\n",
       "   'Pneumonia',\n",
       "   'Global health',\n",
       "   'Medicine',\n",
       "   'Virology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,041',\n",
       "  'reference_count': '6',\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '2006434809',\n",
       "   '2141877163',\n",
       "   '2162112824',\n",
       "   '2156614913']},\n",
       " {'id': '3001456238',\n",
       "  'title': 'Emerging coronaviruses: Genome structure, replication, and pathogenesis.',\n",
       "  'abstract': 'The recent emergence of a novel coronavirus (2019-nCoV), which is causing an outbreak of unusual viral pneumonia in patients in Wuhan, a central city in China, is another warning of the risk of CoVs posed to public health. In this minireview, we provide a brief introduction of the general features of CoVs and describe diseases caused by different CoVs in humans and animals. This review will help understand the biology and potential risk of CoVs that exist in richness in wildlife such as bats.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yu Chen', 'Qianyun Liu', 'Deyin Guo'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Virus classification',\n",
       "   'Outbreak',\n",
       "   'Viral pneumonia',\n",
       "   'Viral replication',\n",
       "   'Genetics',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Genome structure',\n",
       "   'In patient',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,935',\n",
       "  'reference_count': '70',\n",
       "  'references': ['2903899730',\n",
       "   '2470646526',\n",
       "   '2306794997',\n",
       "   '1993577573',\n",
       "   '311927316',\n",
       "   '2046153984',\n",
       "   '2105637133',\n",
       "   '2794573360',\n",
       "   '2148822770',\n",
       "   '2017248106']},\n",
       " {'id': '3004668429',\n",
       "  'title': 'Emerging 2019 Novel Coronavirus (2019-nCoV) Pneumonia.',\n",
       "  'abstract': 'BackgroundThe chest CT findings of patients with 2019 Novel Coronavirus (2019-nCoV) pneumonia have not previously been described in detail.PurposeTo investigate the clinical, laboratory, and imaging findings of emerging 2019-nCoV pneumonia in humans.Materials and MethodsFifty-one patients (25 men and 26 women; age range 16-76 years) with laboratory-confirmed 2019-nCoV infection by using real-time reverse transcription polymerase chain reaction underwent thin-section CT. The imaging findings, clinical data, and laboratory data were evaluated.ResultsFifty of 51 patients (98%) had a history of contact with individuals from the endemic center in Wuhan, China. Fever (49 of 51, 96%) and cough (24 of 51, 47%) were the most common symptoms. Most patients had a normal white blood cell count (37 of 51, 73%), neutrophil count (44 of 51, 86%), and either normal (17 of 51, 35%) or reduced (33 of 51, 65%) lymphocyte count. CT images showed pure ground-glass opacity (GGO) in 39 of 51 (77%) patients and GGO with reticular and/or interlobular septal thickening in 38 of 51 (75%) patients. GGO with consolidation was present in 30 of 51 (59%) patients, and pure consolidation was present in 28 of 51 (55%) patients. Forty-four of 51 (86%) patients had bilateral lung involvement, while 41 of 51 (80%) involved the posterior part of the lungs and 44 of 51 (86%) were peripheral. There were more consolidated lung lesions in patients 5 days or more from disease onset to CT scan versus 4 days or fewer (431 of 712 lesions vs 129 of 612 lesions; P < .001). Patients older than 50 years had more consolidated lung lesions than did those aged 50 years or younger (212 of 470 vs 198 of 854; P < .001). Follow-up CT in 13 patients showed improvement in seven (54%) patients and progression in four (31%) patients.ConclusionPatients with fever and/or cough and with conspicuous ground-glass opacity lesions in the peripheral and posterior lungs on CT images, combined with normal or decreased white blood cells and a history of epidemic exposure, are highly suspected of having 2019 Novel Coronavirus (2019-nCoV) pneumonia.© RSNA, 2020.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Fengxiang',\n",
       "   'Nannan',\n",
       "   'Fei',\n",
       "   'Zhiyong',\n",
       "   'Jie',\n",
       "   'Hongzhou',\n",
       "   'Yun',\n",
       "   'Yebin',\n",
       "   'Yuxin'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Absolute neutrophil count',\n",
       "   'Lung',\n",
       "   'Gastroenterology',\n",
       "   'Retrospective cohort study',\n",
       "   'Lymphocyte',\n",
       "   'Young adult',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   'Internal medicine',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '902',\n",
       "  'reference_count': '10',\n",
       "  'references': ['3002108456',\n",
       "   '2999409984',\n",
       "   '1993577573',\n",
       "   '3003901880',\n",
       "   '2800783955',\n",
       "   '2128312233',\n",
       "   '2015323375',\n",
       "   '2789752210',\n",
       "   '1922522683',\n",
       "   '1997020575']},\n",
       " {'id': '2786098272',\n",
       "  'title': 'Serological Evidence of Bat SARS-Related Coronavirus Infection in Humans, China',\n",
       "  'abstract': 'In our previous works, we have reported genetically diverse SARS-related coronaviruses (SARSr-CoV) in a single bat cave, Yunnan province, China, and suggested that some SARSr-CoVs may have high potential to infect humans without the necessity for an intermediate host. In this report, we developed a specific ELISA based on the nucleocapsid protein of a SARSr-CoV strain and detected its antibody in humans who are highly exposed to bat populations. From 218 human serum samples, 6 were positive against the nucleocapsid protein by ELISA and further confirmed by Western blot. For the first time, we demonstrated the SARSr-CoV had spillover to humans, although did not cause clinical diseases.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Ning Wang',\n",
       "   'Shi-Yue Li',\n",
       "   'Xing-Lou Yang',\n",
       "   'Hui-Min Huang',\n",
       "   'Yu-Ji Zhang',\n",
       "   'Hua Guo',\n",
       "   'Chu-Ming Luo',\n",
       "   'Maureen Miller',\n",
       "   'Guangjian Zhu',\n",
       "   'Aleksei A. Chmura',\n",
       "   'Emily Hagan',\n",
       "   'Ji-Hua Zhou',\n",
       "   'Yun-Zhi Zhang',\n",
       "   'Lin-Fa Wang',\n",
       "   'Peter Daszak',\n",
       "   'Zheng-Li Shi'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Serology',\n",
       "   'Intermediate host',\n",
       "   'Antibody',\n",
       "   'Virology',\n",
       "   'Western blot',\n",
       "   'Medical microbiology',\n",
       "   'Strain (biology)',\n",
       "   'Biology',\n",
       "   'Severe acute respiratory syndrome-related coronavirus',\n",
       "   'View Less'],\n",
       "  'citation_count': '165',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '1993577573',\n",
       "   '2103503670',\n",
       "   '2298153446',\n",
       "   '2134061616',\n",
       "   '2126707939',\n",
       "   '2285897784',\n",
       "   '2200668708',\n",
       "   '2151461700']},\n",
       " {'id': '2021442163',\n",
       "  'title': 'Organ distribution of severe acute respiratory syndrome (SARS) associated coronavirus (SARS-CoV) in SARS patients: implications for pathogenesis and virus transmission pathways.',\n",
       "  'abstract': 'We previously identified the major pathological changes in the respiratory and immune systems of patients who died of severe acute respiratory syndrome (SARS) but gained little information on the organ distribution of SARS-associated coronavirus (SARS-CoV). In the present study, we used a murine monoclonal antibody specific for SARS-CoV nucleoprotein, and probes specific for a SARS-CoV RNA polymerase gene fragment, for immunohistochemistry and in situ hybridization, respectively, to detect SARS-CoV systematically in tissues from patients who died of SARS. SARS-CoV was found in lung, trachea/bronchus, stomach, small intestine, distal convoluted renal tubule, sweat gland, parathyroid, pituitary, pancreas, adrenal gland, liver and cerebrum, but was not detected in oesophagus, spleen, lymph node, bone marrow, heart, aorta, cerebellum, thyroid, testis, ovary, uterus or muscle. These results suggest that, in addition to the respiratory system, the gastrointestinal tract and other organs with detectable SARS-CoV may also be targets of SARS-CoV infection. The pathological changes in these organs may be caused directly by the cytopathic effect mediated by local replication of the SARS-CoV; or indirectly as a result of systemic responses to respiratory failure or the harmful immune response induced by viral infection. In addition to viral spread through a respiratory route, SARS-CoV in the intestinal tract, kidney and sweat glands may be excreted via faeces, urine and sweat, thereby leading to virus transmission. This study provides important information for understanding the pathogenesis of SARS-CoV infection and sheds light on possible virus transmission pathways. This data will be useful for designing new strategies for prevention and treatment of SARS.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Yanqing Ding',\n",
       "   'Li He',\n",
       "   'Qingling Zhang',\n",
       "   'Zhongxi Huang',\n",
       "   'Xiaoyan Che',\n",
       "   'Jinlin Hou',\n",
       "   'Huijun Wang',\n",
       "   'Hong Shen',\n",
       "   'Liwen Qiu',\n",
       "   'Zhuguo Li',\n",
       "   'Jian Geng',\n",
       "   'Junjie Cai',\n",
       "   'Huixia Han',\n",
       "   'Xin Li',\n",
       "   'Wei Kang',\n",
       "   'Desheng Weng',\n",
       "   'Ping Liang',\n",
       "   'Shibo Jiang'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Respiratory disease',\n",
       "   'Lung',\n",
       "   'Pathogenesis',\n",
       "   'Sweat gland',\n",
       "   'Coronaviridae',\n",
       "   'Respiratory system',\n",
       "   'Pathology',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '823',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2116586125',\n",
       "   '1966238900',\n",
       "   '2169198329',\n",
       "   '2134061616',\n",
       "   '2168446943',\n",
       "   '1757215199',\n",
       "   '2158118659']},\n",
       " {'id': '3025232310',\n",
       "  'title': 'Humoral Immune Responses in COVID-19 Patients: A Window on the State of the Art.',\n",
       "  'abstract': 'The novel SARS-CoV-2 is a recently emerging virus causing a human pandemic. A great variety of symptoms associated with COVID-19 disease, ranging from mild to severe symptoms, eventually leading to death. Specific SARS-CoV-2 RT-PCR is the standard method to screen symptomatic people; however, asymptomatic subjects and subjects with undetectable viral load escape from the screening, contributing to viral spread. Currently, the lock down imposed by many governments is an important measure to contain the spread, as there is no specific antiviral therapy or a vaccine and the main treatments are supportive. Therefore, there is urgent need to characterize the virus and the viral-mediated responses, in order to develop specific diagnostic and therapeutic tools to prevent viral transmission and efficiently cure COVID-19 patients. Here, we review the current studies on two viral mediated-responses, specifically the cytokine storm occurring in a subset of patients and the antibody response triggered by the infection. Further studies are needed to explore both the dynamics and the mechanisms of the humoral immune response in COVID-19 patients, in order to guide future vaccine design and antibody-based therapies for the management of the disease.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Gabriel Siracusano', 'Claudia Pastori', 'Lucia Lopalco'],\n",
       "  'related_topics': ['Viral load',\n",
       "   'Disease',\n",
       "   'Cytokine storm',\n",
       "   'Virus',\n",
       "   'Immune system',\n",
       "   'Antibody',\n",
       "   'Pandemic',\n",
       "   'Asymptomatic',\n",
       "   'Immunology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '67',\n",
       "  'reference_count': '75',\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3004280078',\n",
       "   '3001465255',\n",
       "   '3003217347',\n",
       "   '3007643904',\n",
       "   '3005679569',\n",
       "   '3012756997',\n",
       "   '3009314935',\n",
       "   '3013985547']},\n",
       " {'id': '3028321619',\n",
       "  'title': 'SARS-CoV-2 Infection and the Newborn',\n",
       "  'abstract': 'Severe Acute Respiratory Syndrome Coronavirus Type 2 (SARS-CoV-2) affects people at all ages and it may be encountered in pregnant women and newborns also. The information about its clinical features, laboratory findings and prognosis in children and newborns is scarce. All the reported cases in pregnant women were in the 2nd or 3rd trimester and only 1% of them developed severe disease. Miscarriages are rare. Materno-fetal transmission of the disease is controversial. Definitive diagnosis can be made by a history of contact with a proven case, fever, pneumonia and gastrointestinal disorder and a Polymerase chain reaction (PCR) test of nasopharyngeal swabs. Lymphopenia as well as liver and renal dysfunctions may be seen. Suspected or proven cases of newborns with symptoms should be quarantined in the neonatal intensive care unit for at least 14 days with standart and droplet isolation precautions. Asymptomatic infants may be quaratined at home. Transport of the neonates should be performed in a dedicated transport incubator and ambulance with isolation precautions. There is no specific treatment for the disease, but hemodynamic stabilization of the infant, respiratory management and other daily care are essential. Drugs against cytokine storm syndrome such as corticosteroids or tocilizumab are under investigation. Routine antibiotics are not recommended. No deaths have been reported so far in the neonatal population. Families and healthcare staff should receive pyschological support. Since the infection is quite new and knowledge is constantly accumulating, following developments and continuous updates are crucial.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Fahri Ovalı'],\n",
       "  'related_topics': ['Neonatal intensive care unit',\n",
       "   'Gastrointestinal disorder',\n",
       "   'Population',\n",
       "   'Pneumonia',\n",
       "   'Asymptomatic',\n",
       "   'Disease',\n",
       "   'Pregnancy',\n",
       "   'Transmission (medicine)',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '15',\n",
       "  'reference_count': '83',\n",
       "  'references': ['3001118548',\n",
       "   '3008028633',\n",
       "   '3002539152',\n",
       "   '3004318991',\n",
       "   '3007940623',\n",
       "   '3010233963',\n",
       "   '3010604545',\n",
       "   '3005212621',\n",
       "   '3011610993',\n",
       "   '3005679569']},\n",
       " {'id': '3027541845',\n",
       "  'title': 'Psycho-Neuroendocrine-Immune Interactions in COVID-19: Potential Impacts on Mental Health.',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). The impacts of the disease may be beyond the respiratory system, also affecting mental health. Several factors may be involved in the association between COVID-19 and psychiatric outcomes, such as fear inherent in the pandemic, adverse effects of treatments, as well as financial stress, and social isolation. Herein we discuss the growing evidence suggesting that the relationship between SARS-CoV-2 and host may also trigger changes in brain and behavior. Based on the similarity of SARS-CoV-2 with other coronaviruses, it is conceivable that changes in endocrine and immune response in the periphery or in the central nervous system may be involved in the association between SARS-CoV-2 infection and impaired mental health. This is likely to be further enhanced, since millions of people worldwide are isolated in quarantine to minimize the transmission of SARS-CoV-2 and social isolation can also lead to neuroendocrine-immune changes. Accordingly, we highlight here the hypothesis that neuroendocrine-immune interactions may be involved in negative impacts of SARS-CoV-2 infection and social isolation on psychiatric issues.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ícaro Raony',\n",
       "   'Camila Saggioro de Figueiredo',\n",
       "   'Pablo Pandolfo',\n",
       "   'Elizabeth Giestal-de-Araujo',\n",
       "   '',\n",
       "   'Priscilla Oliveira-Silva Bomfim',\n",
       "   '',\n",
       "   '',\n",
       "   'Wilson Savino',\n",
       "   ''],\n",
       "  'related_topics': ['Disease',\n",
       "   'Social isolation',\n",
       "   'Mental health',\n",
       "   'Transmission (medicine)',\n",
       "   'Pandemic',\n",
       "   'Immune system',\n",
       "   'Adverse effect',\n",
       "   'Psychiatry',\n",
       "   'Quarantine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '25',\n",
       "  'reference_count': '153',\n",
       "  'references': ['3001118548',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3006659024',\n",
       "   '2903899730',\n",
       "   '3012421327',\n",
       "   '3009506062',\n",
       "   '3006645647',\n",
       "   '3004348779',\n",
       "   '3015197879']},\n",
       " {'id': '3003637715',\n",
       "  'title': 'Molecular Diagnosis of a Novel Coronavirus (2019-nCoV) Causing an Outbreak of Pneumonia.',\n",
       "  'abstract': 'BACKGROUND: A novel coronavirus of zoonotic origin (2019-nCoV) has recently been identified in patients with acute respiratory disease. This virus is genetically similar to SARS coronavirus and bat SARS-like coronaviruses. The outbreak was initially detected in Wuhan, a major city of China, but has subsequently been detected in other provinces of China. Travel-associated cases have also been reported in a few other countries. Outbreaks in health care workers indicate human-to-human transmission. Molecular tests for rapid detection of this virus are urgently needed for early identification of infected patients. METHODS: We developed two 1-step quantitative real-time reverse-transcription PCR assays to detect two different regions (ORF1b and N) of the viral genome. The primer and probe sets were designed to react with this novel coronavirus and its closely related viruses, such as SARS coronavirus. These assays were evaluated using a panel of positive and negative controls. In addition, respiratory specimens from two 2019-nCoV-infected patients were tested. RESULTS: Using RNA extracted from cells infected by SARS coronavirus as a positive control, these assays were shown to have a dynamic range of at least seven orders of magnitude (2x10-4-2000 TCID50/reaction). Using DNA plasmids as positive standards, the detection limits of these assays were found to be below 10 copies per reaction. All negative control samples were negative in the assays. Samples from two 2019-nCoV-infected patients were positive in the tests. CONCLUSIONS: The established assays can achieve a rapid detection of 2019n-CoV in human samples, thereby allowing early identification of patients.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Daniel K W Chu',\n",
       "   'Yang Pan',\n",
       "   '',\n",
       "   'Samuel M S Cheng',\n",
       "   'Kenrie P Y Hui',\n",
       "   'Pavithra Krishnan',\n",
       "   'Yingzhi Liu',\n",
       "   'Daisy Y M Ng',\n",
       "   'Carrie K C Wan',\n",
       "   'Peng Yang',\n",
       "   '',\n",
       "   'Quanyi Wang',\n",
       "   'Malik Peiris',\n",
       "   'Leo L M Poon'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Virus',\n",
       "   'Outbreak',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Plasmid',\n",
       "   'Pneumonia',\n",
       "   'Viral Epidemiology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '704',\n",
       "  'reference_count': '14',\n",
       "  'references': []},\n",
       " {'id': '3009885589',\n",
       "  'title': 'Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study.',\n",
       "  'abstract': 'Summary  Background  Since December, 2019, Wuhan, China, has experienced an outbreak of coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Epidemiological and clinical characteristics of patients with COVID-19 have been reported but risk factors for mortality and a detailed clinical course of illness, including viral shedding, have not been well described.  Methods  In this retrospective, multicentre cohort study, we included all adult inpatients (≥18 years old) with laboratory-confirmed COVID-19 from Jinyintan Hospital and Wuhan Pulmonary Hospital (Wuhan, China) who had been discharged or had died by Jan 31, 2020. Demographic, clinical, treatment, and laboratory data, including serial samples for viral RNA detection, were extracted from electronic medical records and compared between survivors and non-survivors. We used univariable and multivariable logistic regression methods to explore the risk factors associated with in-hospital death.  Findings  191 patients (135 from Jinyintan Hospital and 56 from Wuhan Pulmonary Hospital) were included in this study, of whom 137 were discharged and 54 died in hospital. 91 (48%) patients had a comorbidity, with hypertension being the most common (58 [30%] patients), followed by diabetes (36 [19%] patients) and coronary heart disease (15 [8%] patients). Multivariable regression showed increasing odds of in-hospital death associated with older age (odds ratio 1·10, 95% CI 1·03–1·17, per year increase; p=0·0043), higher Sequential Organ Failure Assessment (SOFA) score (5·65, 2·61–12·23; p  Interpretation  The potential risk factors of older age, high SOFA score, and d-dimer greater than 1 μg/mL could help clinicians to identify patients with poor prognosis at an early stage. Prolonged viral shedding provides the rationale for a strategy of isolation of infected patients and optimal antiviral interventions in the future.  Funding  Chinese Academy of Medical Sciences Innovation Fund for Medical Sciences; National Science Grant for Distinguished Young Scholars; National Key Research and Development Program of China; The Beijing Science and Technology Project; and Major Projects of National Science and Technology on New Drug Creation and Development.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Fei Zhou',\n",
       "   'Ting Yu',\n",
       "   'Ronghui Du',\n",
       "   'Guohui Fan',\n",
       "   'Ying Liu',\n",
       "   'Zhibo Liu',\n",
       "   'Jie Xiang',\n",
       "   'Yeming Wang',\n",
       "   'Bin Song',\n",
       "   'Xiaoying Gu',\n",
       "   '',\n",
       "   'Lulu Guan',\n",
       "   'Yuan Wei',\n",
       "   'Hui Li',\n",
       "   'Xudong Wu',\n",
       "   'Jiuyang Xu',\n",
       "   'Shengjin Tu',\n",
       "   'Yi Zhang',\n",
       "   'Hua Chen',\n",
       "   'Bin'],\n",
       "  'related_topics': ['Cohort study',\n",
       "   'Retrospective cohort study',\n",
       "   'Odds ratio',\n",
       "   'SOFA score',\n",
       "   'Epidemiology',\n",
       "   'Comorbidity',\n",
       "   'Young adult',\n",
       "   'Risk assessment',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,122',\n",
       "  'reference_count': '40',\n",
       "  'references': []},\n",
       " {'id': '3013893137',\n",
       "  'title': 'Virological assessment of hospitalized patients with COVID-2019.',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is an acute infection of the respiratory tract that emerged in late 20191,2. Initial outbreaks in China involved 13.8% of cases with severe courses, and 6.1% of cases with critical courses3. This severe presentation may result from the virus using a virus receptor that is expressed predominantly in the lung2,4; the same receptor tropism is thought to have determined the pathogenicity-but also aided in the control-of severe acute respiratory syndrome (SARS) in 20035. However, there are reports of cases of COVID-19 in which the patient shows mild upper respiratory tract symptoms, which suggests the potential for pre- or oligosymptomatic transmission6-8. There is an urgent need for information on virus replication, immunity and infectivity in specific sites of the body. Here we report a detailed virological analysis of nine cases of COVID-19 that provides proof of active virus replication in tissues of the upper respiratory tract. Pharyngeal virus shedding was very high during the first week of symptoms, with a peak at 7.11 × 108 RNA copies per throat swab on day 4. Infectious virus was readily isolated from samples derived from the throat or lung, but not from stool samples-in spite of high concentrations of virus RNA. Blood and urine samples never yielded virus. Active replication in the throat was confirmed by the presence of viral replicative RNA intermediates in the throat samples. We consistently detected sequence-distinct virus populations in throat and lung samples from one patient, proving independent replication. The shedding of viral RNA from sputum outlasted the end of symptoms. Seroconversion occurred after 7 days in 50% of patients (and by day 14 in all patients), but was not followed by a rapid decline in viral load. COVID-19 can present as a mild illness of the upper respiratory tract. The confirmation of active virus replication in the upper respiratory tract has implications for the containment of COVID-19.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Roman Wölfel',\n",
       "   'Victor M. Corman',\n",
       "   'Wolfgang Guggemos',\n",
       "   'Michael Seilmaier',\n",
       "   'Sabine Zange',\n",
       "   'Marcel A. Müller',\n",
       "   'Daniela Niemeyer',\n",
       "   'Terry C. Jones',\n",
       "   '',\n",
       "   'Patrick Vollmar',\n",
       "   'Camilla Rothe',\n",
       "   'Michael Hoelscher',\n",
       "   'Tobias Bleicker',\n",
       "   'Sebastian Brünink',\n",
       "   'Julia Schneider',\n",
       "   'Rosina Ehmann',\n",
       "   'Katrin Zwirglmaier',\n",
       "   'Christian Drosten',\n",
       "   'Clemens Wendtner'],\n",
       "  'related_topics': ['Virus receptor',\n",
       "   'Viral shedding',\n",
       "   'Viral load',\n",
       "   'Virus',\n",
       "   'Viral replication',\n",
       "   'Sputum',\n",
       "   'Respiratory tract',\n",
       "   'Seroconversion',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,832',\n",
       "  'reference_count': '34',\n",
       "  'references': ['3001897055',\n",
       "   '3002108456',\n",
       "   '3001195213',\n",
       "   '3003465021',\n",
       "   '3006961006',\n",
       "   '3004239190',\n",
       "   '3009912996',\n",
       "   '3009906937',\n",
       "   '3010338568',\n",
       "   '2132260239']},\n",
       " {'id': '3004824173',\n",
       "  'title': 'A rapid advice guideline for the diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infected pneumonia (standard version)',\n",
       "  'abstract': 'In December 2019, a new type viral pneumonia cases occurred in Wuhan, Hubei Province; and then named “2019 novel coronavirus (2019-nCoV)” by the World Health Organization (WHO) on 12 January 2020. For it is a never been experienced respiratory disease before and with infection ability widely and quickly, it attracted the world’s attention but without treatment and control manual. For the request from frontline clinicians and public health professionals of 2019-nCoV infected pneumonia management, an evidence-based guideline urgently needs to be developed. Therefore, we drafted this guideline according to the rapid advice guidelines methodology and general rules of WHO guideline development; we also added the first-hand management data of Zhongnan Hospital of Wuhan University. This guideline includes the guideline methodology, epidemiological characteristics, disease screening and population prevention, diagnosis, treatment and control (including traditional Chinese Medicine), nosocomial infection prevention and control, and disease nursing of the 2019-nCoV. Moreover, we also provide a whole process of a successful treatment case of the severe 2019-nCoV infected pneumonia and experience and lessons of hospital rescue for 2019-nCoV infections. This rapid advice guideline is suitable for the first frontline doctors and nurses, managers of hospitals and healthcare sections, community residents, public health persons, relevant researchers, and all person who are interested in the 2019-nCoV.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ying-Hui Jin',\n",
       "   'Lin Cai',\n",
       "   'Zhen-Shun Cheng',\n",
       "   'Hong Cheng',\n",
       "   'Tong Deng',\n",
       "   '',\n",
       "   'Yi-Pin Fan',\n",
       "   'Cheng Fang',\n",
       "   'Di Huang',\n",
       "   'Lu-Qi Huang',\n",
       "   'Qiao Huang',\n",
       "   'Yong Han',\n",
       "   'Bo Hu',\n",
       "   'Fen Hu',\n",
       "   'Bing-Hui Li',\n",
       "   '',\n",
       "   'Yi-Rong Li',\n",
       "   'Ke Liang',\n",
       "   'Li-Kai Lin',\n",
       "   'Li-Sha Luo',\n",
       "   'Jing Ma',\n",
       "   'Lin-Lu Ma',\n",
       "   'Zhi-Yong Peng',\n",
       "   'Yun-Bao Pan',\n",
       "   'Zhen-Yu Pan',\n",
       "   'Xue-Qun Ren',\n",
       "   'Hui-Min Sun',\n",
       "   'Ying Wang',\n",
       "   'Yun-Yun Wang',\n",
       "   'Hong Weng',\n",
       "   'Chao-Jie Wei',\n",
       "   'Dong-Fang Wu',\n",
       "   'Jian Xia',\n",
       "   'Yong Xiong',\n",
       "   'Hai-Bo Xu',\n",
       "   'Xiao-Mei Yao',\n",
       "   'Yu-Feng Yuan',\n",
       "   'Tai-Sheng Ye',\n",
       "   'Xiao-Chun Zhang',\n",
       "   'Ying-Wen Zhang',\n",
       "   'Yin-Gao Zhang',\n",
       "   'Hua-Min Zhang',\n",
       "   'Yan Zhao',\n",
       "   'Ming-Juan Zhao',\n",
       "   'Hao Zi',\n",
       "   '',\n",
       "   'Xian-Tao Zeng',\n",
       "   'Yong-Yan Wang',\n",
       "   'Xing-Huan Wang'],\n",
       "  'related_topics': ['Guideline',\n",
       "   'Mass screening',\n",
       "   'Health care',\n",
       "   'Public health',\n",
       "   'Population',\n",
       "   'Nursing care',\n",
       "   'Infection control',\n",
       "   'Pneumonia',\n",
       "   'Medical emergency',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,408',\n",
       "  'reference_count': '21',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '2165010366',\n",
       "   '1803784511',\n",
       "   '3002715510',\n",
       "   '2034462612',\n",
       "   '2150120685',\n",
       "   '1945961678']},\n",
       " {'id': '3003464757',\n",
       "  'title': 'Genomic characterization of the 2019 novel human-pathogenic coronavirus isolated from a patient with atypical pneumonia after visiting Wuhan.',\n",
       "  'abstract': \"A mysterious outbreak of atypical pneumonia in late 2019 was traced to a seafood wholesale market in Wuhan of China. Within a few weeks, a novel coronavirus tentatively named as 2019 novel coronavirus (2019-nCoV) was announced by the World Health Organization. We performed bioinformatics analysis on a virus genome from a patient with 2019-nCoV infection and compared it with other related coronavirus genomes. Overall, the genome of 2019-nCoV has 89% nucleotide identity with bat SARS-like-CoVZXC21 and 82% with that of human SARS-CoV. The phylogenetic trees of their orf1a/b, Spike, Envelope, Membrane and Nucleoprotein also clustered closely with those of the bat, civet and human SARS coronaviruses. However, the external subdomain of Spike's receptor binding domain of 2019-nCoV shares only 40% amino acid identity with other SARS-related coronaviruses. Remarkably, its orf3b encodes a completely novel short protein. Furthermore, its new orf8 likely encodes a secreted protein with an alpha-helix, following with a beta-sheet(s) containing six strands. Learning from the roles of civet in SARS and camel in MERS, hunting for the animal source of 2019-nCoV and its more ancestral virus would be important for understanding the origin and evolution of this novel lineage B betacoronavirus. These findings provide the basis for starting further studies on the pathogenesis, and optimizing the design of diagnostic, antiviral and vaccination strategies for this emerging infection.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Jasper Fuk Woo Chan',\n",
       "   'Kin Hang Kok',\n",
       "   '',\n",
       "   'Zheng Zhu',\n",
       "   'Hin Chu',\n",
       "   '',\n",
       "   'Kelvin Kai Wang To',\n",
       "   'Shuofeng Yuan',\n",
       "   '',\n",
       "   'Kwok Yung Yuen'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Civet',\n",
       "   'Genome',\n",
       "   'Virus',\n",
       "   'Atypical pneumonia',\n",
       "   'Sequence analysis',\n",
       "   'Phylogenetics',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,820',\n",
       "  'reference_count': '24',\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '2799524357',\n",
       "   '2097706568',\n",
       "   '2025170735',\n",
       "   '2030966943',\n",
       "   '2115555188',\n",
       "   '2170933940',\n",
       "   '2162496804',\n",
       "   '2255897570']},\n",
       " {'id': '3009834387',\n",
       "  'title': 'Evidence for gastrointestinal infection of SARS-CoV-2',\n",
       "  'abstract': 'No abstract available\\r\\nKeywords: ACE2; Gastrointestinal Infection; Oral-Fecal Transmission; SARS-CoV-2.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Fei Xiao',\n",
       "   'Meiwen Tang',\n",
       "   'Xiaobin Zheng',\n",
       "   'Ye Liu',\n",
       "   'Xiaofeng Li',\n",
       "   'Hong Shan'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Pneumonia',\n",
       "   'Transmission (medicine)',\n",
       "   'Pandemic',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'Coronavirus Infections',\n",
       "   'Sars virus',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,536',\n",
       "  'reference_count': '13',\n",
       "  'references': ['3001118548',\n",
       "   '3003668884',\n",
       "   '3004280078',\n",
       "   '3003465021',\n",
       "   '3010441732',\n",
       "   '3005272159',\n",
       "   '3031532178',\n",
       "   '2131988685',\n",
       "   '1974901207',\n",
       "   '3034277126']},\n",
       " {'id': '3011863580',\n",
       "  'title': 'Prolonged presence of SARS-CoV-2 viral RNA in faecal samples.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yongjian Wu',\n",
       "   'Cheng Guo',\n",
       "   'Lantian Tang',\n",
       "   'Zhongsi Hong',\n",
       "   'Jianhui Zhou',\n",
       "   'Xin Dong',\n",
       "   'Huan Yin',\n",
       "   'Qiang Xiao',\n",
       "   'Yanping Tang',\n",
       "   'Xiujuan Qu',\n",
       "   'Liangjian Kuang',\n",
       "   'Xiaomin Fang',\n",
       "   'Nischay Mishra',\n",
       "   'Jiahai Lu',\n",
       "   'Hong Shan',\n",
       "   'Guanmin Jiang',\n",
       "   'Xi Huang'],\n",
       "  'related_topics': ['Viral shedding',\n",
       "   'Viral Epidemiology',\n",
       "   'RNA',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral genetics',\n",
       "   'Viral rna',\n",
       "   'View Less'],\n",
       "  'citation_count': '863',\n",
       "  'reference_count': '4',\n",
       "  'references': ['3008696669', '3002533507', '3006846061', '3008352032']},\n",
       " {'id': '3010096538',\n",
       "  'title': 'Features, Evaluation and Treatment Coronavirus (COVID-19)',\n",
       "  'abstract': 'According to the World Health Organization (WHO), viral diseases continue to emerge and represent a serious issue to public health. In the last twenty years, several viral epidemics such as the severe acute respiratory syndrome coronavirus (SARS-CoV) from 2002 to 2003, and H1N1 influenza in 2009, have been recorded. Most recently, the Middle East respiratory syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia in 2012.In a timeline that reaches the present day, an epidemic of cases with unexplained low respiratory infections detected in Wuhan, the largest metropolitan area in China\\'s Hubei province, was first reported to the WHO Country Office in China, on December 31, 2019. Published literature can trace the beginning of symptomatic individuals back to the beginning of December 2019. As they were unable to identify the causative agent, these first cases (n=29) were classified as \"pneumonia of unknown etiology.\" The Chinese Center for Disease Control and Prevention (CDC) and local CDCs organized an intensive outbreak investigation program. The etiology of this illness was attributed to a novel virus belonging to the coronavirus (CoV) family.On February 11, 2020, the WHO Director-General, Dr. Tedros Adhanom Ghebreyesus, announced that the disease caused by this new CoV was a \"COVID-19,\" which is the acronym of \"coronavirus disease 2019\". In the past twenty years, two additional CoVs epidemics have occurred. SARS-CoV provoked a large-scale epidemic beginning in China and involving two dozen countries with approximately 8000 cases and 800 deaths (fatality rate of 9,6%), and the MERS-CoV that began in Saudi Arabia and has approximately 2,500 cases and 800 deaths (fatality rate of 35%) and still causes as sporadic cases.This new virus is very contagious and has quickly spread globally. In a meeting on January 30, 2020, per the International Health Regulations (IHR, 2005), the outbreak was declared by the WHO a Public Health Emergency of International Concern (PHEIC) as it had spread to 18 countries with four countries reporting human-to-human transmission. An additional landmark occurred on February 26, 2020, as the first case of the disease, not imported from China, was recorded in the United States (US). Initially, the new virus was called 2019-nCoV. Subsequently, the task of experts of the International Committee on Taxonomy of Viruses (ICTV) termed it the SARS-CoV-2 virus as it is very similar to the one that caused the SARS outbreak (SARS-CoVs). The CoVs have become the major pathogens of emerging respiratory disease outbreaks. They are a large family of single-stranded RNA viruses (+ssRNA) that can be isolated in different animal species. For reasons yet to be explained, these viruses can cross species barriers and can cause, in humans, illness ranging from the common cold to more severe diseases such as MERS and SARS. Interestingly, these latter viruses have probably originated from bats and then moving into other mammalian hosts — the Himalayan palm civet for SARS-CoV, and the dromedary camel for MERS-CoV — before jumping to humans. The dynamics of SARS-Cov-2 are currently unknown, but there is speculation that it also has an animal origin.The potential for these viruses to grow to become a pandemic worldwide represents a serious public health risk. Concerning COVID-19, the WHO raised the threat to the CoV epidemic to the \"very high\" level, on February 28, 2020. On March 11, as the number of COVID-19 cases outside China has increased 13 times and the number of countries involved has tripled with more than 118,000 cases in 114 countries and over 4,000 deaths, WHO declared the COVID-19 a pandemic.World governments are at work to establish countermeasures to stem the devastating effects and it has been estimated that strict shutdowns may have saved 3 million lives across 11 European countries. Health organizations coordinate information flows and issues directives and guidelines to best mitigate the impact of the threat. At the same time, scientists around the world work tirelessly, and information about the transmission mechanisms, the clinical spectrum of disease, new diagnostics, and prevention and therapeutic strategies are rapidly developing. Many uncertainties remain with regard to both the virus-host interaction and the evolution of the pandemic, with specific reference to the times when it will reach its peak.At the moment, the therapeutic strategies to deal with the infection are only supportive, and prevention aimed at reducing transmission in the community is our best weapon. Aggressive isolation measures in China have led to a progressive reduction of cases. From China, the disease spread to Europe. In Italy, in geographic regions of the north, initially, and subsequently throughout the peninsula, political and health authorities have made incredible efforts to contain a shock wave that has severely tested the health system. Afterward, the COVID-19 quickly crossed the ocean and as of June 20, 2020, about 2,282,000 cases (with 121,000 deaths) have been recorded in the US, whereas Brazil with more than 1,000,000 cases and about 50,000 deaths is the most affected state in South America and the second in the world after the US. Although over time the lethality rate (total number of deaths for a given disease in relation to the total number of patients) of COVID-19 has been significantly lower than that of the SARS and MERS epidemics, the transmission of the SARS-CoV-2 virus is much larger than that of the previous viruses, with a much higher total number of deaths. It has been estimated that about one in five individuals worldwide could be at increased risk of severe COVID-19 disease if they become infected, due to underlying health conditions.In the midst of the crisis, the authors have chosen to use the \"Statpearls\" platform because, within the PubMed scenario, it represents a unique tool that may allow them to make updates in real-time. The aim, therefore, is to collect information and scientific evidence and to provide an overview of the topic that will be continuously updated.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Marco Cascella',\n",
       "   'Michael Rajnik',\n",
       "   'Arturo Cuomo',\n",
       "   'Scott C.',\n",
       "   'Raffaela Di Napoli'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Outbreak',\n",
       "   'Case fatality rate',\n",
       "   'Pandemic',\n",
       "   'Public health',\n",
       "   'International Health Regulations',\n",
       "   'Disease',\n",
       "   'Novel virus',\n",
       "   'Environmental health',\n",
       "   'Geography',\n",
       "   'View Less'],\n",
       "  'citation_count': '837',\n",
       "  'reference_count': '54',\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3009912996',\n",
       "   '2280404143',\n",
       "   '3010233963',\n",
       "   '3010930696',\n",
       "   '3012379316']},\n",
       " {'id': '3006846061',\n",
       "  'title': 'Enteric involvement of coronaviruses: is faecal-oral transmission of SARS-CoV-2 possible?',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Charleen Yeo', 'Sanghvi Kaushal', 'Danson Yeo'],\n",
       "  'related_topics': ['Viral shedding',\n",
       "   'Pneumonia',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral transmission',\n",
       "   'View Less'],\n",
       "  'citation_count': '593',\n",
       "  'reference_count': '14',\n",
       "  'references': ['3005079553',\n",
       "   '3002108456',\n",
       "   '3003465021',\n",
       "   '3004348779',\n",
       "   '2006434809',\n",
       "   '3003464757',\n",
       "   '2144410942',\n",
       "   '2769543984',\n",
       "   '1984335993',\n",
       "   '2064850047']},\n",
       " {'id': '3008443627',\n",
       "  'title': 'An interactive web-based dashboard to track COVID-19 in real time.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ensheng Dong', 'Hongru Du', 'Lauren Gardner'],\n",
       "  'related_topics': ['Dashboard (business)',\n",
       "   'Web application',\n",
       "   'Track (disk drive)',\n",
       "   'Human–computer interaction',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Web browser',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,371',\n",
       "  'reference_count': '1',\n",
       "  'references': ['2406220407']},\n",
       " {'id': '3013967887',\n",
       "  'title': 'Estimates of the severity of coronavirus disease 2019: a model-based analysis.',\n",
       "  'abstract': 'Background  In the face of rapidly changing data, a range of case fatality ratio estimates for coronavirus disease 2019 (COVID-19) have been produced that differ substantially in magnitude. We aimed to provide robust estimates, accounting for censoring and ascertainment biases.  Methods  We collected individual-case data for patients who died from COVID-19 in Hubei, mainland China (reported by national and provincial health commissions to Feb 8, 2020), and for cases outside of mainland China (from government or ministry of health websites and media reports for 37 countries, as well as Hong Kong and Macau, until Feb 25, 2020). These individual-case data were used to estimate the time between onset of symptoms and outcome (death or discharge from hospital). We next obtained age-stratified estimates of the case fatality ratio by relating the aggregate distribution of cases to the observed cumulative deaths in China, assuming a constant attack rate by age and adjusting for demography and age-based and location-based under-ascertainment. We also estimated the case fatality ratio from individual line-list data on 1334 cases identified outside of mainland China. Using data on the prevalence of PCR-confirmed cases in international residents repatriated from China, we obtained age-stratified estimates of the infection fatality ratio. Furthermore, data on age-stratified severity in a subset of 3665 cases from China were used to estimate the proportion of infected individuals who are likely to require hospitalisation.  Findings  Using data on 24 deaths that occurred in mainland China and 165 recoveries outside of China, we estimated the mean duration from onset of symptoms to death to be 17·8 days (95% credible interval [CrI] 16·9-19·2) and to hospital discharge to be 24·7 days (22·9-28·1). In all laboratory confirmed and clinically diagnosed cases from mainland China (n=70 117), we estimated a crude case fatality ratio (adjusted for censoring) of 3·67% (95% CrI 3·56-3·80). However, after further adjusting for demography and under-ascertainment, we obtained a best estimate of the case fatality ratio in China of 1·38% (1·23-1·53), with substantially higher ratios in older age groups (0·32% [0·27-0·38] in those aged   Interpretation  These early estimates give an indication of the fatality ratio across the spectrum of COVID-19 disease and show a strong age gradient in risk of death.  Funding  UK Medical Research Council.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Robert Verity',\n",
       "   'Lucy C Okell',\n",
       "   'Ilaria Dorigatti',\n",
       "   'Peter Winskill',\n",
       "   'Charles Whittaker',\n",
       "   'Natsuko Imai',\n",
       "   'Gina Cuomo-Dannenburg',\n",
       "   'Hayley Thompson',\n",
       "   'Patrick G T Walker',\n",
       "   'Han Fu',\n",
       "   'Amy Dighe',\n",
       "   'Jamie T Griffin',\n",
       "   'Marc Baguelin',\n",
       "   'Sangeeta Bhatia',\n",
       "   'Adhiratha Boonyasiri',\n",
       "   'Anne Cori',\n",
       "   'Zulma Cucunubá',\n",
       "   'Rich FitzJohn',\n",
       "   'Katy Gaythorpe',\n",
       "   'Will Green',\n",
       "   'Arran Hamlet',\n",
       "   'Wes Hinsley',\n",
       "   'Daniel Laydon',\n",
       "   'Gemma Nedjati-Gilani',\n",
       "   'Steven Riley',\n",
       "   'Sabine van Elsland',\n",
       "   'Erik Volz',\n",
       "   'Haowei Wang',\n",
       "   'Yuanrong Wang',\n",
       "   'Xiaoyue Xi',\n",
       "   'Christl A Donnelly',\n",
       "   '',\n",
       "   'Azra C Ghani',\n",
       "   'Neil M Ferguson'],\n",
       "  'related_topics': ['Case fatality rate',\n",
       "   'Mainland China',\n",
       "   'Incidence (epidemiology)',\n",
       "   'Attack rate',\n",
       "   'Credible interval',\n",
       "   'Censoring (clinical trials)',\n",
       "   'Demography',\n",
       "   'China',\n",
       "   'Young adult',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,376',\n",
       "  'reference_count': '27',\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3002539152',\n",
       "   '3008818676',\n",
       "   '3007189521',\n",
       "   '3020184843',\n",
       "   '3001971765']},\n",
       " {'id': '3015571324',\n",
       "  'title': 'Temporal dynamics in viral shedding and transmissibility of COVID-19.',\n",
       "  'abstract': \"We report temporal patterns of viral shedding in 94 patients with laboratory-confirmed COVID-19 and modeled COVID-19 infectiousness profiles from a separate sample of 77 infector-infectee transmission pairs. We observed the highest viral load in throat swabs at the time of symptom onset, and inferred that infectiousness peaked on or before symptom onset. We estimated that 44% (95% confidence interval, 25-69%) of secondary cases were infected during the index cases' presymptomatic stage, in settings with substantial household clustering, active case finding and quarantine outside the home. Disease control measures should be adjusted to account for probable substantial presymptomatic transmission.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Xi He',\n",
       "   'Eric H Y Lau',\n",
       "   'Peng Wu',\n",
       "   'Xilong Deng',\n",
       "   'Jian Wang',\n",
       "   'Xinxin Hao',\n",
       "   'Yiu Chung Lau',\n",
       "   'Jessica Y Wong',\n",
       "   'Yujuan Guan',\n",
       "   'Xinghua Tan',\n",
       "   'Xiaoneng Mo',\n",
       "   'Yanqing Chen',\n",
       "   'Baolin Liao',\n",
       "   'Weilie Chen',\n",
       "   'Fengyu Hu',\n",
       "   'Qing Zhang',\n",
       "   'Mingqiu Zhong',\n",
       "   'Yanrong Wu',\n",
       "   'Lingzhai Zhao',\n",
       "   'Fuchun Zhang',\n",
       "   'Benjamin J Cowling',\n",
       "   'Fang Li',\n",
       "   'Gabriel M Leung'],\n",
       "  'related_topics': ['Viral load',\n",
       "   'Viral shedding',\n",
       "   'Serial interval',\n",
       "   'Viral Epidemiology',\n",
       "   'Confidence interval',\n",
       "   'Transmission (mechanics)',\n",
       "   'Transmissibility (vibration)',\n",
       "   'Throat',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,175',\n",
       "  'reference_count': '22',\n",
       "  'references': ['3003668884',\n",
       "   '3009885589',\n",
       "   '3006961006',\n",
       "   '3003573988',\n",
       "   '3008696669',\n",
       "   '3013893137',\n",
       "   '3012756997',\n",
       "   '3008294222',\n",
       "   '2129542667',\n",
       "   '3009983851']},\n",
       " {'id': '3006642361',\n",
       "  'title': 'The reproductive number of COVID-19 is higher compared to SARS coronavirus.',\n",
       "  'abstract': 'Teaser: Our review found the average R0 for 2019-nCoV to be 3.28, which exceeds WHO estimates of 1.4 to 2.5.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Ying Liu',\n",
       "   'Albert A Gayle',\n",
       "   'Annelies Wilder-Smith',\n",
       "   '',\n",
       "   'Joacim Rocklöv'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease transmission',\n",
       "   'Severe acute respiratory syndrome coronavirus',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral transmission',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,384',\n",
       "  'reference_count': '14',\n",
       "  'references': ['3003668884',\n",
       "   '3003573988',\n",
       "   '3004397688',\n",
       "   '3002764620',\n",
       "   '3004026249',\n",
       "   '3002533591',\n",
       "   '3002747665',\n",
       "   '3001343166',\n",
       "   '3001392146',\n",
       "   '3023259384']},\n",
       " {'id': '3013594674',\n",
       "  'title': 'The effect of human mobility and control measures on the COVID-19 epidemic in China.',\n",
       "  'abstract': 'The ongoing coronavirus disease 2019 (COVID-19) outbreak expanded rapidly throughout China. Major behavioral, clinical, and state interventions were undertaken to mitigate the epidemic and prevent the persistence of the virus in human populations in China and worldwide. It remains unclear how these unprecedented interventions, including travel restrictions, affected COVID-19 spread in China. We used real-time mobility data from Wuhan and detailed case data including travel history to elucidate the role of case importation in transmission in cities across China and to ascertain the impact of control measures. Early on, the spatial distribution of COVID-19 cases in China was explained well by human mobility data. After the implementation of control measures, this correlation dropped and growth rates became negative in most locations, although shifts in the demographics of reported cases were still indicative of local chains of transmission outside of Wuhan. This study shows that the drastic control measures implemented in China substantially mitigated the spread of COVID-19.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Moritz U.G. Kraemer',\n",
       "   '',\n",
       "   'Chia Hung Yang',\n",
       "   'Bernardo Gutierrez',\n",
       "   'Chieh Hsi Wu',\n",
       "   'Brennan Klein',\n",
       "   'David M. Pigott',\n",
       "   'Louis du Plessis',\n",
       "   'Nuno R. Faria',\n",
       "   'Ruoran Li',\n",
       "   'William P. Hanage',\n",
       "   'John S. Brownstein',\n",
       "   'Maylis Layan',\n",
       "   'Alessandro Vespignani',\n",
       "   'Huaiyu Tian',\n",
       "   'Christopher Dye',\n",
       "   'Oliver G. Pybus',\n",
       "   'Samuel V. Scarpino'],\n",
       "  'related_topics': ['China',\n",
       "   'Transmission (mechanics)',\n",
       "   'Outbreak',\n",
       "   'Environmental health',\n",
       "   'Psychological intervention',\n",
       "   'Epidemiological Monitoring',\n",
       "   'Geography',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Demographics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,304',\n",
       "  'reference_count': '38',\n",
       "  'references': ['3001897055',\n",
       "   '3003668884',\n",
       "   '3008028633',\n",
       "   '1951724000',\n",
       "   '3003573988',\n",
       "   '3008818676',\n",
       "   '2097360283',\n",
       "   '3012284084',\n",
       "   '2122825543',\n",
       "   '3004912618']},\n",
       " {'id': '3012789146',\n",
       "  'title': 'The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study.',\n",
       "  'abstract': 'BACKGROUND: In December, 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a novel coronavirus, emerged in Wuhan, China. Since then, the city of Wuhan has taken unprecedented measures in response to the outbreak, including extended school and workplace closures. We aimed to estimate the effects of physical distancing measures on the progression of the COVID-19 epidemic, hoping to provide some insights for the rest of the world. METHODS: To examine how changes in population mixing have affected outbreak progression in Wuhan, we used synthetic location-specific contact patterns in Wuhan and adapted these in the presence of school closures, extended workplace closures, and a reduction in mixing in the general community. Using these matrices and the latest estimates of the epidemiological parameters of the Wuhan outbreak, we simulated the ongoing trajectory of an outbreak in Wuhan using an age-structured susceptible-exposed-infected-removed (SEIR) model for several physical distancing measures. We fitted the latest estimates of epidemic parameters from a transmission model to data on local and internationally exported cases from Wuhan in an age-structured epidemic framework and investigated the age distribution of cases. We also simulated lifting of the control measures by allowing people to return to work in a phased-in way and looked at the effects of returning to work at different stages of the underlying outbreak (at the beginning of March or April). FINDINGS: Our projections show that physical distancing measures were most effective if the staggered return to work was at the beginning of April; this reduced the median number of infections by more than 92% (IQR 66-97) and 24% (13-90) in mid-2020 and end-2020, respectively. There are benefits to sustaining these measures until April in terms of delaying and reducing the height of the peak, median epidemic size at end-2020, and affording health-care systems more time to expand and respond. However, the modelled effects of physical distancing measures vary by the duration of infectiousness and the role school children have in the epidemic. INTERPRETATION: Restrictions on activities in Wuhan, if maintained until April, would probably help to delay the epidemic peak. Our projections suggest that premature and sudden lifting of interventions could lead to an earlier secondary peak, which could be flattened by relaxing the interventions gradually. However, there are limitations to our analysis, including large uncertainties around estimates of R0 and the duration of infectiousness. FUNDING: Bill & Melinda Gates Foundation, National Institute for Health Research, Wellcome Trust, and Health Data Research UK.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Kiesha Prem',\n",
       "   'Yang Liu',\n",
       "   'Timothy W Russell',\n",
       "   'Adam J Kucharski',\n",
       "   'Rosalind M Eggo',\n",
       "   'Nicholas Davies',\n",
       "   'Mark Jit',\n",
       "   'Petra Klepac'],\n",
       "  'related_topics': ['Psychological intervention',\n",
       "   'Outbreak',\n",
       "   'Demography',\n",
       "   'Social distance',\n",
       "   'Distancing',\n",
       "   'Epidemiology',\n",
       "   'Duration (project management)',\n",
       "   'Transmission (mechanics)',\n",
       "   'China',\n",
       "   'Geography',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,337',\n",
       "  'reference_count': '43',\n",
       "  'references': ['3001897055',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3003573988',\n",
       "   '3006659024',\n",
       "   '3009577418',\n",
       "   '3009468976',\n",
       "   '3004912618',\n",
       "   '3004026249',\n",
       "   '3020184843']},\n",
       " {'id': '3001195213',\n",
       "  'title': 'Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR.',\n",
       "  'abstract': 'Background The ongoing outbreak of the recently emerged novel coronavirus (2019-nCoV) poses a challenge for public health laboratories as virus isolates are unavailable while there is growing evidence that the outbreak is more widespread than initially thought, and international spread through travellers does already occur. Aim We aimed to develop and deploy robust diagnostic methodology for use in public health laboratory settings without having virus material available. Methods Here we present a validated diagnostic workflow for 2019-nCoV, its design relying on close genetic relatedness of 2019-nCoV with SARS coronavirus, making use of synthetic nucleic acid technology. Results The workflow reliably detects 2019-nCoV, and further discriminates 2019-nCoV from SARS-CoV. Through coordination between academic and public laboratories, we confirmed assay exclusivity based on 297 original clinical specimens containing a full spectrum of human respiratory viruses. Control material is made available through European Virus Archive – Global (EVAg), a European Union infrastructure project. Conclusion The present study demonstrates the enormous response capacity achieved through coordination of academic and public laboratories in national and European research networks.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Victor M. Corman',\n",
       "   'Olfert Landt',\n",
       "   'Marco Kaiser',\n",
       "   'Richard Molenkamp',\n",
       "   'Adam Meijer',\n",
       "   'Daniel K.W. Chu',\n",
       "   'Tobias Bleicker',\n",
       "   'Sebastian Brünink',\n",
       "   'Julia Schneider',\n",
       "   'Marie Luisa Schmidt',\n",
       "   'Daphne G.J.C. Mulders',\n",
       "   'Bart L. Haagmans',\n",
       "   'Bas Van Der Veer',\n",
       "   'Sharon Van Den Brink',\n",
       "   'Lisa Wijsman',\n",
       "   'Gabriel Goderski',\n",
       "   'Jean Louis Romette',\n",
       "   'Joanna Ellis',\n",
       "   'Maria Zambon',\n",
       "   'Malik Peiris',\n",
       "   'Herman Goossens',\n",
       "   'Chantal Reusken',\n",
       "   'Marion P.G. Koopmans',\n",
       "   'Christian Drosten'],\n",
       "  'related_topics': ['European union',\n",
       "   'Coronavirus',\n",
       "   'Global health',\n",
       "   'Emergency Use Authorization',\n",
       "   'Workflow',\n",
       "   'Public health',\n",
       "   'Outbreak',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,985',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2903899730',\n",
       "   '2132260239',\n",
       "   '1703839189',\n",
       "   '1852588318',\n",
       "   '2793008036',\n",
       "   '2167080692',\n",
       "   '2894950287',\n",
       "   '2884280018',\n",
       "   '2031705962',\n",
       "   '2101063972']},\n",
       " {'id': '2105275554',\n",
       "  'title': 'Loop-mediated isothermal amplification of DNA',\n",
       "  'abstract': 'We have developed a novel method, termed loop-mediated isothermal amplification (LAMP), that amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions. This method employs a DNA polymerase and a set of four specially designed primers that recognize a total of six distinct sequences on the target DNA. An inner primer containing sequences of the sense and antisense strands of the target DNA initiates LAMP. The following strand displacement DNA synthesis primed by an outer primer releases a single-stranded DNA. This serves as template for DNA synthesis primed by the second inner and outer primers that hybridize to the other end of the target, which produces a stem–loop DNA structure. In subsequent LAMP cycling one inner primer hybridizes to the loop on the product and initiates displacement DNA synthesis, yielding the original stem–loop DNA and a new stem–loop DNA with a stem twice as long. The cycling reaction continues with accumulation of 109 copies of target in less than an hour. The final products are stem–loop DNAs with several inverted repeats of the target and cauliflower-like structures with multiple loops formed by annealing between alternately inverted repeats of the target in the same strand. Because LAMP recognizes the target by six distinct sequences initially and by four distinct sequences afterwards, it is expected to amplify the target sequence with high selectivity.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Tsugunori Notomi',\n",
       "   'Hiroto',\n",
       "   'Harumi',\n",
       "   'Toshihiro',\n",
       "   'Keiko',\n",
       "   'Nobuyuki',\n",
       "   'Tetsu'],\n",
       "  'related_topics': ['DNA clamp',\n",
       "   'Primer (molecular biology)',\n",
       "   'Base pair',\n",
       "   'Primase',\n",
       "   'In vitro recombination',\n",
       "   'DNA polymerase',\n",
       "   'Multiple displacement amplification',\n",
       "   'Nucleic acid thermodynamics',\n",
       "   'Molecular biology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,786',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2032118018',\n",
       "   '2050717506',\n",
       "   '2035792726',\n",
       "   '2062756489',\n",
       "   '1990689151',\n",
       "   '2142539585',\n",
       "   '2083121396',\n",
       "   '2082277951',\n",
       "   '1970137322',\n",
       "   '2056636525']},\n",
       " {'id': '3011969828',\n",
       "  'title': '2019 Novel Coronavirus Disease (COVID-19): Paving the Road for Rapid Detection and Point-of-Care Diagnostics.',\n",
       "  'abstract': 'We believe a point-of-care (PoC) device for the rapid detection of the 2019 novel Coronavirus (SARS-CoV-2) is crucial and urgently needed. With this perspective, we give suggestions regarding a potential candidate for the rapid detection of the coronavirus disease 2019 (COVID-19), as well as factors for the preparedness and response to the outbreak of the COVID-19.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Trieu Nguyen', 'Dang Duong Bang', 'Anders Wolff'],\n",
       "  'related_topics': ['Preparedness',\n",
       "   'Outbreak',\n",
       "   'Point-of-care testing',\n",
       "   'Disease',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Potential candidate',\n",
       "   'Rapid detection',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '176',\n",
       "  'reference_count': '32',\n",
       "  'references': ['3001118548',\n",
       "   '3004280078',\n",
       "   '3001195213',\n",
       "   '3003465021',\n",
       "   '3004239190',\n",
       "   '3003573988',\n",
       "   '3001465255',\n",
       "   '3003951199',\n",
       "   '3003637715',\n",
       "   '3000771439']},\n",
       " {'id': '2770752141',\n",
       "  'title': 'Loop-mediated isothermal amplification (LAMP): a versatile technique for detection of micro-organisms.',\n",
       "  'abstract': 'Summary\\r\\nLoop-mediated isothermal amplification (LAMP) amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions by using a DNA polymerase with high displacement strand activity and a set of specifically designed primers to amplify targeted DNA strands. Following its first discovery by Notomi et\\xa0al. (2000 Nucleic Acids Res 28: E63), LAMP was further developed over the years which involved the combination of this technique with other molecular approaches, such as reverse transcription and multiplex amplification for the detection of infectious diseases caused by micro-organisms in humans, livestock and plants. In this review, available types of LAMP techniques will be discussed together with their applications in detection of various micro-organisms. Up to date, there are varieties of LAMP detection methods available including colorimetric and fluorescent detection, real-time monitoring using turbidity metre and detection using lateral flow device which will also be highlighted in this review. Apart from that, commercialization of LAMP technique had also been reported such as lyophilized form of LAMP reagents kit and LAMP primer sets for detection of pathogenic micro-organisms. On top of that, advantages and limitations of this molecular detection method are also described together with its future potential as a diagnostic method for infectious disease.',\n",
       "  'date': '2018',\n",
       "  'authors': ['Y.-P. Wong', 'S. Othman', 'Y.-L. Lau', 'S. Radu', 'H.-Y. Chee'],\n",
       "  'related_topics': ['Loop-mediated isothermal amplification',\n",
       "   'Multiplex',\n",
       "   'Polymerase',\n",
       "   'Primer (molecular biology)',\n",
       "   'DNA',\n",
       "   'Nucleic acid',\n",
       "   'Computational biology',\n",
       "   'Materials science',\n",
       "   'Diagnostic methods',\n",
       "   'View Less'],\n",
       "  'citation_count': '175',\n",
       "  'reference_count': '168',\n",
       "  'references': ['2105275554',\n",
       "   '1979974453',\n",
       "   '1975801865',\n",
       "   '1562219715',\n",
       "   '89741002',\n",
       "   '2063533401',\n",
       "   '1997925861',\n",
       "   '2050515639',\n",
       "   '2141633648',\n",
       "   '2035792726']},\n",
       " {'id': '2263084061',\n",
       "  'title': 'Loop-Mediated Isothermal Amplification Assay for Identification of Five Human Plasmodium Species in Malaysia',\n",
       "  'abstract': 'The lack of rapid, affordable, and accurate diagnostic tests represents the primary hurdle affecting malaria surveillance in resource- and expertise-limited areas. Loop-mediated isothermal amplification (LAMP) is a sensitive, rapid, and cheap diagnostic method. Five species-specific LAMP assays were developed based on 18S rRNA gene. Sensitivity and specificity of LAMP results were calculated as compared with microscopic examination and nested polymerase chain reaction. LAMP reactions were highly sensitive with the detection limit of one copy for Plasmodium vivax, Plasmodium falciparum, and Plasmodium malariae and 10 copies for Plasmodium knowlesi and Plasmodium ovale. LAMP positively detected all human malaria species in all positive samples (N = 134; sensitivity = 100%) within 35 minutes. All negative samples were not amplified by LAMP (N = 67; specificity = 100%). LAMP successfully detected two samples with very low parasitemia. LAMP may offer a rapid, simple, and reliable test for the diagnosis of malaria in areas where malaria is prevalent.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Yee Ling', 'Meng Yee', 'Mun Yik', 'Jenarun', 'Rohela'],\n",
       "  'related_topics': ['Plasmodium malariae',\n",
       "   'Plasmodium ovale',\n",
       "   'Plasmodium vivax',\n",
       "   'Plasmodium knowlesi',\n",
       "   'Plasmodium falciparum',\n",
       "   'Nucleic acid amplification technique',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Nested polymerase chain reaction',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '37',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2105275554',\n",
       "   '2788073857',\n",
       "   '2133724824',\n",
       "   '2066385972',\n",
       "   '2149136689',\n",
       "   '2102131955',\n",
       "   '2043762150',\n",
       "   '1987713777',\n",
       "   '2110753623',\n",
       "   '2144345536']},\n",
       " {'id': '2175815746',\n",
       "  'title': 'Development of reverse-transcription loop-mediated isothermal amplification assay for rapid detection and differentiation of dengue virus serotypes 1-4.',\n",
       "  'abstract': 'Dengue virus (DENV), the most widely prevalent arbovirus, continues to be a threat to human health in the tropics and subtropics. Early and rapid detection of DENV infection during the acute phase of illness is crucial for proper clinical patient management and preventing the spread of infection. The aim of the current study was to develop a specific, sensitive, and robust reverse transcriptase loop-mediated isothermal amplification (RT-LAMP) assay for detection and differentiation of DENV1-4 serotypes. The method detection primers, which were designed to target the different DENV serotypes, were identified by inspection of multiple sequence alignments of the non-structural protein (NS) 2A of DENV1, NS4B of DENV2, NS4A of DENV3 and the 3′ untranslated region of the NS protein of DENV4. No cross-reactions of the four serotypes were observed during the tests. The detection limits of the DENV1-4-specific RT-LAMP assays were approximately 10-copy templates per reaction. The RT-LAMP assays were ten-fold more sensitive than RT-PCR or real-time PCR. The diagnostic rate was 100\\xa0% for clinical strains of DENV, and 98.9\\xa0% of the DENV-infected patients whose samples were tested were detected by RT-LAMP. Importantly, no false-positives were detected with the new equipment and methodology that was used to avoid aerosol contamination of the samples. The RT-LAMP method used in our study is specific, sensitive, and suitable for further investigation as a useful alternative to the current methods used for clinical diagnosis of DENV1-4, especially in hospitals and laboratories that lack sophisticated diagnostic systems.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Sheng-feng Hu',\n",
       "   'Miao Li',\n",
       "   'Lan-lan Zhong',\n",
       "   'Shi-miao Lu',\n",
       "   'Ze-xia Liu',\n",
       "   'Jie-ying Pu',\n",
       "   'Jin-sheng Wen',\n",
       "   'Xi Huang',\n",
       "   ''],\n",
       "  'related_topics': ['Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Dengue virus',\n",
       "   'Nucleic acid amplification technique',\n",
       "   'Dengue fever',\n",
       "   'Arbovirus',\n",
       "   'Reverse transcriptase',\n",
       "   'Parasitology',\n",
       "   'Virology',\n",
       "   'Microbiology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '26',\n",
       "  'reference_count': '43',\n",
       "  'references': ['2105275554',\n",
       "   '2042479028',\n",
       "   '2066385972',\n",
       "   '2128110156',\n",
       "   '2149136689',\n",
       "   '2160892791',\n",
       "   '2101258647',\n",
       "   '1994850469',\n",
       "   '2124449928',\n",
       "   '2141627564']},\n",
       " {'id': '1991420168',\n",
       "  'title': 'Utility of IgM ELISA, TaqMan real-time PCR, reverse transcription PCR, and RT-LAMP assay for the diagnosis of Chikungunya fever.',\n",
       "  'abstract': 'Chikungunya fever a re-emerging infection with expanding geographical boundaries, can mimic symptoms of other infections like dengue, malaria which makes the definitive diagnosis of the infection important. The present study compares the utility of four laboratory diagnostic methods viz. IgM capture ELISA, an in house reverse transcription PCR for the diagnosis of Chikungunya fever, TaqMan real-time PCR, and a one step reverse transcription-loop mediated isothermal amplification assay (RT-LAMP). Out of the 70 serum samples tested, 29 (41%) were positive for Chikungunya IgM antibody by ELISA and 50 (71%) samples were positive by one of the three molecular assays. CHIKV specific nucleic acid was detected in 33/70 (47%) by reverse transcription PCR, 46/70 (66%) by TaqMan real-time PCR, and 43/70 (62%) by RT-LAMP assay. A majority of the samples (62/70; 89%) were positive by at least one of the four assays used in the study. The molecular assays were more sensitive for diagnosis in the early stages of illness (2–5 days post onset) when antibodies were not detectable. In the later stages of illness, the IgM ELISA is a more sensitive diagnostic test. In conclusion we recommend that the IgM ELISA be used as an initial screening test followed one of the molecular assays in samples that are collected in the early phase of illness and negative for CHIKV IgM antibodies. Such as approach would enable rapid confirmation of the diagnosis and implementation of public health measures especially during outbreaks. J. Med. Virol. 84:1771–1778, 2012. © 2012 Wiley Periodicals, Inc.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Vijayalakshmi Reddy',\n",
       "   'Vasanthapuram Ravi',\n",
       "   'Anita Desai',\n",
       "   'Manmohan Parida',\n",
       "   'Ann M. Powers',\n",
       "   'Barbara W. Johnson'],\n",
       "  'related_topics': ['TaqMan',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Chikungunya',\n",
       "   'Dengue fever',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Antibody',\n",
       "   'Virology',\n",
       "   'Malaria',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '65',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2141987735',\n",
       "   '2120801593',\n",
       "   '1977296748',\n",
       "   '2129358311',\n",
       "   '1969557290',\n",
       "   '2067506266',\n",
       "   '2052129607',\n",
       "   '2108924397',\n",
       "   '1966636515',\n",
       "   '2051304065']},\n",
       " {'id': '2073600962',\n",
       "  'title': 'Evaluation of a Direct Reverse Transcription Loop-Mediated Isothermal Amplification Method without RNA Extraction for the Detection of Human Enterovirus 71 Subgenotype C4 in Nasopharyngeal Swab Specimens',\n",
       "  'abstract': 'Human enterovirus 71 (EV71) is the major causative agent of hand, foot, and mouth disease (HFMD) worldwide and has been associated with neurological complications which resulted in fatalities during recent outbreak in Asia pacific region. A direct reverse transcription loop-mediated isothermal amplification (direct RT-LAMP) assay using heat-treated samples without RNA extraction was developed and evaluated for the detection of EV71 subgenotype C4 in nasopharyngeal swab specimens. The analytical sensitivity and specificity of the direct RT-LAMP assay were examined. The detection limit of the direct RT-LAMP assays was 1.6 of a 50% tissue culture infective dose (TCID50) per reaction and no cross-reaction was observed with control viruses including Cosackievirus A (CVA) viruses (CVA2,4,5,7,9,10,14,16, and 24), Coxsackievirus B (CVB) viruses (CVB1,2,3,4, and 5) or ECHO viruses (ECHO3,6,11, and 19). The direct RT-LAMP assay was evaluated and compared to both RT-LAMP and quantitative real-time PCR (qRT-PCR) in detecting EV71 infection with 145 nasopharyngeal swab specimens. The clinical performance demonstrated the sensitivity and specificity of direct RT-LAMP was reported to be 90.3% and 100% respectively, compared to RT-LAMP, and 86.83% and 100% respectively, compared to qRT-PCR. These data demonstrated that the direct RT-LAMP assay can potentially be developed for the point of care screening of EV71 infection in China.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Kai Nie',\n",
       "   'Shun-xiang Qi',\n",
       "   'Yong Zhang',\n",
       "   'Le Luo',\n",
       "   'Yun Xie',\n",
       "   'Meng-jie Yang',\n",
       "   'Yi Zhang',\n",
       "   'Jin Li',\n",
       "   'Hongwei Shen',\n",
       "   'Qi Li',\n",
       "   'Xue-jun Ma'],\n",
       "  'related_topics': ['Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Nucleic acid amplification technique',\n",
       "   'Coxsackievirus',\n",
       "   'Reverse transcriptase',\n",
       "   'RNA extraction',\n",
       "   'Polymerase chain reaction',\n",
       "   'RNA',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'View Less'],\n",
       "  'citation_count': '31',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2105275554',\n",
       "   '2135086664',\n",
       "   '2160892791',\n",
       "   '2057680658',\n",
       "   '2140363623',\n",
       "   '2065860560',\n",
       "   '1975134712',\n",
       "   '1970186243',\n",
       "   '2051085144',\n",
       "   '1984421786']},\n",
       " {'id': '2084576921',\n",
       "  'title': 'Visual detection of turkey coronavirus RNA in tissues and feces by reverse-transcription loop-mediated isothermal amplification (RT-LAMP) with hydroxynaphthol blue dye',\n",
       "  'abstract': 'Abstract   A sensitive reverse-transcription loop-mediated isothermal amplification (RT-LAMP) assay was developed for the rapid visual detection of turkey coronavirus (TCoV) infection. The reaction is performed in one step in a single tube at 65\\xa0°C for 45\\xa0min, with hydroxynaphthol blue (HNB) dye added prior to amplification. The\\xa0detection limit of the RT-LAMP assay was approximately 10 2  EID 50/50\\xa0μl  TCoV genome, and no cross-reaction with other avian viruses was observed. The assay was evaluated further in tissue suspensions prepared from the ileum and ileum–caecal junctions of infected turkey embryos; 100% of these samples were positive in the RT-LAMP assay. All individual feces samples collected in the field were considered positive by both conventional RT-PCR and RT-LAMP. In conclusion, RT-LAMP with HNB dye was shown to be a sensitive, simple assay for the rapid diagnosis of TCoV infection, either directly from feces or in association with virus isolation methods.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Tereza C. Cardoso',\n",
       "   'Heitor F. Ferrari',\n",
       "   'Livia C. Bregano',\n",
       "   'Camila Silva-Frade',\n",
       "   'Ana Carolina G. Rosa',\n",
       "   'Alexandre Lima de Andrade'],\n",
       "  'related_topics': ['Hydroxynaphthol blue',\n",
       "   'Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Turkey coronavirus',\n",
       "   'Feces',\n",
       "   'RNA',\n",
       "   'Molecular biology',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Single tube',\n",
       "   'Visual detection',\n",
       "   'View Less'],\n",
       "  'citation_count': '62',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2105275554',\n",
       "   '1544561280',\n",
       "   '2028571354',\n",
       "   '1997738379',\n",
       "   '2057637320',\n",
       "   '2155485281',\n",
       "   '1997343737',\n",
       "   '1565181081',\n",
       "   '2009652162',\n",
       "   '2038706592']},\n",
       " {'id': '3008090866',\n",
       "  'title': 'Clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia in Wuhan, China: a single-centered, retrospective, observational study.',\n",
       "  'abstract': 'Summary  Background  An ongoing outbreak of pneumonia associated with the severe acute respiratory coronavirus 2 (SARS-CoV-2) started in December, 2019, in Wuhan, China. Information about critically ill patients with SARS-CoV-2 infection is scarce. We aimed to describe the clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia.  Methods  In this single-centered, retrospective, observational study, we enrolled 52 critically ill adult patients with SARS-CoV-2 pneumonia who were admitted to the intensive care unit (ICU) of Wuhan Jin Yin-tan hospital (Wuhan, China) between late December, 2019, and Jan 26, 2020. Demographic data, symptoms, laboratory values, comorbidities, treatments, and clinical outcomes were all collected. Data were compared between survivors and non-survivors. The primary outcome was 28-day mortality, as of Feb 9, 2020. Secondary outcomes included incidence of SARS-CoV-2-related acute respiratory distress syndrome (ARDS) and the proportion of patients requiring mechanical ventilation.  Findings  Of 710 patients with SARS-CoV-2 pneumonia, 52 critically ill adult patients were included. The mean age of the 52 patients was 59·7 (SD 13·3) years, 35 (67%) were men, 21 (40%) had chronic illness, 51 (98%) had fever. 32 (61·5%) patients had died at 28 days, and the median duration from admission to the intensive care unit (ICU) to death was 7 (IQR 3–11) days for non-survivors. Compared with survivors, non-survivors were older (64·6 years [11·2] vs 51·9 years [12·9]), more likely to develop ARDS (26 [81%] patients vs 9 [45%] patients), and more likely to receive mechanical ventilation (30 [94%] patients vs 7 [35%] patients), either invasively or non-invasively. Most patients had organ function damage, including 35 (67%) with ARDS, 15 (29%) with acute kidney injury, 12 (23%) with cardiac injury, 15 (29%) with liver dysfunction, and one (2%) with pneumothorax. 37 (71%) patients required mechanical ventilation. Hospital-acquired infection occurred in seven (13·5%) patients.  Interpretation  The mortality of critically ill patients with SARS-CoV-2 pneumonia is considerable. The survival time of the non-survivors is likely to be within 1–2 weeks after ICU admission. Older patients (>65 years) with comorbidities and ARDS are at increased risk of death. The severity of SARS-CoV-2 pneumonia poses great strain on critical care resources in hospitals, especially if they are not adequately staffed or resourced.  Funding  None.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Xiaobo Yang',\n",
       "   'Yuan Yu',\n",
       "   'Jiqian Xu',\n",
       "   'Huaqing Shu',\n",
       "   \"Jia'an Xia\",\n",
       "   'Hong Liu',\n",
       "   '',\n",
       "   'Yongran Wu',\n",
       "   'Lu Zhang',\n",
       "   'Zhui Yu',\n",
       "   'Minghao Fang',\n",
       "   'Ting Yu',\n",
       "   'Yaxin Wang',\n",
       "   'Shangwen Pan',\n",
       "   'Xiaojing Zou',\n",
       "   'Shiying Yuan',\n",
       "   'You Shang',\n",
       "   ''],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'ARDS',\n",
       "   'Intensive care unit',\n",
       "   'Mechanical ventilation',\n",
       "   'Retrospective cohort study',\n",
       "   'Incidence (epidemiology)',\n",
       "   'Acute kidney injury',\n",
       "   'Pneumothorax',\n",
       "   'Emergency medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,578',\n",
       "  'reference_count': '22',\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3005079553',\n",
       "   '3002108456',\n",
       "   '3003465021',\n",
       "   '3004239190',\n",
       "   '2470646526',\n",
       "   '2026274122',\n",
       "   '3005403371',\n",
       "   '2286228001']},\n",
       " {'id': '3007940623',\n",
       "  'title': 'Pathological findings of COVID-19 associated with acute respiratory distress syndrome.',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Zhe Xu',\n",
       "   'Lei Shi',\n",
       "   'Yijin Wang',\n",
       "   'Jiyuan Zhang',\n",
       "   'Lei Huang',\n",
       "   'Chao Zhang',\n",
       "   'Shuhong Liu',\n",
       "   'Peng Zhao',\n",
       "   'Hongxia Liu',\n",
       "   'Li Zhu',\n",
       "   'Yanhong Tai',\n",
       "   'Changqing Bai',\n",
       "   'Tingting Gao',\n",
       "   'Jinwen Song',\n",
       "   'Peng Xia',\n",
       "   'Jinghui Dong',\n",
       "   'Jingmin Zhao',\n",
       "   'Fu Sheng Wang'],\n",
       "  'related_topics': ['Cytokine storm',\n",
       "   'Medicine',\n",
       "   'Pathological',\n",
       "   'Internal medicine',\n",
       "   'MEDLINE',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Acute respiratory distress',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,395',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '3003217347',\n",
       "   '2256430766',\n",
       "   '2158118659']},\n",
       " {'id': '3011242477',\n",
       "  'title': 'COVID-19 and Italy: what next?',\n",
       "  'abstract': \"Summary  The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already taken on pandemic proportions, affecting over 100 countries in a matter of weeks. A global response to prepare health systems worldwide is imperative. Although containment measures in China have reduced new cases by more than 90%, this reduction is not the case elsewhere, and Italy has been particularly affected. There is now grave concern regarding the Italian national health system's capacity to effectively respond to the needs of patients who are infected and require intensive care for SARS-CoV-2 pneumonia. The percentage of patients in intensive care reported daily in Italy between March 1 and March 11, 2020, has consistently been between 9% and 11% of patients who are actively infected. The number of patients infected since Feb 21 in Italy closely follows an exponential trend. If this trend continues for 1 more week, there will be 30\\u2008000 infected patients. Intensive care units will then be at maximum capacity; up to 4000 hospital beds will be needed by mid-April, 2020. Our analysis might help political leaders and health authorities to allocate enough resources, including personnel, beds, and intensive care facilities, to manage the situation in the next few days and weeks. If the Italian outbreak follows a similar trend as in Hubei province, China, the number of newly infected patients could start to decrease within 3–4 days, departing from the exponential trend. However, this cannot currently be predicted because of differences between social distancing measures and the capacity to quickly build dedicated facilities in China.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Andrea Remuzzi', 'Giuseppe Remuzzi'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Global health',\n",
       "   'Pandemic',\n",
       "   'China',\n",
       "   'Environmental health',\n",
       "   'Outbreak',\n",
       "   'Social distance',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,309',\n",
       "  'reference_count': '2',\n",
       "  'references': ['3003668884', '3007613835']},\n",
       " {'id': '3010449299',\n",
       "  'title': 'Air, Surface Environmental, and Personal Protective Equipment Contamination by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) From a Symptomatic Patient.',\n",
       "  'abstract': 'This study documents results of SARS-CoV-2 polymerase chain reaction (PCR) testing of environmental surfaces and personal protective equipment surrounding 3 COVID-19 patients in isolation rooms in a Singapore hospital.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Sean Wei Xiang Ong',\n",
       "   'Yian Kim Tan',\n",
       "   'Po Ying Chia',\n",
       "   'Tau Hong Lee',\n",
       "   'Oon Tek Ng',\n",
       "   'Michelle Su Yen Wong',\n",
       "   'Kalisvar Marimuthu'],\n",
       "  'related_topics': ['Isolation (health care)',\n",
       "   'Coronavirus',\n",
       "   'Personal protective equipment',\n",
       "   'Equipment Contamination',\n",
       "   'Pneumonia',\n",
       "   'Viral shedding',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Emergency medicine',\n",
       "   'Medicine',\n",
       "   'Contamination',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,707',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3005079553',\n",
       "   '3001195213',\n",
       "   '3010338568',\n",
       "   '1815575713',\n",
       "   '2250074178']},\n",
       " {'id': '3018334611',\n",
       "  'title': 'Aerodynamic analysis of SARS-CoV-2 in two Wuhan hospitals.',\n",
       "  'abstract': 'The ongoing outbreak of coronavirus disease 2019\\xa0(COVID-19) has spread rapidly on a global scale. Although it is clear that severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is transmitted through human respiratory droplets and direct contact, the potential for aerosol transmission is poorly understood1-3. Here we investigated the aerodynamic nature of SARS-CoV-2 by measuring viral RNA in aerosols in different areas of two Wuhan hospitals during the outbreak of COVID-19 in February and March 2020. The concentration of SARS-CoV-2 RNA in aerosols that was detected in isolation wards and ventilated patient rooms was very low, but it was higher in the toilet areas used by the patients. Levels of airborne SARS-CoV-2 RNA in the most public areas was undetectable, except in two areas that were prone to crowding; this increase was possibly due to individuals infected with SARS-CoV-2 in the crowd. We found that some medical staff areas initially had high concentrations of viral RNA with aerosol size distributions that showed peaks in the submicrometre and/or supermicrometre regions; however, these levels were reduced to undetectable levels after implementation of rigorous sanitization procedures. Although we have not established the infectivity of the virus detected in these hospital areas, we propose that SARS-CoV-2 may have the potential to be transmitted through aerosols. Our results indicate that room ventilation, open space, sanitization of protective apparel, and proper use and disinfection of toilet areas can effectively limit the concentration of SARS-CoV-2 RNA in aerosols. Future work should explore the infectivity of aerosolized virus.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Yuan Liu',\n",
       "   'Zhi Ning',\n",
       "   'Yu Chen',\n",
       "   'Ming Guo',\n",
       "   'Yingle Liu',\n",
       "   'Nirmal Kumar Gali',\n",
       "   'Li Sun',\n",
       "   'Yusen Duan',\n",
       "   'Jing Cai',\n",
       "   'Dane Westerdahl',\n",
       "   'Xinjin Liu',\n",
       "   'Ke Xu',\n",
       "   'Kin fai Ho',\n",
       "   'Haidong Kan',\n",
       "   'Qingyan Fu',\n",
       "   'Ke Lan'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Infectivity',\n",
       "   'Aerosol',\n",
       "   'Virus',\n",
       "   'Isolation (health care)',\n",
       "   'Viral Epidemiology',\n",
       "   'Betacoronavirus',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,067',\n",
       "  'reference_count': '12',\n",
       "  'references': ['3004280078',\n",
       "   '3012099172',\n",
       "   '3010604545',\n",
       "   '3009906937',\n",
       "   '3010449299',\n",
       "   '3005510968',\n",
       "   '3027866910',\n",
       "   '3018724240',\n",
       "   '2215636165',\n",
       "   '2147350479']},\n",
       " {'id': '3015704123',\n",
       "  'title': 'Aerosol and Surface Distribution of Severe Acute Respiratory Syndrome Coronavirus 2 in Hospital Wards, Wuhan, China, 2020.',\n",
       "  'abstract': 'To determine distribution of severe acute respiratory syndrome coronavirus 2 in hospital wards in Wuhan, China, we tested air and surface samples. Contamination was greater in intensive care units than general wards. Virus was widely distributed on floors, computer mice, trash cans, and sickbed handrails and was detected in air ≈4 m from patients.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Zhen Dong Guo',\n",
       "   'Zhong Yi Wang',\n",
       "   'Shou Feng Zhang',\n",
       "   'Xiao Li',\n",
       "   'Lin Li',\n",
       "   'Chao Li',\n",
       "   'Yan Cui',\n",
       "   'Rui Bin Fu',\n",
       "   'Yun Zhu Dong',\n",
       "   'Xiang Yang Chi',\n",
       "   'Meng Yao Zhang',\n",
       "   'Kun Liu',\n",
       "   'Cheng Cao',\n",
       "   'Bin Liu',\n",
       "   'Ke Zhang',\n",
       "   'Yu Wei Gao',\n",
       "   'Bing Lu',\n",
       "   'Wei Chen',\n",
       "   ''],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Emergency medicine',\n",
       "   'Medicine',\n",
       "   'Aerosol',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Air microbiology',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '682',\n",
       "  'reference_count': '7',\n",
       "  'references': ['3011242477',\n",
       "   '3010223921',\n",
       "   '3010449299',\n",
       "   '3010149441',\n",
       "   '3008429297',\n",
       "   '3010633777',\n",
       "   '2614711400']},\n",
       " {'id': '3030968929',\n",
       "  'title': 'Detection of air and surface contamination by SARS-CoV-2 in hospital rooms of infected patients.',\n",
       "  'abstract': 'Understanding the particle size distribution in the air and patterns of environmental contamination of SARS-CoV-2 is essential for infection prevention policies. Here we screen surface and air samples from hospital rooms of COVID-19 patients for SARS-CoV-2 RNA. Environmental sampling is conducted in three airborne infection isolation rooms (AIIRs) in the ICU and 27 AIIRs in the general ward. 245 surface samples are collected. 56.7% of rooms have at least one environmental surface contaminated. High touch surface contamination is shown in ten (66.7%) out of 15 patients in the first week of illness, and three (20%) beyond the first week of illness (p = 0.01, χ2 test). Air sampling is performed in three of the 27 AIIRs in the general ward, and detects SARS-CoV-2 PCR-positive particles of sizes >4 µm and 1-4 µm in two rooms, despite these rooms having 12 air changes per hour. This warrants further study of the airborne transmission potential of SARS-CoV-2.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Po Ying Chia',\n",
       "   '',\n",
       "   'Kristen Kelli Coleman',\n",
       "   'Yian Kim Tan',\n",
       "   'Sean Wei Xiang Ong',\n",
       "   'Marcus Gum',\n",
       "   'Sok Kiang Lau',\n",
       "   'Xiao Fang Lim',\n",
       "   'Ai Sim Lim',\n",
       "   'Stephanie Sutjipto',\n",
       "   'Pei Hua Lee',\n",
       "   'Barnaby Edward Young',\n",
       "   '',\n",
       "   'Donald K Milton',\n",
       "   'Gregory C Gray',\n",
       "   '',\n",
       "   'Stephan Schuster',\n",
       "   'Timothy Barkham',\n",
       "   '',\n",
       "   'Partha Pratim De',\n",
       "   '',\n",
       "   'Shawn Vasoo',\n",
       "   '',\n",
       "   'Monica Chan',\n",
       "   'Brenda Sze Peng Ang',\n",
       "   'Boon Huan Tan',\n",
       "   'Yee-Sin Leo',\n",
       "   'Oon-Tek Ng',\n",
       "   '',\n",
       "   'Michelle Su Yen Wong',\n",
       "   'Kalisvar Marimuthu',\n",
       "   ''],\n",
       "  'related_topics': ['Airborne transmission',\n",
       "   'Air changes per hour',\n",
       "   'Isolation (health care)',\n",
       "   'Contamination',\n",
       "   'Infection control',\n",
       "   'Veterinary medicine',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'General ward',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '274',\n",
       "  'reference_count': '17',\n",
       "  'references': ['3002539152',\n",
       "   '3001195213',\n",
       "   '3006961006',\n",
       "   '3012099172',\n",
       "   '3008696669',\n",
       "   '3013893137',\n",
       "   '3010604545',\n",
       "   '3010338568',\n",
       "   '2132260239',\n",
       "   '3010449299']},\n",
       " {'id': '2158121945',\n",
       "  'title': 'Guidelines for environmental infection control in health-care facilities. Recommendations of CDC and the Healthcare Infection Control Practices Advisory Committee (HICPAC).',\n",
       "  'abstract': 'The health-care facility environment is rarely implicated in disease transmission, except among patients who are immunocompromised. Nonetheless, inadvertent exposures to environmental pathogens (e.g., Aspergillus spp. and Legionella spp.) or airborne pathogens (e.g., Mycobacterium tuberculosis and varicella-zoster virus) can result in adverse patient outcomes and cause illness among health-care workers. Environmental infection-control strategies and engineering controls can effectively prevent these infections. The incidence of health-care--associated infections and pseudo-outbreaks can be minimized by 1) appropriate use of cleaners and disinfectants; 2) appropriate maintenance of medical equipment (e.g., automated endoscope reprocessors or hydrotherapy equipment); 3) adherence to water-quality standards for hemodialysis, and to ventilation standards for specialized care environments (e.g., airborne infection isolation rooms, protective environments, or operating rooms); and 4) prompt management of water intrusion into the facility. Routine environmental sampling is not usually advised, except for water quality determinations in hemodialysis settings and other situations where sampling is directed by epidemiologic principles, and results can be applied directly to infection-control decisions. This report reviews previous guidelines and strategies for preventing environment-associated infections in health-care facilities and offers recommendations. These include 1) evidence-based recommendations supported by studies; 2) requirements of federal agencies (e.g., Food and Drug Administration, U.S. Environmental Protection Agency, U.S. Department of Labor, Occupational Safety and Health Administration, and U.S. Department of Justice); 3) guidelines and standards from building and equipment professional organizations (e.g., American Institute of Architects, Association for the Advancement of Medical Instrumentation, and American Society of Heating, Refrigeration, and Air-Conditioning Engineers); 4) recommendations derived from scientific theory or rationale; and 5) experienced opinions based upon infection-control and engineering practices. The report also suggests a series of performance measurements as a means to evaluate infection-control efforts.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Lynne Sehulster'],\n",
       "  'related_topics': ['Isolation (health care)',\n",
       "   'Infection control',\n",
       "   'Health care',\n",
       "   'Occupational safety and health',\n",
       "   'Medical equipment',\n",
       "   'Justice (ethics)',\n",
       "   'Professional association',\n",
       "   'Medical emergency',\n",
       "   'Guideline',\n",
       "   'Environmental health',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,354',\n",
       "  'reference_count': '500',\n",
       "  'references': ['1856219842',\n",
       "   '2798795107',\n",
       "   '1833207062',\n",
       "   '2093487930',\n",
       "   '2905912541',\n",
       "   '1536339477',\n",
       "   '2465608195',\n",
       "   '2153911335',\n",
       "   '2315217144',\n",
       "   '1589603082']},\n",
       " {'id': '3015636815',\n",
       "  'title': 'Rapid Detection of COVID-19 Causative Virus (SARS-CoV-2) in Human Nasopharyngeal Swab Specimens Using Field-Effect Transistor-Based Biosensor.',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is a newly emerging human infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2, previously called 2019-nCoV). Based on the rapid increase in the rate of human infection, the World Health Organization (WHO) has classified the COVID-19 outbreak as a pandemic. Because no specific drugs or vaccines for COVID-19 are yet available, early diagnosis and management are crucial for containing the outbreak. Here, we report a field-effect transistor (FET)-based biosensing device for detecting SARS-CoV-2 in clinical samples. The sensor was produced by coating graphene sheets of the FET with a specific antibody against SARS-CoV-2 spike protein. The performance of the sensor was determined using antigen protein, cultured virus, and nasopharyngeal swab specimens from COVID-19 patients. Our FET device could detect the SARS-CoV-2 spike protein at concentrations of 1 fg/mL in phosphate-buffered saline and 100 fg/mL clinical transport medium. In addition, the FET sensor successfully detected SARS-CoV-2 in culture medium (limit of detection [LOD]: 1.6 × 101 pfu/mL) and clinical samples (LOD: 2.42 × 102 copies/mL). Thus, we have successfully fabricated a promising FET biosensor for SARS-CoV-2; our device is a highly sensitive immunological diagnostic method for COVID-19 that requires no sample pretreatment or labeling.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Giwan Seo',\n",
       "   'Geonhee Lee',\n",
       "   'Mi Jeong Kim',\n",
       "   'Seung Hwa Baek',\n",
       "   'Minsuk Choi',\n",
       "   'Keun Bon Ku',\n",
       "   'Chang Seop Lee',\n",
       "   'Sangmi Jun',\n",
       "   'Daeui Park',\n",
       "   'Hong Gi Kim',\n",
       "   'Seong Jun Kim',\n",
       "   'Jeong O. Lee',\n",
       "   'Bum Tae Kim',\n",
       "   'Edmond Changkyun Park',\n",
       "   'Seung Il Kim'],\n",
       "  'related_topics': ['Virus',\n",
       "   'Antigen',\n",
       "   'Detection limit',\n",
       "   'Biosensor',\n",
       "   'Outbreak',\n",
       "   'Immunological Diagnostic Method',\n",
       "   'Virology',\n",
       "   'Field-effect transistor',\n",
       "   'Chemistry',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'View Less'],\n",
       "  'citation_count': '379',\n",
       "  'reference_count': '20',\n",
       "  'references': ['3003668884',\n",
       "   '3004280078',\n",
       "   '2014935324',\n",
       "   '3004318991',\n",
       "   '3006961006',\n",
       "   '3003217347',\n",
       "   '3008696669',\n",
       "   '3007643904',\n",
       "   '3009906937',\n",
       "   '2470646526']},\n",
       " {'id': '3018724240',\n",
       "  'title': 'SARS-CoV-2 can be detected in urine, blood, anal swabs, and oropharyngeal swabs specimens.',\n",
       "  'abstract': \"Purpose  The purpose of this study was to detect severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) ribonucleic acid (RNA) in urine and blood specimens, and anal and oropharyngeal swabs from patients with confirmed SARS-CoV-2 infection, and correlated positive results with clinical findings.  Methods  Patients with confirmed SARS-CoV-2 infections were included in this study. Patients' demographic and clinical data were recorded. Quantitative real-time polymerase chain reaction was used to detect SARS-CoV-2 RNA in urine and blood specimens, and anal and oropharyngeal swabs. The study is registered at ClinicalTrials.gov (No. NCT04279782, 19 February, 2020).  Results  SARS-CoV-2 RNA was present in all four specimen types, though not all specimen types were positive simultaneously. The presence of viral RNA was not necessarily predictive of clinical symptoms, for example, the presence of viral RNA in the urine did not necessarily predict urinary tract symptoms.  Conclusions  SARS-CoV-2 can infect multiple systems, including the urinary tract. Testing different specimen types may be useful for monitoring disease changes and progression, and for establishing a prognosis.\",\n",
       "  'date': '2020',\n",
       "  'authors': ['Liang Peng',\n",
       "   'Jing Liu',\n",
       "   'Wenxiong Xu',\n",
       "   'Qiumin Luo',\n",
       "   'Dabiao Chen',\n",
       "   'Ziying Lei',\n",
       "   'Zhanlian Huang',\n",
       "   'Xuejun Li',\n",
       "   'Keji Deng',\n",
       "   'Bingliang Lin',\n",
       "   'Zhiliang Gao'],\n",
       "  'related_topics': ['Urinary system',\n",
       "   'Urine',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'RNA',\n",
       "   'Polymerase chain reaction',\n",
       "   'Gastroenterology',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'Internal medicine',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'URINE BLOOD',\n",
       "   'Viral rna',\n",
       "   'View Less'],\n",
       "  'citation_count': '208',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '3003465021',\n",
       "   '3010604545',\n",
       "   '3011060952']},\n",
       " {'id': '3008028633',\n",
       "  'title': 'Characteristics of and Important Lessons From the Coronavirus Disease 2019 (COVID-19) Outbreak in China: Summary of a Report of 72 314 Cases From the Chinese Center for Disease Control and Prevention',\n",
       "  'abstract': '',\n",
       "  'date': '2020',\n",
       "  'authors': ['Zunyou Wu', 'Jennifer M. McGoogan'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Pandemic',\n",
       "   'China',\n",
       "   'Medicine',\n",
       "   'Viral Epidemiology',\n",
       "   'Betacoronavirus',\n",
       "   'Family medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease control',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,282',\n",
       "  'reference_count': '10',\n",
       "  'references': ['3006533361',\n",
       "   '3024919756',\n",
       "   '3082757469',\n",
       "   '3006044875',\n",
       "   '3005409672',\n",
       "   '3004434564',\n",
       "   '3033263492',\n",
       "   '3031410613',\n",
       "   '3026023364',\n",
       "   '3034094473']},\n",
       " {'id': '3010930696',\n",
       "  'title': 'Hydroxychloroquine and azithromycin as a treatment of COVID-19: results of an open-label non-randomized clinical trial.',\n",
       "  'abstract': 'Background  Chloroquine and hydroxychloroquine have been found to be efficient on SARS-CoV-2, and reported to be efficient in Chinese COV-19 patients. We evaluate the role of hydroxychloroquine on respiratory viral loads.  Patients and methods  French Confirmed COVID-19 patients were included in a single arm protocol from early March to March 16th, to receive 600mg of hydroxychloroquine daily and their viral load in nasopharyngeal swabs was tested daily in a hospital setting. Depending on their clinical presentation, azithromycin was added to the treatment. Untreated patients from another center and cases refusing the protocol were included as negative controls. Presence and absence of virus at Day6-post inclusion was considered the end point.  Results  Six patients were asymptomatic, 22 had upper respiratory tract infection symptoms and eight had lower respiratory tract infection symptoms. Twenty cases were treated in this study and showed a significant reduction of the viral carriage at D6-post inclusion compared to controls, and much lower average carrying duration than reported of untreated patients in the literature. Azithromycin added to hydroxychloroquine was significantly more efficient for virus elimination.  Conclusion  Despite its small sample size our survey shows that hydroxychloroquine treatment is significantly associated with viral load reduction/disappearance in COVID-19 patients and its effect is reinforced by azithromycin.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Philippe Gautret',\n",
       "   'Jean Christophe Lagier',\n",
       "   'Philippe Parola',\n",
       "   'Van Thuan Hoang',\n",
       "   'Line Meddeb',\n",
       "   'Morgane Mailhe',\n",
       "   'Barbara Doudier',\n",
       "   'Johan Courjon',\n",
       "   'Valérie Giordanengo',\n",
       "   'Vera Esteves Vieira',\n",
       "   'Hervé Tissot Dupont',\n",
       "   'Stéphane Honoré',\n",
       "   'Philippe Colson',\n",
       "   'Eric Chabrière',\n",
       "   'Bernard La Scola',\n",
       "   'Jean Marc Rolain',\n",
       "   'Philippe Brouqui',\n",
       "   'Didier Raoult'],\n",
       "  'related_topics': ['Hydroxychloroquine',\n",
       "   'Viral load',\n",
       "   'Lower respiratory tract infection',\n",
       "   'Upper respiratory tract infection',\n",
       "   'Azithromycin',\n",
       "   'Asymptomatic',\n",
       "   'Randomized controlled trial',\n",
       "   'Clinical trial',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,339',\n",
       "  'reference_count': '30',\n",
       "  'references': ['3009885589',\n",
       "   '3008028633',\n",
       "   '3005212621',\n",
       "   '3006645647',\n",
       "   '3009577418',\n",
       "   '3008763357',\n",
       "   '3010277308',\n",
       "   '2302013022',\n",
       "   '3024400578',\n",
       "   '3011032288']},\n",
       " {'id': '3014294089',\n",
       "  'title': 'Baseline Characteristics and Outcomes of 1591 Patients Infected With SARS-CoV-2 Admitted to ICUs of the Lombardy Region, Italy.',\n",
       "  'abstract': 'Importance  In December 2019, a novel coronavirus (severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) emerged in China and has spread globally, creating a pandemic. Information about the clinical characteristics of infected patients who require intensive care is limited.  Objective  To characterize patients with coronavirus disease 2019 (COVID-19) requiring treatment in an intensive care unit (ICU) in the Lombardy region of Italy.  Design, Setting, and Participants  Retrospective case series of 1591 consecutive patients with laboratory-confirmed COVID-19 referred for ICU admission to the coordinator center (Fondazione IRCCS Ca’ Granda Ospedale Maggiore Policlinico, Milan, Italy) of the COVID-19 Lombardy ICU Network and treated at one of the ICUs of the 72 hospitals in this network between February 20 and March 18, 2020. Date of final follow-up was March 25, 2020.  Exposures  SARS-CoV-2 infection confirmed by real-time reverse transcriptase–polymerase chain reaction (RT-PCR) assay of nasal and pharyngeal swabs.  Main Outcomes and Measures  Demographic and clinical data were collected, including data on clinical management, respiratory failure, and patient mortality. Data were recorded by the coordinator center on an electronic worksheet during telephone calls by the staff of the COVID-19 Lombardy ICU Network.  Results  Of the 1591 patients included in the study, the median (IQR) age was 63 (56-70) years and 1304 (82%) were male. Of the 1043 patients with available data, 709 (68%) had at least 1 comorbidity and 509 (49%) had hypertension. Among 1300 patients with available respiratory support data, 1287 (99% [95% CI, 98%-99%]) needed respiratory support, including 1150 (88% [95% CI, 87%-90%]) who received mechanical ventilation and 137 (11% [95% CI, 9%-12%]) who received noninvasive ventilation. The median positive end-expiratory pressure (PEEP) was 14 (IQR, 12-16) cm H2O, and Fio2was greater than 50% in 89% of patients. The median Pao2/Fio2was 160 (IQR, 114-220). The median PEEP level was not different between younger patients (n\\u2009=\\u2009503 aged ≤63 years) and older patients (n\\u2009=\\u2009514 aged ≥64 years) (14 [IQR, 12-15] vs 14 [IQR, 12-16] cm H2O, respectively; median difference, 0 [95% CI, 0-0];P\\u2009=\\u2009.94). Median Fio2was lower in younger patients: 60% (IQR, 50%-80%) vs 70% (IQR, 50%-80%) (median difference, −10% [95% CI, −14% to 6%];P\\u2009=\\u2009.006), and median Pao2/Fio2was higher in younger patients: 163.5 (IQR, 120-230) vs 156 (IQR, 110-205) (median difference, 7 [95% CI, −8 to 22];P\\u2009=\\u2009.02). Patients with hypertension (n\\u2009=\\u2009509) were older than those without hypertension (n\\u2009=\\u2009526) (median [IQR] age, 66 years [60-72] vs 62 years [54-68];P\\u2009  Conclusions and Relevance  In this case series of critically ill patients with laboratory-confirmed COVID-19 admitted to ICUs in Lombardy, Italy, the majority were older men, a large proportion required mechanical ventilation and high levels of PEEP, and ICU mortality was 26%.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Giacomo Grasselli',\n",
       "   '',\n",
       "   'Alberto Zangrillo',\n",
       "   'Alberto Zanella',\n",
       "   '',\n",
       "   'Massimo Antonelli',\n",
       "   '',\n",
       "   'Luca Cabrini',\n",
       "   'Antonio Castelli',\n",
       "   'Danilo Cereda',\n",
       "   'Antonio Coluccello',\n",
       "   'Giuseppe Foti',\n",
       "   'Roberto Fumagalli',\n",
       "   'Giorgio Iotti',\n",
       "   'Nicola Latronico',\n",
       "   'Luca Lorini',\n",
       "   'Stefano Merler',\n",
       "   'Giuseppe Natalini',\n",
       "   'Alessandra Piatti',\n",
       "   'Marco Vito Ranieri',\n",
       "   'Anna Mara Scandroglio',\n",
       "   'Enrico Storti',\n",
       "   'Maurizio Cecconi',\n",
       "   'Antonio Pesenti',\n",
       "   ''],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Intensive care unit',\n",
       "   'Respiratory failure',\n",
       "   'Mechanical ventilation',\n",
       "   'Retrospective cohort study',\n",
       "   'Artificial ventilation',\n",
       "   'Positive end-expiratory pressure',\n",
       "   'Comorbidity',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,036',\n",
       "  'reference_count': '9',\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3009885589',\n",
       "   '3008090866',\n",
       "   '3011508296',\n",
       "   '3011559677',\n",
       "   '3014538785',\n",
       "   '2158525457']},\n",
       " {'id': '2120419212',\n",
       "  'title': 'A discriminatively trained, multiscale, deformable part model',\n",
       "  'abstract': 'This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.',\n",
       "  'date': '2008',\n",
       "  'authors': ['P. Felzenszwalb', 'D. McAllester', 'D. Ramanan'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Support vector machine',\n",
       "   'Discriminative model',\n",
       "   'Pascal (programming language)',\n",
       "   'Object detection',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Histogram',\n",
       "   'Computer science',\n",
       "   'Grammar',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,069',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2161969291',\n",
       "   '2154422044',\n",
       "   '1576520375',\n",
       "   '2030536784',\n",
       "   '2112020727',\n",
       "   '2186094539',\n",
       "   '2101534792',\n",
       "   '1518641734',\n",
       "   '1970255615',\n",
       "   '2166770390']},\n",
       " {'id': '2152826865',\n",
       "  'title': 'Active appearance models',\n",
       "  'abstract': 'We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.',\n",
       "  'date': '2001',\n",
       "  'authors': ['T.F. Cootes', 'G.J. Edwards', 'C.J. Taylor'],\n",
       "  'related_topics': ['Active appearance model',\n",
       "   'Active shape model',\n",
       "   'Point distribution model',\n",
       "   'Statistical model',\n",
       "   'Matching (statistics)',\n",
       "   'Iterative method',\n",
       "   'Image segmentation',\n",
       "   'Blossom algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,255',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2138451337',\n",
       "   '2152826865',\n",
       "   '2038952578',\n",
       "   '2408227189',\n",
       "   '2095757522',\n",
       "   '2103876808',\n",
       "   '2159173611',\n",
       "   '2129150631',\n",
       "   '2293264518',\n",
       "   '2133001582']},\n",
       " {'id': '2145072179',\n",
       "  'title': 'PCA-SIFT: a more distinctive representation for local image descriptors',\n",
       "  'abstract': \"Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.\",\n",
       "  'date': '2004',\n",
       "  'authors': ['Yan Ke', 'R. Sukthankar'],\n",
       "  'related_topics': ['Principal curvature-based region detector',\n",
       "   'Feature detection (computer vision)',\n",
       "   'GLOH',\n",
       "   'Feature (computer vision)',\n",
       "   'Image gradient',\n",
       "   'Feature extraction',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Image processing',\n",
       "   'Image retrieval',\n",
       "   'Image registration',\n",
       "   'Histogram',\n",
       "   'Principal component analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Normalization (statistics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,915',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2151103935',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '1902027874',\n",
       "   '2154422044',\n",
       "   '2148694408',\n",
       "   '2119747362',\n",
       "   '2111308925',\n",
       "   '2098693229',\n",
       "   '1541642243']},\n",
       " {'id': '1576520375',\n",
       "  'title': 'Making large scale SVM learning practical',\n",
       "  'abstract': 'Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Thorsten'],\n",
       "  'related_topics': ['Ranking SVM',\n",
       "   'Support vector machine',\n",
       "   'Quadratic programming',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Quadratic equation',\n",
       "   'Constraint (information theory)',\n",
       "   'Scale (chemistry)',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,933',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2115763357',\n",
       "  'title': 'A general framework for object detection',\n",
       "  'abstract': 'This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.',\n",
       "  'date': '1998',\n",
       "  'authors': ['C.P. Papageorgiou', 'M. Oren', 'T. Poggio'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Object detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Face detection',\n",
       "   'Representation (systemics)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Wavelet',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,117',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2132984323',\n",
       "   '2087347434',\n",
       "   '2124351082',\n",
       "   '2125848778',\n",
       "   '2104671481',\n",
       "   '2159173611',\n",
       "   '2137346077',\n",
       "   '2056695679',\n",
       "   '1676612073',\n",
       "   '2030989822']},\n",
       " {'id': '2161381512',\n",
       "  'title': 'Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks',\n",
       "  'abstract': 'Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large- scale visual recognition challenge (ILSVRC2012). The suc- cess of CNNs is attributed to their ability to learn rich mid- level image representations as opposed to hand-designed low-level features used in other image classification meth- ods. Learning CNNs, however, amounts to estimating mil- lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi- ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep- resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Maxime Oquab', '', 'Leon Bottou', 'Ivan Laptev', 'Josef Sivic'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Contextual image classification',\n",
       "   'Pascal (programming language)',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,959',\n",
       "  'reference_count': '53',\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '2031489346']},\n",
       " {'id': '2166851633',\n",
       "  'title': 'Stochastic variational inference',\n",
       "  'abstract': 'We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Matthew D. Hoffman',\n",
       "   'David M. Blei',\n",
       "   'Chong Wang',\n",
       "   'John Paisley'],\n",
       "  'related_topics': ['Frequentist inference',\n",
       "   'Variational message passing',\n",
       "   'Fiducial inference',\n",
       "   'Predictive inference',\n",
       "   'Inference',\n",
       "   'Statistical inference',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Bayesian inference',\n",
       "   'Theoretical computer science',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,963',\n",
       "  'reference_count': '97',\n",
       "  'references': ['1880262756',\n",
       "   '1503398984',\n",
       "   '2125838338',\n",
       "   '1506806321',\n",
       "   '1511986666',\n",
       "   '1981457167',\n",
       "   '2159080219',\n",
       "   '2001082470',\n",
       "   '2174706414',\n",
       "   '2158266063']},\n",
       " {'id': '2097268041',\n",
       "  'title': 'Deep AutoRegressive Networks',\n",
       "  'abstract': 'We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Karol Gregor',\n",
       "   'Ivo Danihelka',\n",
       "   'Andriy Mnih',\n",
       "   'Charles Blundell',\n",
       "   'Daan Wierstra'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Approximate inference',\n",
       "   'Feedforward neural network',\n",
       "   'Autoregressive model',\n",
       "   'MNIST database',\n",
       "   'Estimation theory',\n",
       "   'Upper and lower bounds',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '223',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2310919327',\n",
       "   '3120740533',\n",
       "   '2025768430',\n",
       "   '3140968660',\n",
       "   '1810943226',\n",
       "   '2952509347',\n",
       "   '189596042',\n",
       "   '2096192494',\n",
       "   '2108677974',\n",
       "   '2134842679']},\n",
       " {'id': '2963173382',\n",
       "  'title': 'Black Box Variational Inference',\n",
       "  'abstract': 'Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires signicant model-specic analysis. These eorts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a \\\\black box\" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid dicult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We nd that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Rajesh Ranganath', 'Sean Gerrish', 'David M. Blei'],\n",
       "  'related_topics': ['Inference',\n",
       "   'Black box',\n",
       "   'Stochastic optimization',\n",
       "   'Monte Carlo method',\n",
       "   'Sampling (statistics)',\n",
       "   'Latent variable',\n",
       "   'Algorithm',\n",
       "   'Variance (accounting)',\n",
       "   'Distribution (mathematics)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '799',\n",
       "  'reference_count': '20',\n",
       "  'references': ['1663973292',\n",
       "   '1959608418',\n",
       "   '2146502635',\n",
       "   '2166851633',\n",
       "   '2120340025',\n",
       "   '1516111018',\n",
       "   '2951493172',\n",
       "   '2128925311',\n",
       "   '2119196781',\n",
       "   '3104819538']},\n",
       " {'id': '2951493172',\n",
       "  'title': 'Variational Bayesian Inference with Stochastic Search',\n",
       "  'abstract': 'Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.',\n",
       "  'date': '2012',\n",
       "  'authors': ['John Paisley', 'David Blei', 'Michael Jordan'],\n",
       "  'related_topics': ['Bayesian inference',\n",
       "   'Stochastic optimization',\n",
       "   'Marginal likelihood',\n",
       "   'Upper and lower bounds',\n",
       "   'Posterior probability',\n",
       "   'Control variates',\n",
       "   'Inference',\n",
       "   'Bayesian probability',\n",
       "   'Applied mathematics',\n",
       "   'Logistic regression',\n",
       "   'Computer science',\n",
       "   'Joint likelihood',\n",
       "   'View Less'],\n",
       "  'citation_count': '306',\n",
       "  'reference_count': '10',\n",
       "  'references': ['1880262756',\n",
       "   '2158266063',\n",
       "   '2108677974',\n",
       "   '2115979064',\n",
       "   '2127498532',\n",
       "   '2187741934',\n",
       "   '2117111086',\n",
       "   '1496451467',\n",
       "   '2142508340',\n",
       "   '2162995719']},\n",
       " {'id': '2171490498',\n",
       "  'title': 'Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition',\n",
       "  'abstract': 'Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Koray Kavukcuoglu', \"Marc'Aurelio Ranzato\", 'Yann LeCun'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   '3D single-object recognition',\n",
       "   'Neural coding',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Basis function',\n",
       "   'Representation (systemics)',\n",
       "   'Pattern recognition',\n",
       "   'Algorithm',\n",
       "   'Image (mathematics)',\n",
       "   'Inference',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '305',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2310919327',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2063978378',\n",
       "   '2078204800',\n",
       "   '2166049352',\n",
       "   '2151693816',\n",
       "   '2113606819',\n",
       "   '2154332973',\n",
       "   '2139427956']},\n",
       " {'id': '2119196781',\n",
       "  'title': 'Variational Bayesian Inference with Stochastic Search',\n",
       "  'abstract': 'Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.',\n",
       "  'date': '2012',\n",
       "  'authors': ['David M. Blei', 'Michael I. Jordan', 'John W. Paisley'],\n",
       "  'related_topics': ['Bayesian linear regression',\n",
       "   'Bayesian inference',\n",
       "   'Stochastic optimization',\n",
       "   'Marginal likelihood',\n",
       "   'Upper and lower bounds',\n",
       "   'Posterior probability',\n",
       "   'Control variates',\n",
       "   'Inference',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '404',\n",
       "  'reference_count': '14',\n",
       "  'references': ['1880262756',\n",
       "   '2158266063',\n",
       "   '2108677974',\n",
       "   '1516111018',\n",
       "   '2165599843',\n",
       "   '2115979064',\n",
       "   '2127498532',\n",
       "   '2187741934',\n",
       "   '2117111086',\n",
       "   '1496451467']},\n",
       " {'id': '3104819538',\n",
       "  'title': 'Fixed-form variational posterior approximation through stochastic linear regression',\n",
       "  'abstract': 'textabstractWe propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribu- tion. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approxi- mation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several exam- ples illustrate the speed and accuracy of our approximation method in practice.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Tim', 'David A.'],\n",
       "  'related_topics': ['Exponential family',\n",
       "   'Posterior probability',\n",
       "   'Stochastic approximation',\n",
       "   'Divergence (statistics)',\n",
       "   'Approximate inference',\n",
       "   'Distribution (mathematics)',\n",
       "   'Bayesian probability',\n",
       "   'Linear regression',\n",
       "   'Applied mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '187',\n",
       "  'reference_count': '37',\n",
       "  'references': ['1663973292',\n",
       "   '114517082',\n",
       "   '2166851633',\n",
       "   '2120340025',\n",
       "   '1992208280',\n",
       "   '1516111018',\n",
       "   '1545319692',\n",
       "   '2165599843',\n",
       "   '3125096521',\n",
       "   '1515272691']},\n",
       " {'id': '4919037',\n",
       "  'title': 'Regularization of Neural Networks using DropConnect',\n",
       "  'abstract': 'We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Li Wan',\n",
       "   'Matthew Zeiler',\n",
       "   'Sixin Zhang',\n",
       "   'Yann Le Cun',\n",
       "   'Rob Fergus'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Pattern recognition',\n",
       "   'Regularization (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,315',\n",
       "  'reference_count': '13',\n",
       "  'references': ['3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '1665214252',\n",
       "   '2131241448',\n",
       "   '2335728318',\n",
       "   '2141125852',\n",
       "   '2134557905',\n",
       "   '2963574257',\n",
       "   '188867022']},\n",
       " {'id': '2206858481',\n",
       "  'title': 'Visualizing and Understanding Convolutional Neural Networks',\n",
       "  'abstract': 'Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\\\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Matthew D', 'Rob'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Softmax function',\n",
       "   'Classifier (linguistics)',\n",
       "   'Benchmark (computing)',\n",
       "   'Feature (machine learning)',\n",
       "   'Network model',\n",
       "   'Visualization',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Function (engineering)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '474',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2120480077',\n",
       "  'title': 'Building high-level features using large scale unsupervised learning',\n",
       "  'abstract': 'We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Quoc V. Le'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Unsupervised learning',\n",
       "   'Object detection',\n",
       "   'Facial recognition system',\n",
       "   'Detector',\n",
       "   'Pattern recognition',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,551',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2108598243',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2168231600',\n",
       "   '2546302380',\n",
       "   '2110798204',\n",
       "   '1782590233',\n",
       "   '2130325614']},\n",
       " {'id': '2150165932',\n",
       "  'title': 'How to Explain Individual Classification Decisions',\n",
       "  'abstract': 'After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.',\n",
       "  'date': '2010',\n",
       "  'authors': ['David Baehrens',\n",
       "   'Timon Schroeter',\n",
       "   'Stefan Harmeling',\n",
       "   'Motoaki Kawanabe',\n",
       "   'Katja Hansen',\n",
       "   'Klaus-Robert Müller'],\n",
       "  'related_topics': ['Decision tree',\n",
       "   'Classifier (UML)',\n",
       "   'Black box',\n",
       "   'Kernel method',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Classification methods',\n",
       "   'View Less'],\n",
       "  'citation_count': '668',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2156909104',\n",
       "   '1746819321',\n",
       "   '1554663460',\n",
       "   '1480376833',\n",
       "   '2119479037',\n",
       "   '3023786531',\n",
       "   '740415',\n",
       "   '2108995755',\n",
       "   '1618905105',\n",
       "   '1564947197']},\n",
       " {'id': '1948751323',\n",
       "  'title': 'Hypercolumns for object segmentation and fine-grained localization',\n",
       "  'abstract': 'Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Bharath Hariharan',\n",
       "   'Pablo Arbelaez',\n",
       "   'Ross Girshick',\n",
       "   'Jitendra Malik'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Pixel',\n",
       "   'Feature (computer vision)',\n",
       "   'Feature extraction',\n",
       "   'Segmentation',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Artificial neural network',\n",
       "   'Computer vision',\n",
       "   'Point (geometry)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,476',\n",
       "  'reference_count': '42',\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2102605133',\n",
       "   '1903029394',\n",
       "   '2168356304',\n",
       "   '2109255472',\n",
       "   '2156303437',\n",
       "   '2022508996',\n",
       "   '2118585731',\n",
       "   '1507506748']},\n",
       " {'id': '2167510172',\n",
       "  'title': 'Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images',\n",
       "  'abstract': 'We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 × 512 × 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Dan Ciresan',\n",
       "   'Alessandro Giusti',\n",
       "   'Luca M. Gambardella',\n",
       "   'Jürgen Schmidhuber'],\n",
       "  'related_topics': ['Pixel',\n",
       "   'Ground truth',\n",
       "   'Artificial neural network',\n",
       "   'Segmentation',\n",
       "   'Gradient descent',\n",
       "   'Image warping',\n",
       "   'Computer vision',\n",
       "   'Classifier (UML)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,455',\n",
       "  'reference_count': '28',\n",
       "  'references': ['2310919327',\n",
       "   '2141125852',\n",
       "   '2143516773',\n",
       "   '2156163116',\n",
       "   '2148461049',\n",
       "   '1624854622',\n",
       "   '2132424367',\n",
       "   '1969013163',\n",
       "   '1523493493',\n",
       "   '2149194912']},\n",
       " {'id': '1893585201',\n",
       "  'title': 'Learning to generate chairs with convolutional neural networks',\n",
       "  'abstract': 'We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Alexey Dosovitskiy', 'Jost Tobias Springenberg', 'Thomas Brox'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Similarity (geometry)',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '682',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2099471712',\n",
       "   '2155893237',\n",
       "   '2136922672',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '1959608418',\n",
       "   '2100495367',\n",
       "   '2155541015']},\n",
       " {'id': '2148349024',\n",
       "  'title': 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks',\n",
       "  'abstract': \"Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).\",\n",
       "  'date': '2014',\n",
       "  'authors': ['Alexey Dosovitskiy',\n",
       "   'Jost Tobias Springenberg',\n",
       "   'Martin Riedmiller',\n",
       "   'Thomas Brox'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Unsupervised learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Convolutional neural network',\n",
       "   'Feature learning',\n",
       "   'Feature (machine learning)',\n",
       "   'Discriminative model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '657',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2155893237',\n",
       "   '1849277567',\n",
       "   '2109255472',\n",
       "   '3118608800',\n",
       "   '1904365287',\n",
       "   '2155541015',\n",
       "   '2158899491',\n",
       "   '2963911037']},\n",
       " {'id': '2159269332',\n",
       "  'title': 'A universal image quality index',\n",
       "  'abstract': 'We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality_index/demo.html.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Zhou Wang', 'A.C. Bovik'],\n",
       "  'related_topics': ['Distortion',\n",
       "   'Image quality',\n",
       "   'Image processing',\n",
       "   'Human visual system model',\n",
       "   'Mean squared error',\n",
       "   'Signal-to-noise ratio',\n",
       "   'Metric (mathematics)',\n",
       "   'Luminance',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,955',\n",
       "  'reference_count': '3',\n",
       "  'references': ['2153777140', '2912116903', '1543242897']},\n",
       " {'id': '2142276208',\n",
       "  'title': 'A new, fast, and efficient image codec based on set partitioning in hierarchical trees',\n",
       "  'abstract': 'Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.',\n",
       "  'date': '1996',\n",
       "  'authors': ['A. Said', 'W.A. Pearlman'],\n",
       "  'related_topics': ['Set partitioning in hierarchical trees',\n",
       "   'Data compression',\n",
       "   'Entropy encoding',\n",
       "   'Image compression',\n",
       "   'Transform coding',\n",
       "   'Wavelet transform',\n",
       "   'Bit plane',\n",
       "   'Image processing',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,615',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2053691921',\n",
       "   '2148593155',\n",
       "   '2103504761',\n",
       "   '2129652681',\n",
       "   '1931641413',\n",
       "   '2166087152',\n",
       "   '2058719583',\n",
       "   '1592970628',\n",
       "   '2147399030',\n",
       "   '2117465325']},\n",
       " {'id': '2118217749',\n",
       "  'title': 'JPEG2000 : image compression fundamentals, standards, and practice',\n",
       "  'abstract': 'This is nothing less than a totally essential reference for engineers and researchers in any field of work that involves the use of compressed imagery. Beginning with a thorough and up-to-date overview of the fundamentals of image compression, the authors move on to provide a complete description of the JPEG2000 standard. They then devote space to the implementation and exploitation of that standard. The final section describes other key image compression systems. This work has specific applications for those involved in the development of software and hardware solutions for multimedia, internet, and medical imaging applications.',\n",
       "  'date': '2001',\n",
       "  'authors': ['David S. Taubman', 'Michael W.'],\n",
       "  'related_topics': ['Image compression',\n",
       "   'The Internet',\n",
       "   'Software',\n",
       "   'JPEG 2000',\n",
       "   'JPIP',\n",
       "   'Multimedia',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Work (electrical)',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,971',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2053691921',\n",
       "  'title': 'Embedded image coding using zerotrees of wavelet coefficients',\n",
       "  'abstract': 'The embedded zerotree wavelet algorithm (EZW) is a simple, yet remarkably effective, image compression algorithm, having the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. The embedded code represents a sequence of binary decisions that distinguish an image from the \"null\" image. Using an embedded coding algorithm, an encoder can terminate the encoding at any point thereby allowing a target rate or target distortion metric to be met exactly. Also, given a bit stream, the decoder can cease decoding at any point in the bit stream and still produce exactly the same image that would have been encoded at the bit rate corresponding to the truncated bit stream. In addition to producing a fully embedded bit stream, the EZW consistently produces compression results that are competitive with virtually all known compression algorithms on standard test images. Yet this performance is achieved with a technique that requires absolutely no training, no pre-stored tables or codebooks, and requires no prior knowledge of the image source. The EZW algorithm is based on four key concepts: (1) a discrete wavelet transform or hierarchical subband decomposition, (2) prediction of the absence of significant information across scales by exploiting the self-similarity inherent in images, (3) entropy-coded successive-approximation quantization, and (4) universal lossless data compression which is achieved via adaptive arithmetic coding. >',\n",
       "  'date': '1993',\n",
       "  'authors': ['J.M. Shapiro'],\n",
       "  'related_topics': ['Data compression',\n",
       "   'Set partitioning in hierarchical trees',\n",
       "   'Lossless compression',\n",
       "   'Arithmetic coding',\n",
       "   'Signal compression',\n",
       "   'Wavelet transform',\n",
       "   'Binary image',\n",
       "   'Discrete wavelet transform',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,975',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2132984323',\n",
       "   '2098914003',\n",
       "   '2140196014',\n",
       "   '1996021349',\n",
       "   '2156447271',\n",
       "   '1970352604',\n",
       "   '2103504761',\n",
       "   '2129652681',\n",
       "   '2166982406',\n",
       "   '2186435531']},\n",
       " {'id': '2153777140',\n",
       "  'title': 'Image quality measures and their performance',\n",
       "  'abstract': \"A number of quality measures are evaluated for gray scale image compression. They are all bivariate, exploiting the differences between corresponding pixels in the original and degraded images. It is shown that although some numerical measures correlate well with the observers' response for a given compression technique, they are not reliable for an evaluation across different techniques. A graphical measure called Hosaka plots, however, can be used to appropriately specify not only the amount, but also the type of degradation in reconstructed images.\",\n",
       "  'date': '1995',\n",
       "  'authors': ['A.M. Eskicioglu', 'P.S. Fisher'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Image compression',\n",
       "   'Data compression',\n",
       "   'Image resolution',\n",
       "   'Grayscale',\n",
       "   'Pixel',\n",
       "   'Transform coding',\n",
       "   'Histogram',\n",
       "   'Iterative reconstruction',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,002',\n",
       "  'reference_count': '3',\n",
       "  'references': ['3021180913', '1487065163', '2170745401']},\n",
       " {'id': '2912116903',\n",
       "  'title': 'Image dissimilarity',\n",
       "  'abstract': '',\n",
       "  'date': '1998',\n",
       "  'authors': ['Jean-Bernard', 'Lydia'],\n",
       "  'related_topics': ['Image (mathematics)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '141',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2103504761',\n",
       "   '1991605728',\n",
       "   '2103232506',\n",
       "   '2134774992',\n",
       "   '42232744',\n",
       "   '1990873664',\n",
       "   '2108657140',\n",
       "   '1551978325',\n",
       "   '2056930330',\n",
       "   '2118491738']},\n",
       " {'id': '2107790757',\n",
       "  'title': 'Shiftable multiscale transforms',\n",
       "  'abstract': 'One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >',\n",
       "  'date': '1992',\n",
       "  'authors': ['E.P. Simoncelli',\n",
       "   'W.T. Freeman',\n",
       "   'E.H. Adelson',\n",
       "   'D.J. Heeger'],\n",
       "  'related_topics': ['Wavelet',\n",
       "   'Orthogonal wavelet',\n",
       "   'Wavelet transform',\n",
       "   'Stereophotography',\n",
       "   'Nyquist–Shannon sampling theorem',\n",
       "   'Image processing',\n",
       "   'Orthogonal transformation',\n",
       "   'Signal processing',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,960',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2170120409',\n",
       "   '2132984323',\n",
       "   '2098914003',\n",
       "   '1996021349',\n",
       "   '2103504761',\n",
       "   '1991605728',\n",
       "   '2118877769',\n",
       "   '2109863423',\n",
       "   '2166982406',\n",
       "   '1627054999']},\n",
       " {'id': '2158564760',\n",
       "  'title': 'Why is image quality assessment so difficult',\n",
       "  'abstract': 'Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in recent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the literature. Furthermore, we propose a new philosophy in designing image quality metrics: The main function of the human eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approximation of perceived image distortion. Based on the new philosophy, we implemented a simple but effective image quality indexing algorithm, which is very promising as shown by our current results.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Zhou Wang', 'Alan C. Bovik', 'Ligang Lu'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Subjective video quality',\n",
       "   'Image processing',\n",
       "   'Human visual system model',\n",
       "   'Distortion',\n",
       "   'Search engine indexing',\n",
       "   'Field (computer science)',\n",
       "   'Machine learning',\n",
       "   'Function (engineering)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '926',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2159269332',\n",
       "   '2153777140',\n",
       "   '2912116903',\n",
       "   '2145792107',\n",
       "   '1543242897',\n",
       "   '2015937190',\n",
       "   '1527289589',\n",
       "   '42232744',\n",
       "   '2029826041',\n",
       "   '1990873664']},\n",
       " {'id': '2124731682',\n",
       "  'title': 'Image compression via joint statistical characterization in the wavelet domain',\n",
       "  'abstract': 'We develop a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, are found to be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which subband coefficients are encoded one bitplane at a time using a nonadaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature.',\n",
       "  'date': '1999',\n",
       "  'authors': ['R.W. Buccigrossi', 'E.P. Simoncelli'],\n",
       "  'related_topics': ['Wavelet transform',\n",
       "   'Wavelet',\n",
       "   'Statistical model',\n",
       "   'Image processing',\n",
       "   'Image compression',\n",
       "   'Data compression',\n",
       "   'Markov model',\n",
       "   'Transform coding',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '779',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2132984323',\n",
       "   '2151693816',\n",
       "   '2053691921',\n",
       "   '2408227189',\n",
       "   '2148593155',\n",
       "   '2156447271',\n",
       "   '2103504761',\n",
       "   '1490632837',\n",
       "   '2107790757',\n",
       "   '2180838288']},\n",
       " {'id': '2115838129',\n",
       "  'title': 'Linear transform for simultaneous diagonalization of covariance and perceptual metric matrix in image coding',\n",
       "  'abstract': 'Two types ofredundancies are contained in images: statistical redundancy and psychovisual redundancy. Image representation techniques for image coding should remove both redundancies in order to obtain good results. In order to establish an appropriate representation, the standard approach to transform coding only considers the statistical redundancy, whereas the psychovisual factors are introduced after the selection ofthe representation as a simple scalar weighting in the transform domain. In this work, we take into account the psychovisual factors in the de8nition of the representation together with the statistical factors, by means of the perceptual metric and the covariance matrix, respectively. In general the ellipsoids described by these matrices are not aligned. Therefore, the optimal basis for image representation should simultaneously diagonalize both matrices. This approach to the basis selection problem has several advantages in the particular application ofimage coding. As the transform domain is Euclidean (by de8nition), the quantizer design is highly simpli8ed and at the same time, the use ofscalar quantizers is truly justi8ed. The proposed representation is compared to covariance-based representations such as the DCT and the KLT or PCA using standard JPEG-like and Max-Lloyd quantizers. ? 2003 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Irene Epifanio', 'Jaime Gutierrez', 'Jesus Malo'],\n",
       "  'related_topics': ['Transform coding',\n",
       "   'Covariance matrix',\n",
       "   'Covariance',\n",
       "   'Image compression',\n",
       "   'Discrete cosine transform',\n",
       "   'Matrix (mathematics)',\n",
       "   'Redundancy (information theory)',\n",
       "   'Weighting',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '28',\n",
       "  'reference_count': '31',\n",
       "  'references': ['1548802052',\n",
       "   '2798909945',\n",
       "   '2140196014',\n",
       "   '1634005169',\n",
       "   '2145889472',\n",
       "   '3017143921',\n",
       "   '2137234026',\n",
       "   '2134383396',\n",
       "   '98769269',\n",
       "   '1500256440']},\n",
       " {'id': '2131975293',\n",
       "  'title': 'Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing',\n",
       "  'abstract': 'We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Matei Zaharia',\n",
       "   'Mosharaf Chowdhury',\n",
       "   'Tathagata Das',\n",
       "   'Ankur Dave',\n",
       "   'Justin Ma',\n",
       "   'Murphy McCauley',\n",
       "   'Michael J. Franklin',\n",
       "   'Scott Shenker',\n",
       "   'Ion Stoica'],\n",
       "  'related_topics': ['Distributed memory',\n",
       "   'Shared memory',\n",
       "   'Computer cluster',\n",
       "   'Fault tolerance',\n",
       "   'Programming paradigm',\n",
       "   'Spark (mathematics)',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Distributed computing',\n",
       "   'State (computer science)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,211',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2173213060',\n",
       "   '1554944419',\n",
       "   '3013264884',\n",
       "   '2170616854',\n",
       "   '2100830825',\n",
       "   '2098935637',\n",
       "   '2096125134',\n",
       "   '2163961697',\n",
       "   '2060204338',\n",
       "   '2109722477']},\n",
       " {'id': '2125389028',\n",
       "  'title': 'Conditional Generative Adversarial Nets',\n",
       "  'abstract': 'Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Mehdi', 'Simon'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Generative Design',\n",
       "   'MNIST database',\n",
       "   'Generative grammar',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Generator (mathematics)',\n",
       "   'Class (computer programming)',\n",
       "   'Computer science',\n",
       "   'Image translation',\n",
       "   'Image (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,725',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2097117768',\n",
       "   '2099471712',\n",
       "   '1614298861',\n",
       "   '1904365287',\n",
       "   '2546302380',\n",
       "   '2294059674',\n",
       "   '2123024445',\n",
       "   '2951446714',\n",
       "   '154472438',\n",
       "   '1496559305']},\n",
       " {'id': '2962897886',\n",
       "  'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models',\n",
       "  'abstract': 'We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation - rules for gradient backpropagation through stochastic variables - and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Danilo Jimenez Rezende', 'Shakir Mohamed', 'Daan Wierstra'],\n",
       "  'related_topics': ['Approximate inference',\n",
       "   'Bayesian inference',\n",
       "   'Backpropagation',\n",
       "   'Inference',\n",
       "   'Missing data',\n",
       "   'Posterior probability',\n",
       "   'Algorithm',\n",
       "   'Upper and lower bounds',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,173',\n",
       "  'reference_count': '33',\n",
       "  'references': ['1959608418',\n",
       "   '2145094598',\n",
       "   '2335728318',\n",
       "   '2166851633',\n",
       "   '2951446714',\n",
       "   '2044758663',\n",
       "   '2108677974',\n",
       "   '2097268041',\n",
       "   '2963173382',\n",
       "   '2167433878']},\n",
       " {'id': '2136504847',\n",
       "  'title': 'Semi-Supervised Learning Literature Survey',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': ['Xiaojin'],\n",
       "  'related_topics': ['Literature survey',\n",
       "   'Semi-supervised learning',\n",
       "   'Co-training',\n",
       "   'Technical report',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,476',\n",
       "  'reference_count': '157',\n",
       "  'references': ['1880262756',\n",
       "   '2148603752',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328',\n",
       "   '2125838338',\n",
       "   '2165874743',\n",
       "   '2097308346',\n",
       "   '2114524997',\n",
       "   '1479807131']},\n",
       " {'id': '1676820704',\n",
       "  'title': 'Solving multiclass learning problems via error-correcting output codes',\n",
       "  'abstract': 'Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Thomas G. Dietterich', 'Ghulum Bakiri'],\n",
       "  'related_topics': ['Multiclass classification',\n",
       "   'Multi-task learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Overfitting',\n",
       "   'Pruning (decision trees)',\n",
       "   'Generalization',\n",
       "   'Backpropagation',\n",
       "   'Concept learning',\n",
       "   'Theoretical computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,561',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2154642048',\n",
       "   '1594031697',\n",
       "   '2147800946',\n",
       "   '2173629880',\n",
       "   '2019363670',\n",
       "   '2093717447',\n",
       "   '3036751298',\n",
       "   '2176028050',\n",
       "   '1667614912',\n",
       "   '1980501707']},\n",
       " {'id': '2407712691',\n",
       "  'title': 'Deep Learning via Semi-Supervised Embedding',\n",
       "  'abstract': 'We show how nonlinear embedding algorithms popular for use with \"shallow\" semi-supervised learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This trick provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Jason', 'Frédéric', 'Hossein', 'Ronan'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Kernel method',\n",
       "   'Embedding',\n",
       "   'Deep learning',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear embedding',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,016',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2001141328',\n",
       "   '2097308346',\n",
       "   '1479807131',\n",
       "   '2139427956',\n",
       "   '2914746235',\n",
       "   '2159291644',\n",
       "   '2145038566',\n",
       "   '2148029428']},\n",
       " {'id': '2158049734',\n",
       "  'title': 'Semi-Supervised Learning for Natural Language',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': ['Percy'],\n",
       "  'related_topics': ['Informatics engineering',\n",
       "   'Semi-supervised learning',\n",
       "   'Applied science',\n",
       "   'Natural language',\n",
       "   'Software engineering',\n",
       "   'Computer science',\n",
       "   'Mechanical engineering',\n",
       "   'View Less'],\n",
       "  'citation_count': '408',\n",
       "  'reference_count': '74',\n",
       "  'references': ['2147880316',\n",
       "   '2139212933',\n",
       "   '2121947440',\n",
       "   '2048679005',\n",
       "   '2008652694',\n",
       "   '2097089247',\n",
       "   '2138745909',\n",
       "   '2156515921',\n",
       "   '2107008379',\n",
       "   '2144578941']},\n",
       " {'id': '2122457239',\n",
       "  'title': 'Semi-Supervised Learning in Gigantic Image Collections',\n",
       "  'abstract': 'With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. \"Clean labels\" can be manually obtained on a small fraction, \"noisy labels\" may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Rob Fergus', 'Yair Weiss', 'Antonio Torralba'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Laplacian matrix',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Image (mathematics)',\n",
       "   'Normalization (statistics)',\n",
       "   'Fraction (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '325',\n",
       "  'reference_count': '27',\n",
       "  'references': ['3118608800',\n",
       "   '2110764733',\n",
       "   '2145607950',\n",
       "   '1566135517',\n",
       "   '2293597654',\n",
       "   '1560724230',\n",
       "   '1479807131',\n",
       "   '2111993661',\n",
       "   '2154455818',\n",
       "   '2104290444']},\n",
       " {'id': '2963207607',\n",
       "  'title': 'Explaining and Harnessing Adversarial Examples',\n",
       "  'abstract': \"Abstract: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.\",\n",
       "  'date': '2015',\n",
       "  'authors': ['Ian J. Goodfellow', 'Jonathon Shlens', 'Christian Szegedy'],\n",
       "  'related_topics': ['Adversarial machine learning',\n",
       "   'Overfitting',\n",
       "   'MNIST database',\n",
       "   'Test set',\n",
       "   'Artificial neural network',\n",
       "   'Adversarial system',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Backdoor',\n",
       "   'Nonlinear system',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,443',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2963382180',\n",
       "  'title': 'Striving for Simplicity: The All Convolutional Net',\n",
       "  'abstract': 'Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Jost Tobias Springenberg',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox',\n",
       "   'Martin A. Riedmiller'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pipeline (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Convolution',\n",
       "   'Deconvolution',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'Range (mathematics)',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,236',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1479807131',\n",
       "  'title': 'Semi-Supervised Learning',\n",
       "  'abstract': 'In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series',\n",
       "  'date': '2010',\n",
       "  'authors': ['Olivier Chapelle', 'Bernhard Schlkopf', 'Alexander Zien'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'Transduction (machine learning)',\n",
       "   'Co-training',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Generative grammar',\n",
       "   'Computation',\n",
       "   'Artificial intelligence',\n",
       "   'Manifold regularization',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,990',\n",
       "  'reference_count': '374',\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2158714788',\n",
       "   '2055043387',\n",
       "   '1480376833',\n",
       "   '2119821739',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '3124955340',\n",
       "   '2912934387']},\n",
       " {'id': '2112076978',\n",
       "  'title': 'Experiments with a new boosting algorithm',\n",
       "  'abstract': 'In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman\\'s \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Yoav Freund', 'Robert E. Schapire'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'Gradient boosting',\n",
       "   'AdaBoost',\n",
       "   'LPBoost',\n",
       "   'LogitBoost',\n",
       "   'Ensembles of classifiers',\n",
       "   'Stability (learning theory)',\n",
       "   'Cascading classifiers',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Boosting methods for object categorization',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,827',\n",
       "  'reference_count': '19',\n",
       "  'references': ['3124955340',\n",
       "   '2912934387',\n",
       "   '2125055259',\n",
       "   '1504694836',\n",
       "   '1670263352',\n",
       "   '1966280301',\n",
       "   '2093717447',\n",
       "   '2132166479',\n",
       "   '2070534370',\n",
       "   '2137291015']},\n",
       " {'id': '1975846642',\n",
       "  'title': 'Boosting the margin: a new explanation for the effectiveness of voting methods',\n",
       "  'abstract': \"One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.\",\n",
       "  'date': '1998',\n",
       "  'authors': ['Robert E. Schapire',\n",
       "   'Yoav Freund',\n",
       "   'Peter Bartlett',\n",
       "   'Wee Sun Lee'],\n",
       "  'related_topics': ['Margin classifier',\n",
       "   'BrownBoost',\n",
       "   'LPBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'Support vector machine',\n",
       "   'Voting',\n",
       "   'Classification rule',\n",
       "   'LogitBoost',\n",
       "   'Algorithm',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,525',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2156909104',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '1594031697',\n",
       "   '2112076978',\n",
       "   '2087347434',\n",
       "   '1605688901',\n",
       "   '2032210760',\n",
       "   '2982720039']},\n",
       " {'id': '2152761983',\n",
       "  'title': 'An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants',\n",
       "  'abstract': 'Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only “hard” areas but also outliers and noise.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Eric Bauer', 'Ron Kohavi'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'AdaBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'LPBoost',\n",
       "   'Ensembles of classifiers',\n",
       "   'Mean squared error',\n",
       "   'Statistical classification',\n",
       "   'Decision tree',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,325',\n",
       "  'reference_count': '50',\n",
       "  'references': ['1995945562',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '2125055259',\n",
       "   '2084812512',\n",
       "   '2112076978',\n",
       "   '1975846642',\n",
       "   '1680392829',\n",
       "   '2140785063',\n",
       "   '3017143921']},\n",
       " {'id': '2113242816',\n",
       "  'title': 'The random subspace method for constructing decision forests',\n",
       "  'abstract': \"Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.\",\n",
       "  'date': '1998',\n",
       "  'authors': ['Tin Kam Ho'],\n",
       "  'related_topics': ['Random forest',\n",
       "   'Random subspace method',\n",
       "   'Decision tree',\n",
       "   'Overfitting',\n",
       "   'Ensembles of classifiers',\n",
       "   'Binary tree',\n",
       "   'Support vector machine',\n",
       "   'Feature vector',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,197',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2156909104',\n",
       "   '2912934387',\n",
       "   '2125055259',\n",
       "   '1594031697',\n",
       "   '2112076978',\n",
       "   '2149706766',\n",
       "   '2101522199',\n",
       "   '1966280301',\n",
       "   '2102734279',\n",
       "   '1930624869']},\n",
       " {'id': '1605688901',\n",
       "  'title': 'An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization',\n",
       "  'abstract': 'Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Thomas G. Dietterich'],\n",
       "  'related_topics': ['Gradient boosting',\n",
       "   'BrownBoost',\n",
       "   'Ensembles of classifiers',\n",
       "   'Boosting (machine learning)',\n",
       "   'Random forest',\n",
       "   'LPBoost',\n",
       "   'Ensemble learning',\n",
       "   'Decision tree',\n",
       "   'Machine learning',\n",
       "   'Data mining',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,119',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2912934387',\n",
       "   '2112076978',\n",
       "   '2152761983',\n",
       "   '2982720039',\n",
       "   '1966280301',\n",
       "   '2167277498',\n",
       "   '2073738917',\n",
       "   '2976840617',\n",
       "   '1562197959',\n",
       "   '1850527962']},\n",
       " {'id': '2120240539',\n",
       "  'title': 'Shape quantization and recognition with randomized trees',\n",
       "  'abstract': 'We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred L AT E X symbols. Stateof-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on L AT E X symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Yali Amit', 'Donald Geman'],\n",
       "  'related_topics': ['Feature vector',\n",
       "   'Tree-depth',\n",
       "   'Decision tree',\n",
       "   'Posterior probability',\n",
       "   'Artificial neural network',\n",
       "   'Classifier (UML)',\n",
       "   'Partially ordered set',\n",
       "   'A priori and a posteriori',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,361',\n",
       "  'reference_count': '49',\n",
       "  'references': ['2099111195',\n",
       "   '2912934387',\n",
       "   '1594031697',\n",
       "   '2149706766',\n",
       "   '2101522199',\n",
       "   '1676820704',\n",
       "   '2165758113',\n",
       "   '2154579312',\n",
       "   '2076118331',\n",
       "   '2168228682']},\n",
       " {'id': '2099968818',\n",
       "  'title': 'Boosting in the limit: maximizing the margin of learned ensembles',\n",
       "  'abstract': 'The \"minimum margin\" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \"LPboosting\" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Adam J. Grove', 'Dale Schuurmans'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'LPBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'AdaBoost',\n",
       "   'Linear programming',\n",
       "   'Machine learning',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Classifier (UML)',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization error',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '365',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2119821739',\n",
       "   '3124955340',\n",
       "   '2125055259',\n",
       "   '2112076978',\n",
       "   '2152761983',\n",
       "   '1504694836',\n",
       "   '2982720039',\n",
       "   '1966280301',\n",
       "   '2266946488',\n",
       "   '1553313034']},\n",
       " {'id': '2067885219',\n",
       "  'title': 'Arcing classifier (with discussion and a rejoinder by the author)',\n",
       "  'abstract': 'Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym “arcing”) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Leo'],\n",
       "  'related_topics': ['Test set',\n",
       "   'Boosting (machine learning)',\n",
       "   'Weighted voting',\n",
       "   'BrownBoost',\n",
       "   'Resampling',\n",
       "   'LPBoost',\n",
       "   'Ensemble learning',\n",
       "   'Artificial neural network',\n",
       "   'Pattern recognition',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,925',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1580948147',\n",
       "  'title': 'Randomizing Outputs to Increase Prediction Accuracy',\n",
       "  'abstract': 'Bagging and boosting reduce error by changing both the inputs and outputs to form perturbed training sets, growing predictors on these perturbed training sets and combining them. An interesting question is whether it is possible to get comparable performance by perturbing the outputs alone. Two methods of randomizing outputs are experimented with. One is called output smearing and the other output flipping. Both are shown to consistently do better than bagging.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'related_topics': ['Boosting (machine learning)',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '300',\n",
       "  'reference_count': '14',\n",
       "  'references': ['3124955340',\n",
       "   '2912934387',\n",
       "   '1594031697',\n",
       "   '2112076978',\n",
       "   '1605688901',\n",
       "   '2102201073',\n",
       "   '2067885219',\n",
       "   '2076118331',\n",
       "   '2172195373',\n",
       "   '2073738917']},\n",
       " {'id': '2145889472',\n",
       "  'title': 'Emergence of simple-cell receptive field properties by learning a sparse code for natural images',\n",
       "  'abstract': 'The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Bruno A. Olshausen', '', 'David J. Field'],\n",
       "  'related_topics': ['Efficient coding hypothesis',\n",
       "   'Simple cell',\n",
       "   'Sparse image',\n",
       "   'Unsupervised learning',\n",
       "   'Receptive field',\n",
       "   'Visual cortex',\n",
       "   'Wavelet transform',\n",
       "   'Representation (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,525',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2108384452',\n",
       "   '1993845689',\n",
       "   '2180838288',\n",
       "   '2167034998',\n",
       "   '2120838001',\n",
       "   '2122925692',\n",
       "   '1914401667',\n",
       "   '2106884367',\n",
       "   '2911607583',\n",
       "   '2117731089']},\n",
       " {'id': '2122922389',\n",
       "  'title': 'Self-taught learning: transfer learning from unlabeled data',\n",
       "  'abstract': 'We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Rajat Raina',\n",
       "   'Alexis Battle',\n",
       "   'Honglak Lee',\n",
       "   'Benjamin Packer',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Multi-task learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Instance-based learning',\n",
       "   'Online machine learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Inductive transfer',\n",
       "   'Transfer of learning',\n",
       "   'Fisher kernel',\n",
       "   'Support vector machine',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,808',\n",
       "  'reference_count': '25',\n",
       "  'references': ['1880262756',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2135046866',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2063978378',\n",
       "   '2166049352',\n",
       "   '2147152072',\n",
       "   '2113606819']},\n",
       " {'id': '2139427956',\n",
       "  'title': 'Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition',\n",
       "  'abstract': 'We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.',\n",
       "  'date': '2007',\n",
       "  'authors': ['M.A. Ranzato', 'Fu Jie Huang', 'Y.-L. Boureau', 'Yann LeCun'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'MNIST database',\n",
       "   'Feature extraction',\n",
       "   'Caltech 101',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Invariant (mathematics)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Object detection',\n",
       "   'Sigmoid function',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,313',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2151103935',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2162915993',\n",
       "   '2110798204',\n",
       "   '2166049352',\n",
       "   '1624854622',\n",
       "   '2172174689',\n",
       "   '2168002178',\n",
       "   '2105464873']},\n",
       " {'id': '2118020653',\n",
       "  'title': 'Machine learning in automated text categorization',\n",
       "  'abstract': 'The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Fabrizio Sebastiani'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Categorization',\n",
       "   'Document classification',\n",
       "   'Classifier (UML)',\n",
       "   'Knowledge engineering',\n",
       "   'Email filtering',\n",
       "   'Rocchio algorithm',\n",
       "   'Software portability',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,883',\n",
       "  'reference_count': '165',\n",
       "  'references': ['1574901103',\n",
       "   '2149684865',\n",
       "   '2147152072',\n",
       "   '2435251607',\n",
       "   '2097089247',\n",
       "   '2053463056',\n",
       "   '2114535528',\n",
       "   '2005422315',\n",
       "   '2140785063',\n",
       "   '1978394996']},\n",
       " {'id': '2493916176',\n",
       "  'title': 'Enriching Word Vectors with Subword Information',\n",
       "  'abstract': 'Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.\\xa0Popular models to learn such representations \\xa0ignore the morphology of words, by assigning a distinct vector to each word.\\xa0This is a limitation, especially for languages with large vocabularies and many rare words.\\xa0In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.\\xa0A vector representation is associated to each character n-gram, words being represented as the sum of these representations.\\xa0Our method is fast, allowing to train models on large corpora quickly and allows to compute word representations for words that did not appear in the training data.\\xa0We evaluate our word representations on nine different languages, both on word similarity and analogy tasks.\\xa0By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.',\n",
       "  'date': '2017',\n",
       "  'authors': ['Piotr Bojanowski',\n",
       "   'Edouard Grave',\n",
       "   'Armand Joulin',\n",
       "   'Tomas Mikolov'],\n",
       "  'related_topics': ['Word lists by frequency',\n",
       "   'Word embedding',\n",
       "   'Word (computer architecture)',\n",
       "   'Character (mathematics)',\n",
       "   'Similarity (psychology)',\n",
       "   'Natural language processing',\n",
       "   'Analogy',\n",
       "   'Speech recognition',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word representation',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,864',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2117130368',\n",
       "   '2962784628',\n",
       "   '1810943226',\n",
       "   '2963012544',\n",
       "   '2251012068',\n",
       "   '2147152072',\n",
       "   '1662133657',\n",
       "   '1938755728']},\n",
       " {'id': '2113459411',\n",
       "  'title': 'Learning Word Vectors for Sentiment Analysis',\n",
       "  'abstract': 'Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.',\n",
       "  'date': '2011',\n",
       "  'authors': ['Andrew L. Maas',\n",
       "   'Raymond E. Daly',\n",
       "   'Peter T. Pham',\n",
       "   'Dan Huang',\n",
       "   'Andrew Y. Ng',\n",
       "   'Christopher Potts'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Leverage (statistics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,792',\n",
       "  'reference_count': '28',\n",
       "  'references': ['1880262756',\n",
       "   '2117130368',\n",
       "   '2118585731',\n",
       "   '2166706824',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '2147152072',\n",
       "   '2114524997',\n",
       "   '1662133657',\n",
       "   '2163455955']},\n",
       " {'id': '2164019165',\n",
       "  'title': 'Improving Word Representations via Global Context and Multiple Word Prototypes',\n",
       "  'abstract': 'Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.',\n",
       "  'date': '2012',\n",
       "  'authors': ['Eric Huang',\n",
       "   'Richard Socher',\n",
       "   'Christopher Manning',\n",
       "   'Andrew Ng'],\n",
       "  'related_topics': ['Word lists by frequency',\n",
       "   'Word (computer architecture)',\n",
       "   'Language model',\n",
       "   'Polysemy',\n",
       "   'Context (language use)',\n",
       "   'Semantics',\n",
       "   'Natural language processing',\n",
       "   'Homonym',\n",
       "   'Computer science',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,364',\n",
       "  'reference_count': '36',\n",
       "  'references': ['1532325895',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2118020653',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '2081580037',\n",
       "   '1970381522',\n",
       "   '2131462252']},\n",
       " {'id': '1544827683',\n",
       "  'title': 'Teaching machines to read and comprehend',\n",
       "  'abstract': 'Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Karl Moritz Hermann',\n",
       "   'Tomáš Kočiský',\n",
       "   'Edward Grefenstette',\n",
       "   'Lasse Espeholt',\n",
       "   'Will Kay',\n",
       "   'Mustafa Suleyman',\n",
       "   'Phil Blunsom'],\n",
       "  'related_topics': ['Reading comprehension',\n",
       "   'Natural language',\n",
       "   'Class (computer programming)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Test (assessment)',\n",
       "   'Scale (chemistry)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,994',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2064675550',\n",
       "   '2158899491',\n",
       "   '2120615054',\n",
       "   '1793121960',\n",
       "   '2962741254',\n",
       "   '2147527908',\n",
       "   '2144499799',\n",
       "   '2125436846']},\n",
       " {'id': '2125436846',\n",
       "  'title': 'MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text',\n",
       "  'abstract': 'We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone’s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today’s computers and algorithms.',\n",
       "  'date': '2013',\n",
       "  'authors': ['Matthew Richardson', 'Christopher J.C. Burges', 'Erin Renshaw'],\n",
       "  'related_topics': ['Reading comprehension',\n",
       "   'Comprehension',\n",
       "   'Relationship extraction',\n",
       "   'Textual entailment',\n",
       "   'Semantic role labeling',\n",
       "   'Information extraction',\n",
       "   'Multiple choice',\n",
       "   'Causal reasoning',\n",
       "   'Parsing',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '529',\n",
       "  'reference_count': '25',\n",
       "  'references': ['3122078363',\n",
       "   '2525127255',\n",
       "   '2126631960',\n",
       "   '2167090521',\n",
       "   '3126123353',\n",
       "   '1979532929',\n",
       "   '2989499211',\n",
       "   '2096979215',\n",
       "   '2097550833',\n",
       "   '2142898321']},\n",
       " {'id': '2964267515',\n",
       "  'title': \"The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations\",\n",
       "  'abstract': \"Abstract: We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.\",\n",
       "  'date': '2016',\n",
       "  'authors': ['Felix Hill', 'Antoine Bordes', 'Sumit Chopra', 'Jason Weston'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Reading (process)',\n",
       "   'Encoding (memory)',\n",
       "   'Goldilocks principle',\n",
       "   'Explicit memory',\n",
       "   'Generality',\n",
       "   'Natural language processing',\n",
       "   'Function (engineering)',\n",
       "   'Standard language',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '437',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2962809918',\n",
       "  'title': 'A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task',\n",
       "  'abstract': 'Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1',\n",
       "  'date': '2016',\n",
       "  'authors': ['Danqi Chen', 'Jason Bolton', 'Christopher D. Manning'],\n",
       "  'related_topics': ['Task (project management)',\n",
       "   'Reading comprehension',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Artificial neural network',\n",
       "   'Factor (programming language)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '447',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2250539671',\n",
       "   '1902237438',\n",
       "   '1544827683',\n",
       "   '1793121960',\n",
       "   '2250861254',\n",
       "   '2125436846',\n",
       "   '2964267515',\n",
       "   '2584341106',\n",
       "   '2964091467',\n",
       "   '2962790689']},\n",
       " {'id': '2171278097',\n",
       "  'title': 'Building Watson: An Overview of the DeepQA Project',\n",
       "  'abstract': 'IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.',\n",
       "  'date': '2010',\n",
       "  'authors': ['David A. Ferrucci',\n",
       "   'Eric W. Brown',\n",
       "   'Jennifer Chu-Carroll',\n",
       "   'James Fan',\n",
       "   'David Gondek',\n",
       "   'Aditya Kalyanpur',\n",
       "   'Adam Lally',\n",
       "   'J. William Murdock',\n",
       "   'Eric Nyberg',\n",
       "   'John M. Prager',\n",
       "   'Nico Schlaefer',\n",
       "   'Christopher A. Welty'],\n",
       "  'related_topics': ['Watson',\n",
       "   'IBM',\n",
       "   'Champion',\n",
       "   'Architecture',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Field (computer science)',\n",
       "   'Operations research',\n",
       "   'Extensible architecture',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,764',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2081580037',\n",
       "   '2047221353',\n",
       "   '2096797897',\n",
       "   '2150884987',\n",
       "   '2087064593',\n",
       "   '2988119488',\n",
       "   '2107658650',\n",
       "   '2158823144',\n",
       "   '2122537498',\n",
       "   '2080278171']},\n",
       " {'id': '2962790689',\n",
       "  'title': 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks',\n",
       "  'abstract': 'Abstract: One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.',\n",
       "  'date': '2016',\n",
       "  'authors': ['Jason Weston',\n",
       "   'Antoine Bordes',\n",
       "   'Sumit Chopra',\n",
       "   'Alexander M. Rush',\n",
       "   'Bart van Merriënboer',\n",
       "   'Armand Joulin',\n",
       "   'Tomas Mikolov'],\n",
       "  'related_topics': ['Question answering',\n",
       "   'Set (psychology)',\n",
       "   'AI-complete',\n",
       "   'Natural language',\n",
       "   'Chaining',\n",
       "   'Reading comprehension',\n",
       "   'Human–computer interaction',\n",
       "   'Computer science',\n",
       "   'Simple (philosophy)',\n",
       "   'Measure (data warehouse)',\n",
       "   'View Less'],\n",
       "  'citation_count': '654',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2251818205',\n",
       "  'title': 'WikiQA: A Challenge Dataset for Open-Domain Question Answering',\n",
       "  'abstract': 'We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.',\n",
       "  'date': '2015',\n",
       "  'authors': ['Yi Yang', 'Wen-tau Yih', 'Christopher Meek'],\n",
       "  'related_topics': ['Sentence',\n",
       "   'Question answering',\n",
       "   'Selection (linguistics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Information retrieval',\n",
       "   'Matching (statistics)',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Component (UML)',\n",
       "   'Process (engineering)',\n",
       "   'View Less'],\n",
       "  'citation_count': '554',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2153579005',\n",
       "   '2131744502',\n",
       "   '2070246124',\n",
       "   '2118091490',\n",
       "   '1591825359',\n",
       "   '1514986335',\n",
       "   '2125313055',\n",
       "   '2989499211',\n",
       "   '2120735855',\n",
       "   '2251921768']},\n",
       " {'id': '2251349042',\n",
       "  'title': 'Learning to Automatically Solve Algebra Word Problems',\n",
       "  'abstract': 'We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.',\n",
       "  'date': '2014',\n",
       "  'authors': ['Nate Kushman',\n",
       "   'Yoav Artzi',\n",
       "   'Luke Zettlemoyer',\n",
       "   'Regina Barzilay'],\n",
       "  'related_topics': ['Word problem (mathematics education)',\n",
       "   'System of linear equations',\n",
       "   'Task (project management)',\n",
       "   'Construct (python library)',\n",
       "   'Theoretical computer science',\n",
       "   'Natural language processing',\n",
       "   'Algebra',\n",
       "   'Computer science',\n",
       "   'Algebra over a field',\n",
       "   'Artificial intelligence',\n",
       "   'Sentence boundary disambiguation',\n",
       "   'View Less'],\n",
       "  'citation_count': '227',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2252136820',\n",
       "   '1508977358',\n",
       "   '2163561827',\n",
       "   '2123661878',\n",
       "   '1496189301',\n",
       "   '2189089430',\n",
       "   '2251673953',\n",
       "   '2118781169',\n",
       "   '1923162067',\n",
       "   '1559723967']},\n",
       " {'id': '2169415915',\n",
       "  'title': 'Constructing free-energy approximations and generalized belief propagation algorithms',\n",
       "  'abstract': 'Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the \"Bethe method\", the \"junction graph method\", the \"cluster variation method\", and the \"region graph method\". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.',\n",
       "  'date': '2005',\n",
       "  'authors': ['J.S. Yedidia', 'W.T. Freeman', 'Y. Weiss'],\n",
       "  'related_topics': ['Belief propagation',\n",
       "   'Approximation algorithm',\n",
       "   'Factor graph',\n",
       "   'Graph (abstract data type)',\n",
       "   'Graph theory',\n",
       "   'Stationary point',\n",
       "   'Fixed point',\n",
       "   'Coding theory',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,755',\n",
       "  'reference_count': '56',\n",
       "  'references': ['2099111195',\n",
       "   '2125838338',\n",
       "   '2798766386',\n",
       "   '2137813581',\n",
       "   '2159080219',\n",
       "   '2121606987',\n",
       "   '2987657883',\n",
       "   '1516111018',\n",
       "   '1530042113',\n",
       "   '1746680969']},\n",
       " {'id': '2158164339',\n",
       "  'title': 'Modeling Human Motion Using Binary Latent Variables',\n",
       "  'abstract': 'We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \"visible\" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Graham W. Taylor', 'Geoffrey E. Hinton', 'Sam T. Roweis'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Latent class model',\n",
       "   'Generative model',\n",
       "   'Motion capture',\n",
       "   'Motion (physics)',\n",
       "   'Inference',\n",
       "   'Set (abstract data type)',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '878',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2116064496',\n",
       "   '2124914669',\n",
       "   '2293741035',\n",
       "   '2114153178',\n",
       "   '2147010501',\n",
       "   '2248685949',\n",
       "   '1991942383',\n",
       "   '2123236823']},\n",
       " {'id': '66838807',\n",
       "  'title': 'On Contrastive Divergence Learning.',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': ['Miguel Á.', 'Geoffrey E.'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Linguistics',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Contrastive divergence',\n",
       "   'View Less'],\n",
       "  'citation_count': '794',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2116064496',\n",
       "   '2130416410',\n",
       "   '2157444450',\n",
       "   '1651266332',\n",
       "   '2095844239',\n",
       "   '1802356529',\n",
       "   '1568229137',\n",
       "   '1813659000',\n",
       "   '2482531687',\n",
       "   '2130313186']},\n",
       " {'id': '2064630666',\n",
       "  'title': 'Representational power of restricted boltzmann machines and deep belief networks',\n",
       "  'abstract': 'Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Nicolas Le Roux', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'Deep belief network',\n",
       "   'Boltzmann machine',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Statistical model',\n",
       "   'Inference',\n",
       "   'Artificial intelligence',\n",
       "   'Block (data storage)',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '782',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2137983211',\n",
       "   '3146803896',\n",
       "   '2172174689',\n",
       "   '2613634265',\n",
       "   '2124914669',\n",
       "   '205159212']},\n",
       " {'id': '1513873506',\n",
       "  'title': 'Annealed importance sampling',\n",
       "  'abstract': 'Simulated annealing—moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions—has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Radford M. Neal'],\n",
       "  'related_topics': ['Slice sampling',\n",
       "   'Importance sampling',\n",
       "   'Markov chain Monte Carlo',\n",
       "   'Markov chain',\n",
       "   'Umbrella sampling',\n",
       "   'Autocorrelation',\n",
       "   'Sequence',\n",
       "   'Thermodynamic integration',\n",
       "   'Statistical physics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,299',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2581275558',\n",
       "   '2130416410',\n",
       "   '1567512734',\n",
       "   '2149801992',\n",
       "   '2615953416',\n",
       "   '2057565703',\n",
       "   '2013164703',\n",
       "   '1565709818',\n",
       "   '2138309709',\n",
       "   '2033057584']},\n",
       " {'id': '2135094946',\n",
       "  'title': 'A new class of upper bounds on the log partition function',\n",
       "  'abstract': 'We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants.',\n",
       "  'date': '2005',\n",
       "  'authors': ['M.J. Wainwright', 'T.S. Jaakkola', 'A.S. Willsky'],\n",
       "  'related_topics': ['Partition function (quantum field theory)',\n",
       "   'Upper and lower bounds',\n",
       "   'Fixed point',\n",
       "   'Local optimum',\n",
       "   'Variational method',\n",
       "   'Belief propagation',\n",
       "   'Factor graph',\n",
       "   'Markov random field',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '582',\n",
       "  'reference_count': '40',\n",
       "  'references': ['2798766386',\n",
       "   '2137813581',\n",
       "   '2004308928',\n",
       "   '1516111018',\n",
       "   '2169415915',\n",
       "   '1530042113',\n",
       "   '2914659449',\n",
       "   '2019599312',\n",
       "   '1515272691',\n",
       "   '1513861746']},\n",
       " {'id': '2102381086',\n",
       "  'title': 'Introduction to WordNet: An On-line Lexical Database',\n",
       "  'abstract': 'Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.',\n",
       "  'date': '1990',\n",
       "  'authors': ['George A. Miller',\n",
       "   'Richard Beckwith',\n",
       "   'Christiane Fellbaum',\n",
       "   'Derek Gross',\n",
       "   'Katherine J. Miller'],\n",
       "  'related_topics': ['Lexical database',\n",
       "   'WordNet',\n",
       "   'eXtended WordNet',\n",
       "   'EuroWordNet',\n",
       "   'Train of thought',\n",
       "   'Natural language processing',\n",
       "   'Simple (philosophy)',\n",
       "   'Coh-Metrix',\n",
       "   'Computer science',\n",
       "   'Interrupt',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,740',\n",
       "  'reference_count': '81',\n",
       "  'references': ['1933657216',\n",
       "   '2103318667',\n",
       "   '2090626368',\n",
       "   '1483126227',\n",
       "   '2017668967',\n",
       "   '2123987305',\n",
       "   '2040300040',\n",
       "   '2013596317',\n",
       "   '2052262800',\n",
       "   '2059799772']},\n",
       " {'id': '2103318667',\n",
       "  'title': 'Contextual correlates of semantic similarity',\n",
       "  'abstract': 'Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.',\n",
       "  'date': '1991',\n",
       "  'authors': ['George A. Miller', 'Walter G. Charles'],\n",
       "  'related_topics': ['Semantic similarity',\n",
       "   'Similarity (psychology)',\n",
       "   'Contextual Associations',\n",
       "   'Semantics',\n",
       "   'Sentence',\n",
       "   'Noun',\n",
       "   'Syntax',\n",
       "   'Pragmatics',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Psychology',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,009',\n",
       "  'reference_count': '31',\n",
       "  'references': ['1483126227',\n",
       "   '1971220772',\n",
       "   '2017580301',\n",
       "   '2114826854',\n",
       "   '13823885',\n",
       "   '2109334311',\n",
       "   '2064332540',\n",
       "   '1536719366',\n",
       "   '1634667895',\n",
       "   '2020159140']},\n",
       " {'id': '2065157922',\n",
       "  'title': 'A semantic concordance',\n",
       "  'abstract': 'A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances am proposed.',\n",
       "  'date': '1993',\n",
       "  'authors': ['George A. Miller',\n",
       "   'Claudia Leacock',\n",
       "   'Randee Tengi',\n",
       "   'Ross T. Bunker'],\n",
       "  'related_topics': ['Explicit semantic analysis',\n",
       "   'Semantic similarity',\n",
       "   'Semantic compression',\n",
       "   'WordNet',\n",
       "   'Semantic property',\n",
       "   'Lexicon',\n",
       "   'Brown Corpus',\n",
       "   'Semantic HTML',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '791',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2102381086',\n",
       "   '2081687495',\n",
       "   '1483126227',\n",
       "   '2017668967',\n",
       "   '2007780422',\n",
       "   '2012908435']},\n",
       " {'id': '1483126227',\n",
       "  'title': 'FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR',\n",
       "  'abstract': '',\n",
       "  'date': '1983',\n",
       "  'authors': ['W. Nelson', 'Henry', 'Andrew W.'],\n",
       "  'related_topics': ['Lexicon',\n",
       "   'Grammar',\n",
       "   'Brown Corpus',\n",
       "   'Regular and irregular verbs',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Frequency analysis',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,935',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2017668967',\n",
       "  'title': 'Semantic networks of English.',\n",
       "  'abstract': 'Principles of lexical semantics developed in the course of building an on-line lexical database are discussed. The approach is relational rather than componential. The fundamental semantic relation is synonymy, which is required in order to define the lexicalized concepts that words can be used to express. Other semantic relations between these concepts are then described. No single set of semantic relations or organizational structure is adequate for the entire lexicon: nouns, adjectives, and verbs each have their own semantic relations and their own organization determined by the role they must play in the construction of linguistic messages.',\n",
       "  'date': '1991',\n",
       "  'authors': ['George A. Miller', 'Christiane Fellbaum'],\n",
       "  'related_topics': ['Semantic computing',\n",
       "   'Semantic network',\n",
       "   'Semantic similarity',\n",
       "   'Semantic property',\n",
       "   'Componential analysis',\n",
       "   'Lexical database',\n",
       "   'Lexical semantics',\n",
       "   'Semantic compression',\n",
       "   'Linguistics',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '490',\n",
       "  'reference_count': '41',\n",
       "  'references': ['2103318667',\n",
       "   '1503404806',\n",
       "   '2123987305',\n",
       "   '2040300040',\n",
       "   '2013596317',\n",
       "   '2052262800',\n",
       "   '2059799772',\n",
       "   '1576632330',\n",
       "   '2025589690',\n",
       "   '2069736034']},\n",
       " {'id': '1518768680',\n",
       "  'title': 'Towards building contextual representations of word senses using statistical models',\n",
       "  'abstract': 'A b s t r a c t Automatic corpus-based sense resolution, or sense dlsambiguation, techniques tend to focus either on very local context or on topical context. Both components axe needed for word sense resolution. A contextual representation of a word sense consists of topical context and local context. Our goal is to construct contextual representations by automatically extracting topical and local information from textual corpora. We review an experiment evaluating three statistical classifiers that automatically extract topical context. An experiment designed to examine human subject performance with similar input is described. Finally, we investigate a method for automatically extracting local context from a corpus. Preliminary results show improved perfor-',\n",
       "  'date': '1996',\n",
       "  'authors': ['Claudia', 'Geoffrey', 'Ellen M.'],\n",
       "  'related_topics': ['Context (language use)',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Statistical model',\n",
       "   'Construct (python library)',\n",
       "   'Representation (systemics)',\n",
       "   'Resolution (logic)',\n",
       "   'Focus (linguistics)',\n",
       "   'Computer science',\n",
       "   'Subject (grammar)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '100',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2154642048',\n",
       "   '2103318667',\n",
       "   '1977182536',\n",
       "   '2165612380',\n",
       "   '2047620598',\n",
       "   '1999114220',\n",
       "   '2066444522',\n",
       "   '2148426685',\n",
       "   '2090543924']},\n",
       " {'id': '13823885',\n",
       "  'title': 'The categorization of sentential contexts',\n",
       "  'abstract': 'A new experimental method, involving the sorting of linguistic contexts, is shown to be effective in discriminating the contexts of polysemous words as well as the contexts of synonyms of those words. These results are interpreted as support for the claim that the method of sorting linguistic contexts is a valid technique for studying the contextual information available to support inferences about word meanings.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Walter G. Charles'],\n",
       "  'related_topics': ['Context (archaeology)',\n",
       "   'Categorization',\n",
       "   'Psycholinguistics',\n",
       "   'Sorting',\n",
       "   'Lexico',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Contextual information',\n",
       "   'Semantic relation',\n",
       "   'View Less'],\n",
       "  'citation_count': '9',\n",
       "  'reference_count': '4',\n",
       "  'references': ['2017580301', '1995875735', '2045585593', '2316811974']},\n",
       " {'id': '1997063559',\n",
       "  'title': 'Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images*',\n",
       "  'abstract': 'We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, non-linear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low-energy states (‘annealing’), or what is the same thing, the most probable states under the Gib...',\n",
       "  'date': '1993',\n",
       "  'authors': ['Stuart Geman', 'Donald Geman'],\n",
       "  'related_topics': ['Gibbs sampling',\n",
       "   'Categorical distribution',\n",
       "   'Boltzmann distribution',\n",
       "   'Markov random field',\n",
       "   'Posterior probability',\n",
       "   'Physical system',\n",
       "   'Statistical mechanics',\n",
       "   'Multiplicative function',\n",
       "   'Statistical physics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '26,469',\n",
       "  'reference_count': '48',\n",
       "  'references': ['2581275558',\n",
       "   '2150060382',\n",
       "   '1622620102',\n",
       "   '2154061444',\n",
       "   '2114220616',\n",
       "   '1979622972',\n",
       "   '2065301447',\n",
       "   '2107792892',\n",
       "   '2056760934',\n",
       "   '1567885833']},\n",
       " {'id': '2049633694',\n",
       "  'title': 'Maximum likelihood from incomplete data via the EM algorithm',\n",
       "  'abstract': '',\n",
       "  'date': '1977',\n",
       "  'authors': ['Arthur P. Dempster', 'Nan M. Laird', 'Donald B. Rubin'],\n",
       "  'related_topics': ['Maximum likelihood sequence estimation',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'MM algorithm',\n",
       "   'Mixture model',\n",
       "   'Baum–Welch algorithm',\n",
       "   'Cluster-weighted modeling',\n",
       "   'Observed information',\n",
       "   'Generalized iterative scaling',\n",
       "   'Mathematics',\n",
       "   'Statistics',\n",
       "   'Pattern recognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '65,759',\n",
       "  'reference_count': '74',\n",
       "  'references': ['2100358124',\n",
       "   '2327022120',\n",
       "   '2074673068',\n",
       "   '2403035479',\n",
       "   '1982585616',\n",
       "   '1575431606',\n",
       "   '2086699924',\n",
       "   '2144578442',\n",
       "   '2000084758',\n",
       "   '2121493622']},\n",
       " {'id': '1964724001',\n",
       "  'title': 'Exploratory Projection Pursuit',\n",
       "  'abstract': 'Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ...',\n",
       "  'date': '1987',\n",
       "  'authors': ['Jerome H. Friedman'],\n",
       "  'related_topics': ['Projection pursuit',\n",
       "   'Exploratory data analysis',\n",
       "   'Cluster analysis',\n",
       "   'Covariance',\n",
       "   'Density estimation',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Data mapping',\n",
       "   'Statistical graphics',\n",
       "   'Data mining',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,161',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2954064014',\n",
       "   '2800289289',\n",
       "   '2155199877',\n",
       "   '2029469881',\n",
       "   '2082612735',\n",
       "   '1573763320',\n",
       "   '1968104963',\n",
       "   '2161831609',\n",
       "   '2052740976',\n",
       "   '1983993791']},\n",
       " {'id': '2121407732',\n",
       "  'title': 'Finite Mixture Distributions',\n",
       "  'abstract': \"1 General introduction.- 1.1 Introduction.- 1.2 Some applications of finite mixture distributions.- 1.3 Definition.- 1.4 Estimation methods.- 1.4.1 Maximum likelihood.- 1.4.2 Bayesian estimation.- 1.4.3 Inversion and error minimization.- 1.4.4 Other methods.- 1.4.5 Estimating the number of components.- 1.5 Summary.- 2 Mixtures of normal distributions.- 2.1 Introduction.- 2.2 Some descriptive properties of mixtures of normal distributions.- 2.3 Estimating the parameters in normal mixture distributions.- 2.3.1 Method of moments estimation.- 2.3.2 Maximum likelihood estimation.- 2.3.3 Maximum likelihood estimates for grouped data.- 2.3.4 Obtaining initial parameter values for the maximum likelihood estimation algorithms.- 2.3.5 Graphical estimation techniques.- 2.3.6 Other estimation methods.- 2.4 Summary.- 3 Mixtures of exponential and other continuous distributions.- 3.1 Exponential mixtures.- 3.2 Estimating exponential mixture parameters.- 3.2.1 The method of moments and generalizations.- 3.2.2 Maximum likelihood.- 3.3 Properties of exponential mixtures.- 3.4 Other continuous distributions.- 3.4.1 Non-central chi-squared distribution.- 3.4.2 Non-central F distribution.- 3.4.3 Beta distributions.- 3.4.4 Doubly non-central t distribution.- 3.4.5 Planck's distribution.- 3.4.6 Logistic.- 3.4.7 Laplace.- 3.4.8 Weibull.- 3.4.9 Gamma.- 3.5 Mixtures of different component types.- 3.6 Summary.- 4 Mixtures of discrete distributions.- 4.1 Introduction.- 4.2 Mixtures of binomial distributions.- 4.2.1 Moment estimators for binomial mixtures.- 4.2.2 Maximum likelihood estimators for mixtures of binomial distributions.- 4.2.3 Other estimation methods for mixtures of binomial distributions.- 4.3 Mixtures of Poisson distributions.- 4.3.1 Moment estimators for mixtures of Poisson distributions.- 4.3.2 Maximum likelihood estimators for a Poisson mixture.- 4.4 Mixtures of Poisson and binomial distributions.- 4.5 Mixtures of other discrete distributions.- 4.6 Summary.- 5 Miscellaneous topics.- 5.1 Introduction.- 5.2 Determining the number of components in a mixture.- 5.2.1 Informal diagnostic tools for the detection of mixtures.- 5.2.2 Testing hypotheses on the number of components in a mixture.- 5.3 Probability density function estimation.- 5.4 Miscellaneous problems.- 5.5 Summary.- References.\",\n",
       "  'date': '1981',\n",
       "  'authors': ['Brian Everitt', 'D. J.'],\n",
       "  'related_topics': ['Estimating equations',\n",
       "   'Normal distribution',\n",
       "   'Poisson distribution',\n",
       "   'Beta distribution',\n",
       "   'Estimator',\n",
       "   'F-distribution',\n",
       "   'Bayes estimator',\n",
       "   'Weibull distribution',\n",
       "   'Applied mathematics',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,046',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2725061391',\n",
       "  'title': 'A mean field theory learning algorithm for neural networks',\n",
       "  'abstract': '',\n",
       "  'date': '1987',\n",
       "  'authors': ['Carsten', 'James R.'],\n",
       "  'related_topics': ['Types of artificial neural networks',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Competitive learning',\n",
       "   'Unsupervised learning',\n",
       "   'Rprop',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '682',\n",
       "  'reference_count': '3',\n",
       "  'references': ['2154642048', '2293063825', '1507849272']},\n",
       " {'id': '2315016682',\n",
       "  'title': 'Feature extraction using an unsupervised neural network',\n",
       "  'abstract': 'A novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing distinguishing features in the data is presented. A statistical framework for the parameter estimation problem associated with this neural network is given and its connection to exploratory projection pursuit methods is established. The network is shown to minimize a loss function (projection index) over a set of parameters, yielding an optimal decision rule under some norm. A specific projection index that favors directions possessing multimodality is presented. This leads to a similar form to the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982).\\r\\n\\r\\nThe importance of a dimensionality reduction principal based, solely on distinguishing features, is demonstrated using a linguistically motivated phoneme recognition experiment, and compared with feature extraction using principal components and back-propagation network.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Nathan'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Time delay neural network',\n",
       "   'Projection pursuit',\n",
       "   'Feature extraction',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Projection (set theory)',\n",
       "   'Principal component analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '121',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1991848143',\n",
       "  'title': 'Self-Organization and Associative Memory',\n",
       "  'abstract': '1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References.',\n",
       "  'date': '1984',\n",
       "  'authors': ['Teuvo'],\n",
       "  'related_topics': ['Holographic associative memory',\n",
       "   'Content-addressable memory',\n",
       "   'Autoassociative memory',\n",
       "   'Artificial neural network',\n",
       "   'Memory model',\n",
       "   'Perceptron',\n",
       "   'Learning vector quantization',\n",
       "   'Associative property',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '12,606',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2107636931',\n",
       "  'title': 'GTM: the generative topographic mapping',\n",
       "  'abstract': 'Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping, for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Christopher M. Bishop',\n",
       "   'Markus Svensén',\n",
       "   'Christopher K. I. Williams'],\n",
       "  'related_topics': ['Generative topographic map',\n",
       "   'Latent variable model',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Self-organizing map',\n",
       "   'Toy problem',\n",
       "   'Probability density function',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,760',\n",
       "  'reference_count': '38',\n",
       "  'references': ['1554663460',\n",
       "   '1679913846',\n",
       "   '2049633694',\n",
       "   '2044758663',\n",
       "   '2125027820',\n",
       "   '65738273',\n",
       "   '2146610201',\n",
       "   '2166698530',\n",
       "   '2137969290',\n",
       "   '2051719061']},\n",
       " {'id': '2122538988',\n",
       "  'title': 'Nonlinear principal component analysis using autoassociative neural networks',\n",
       "  'abstract': 'Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Mark A. Kramer'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Principal component analysis',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial neural network',\n",
       "   'Feature vector',\n",
       "   'Curse of dimensionality',\n",
       "   'Nonlinear system',\n",
       "   'Exploratory data analysis',\n",
       "   'Pattern recognition',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,538',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2154642048',\n",
       "   '1965324089',\n",
       "   '2103496339',\n",
       "   '3017143921',\n",
       "   '2158863190',\n",
       "   '1507849272',\n",
       "   '2131329059',\n",
       "   '3121126077',\n",
       "   '2078626246',\n",
       "   '145476170']},\n",
       " {'id': '2047870719',\n",
       "  'title': 'Topology representing networks',\n",
       "  'abstract': 'Abstract   A Hebbian adaptation rule with winner-take-all like competition is introduced. It is shown that this competitive Hebbian rule forms so-called Delaunay triangulations, which play an important role in computational geometry for efficiently solving proximity problems. Given a set of neural units i, i = 1,…, N, the synaptic weights of which can be interpreted as pointers wi, i = 1,…, N in RD, the competitive Hebbian rule leads to a connectivity structure between the units i that corresponds to the Delaunay triangulation of the set of pointers wi. Such competitive Hebbian rule develops connections (Cij > 0) between neural units i, j with neighboring receptive fields (Voronoi polygons) Vi, Vj, whereas between all other units i, j no connections evolve (Cij = 0). Combined with a procedure that distributes the pointers wi over a given feature manifold M, for example, a submanifold M ⊂ RD, the competitive Hebbian rule provides a novel approach to the problem of constructing topology preserving feature maps and representing intricately structured manifolds. The competitive Hebbian rule connects only neural units, the receptive fields (Voronoi polygons) Vi, Vj of which are adjacent on the given manifold M. This leads to a connectivity structure that defines a perfectly topology preserving map and forms a discrete, path preserving representation of M, also in cases where M has an intricate topology. This makes this novel approach particularly useful in all applications where neighborhood relations have to be exploited or the shape and topology of submanifolds have to be take into account.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Thomas Martinetz', '', 'Klaus Schulten'],\n",
       "  'related_topics': ['Hebbian theory',\n",
       "   'Delaunay triangulation',\n",
       "   'Proximity problems',\n",
       "   'Manifold',\n",
       "   'Computational geometry',\n",
       "   'Path (graph theory)',\n",
       "   'Submanifold',\n",
       "   'Topology (chemistry)',\n",
       "   'Topology',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,130',\n",
       "  'reference_count': '48',\n",
       "  'references': ['2046079134',\n",
       "   '1991848143',\n",
       "   '3017143921',\n",
       "   '65738273',\n",
       "   '22297218',\n",
       "   '2913399920',\n",
       "   '2005314985',\n",
       "   '2166322089',\n",
       "   '2002182716',\n",
       "   '2098929365']},\n",
       " {'id': '2070320140',\n",
       "  'title': 'Image representations for visual learning.',\n",
       "  'abstract': 'Computer vision researchers are developing new approaches to object recognition and detection that are based almost directly on images and avoid the use of intermediate three-dimensional models. Many of these techniques depend on a representation of images that induce a linear vector space structure and in principle requires dense feature correspondence. This image representation allows the use of learning techniques for the analysis of images (for computer vision) as well as for the synthesis of images (for computer graphics).',\n",
       "  'date': '1996',\n",
       "  'authors': ['David Beymer', 'Tomaso Poggio'],\n",
       "  'related_topics': ['Image-based modeling and rendering',\n",
       "   'Image processing',\n",
       "   'Scale-space axioms',\n",
       "   'Feature (computer vision)',\n",
       "   'Computer graphics',\n",
       "   'Visual learning',\n",
       "   'Image analysis',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '402',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2138451337',\n",
       "   '2123977795',\n",
       "   '2095757522',\n",
       "   '2138835141',\n",
       "   '2135463994',\n",
       "   '2142912032',\n",
       "   '94523489',\n",
       "   '1981025032',\n",
       "   '2053197265',\n",
       "   '1963565426']},\n",
       " {'id': '1513400187',\n",
       "  'title': 'Data Structures and Network Algorithms',\n",
       "  'abstract': 'Foundations Disjoint Sets Heaps Search Trees Linking and Cutting Trees Minimum Spanning Trees Shortest Paths Network Flows Matchings.',\n",
       "  'date': '1983',\n",
       "  'authors': ['Robert Endre Tarjan'],\n",
       "  'related_topics': ['Weight-balanced tree',\n",
       "   'Spanning tree',\n",
       "   'Disjoint sets',\n",
       "   'Fibonacci heap',\n",
       "   'Expected linear time MST algorithm',\n",
       "   'Data structure',\n",
       "   'Flow network',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics',\n",
       "   'Network algorithms',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,065',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2019020850',\n",
       "  'title': 'Data Visualization by Multimensional Scaling: A Deterministic Annealing Approach',\n",
       "  'abstract': 'Abstract   Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidean space. The quality of a data embedding is measured by a stress function which compares proximity values with Euclidean distances of the respective points. The corresponding minimization problem is non-convex and sensitive to local minima. We present a novel deterministic annealing algorithm for the frequently used objective SSTRESS and for Sammon mapping, derived in the framework of maximum entropy estimation. Experimental results demonstrate the superiority of our optimization technique compared to conventional gradient descent methods.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Hansjoerg Klock', 'Joachim M. Buhmann'],\n",
       "  'related_topics': ['Nonlinear dimensionality reduction',\n",
       "   'Sammon mapping',\n",
       "   'Stress majorization',\n",
       "   'Gradient descent',\n",
       "   'Euclidean space',\n",
       "   'Maxima and minima',\n",
       "   'Principle of maximum entropy',\n",
       "   'Scaling',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '85',\n",
       "  'reference_count': '36',\n",
       "  'references': ['2117812871',\n",
       "   '2581275558',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '3017143921',\n",
       "   '2095757522',\n",
       "   '2159537329',\n",
       "   '2045682702',\n",
       "   '2077990749',\n",
       "   '2148394752']},\n",
       " {'id': '2099741732',\n",
       "  'title': 'Independent component analysis, a new concept?',\n",
       "  'abstract': 'Abstract   The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Pierre Comon'],\n",
       "  'related_topics': ['Independent component analysis',\n",
       "   'FastICA',\n",
       "   'Principal component analysis',\n",
       "   'Blind signal separation',\n",
       "   'Mutual information',\n",
       "   'Independence (probability theory)',\n",
       "   'Infomax',\n",
       "   'Multivariate random variable',\n",
       "   'Algorithm',\n",
       "   'Calculus',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,026',\n",
       "  'reference_count': '49',\n",
       "  'references': ['1996355918',\n",
       "   '2171074980',\n",
       "   '2018388266',\n",
       "   '2098301339',\n",
       "   '2114018052',\n",
       "   '1560089794',\n",
       "   '2796930440',\n",
       "   '2225937484',\n",
       "   '1995963238',\n",
       "   '2140352766']},\n",
       " {'id': '2108384452',\n",
       "  'title': 'An information-maximization approach to blind separation and blind deconvolution',\n",
       "  'abstract': 'We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Anthony J. Bell', 'Terrence J. Sejnowski'],\n",
       "  'related_topics': ['Blind signal separation',\n",
       "   'Blind deconvolution',\n",
       "   'Independent component analysis',\n",
       "   'Source separation',\n",
       "   'Maximization',\n",
       "   'Infomax',\n",
       "   'Deconvolution',\n",
       "   'Information transfer',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,116',\n",
       "  'reference_count': '50',\n",
       "  'references': ['2124776405',\n",
       "   '2099741732',\n",
       "   '2019502123',\n",
       "   '1996355918',\n",
       "   '2038085771',\n",
       "   '2180838288',\n",
       "   '1667165204',\n",
       "   '2096789154',\n",
       "   '2006544565',\n",
       "   '2056211671']},\n",
       " {'id': '2123977795',\n",
       "  'title': 'Visual learning and recognition of 3-D objects from appearance',\n",
       "  'abstract': \"The problem of automatically learning object models for recognition and pose estimation is addressed. In contrast to the traditional approach, the recognition problem is formulated as one of matching appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties and constant for a rigid object, pose and illumination vary from scene to scene. A compact representation of object appearance is proposed that is parametrized by pose and illumination. For each object of interest, a large set of images is obtained by automatically varying pose and illumination. This image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a manifold. Given an unknown input image, the recognition system projects the image to eigenspace. The object is recognized based on the manifold it lies on. The exact position of the projection on the manifold determines the object's pose in the image. A variety of experiments are conducted using objects with complex appearance characteristics. The performance of the recognition and pose estimation algorithms is studied using over a thousand input images of sample objects. Sensitivity of recognition to the number of eigenspace dimensions and the number of learning samples is analyzed. For the objects used, appearance representation in eigenspaces with less than 20 dimensions produces accurate recognition results with an average pose estimation error of about 1.0 degree. A near real-time recognition system with 20 complex objects in the database has been developed. The paper is concluded with a discussion on various issues related to the proposed learning and recognition methodology.\",\n",
       "  'date': '1995',\n",
       "  'authors': ['Hiroshi Murase', 'Shree K. Nayar'],\n",
       "  'related_topics': ['3D pose estimation',\n",
       "   'Pose',\n",
       "   '3D single-object recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Learning object',\n",
       "   'Projection (set theory)',\n",
       "   'Deep-sky object',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,032',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2170120409',\n",
       "   '2098693229',\n",
       "   '2143956139',\n",
       "   '2130259898',\n",
       "   '2135346934',\n",
       "   '2053197265',\n",
       "   '1996773532',\n",
       "   '2086479969',\n",
       "   '2026311529',\n",
       "   '2105815873']},\n",
       " {'id': '2587818897',\n",
       "  'title': 'Decadal trends in the North Atlantic oscillation: regional temperatures and precipitation',\n",
       "  'abstract': 'Greenland ice-core data have revealed large decadal climate variations over the North Atlantic that can be related to a major source of low-frequency variability, the North Atlantic Oscillation. Over the past decade, the Oscillation has remained in one extreme phase during the winters, contributing significantly to the recent wintertime warmth across Europe and to cold conditions in the northwest Atlantic. An evaluation of the atmospheric moisture budget reveals coherent large-scale changes since 1980 that are linked to recent dry conditions over southern Europe and the Mediterranean, whereas northern Europe and parts of Scandinavia have generally experienced wetter than normal conditions.',\n",
       "  'date': '1996',\n",
       "  'authors': ['J. W.'],\n",
       "  'related_topics': ['North Atlantic oscillation',\n",
       "   'Atlantic Equatorial mode',\n",
       "   'Atlantic multidecadal oscillation',\n",
       "   'Mediterranean climate',\n",
       "   'Precipitation',\n",
       "   'Climatology',\n",
       "   'Oceanography',\n",
       "   'Environmental science',\n",
       "   'Atmospheric moisture',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,837',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1594524188',\n",
       "  'title': 'Content-addressable memories',\n",
       "  'abstract': '1 Associative Memory, Content Addressing, and Associative Recall.- 1.1 Introduction.- 1.1.1 Various Motives for the Development of Content-Addressable Memories.- 1.1.2 Definitions and Explanations of Some Basic Concepts.- 1.2 The Two Basic Implementations of Content Addressing.- 1.2.1 Software Implementation: Hash Coding.- 1.2.2 Hardware Implementation: The CAM.- 1.3 Associations.- 1.3.1 Representation and Retrieval of Associated Items.- 1.3.2 Structures of Associations.- 1.4 Associative Recall: Extensions of Concepts.- 1.4.1 The Classical Laws of Association.- 1.4.2 Similarity Measures.- 1.4.3 The Problem of Infinite Memory.- 1.4.4 Distributed Memory and optimal Associative Mappings.- 1.4.5 Sequential Recollections.- 2 Content Addressing by Software.- 2.1 Hash Coding and Formatted Data Structures.- 2.2 Hashing Functions.- 2.3 Handling of Collisions.- 2.3.1 Some Basic Concepts.- 2.3.2 Open Addressing.- 2.3.3 Chaining (Coalesced).- 2.3.4 Chaining Through a Separate overflow Area.- 2.3.5 Rehashing.- 2.3.6 Shortcut Methods for Speedup of Searching.- 2.4 Organizational Features and Formats of Hash Tables.- 2.4.1 Direct and Indirect Addressing.- 2.4.2 Basic Formats of Hash Tables.- 2.4.3 An Example of Special Hash Table Organization.- 2.5 Evaluation of Different Schemes in Hash Coding.- 2.5.1 Average Length of Search with Different Collision Handling Methods.- 2.5.2 Effect of Hashing Function on the Length of Search.- 2.5.3 Special Considerations for the Case in Which the Search Is Unsuccessful.- 2.6 Multi-Key Search.- 2.6.1 Lists and List Structures.- 2.6.2 An Example of Implementation of Multi-Key Search by Hash Index Tables.- 2.6.3 The Use of Compound Keywords in Hashing.- 2.7 Implementation of Proximity Search by Hash Coding.- 2.8 The TRIE Memory.- 2.9 Survey of Literature on Hash Coding and Related Topics.- 3 Logic Principles of Content-Addressable Memories.- 3.1 Present-Day Needs for Hardware CAMs.- 3.2 The Logic of Comparison Operations.- 3.3 The All-Parallel CAM.- 3.3.1 Circuit Logic of a CAM Bit Cell.- 3.3.2 Handling of Responses from the CAM Array.- 3.3.3 The Complete CAM Organization.- 3.3.4 Magnitude Search with the All-Parallel CAM.- 3.4 The Word-Parallel, Bit-Serial CAM.- 3.4.1 Implementation of the CAM by the Linear-Select Memory Principle.- 3.4.2 Skew Addressing.- 3.4.3 Shift Register Implementation.- 3.4.4 The Results Storage.- 3.4.5 Searching on More Complex Specifications.- 3.5 The Word-Serial, Bit-Parallel CAM.- 3.6 Byte-Serial Content-Addressable Search.- 3.6.1 Coding by the Characters.- 3.6.2 Specifications Used in Document Retrieval.- 3.6.3 A Record-Parallel, Byte-Serial CAM for Document Retrieval.- 3.7 Functional Memories.- 3.7.1 The Logic of the Bit Cell in the FM.- 3.7.2 Functional Memory 1.- 3.7.3 Functional Memory 2.- 3.7.4 Read-only Functional Memory.- 3.8 A Formalism for the Description of Micro-Operations in the CAM.- 3.9 Survey of Literature on CAMs.- 4 CAM Hardware.- 4.1 The State-of-the-Art of the Electronic CAM Devices.- 4.2 Circuits for All-Parallel CAMs.- 4.2.1 Active Electronic Circuits for CAM Bit Cells.- 4.2.2 Cryotron-Element CAMs.- 4.2.3 Josephson Junctions and SQUIDs for Memories.- 4.3 Circuits for Bit-Serial and Word-Serial CAMs.- 4.3.1 Semiconductor RAM Modules for the CAM.- 4.3.2 Magnetic Memory Implementations of the CAM.- 4.3.3 Shift Registers for Content-Addressable Memory.- 4.3.4 The Charge-Coupled Device (CCD).- 4.3.5 The Magnetic-Bubble Memory (MBM).- 4.4 Optical Content-Addressable Memories.- 4.4.1 Magneto-Optical Memories.- 4.4.2 Holographic Content-Addressable Memories.- 5 The CAM as a System Part.- 5.1 The CAM in Virtual Memory Systems.- 5.1.1 The Memory Hierarchy.- 5.1.2 The Concept of Virtual Memory and the Cache.- 5.1.3 Memory Mappings for the Cache.- 5.1.4 Replacement Algorithms.- 5.1.5 Updating of Multilevel Memories.- 5.1.6 Automatic Control of the Cache Operations.- 5.1.7 Buffering in a Multiprocessor System.- 5.1.8 Additional Literature on Memory Organizations and Their Evaluation.- 5.2 Utilization of the CAM in Dynamic Memory Allocation.- 5.2.1 Memory Map and Address Conversion.- 5.2.2 Loading of a Program Segment.- 5.3 Content-Addressable Buffer.- 5.4 Programmable Logic.- 5.4.1 RAM Implementation.- 5.4.2 CAM Implementation.- 5.4.3 FM Implementation.- 5.4.4 Other Implementations of Programmable Logic.- 5.4.5 Applications of the CAM in Various Control Operations.- 6 Content-Addressable Processors.- 6.1. Some Trends in Content-Addressable Memory Functions.- 6.2 Distributed-Logic Memories (DLMs).- 6.3 The Augmented Content-Addressable Memory (ACAM).- 6.4 The Association-Storing Processor (ASP).- 6.5 Content-Addressable Processors with High-Level Processing Elements.- 6.5.1 The Basic Array Processor Architecture.- 6.5.2 The Associative Control Switch (ACS) Architecture.- 6.5.3 An Example of Control-Addressable Array Processors: RADCAP.- 6.5.4 An Example of Content-Addressable Ensemble Processors: PEPE.- 6.6 Bit-Slice Content-Addressable Processors.- 6.6.1 The STARAN Computer.- 6.6.2 Orthogonal Computers.- 6.7 An Overview of Parallel Processors.- 6.7.1 Categorizations of Computer Architectures.- 6.7.2 Survey of Additional Literature on Content-Addressable and Parallel Processing.- 7 Review of Research Since 1979.- 7.1 Research on Hash Coding.- 7.1.1 Review Articles.- 7.1.2 Hashing Functions.- 7.1.3 Handling of Collisions.- 7.1.4 Hash Table Organization.- 7.1.5 Linear Hashing.- 7.1.6 Dynamic, Extendible, and External Hashing.- 7.1.7 Multiple-Key and Partial-Match Hashing.- 7.1.8 Hash-Coding Applications.- 7.1.9 Hash-Coding Hardware.- 7.2 CAM Hardware.- 7.2.1 CAM Cells.- 7.2.2 CAM Arrays.- 7.2.3 Dynamic Memories.- 7.2.4 CAM Systems.- 7.3 CAM Applications.- 7.4 Content-Addressable Parallel Processors.- 7.4.1 Architectures for Content-Addressable Processors.- 7.4.2 Data Base Machines.- 7.4.3 Applications of Content-Addressable Processors.- 7.5 Optical Associative Memories.- 7.5.1 General.- 7.5.2 Holographic Content-Addressable Memories.- References.',\n",
       "  'date': '1980',\n",
       "  'authors': ['Teuvo'],\n",
       "  'related_topics': ['Memory map',\n",
       "   'Hash table',\n",
       "   'Content-addressable memory',\n",
       "   'Hash function',\n",
       "   'Distributed memory',\n",
       "   'Linear hashing',\n",
       "   'Open addressing',\n",
       "   'Virtual memory',\n",
       "   'Parallel computing',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '474',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2086789740',\n",
       "  'title': 'Perceptrons: An Introduction to Computational Geometry',\n",
       "  'abstract': \"Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons\",\n",
       "  'date': '1969',\n",
       "  'authors': ['Marvin Lee', 'Seymour'],\n",
       "  'related_topics': ['Computational geometry',\n",
       "   'Perceptron',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Cognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,270',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2970228278',\n",
       "  'title': 'The Neurophysiological Basis of Mind',\n",
       "  'abstract': '',\n",
       "  'date': '1953',\n",
       "  'authors': ['Sir John C.'],\n",
       "  'related_topics': ['Neurophysiology',\n",
       "   'Basis (linear algebra)',\n",
       "   'Cognitive science',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '584',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2076870593',\n",
       "  'title': 'The neuronal basis of behavior in Tritonia. I. Functional organization of the central nervous system.',\n",
       "  'abstract': 'An account is presented of the brain (paired cerebral, pleural, and pedal ganglia) of the nudibranch mollusc Tritonia diomedia. The major efferent nerve fibers are related to their nerve cell bodies and their functional roles identified as far as possible. The channels of sesory input relating to some of these neurons are given so as to provide an overall view of the organization of the brain.\\r\\n\\r\\n\\r\\n\\r\\nA standardized system of abbreviation and notation for the central ganglia, nerve trunks, and gaint somata is proposed. The system of references is intended to provide a guide to the location in the ganglia of many of the smaller neurons of which the functional attributes are known, but which cannot be consistently recognized on visual criteria alone. A system of rectangular coordinates is proposed which is applied to the natural outline of the ganglia. In addition, a system of cell notation is described which is independent of the co-ordinates used to define the position of the cell on the grid. Cell which by reason of their size, pigmentation, characteristic location and physiological attributes are consistently recognizable from animal to animal are numbered. Two principles were followed in numbering cells; (i) the series begins at unity in each ganglion; (ii) cell homologues in opposite ganglia are given the same number, but distinguished by prefixing the abbreviation for the ganglion in which they occur. It is considered that the system will facilitate the exchange of information between workers on the same species, and also benefit the comparison of neural organization of behavior in closely related forms.\\r\\n\\r\\n\\r\\n\\r\\nThe brain is organized in an almost exactly bilaterally symmetrical manner. There are a few bilateral neural pathways, but the major functional routes are ipsilateral. A few motorneurons, which are uniquely identifiable anatomically, cause unique, discrete movements. Others are in small groups sharing overlapping or similar functions.',\n",
       "  'date': '1973',\n",
       "  'authors': ['A. O. D. Willows', 'D. A. Dorsett', 'G. Hoyle'],\n",
       "  'related_topics': ['Efferent nerve',\n",
       "   'Ganglion',\n",
       "   'Central nervous system',\n",
       "   'Neuroscience',\n",
       "   'Tritonia (gastropod)',\n",
       "   'Anatomy',\n",
       "   'Basis (universal algebra)',\n",
       "   'Biology',\n",
       "   'Cell bodies',\n",
       "   'Functional organization',\n",
       "   'Rectangular coordinates',\n",
       "   'View Less'],\n",
       "  'citation_count': '202',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2167531287',\n",
       "   '1910258660',\n",
       "   '2074308372',\n",
       "   '1910429665',\n",
       "   '2167089286',\n",
       "   '2076915589',\n",
       "   '1991985107',\n",
       "   '2055408552',\n",
       "   '2042776884',\n",
       "   '2020100877']},\n",
       " {'id': '2137983211',\n",
       "  'title': 'Multilayer feedforward networks are universal approximators',\n",
       "  'abstract': 'Abstract   This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.',\n",
       "  'date': '1989',\n",
       "  'authors': ['K. Hornik', 'M. Stinchcombe', 'H. White'],\n",
       "  'related_topics': ['Universal approximation theorem',\n",
       "   'Borel measure',\n",
       "   'Feed forward',\n",
       "   'Function (mathematics)',\n",
       "   'Stone–Weierstrass theorem',\n",
       "   'Algorithm',\n",
       "   'Degree (graph theory)',\n",
       "   'Class (philosophy)',\n",
       "   'Mathematics',\n",
       "   'Finite dimensional space',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,241',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2103496339',\n",
       "   '3146803896',\n",
       "   '2056099894',\n",
       "   '2416739038',\n",
       "   '2090270852',\n",
       "   '1654142532',\n",
       "   '1581292930',\n",
       "   '2097415784',\n",
       "   '135768573']},\n",
       " {'id': '3004157836',\n",
       "  'title': 'Numerical Recipes, The Art of Scientific Computing',\n",
       "  'abstract': '',\n",
       "  'date': '1987',\n",
       "  'authors': ['William H.', 'Brian P.', 'Saul A.', 'William T.', 'Harvey'],\n",
       "  'related_topics': ['Data science', 'Physics'],\n",
       "  'citation_count': '20,866',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2140196014',\n",
       "  'title': 'The JPEG still picture compression standard',\n",
       "  'abstract': \"A joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG's proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT (discrete cosine transform)-based method is specified for 'lossy' compression, and a predictive method for 'lossless' compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. The author provides an overview of the JPEG standard, and focuses in detail on the Baseline method. >\",\n",
       "  'date': '1992',\n",
       "  'authors': ['G.K. Wallace'],\n",
       "  'related_topics': ['Lossless JPEG',\n",
       "   'JPEG 2000',\n",
       "   'JPEG',\n",
       "   'JPEG File Interchange Format',\n",
       "   'Quantization (image processing)',\n",
       "   'Lossy compression',\n",
       "   'Data compression',\n",
       "   'Compression artifact',\n",
       "   'Lossless compression',\n",
       "   'Transform coding',\n",
       "   'Discrete cosine transform',\n",
       "   'Signal compression',\n",
       "   'Grayscale',\n",
       "   'Digital image',\n",
       "   'Quantization (signal processing)',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,448',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2019972422',\n",
       "   '2026723094',\n",
       "   '1501108238',\n",
       "   '2073501560',\n",
       "   '2162168771',\n",
       "   '1572731687',\n",
       "   '1561761812',\n",
       "   '2150013946',\n",
       "   '1992371956']},\n",
       " {'id': '1634005169',\n",
       "  'title': 'Vector Quantization and Signal Compression',\n",
       "  'abstract': '1 Introduction.- 1.1 Signals, Coding, and Compression.- 1.2 Optimality.- 1.3 How to Use this Book.- 1.4 Related Reading.- I Basic Tools.- 2 Random Processes and Linear Systems.- 2.1 Introduction.- 2.2 Probability.- 2.3 Random Variables and Vectors.- 2.4 Random Processes.- 2.5 Expectation.- 2.6 Linear Systems.- 2.7 Stationary and Ergodic Properties.- 2.8 Useful Processes.- 2.9 Problems.- 3 Sampling.- 3.1 Introduction.- 3.2 Periodic Sampling.- 3.3 Noise in Sampling.- 3.4 Practical Sampling Schemes.- 3.5 Sampling Jitter.- 3.6 Multidimensional Sampling.- 3.7 Problems.- 4 Linear Prediction.- 4.1 Introduction.- 4.2 Elementary Estimation Theory.- 4.3 Finite-Memory Linear Prediction.- 4.4 Forward and Backward Prediction.- 4.5 The Levinson-Durbin Algorithm.- 4.6 Linear Predictor Design from Empirical Data.- 4.7 Minimum Delay Property.- 4.8 Predictability and Determinism.- 4.9 Infinite Memory Linear Prediction.- 4.10 Simulation of Random Processes.- 4.11 Problems.- II Scalar Coding.- 5 Scalar Quantization I.- 5.1 Introduction.- 5.2 Structure of a Quantizer.- 5.3 Measuring Quantizer Performance.- 5.4 The Uniform Quantizer.- 5.5 Nonuniform Quantization and Companding.- 5.6 High Resolution: General Case.- 5.7 Problems.- 6 Scalar Quantization II.- 6.1 Introduction.- 6.2 Conditions for Optimality.- 6.3 High Resolution Optimal Companding.- 6.4 Quantizer Design Algorithms.- 6.5 Implementation.- 6.6 Problems.- 7 Predictive Quantization.- 7.1 Introduction.- 7.2 Difference Quantization.- 7.3 Closed-Loop Predictive Quantization.- 7.4 Delta Modulation.- 7.5 Problems.- 8 Bit Allocation and Transform Coding.- 8.1 Introduction.- 8.2 The Problem of Bit Allocation.- 8.3 Optimal Bit Allocation Results.- 8.4 Integer Constrained Allocation Techniques.- 8.5 Transform Coding.- 8.6 Karhunen-Loeve Transform.- 8.7 Performance Gain of Transform Coding.- 8.8 Other Transforms.- 8.9 Sub-band Coding.- 8.10 Problems.- 9 Entropy Coding.- 9.1 Introduction.- 9.2 Variable-Length Scalar Noiseless Coding.- 9.3 Prefix Codes.- 9.4 Huffman Coding.- 9.5 Vector Entropy Coding.- 9.6 Arithmetic Coding.- 9.7 Universal and Adaptive Entropy Coding.- 9.8 Ziv-Lempel Coding.- 9.9 Quantization and Entropy Coding.- 9.10 Problems.- III Vector Coding.- 10 Vector Quantization I.- 10.1 Introduction.- 10.2 Structural Properties and Characterization.- 10.3 Measuring Vector Quantizer Performance.- 10.4 Nearest Neighbor Quantizers.- 10.5 Lattice Vector Quantizers.- 10.6 High Resolution Distortion Approximations.- 10.7 Problems.- 11 Vector Quantization II.- 11.1 Introduction.- 11.2 Optimality Conditions for VQ.- 11.3 Vector Quantizer Design.- 11.4 Design Examples.- 11.5 Problems.- 12 Constrained Vector Quantization.- 12.1 Introduction.- 12.2 Complexity and Storage Limitations.- 12.3 Structurally Constrained VQ.- 12.4 Tree-Structured VQ.- 12.5 Classified VQ.- 12.6 Transform VQ.- 12.7 Product Code Techniques.- 12.8 Partitioned VQ.- 12.9 Mean-Removed VQ.- 12.10 Shape-Gain VQ.- 12.11 Multistage VQ.- 12.12 Constrained Storage VQ.- 12.13 Hierarchical and Multiresolution VQ.- 12.14 Nonlinear Interpolative VQ.- 12.15 Lattice Codebook VQ.- 12.16 Fast Nearest Neighbor Encoding.- 12.17 Problems.- 13 Predictive Vector Quantization.- 13.1 Introduction.- 13.2 Predictive Vector Quantization.- 13.3 Vector Linear Prediction.- 13.4 Predictor Design from Empirical Data.- 13.5 Nonlinear Vector Prediction.- 13.6 Design Examples.- 13.7 Problems.- 14 Finite-State Vector Quantization.- 14.1 Recursive Vector Quantizers.- 14.2 Finite-State Vector Quantizers.- 14.3 Labeled-States and Labeled-Transitions.- 14.4 Encoder/Decoder Design.- 14.5 Next-State Function Design.- 14.6 Design Examples.- 14.7 Problems.- 15 Tree and Trellis Encoding.- 15.1 Delayed Decision Encoder.- 15.2 Tree and Trellis Coding.- 15.3 Decoder Design.- 15.4 Predictive Trellis Encoders.- 15.5 Other Design Techniques.- 15.6 Problems.- 16 Adaptive Vector Quantization.- 16.1 Introduction.- 16.2 Mean Adaptation.- 16.3 Gain-Adaptive Vector Quantization.- 16.4 Switched Codebook Adaptation.- 16.5 Adaptive Bit Allocation.- 16.6 Address VQ.- 16.7 Progressive Code Vector Updating.- 16.8 Adaptive Codebook Generation.- 16.9 Vector Excitation Coding.- 16.10 Problems.- 17 Variable Rate Vector Quantization.- 17.1 Variable Rate Coding.- 17.2 Variable Dimension VQ.- 17.3 Alternative Approaches to Variable Rate VQ.- 17.4 Pruned Tree-Structured VQ.- 17.5 The Generalized BFOS Algorithm.- 17.6 Pruned Tree-Structured VQ.- 17.7 Entropy Coded VQ.- 17.8 Greedy Tree Growing.- 17.9 Design Examples.- 17.10 Bit Allocation Revisited.- 17.11 Design Algorithms.- 17.12 Problems.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Allen Gersho', 'Robert M. Gray'],\n",
       "  'related_topics': ['Vector quantization',\n",
       "   'Quantization (signal processing)',\n",
       "   'Linde–Buzo–Gray algorithm',\n",
       "   'Huffman coding',\n",
       "   'Arithmetic coding',\n",
       "   'Entropy encoding',\n",
       "   'Coding theory',\n",
       "   'Transform coding',\n",
       "   'Algorithm',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,317',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '3146803896',\n",
       "  'title': 'Multilayer feedforward networks are universal approximators',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['', '', ''],\n",
       "  'related_topics': ['Universal approximation theorem',\n",
       "   'Feed forward',\n",
       "   'Computer science',\n",
       "   'Control theory',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,971',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1971735090',\n",
       "  'title': 'On the approximate realization of continuous mappings by neural networks',\n",
       "  'abstract': \"Abstract   In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.\",\n",
       "  'date': '1989',\n",
       "  'authors': ['K. Funahashi'],\n",
       "  'related_topics': ['Universal approximation theorem',\n",
       "   'Sigmoid function',\n",
       "   'Artificial neural network',\n",
       "   'Realization (systems)',\n",
       "   'Backpropagation',\n",
       "   'Point (geometry)',\n",
       "   'Topology',\n",
       "   'Discrete mathematics',\n",
       "   'Unit (ring theory)',\n",
       "   'Mathematics',\n",
       "   'Hidden layer',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,215',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2154642048',\n",
       "   '2042264548',\n",
       "   '1554576613',\n",
       "   '3036751298',\n",
       "   '1613359937',\n",
       "   '3108739439',\n",
       "   '2152088994',\n",
       "   '3040500874',\n",
       "   '2105393299',\n",
       "   '2189011649']},\n",
       " {'id': '2913399920',\n",
       "  'title': 'Vector quantization',\n",
       "  'abstract': 'A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications.',\n",
       "  'date': '1984',\n",
       "  'authors': ['R. Gray'],\n",
       "  'related_topics': ['Vector quantization',\n",
       "   'Quantization (signal processing)',\n",
       "   'Speech coding',\n",
       "   'Linde–Buzo–Gray algorithm',\n",
       "   'Data compression',\n",
       "   'Information theory',\n",
       "   'Speech processing',\n",
       "   'Scalar (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,332',\n",
       "  'reference_count': '44',\n",
       "  'references': ['2134383396',\n",
       "   '2127218421',\n",
       "   '1995875735',\n",
       "   '2583466288',\n",
       "   '2142228262',\n",
       "   '2021760654',\n",
       "   '2164240509',\n",
       "   '2150418026',\n",
       "   '2119352491',\n",
       "   '2040336387']},\n",
       " {'id': '2096710051',\n",
       "  'title': 'Detection of signals by information theoretic criteria',\n",
       "  'abstract': 'A new approach is presented to the problem of detecting the number of signals in a multichannel time-series, based on the application of the information theoretic criteria for model selection introduced by Akaike (AIC) and by Schwartz and Rissanen (MDL). Unlike the conventional hypothesis testing based approach, the new approach does not requite any subjective threshold settings; the number of signals is obtained merely by minimizing the AIC or the MDL criteria. Simulation results that illustrate the performance of the new method for the detection of the number of signals received by a sensor array are presented.',\n",
       "  'date': '1985',\n",
       "  'authors': ['M. Wax', 'T. Kailath'],\n",
       "  'related_topics': ['Akaike information criterion',\n",
       "   'Statistical hypothesis testing',\n",
       "   'Model selection',\n",
       "   'Information theory',\n",
       "   'Detection theory',\n",
       "   'Sensor array',\n",
       "   'Covariance matrix',\n",
       "   'Signal processing',\n",
       "   'Algorithm',\n",
       "   'Electronic engineering',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,238',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2168175751',\n",
       "   '2142635246',\n",
       "   '2113638573',\n",
       "   '2058815839',\n",
       "   '2054658115',\n",
       "   '2106596127',\n",
       "   '1974513581',\n",
       "   '2019833178',\n",
       "   '2165887549',\n",
       "   '1500470240']},\n",
       " {'id': '2017977879',\n",
       "  'title': 'Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting',\n",
       "  'abstract': 'Abstract Locally weighted regression, or loess, is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. With local fitting we can estimate a much wider class of regression surfaces than with the usual classes of parametric functions, such as polynomials. The goal of this article is to show, through applications, how loess can be used for three purposes: data exploration, diagnostic checking of parametric models, and providing a nonparametric regression surface. Along the way, the following methodology is introduced: (a) a multivariate smoothing procedure that is an extension of univariate locally weighted regression; (b) statistical procedures that are analogous to those used in the least-squares fitting of parametric functions; (c) several graphical methods that are useful tools for understanding loess estimates and checking the a...',\n",
       "  'date': '1988',\n",
       "  'authors': ['William S. Cleveland', 'Susan J. Devlin'],\n",
       "  'related_topics': ['Local regression',\n",
       "   'Nonparametric regression',\n",
       "   'Regression diagnostic',\n",
       "   'Segmented regression',\n",
       "   'Polynomial regression',\n",
       "   'Regression analysis',\n",
       "   'Smoothing',\n",
       "   'Univariate',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,937',\n",
       "  'reference_count': '40',\n",
       "  'references': ['2611591252',\n",
       "   '1506069954',\n",
       "   '2024081693',\n",
       "   '2166163519',\n",
       "   '2091886411',\n",
       "   '2801830100',\n",
       "   '2030748132',\n",
       "   '3000332379',\n",
       "   '2112081648',\n",
       "   '2025320861']},\n",
       " {'id': '2166116275',\n",
       "  'title': 'Universal approximation bounds for superpositions of a sigmoidal function',\n",
       "  'abstract': 'Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings. >',\n",
       "  'date': '1993',\n",
       "  'authors': ['A.R. Barron'],\n",
       "  'related_topics': ['Approximation error',\n",
       "   'Linear approximation',\n",
       "   'Function approximation',\n",
       "   'Universal approximation theorem',\n",
       "   'Approximation theory',\n",
       "   'Sigmoid function',\n",
       "   'Series expansion',\n",
       "   'Mean squared error',\n",
       "   'Applied mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,956',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2137983211',\n",
       "   '2103496339',\n",
       "   '2084544490',\n",
       "   '2095734615',\n",
       "   '1559907478',\n",
       "   '2044828368',\n",
       "   '2108959409',\n",
       "   '2095546965',\n",
       "   '2112027492',\n",
       "   '2151029520']},\n",
       " {'id': '5731987',\n",
       "  'title': 'Original Contribution: Principal components, minor components, and linear neural networks',\n",
       "  'abstract': 'Many neural network realizations have been recently proposed for the statistical technique of Principal Component Analysis (PCA). Explicit connections between numerical constrained adaptive algorithms and neural networks with constrained Hebbian learning rules are reviewed. The Stochastic Gradient Ascent (SGA) neural network is proposed and shown to be closely related to the Generalized Hebbian Algorithm (GHA). The SGA behaves better for extracting the less dominant eigenvectors. The SGA algorithm is further extended to the case of learning minor components. The symmetrical Subspace Network is known to give a rotated basis of the dominant eigenvector subspace, but usually not the true eigenvectors themselves. Two extensions are proposed: in the first one, each neuron has a scalar parameter which breaks the symmetry. True eigenvectors are obtained in a local and fully parallel learning rule. In the second one, the case of an arbitrary number of parallel neurons is considered, not necessarily less than the input vector dimension.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Erkki Oja'],\n",
       "  'related_topics': ['Generalized Hebbian Algorithm',\n",
       "   \"Oja's rule\",\n",
       "   'Artificial neural network',\n",
       "   'Hebbian theory',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Principal component analysis',\n",
       "   'Gradient descent',\n",
       "   'Subspace topology',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,164',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2115907784',\n",
       "   '2131329059',\n",
       "   '2122925692',\n",
       "   '2078626246',\n",
       "   '2432567885',\n",
       "   '2023963201',\n",
       "   '2017257315',\n",
       "   '1564660545',\n",
       "   '2133884101',\n",
       "   '1981479913']},\n",
       " {'id': '2142228262',\n",
       "  'title': 'Asymptotically optimal block quantization',\n",
       "  'abstract': 'In 1948 W. R. Bennett used a companding model for nonuniform quantization and proposed the formula D \\\\: = \\\\: \\\\frac{1}{12N^{2}} \\\\: \\\\int \\\\: p(x) [ E(x) ]^{-2} \\\\dx for the mean-square quantizing error where N is the number of levels, p (x) is the probability density of the input, and E \\\\prime (x) is the slope of the compressor curve. The formula, an approximation based on the assumption that the number of levels is large and overload distortion is negligible, is a useful tool for analytical studies of quantization. This paper gives a heuristic argument generalizing Bennett\\'s formula to block quantization where a vector of random variables is quantized. The approach is again based on the asymptotic situation where N , the number of quantized output vectors, is very large. Using the resulting heuristic formula, an optimization is performed leading to an expression for the minimum quantizing noise attainable for any block quantizer of a given block size k . The results are consistent with Zador\\'s results and specialize to known results for the one- and two-dimensional cases and for the case of infinite block length (k \\\\rightarrow \\\\infty) . The same heuristic approach also gives an alternate derivation of a bound of Elias for multidimensional quantization. Our approach leads to a rigorous method for obtaining upper bounds on the minimum distortion for block quantizers. In particular, for k = 3 we give a tight upper bound that may in fact be exact. The idea of representing a block quantizer by a block \"compressor\" mapping followed with an optimal quantizer for uniformly distributed random vectors is also explored. It is not always possible to represent an optimal quantizer with this block companding model.',\n",
       "  'date': '1979',\n",
       "  'authors': ['A. Gersho'],\n",
       "  'related_topics': ['Quantization (signal processing)',\n",
       "   'Upper and lower bounds',\n",
       "   'Heuristic argument',\n",
       "   'Block size',\n",
       "   'Random variable',\n",
       "   'Asymptotically optimal algorithm',\n",
       "   'Companding',\n",
       "   'Multidimensional systems',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,342',\n",
       "  'reference_count': '12',\n",
       "  'references': ['1978382377',\n",
       "   '2146519989',\n",
       "   '2004003571',\n",
       "   '2156908459',\n",
       "   '2001968606',\n",
       "   '2137263269',\n",
       "   '1589055062',\n",
       "   '2068071220',\n",
       "   '1972736931',\n",
       "   '2152316618']},\n",
       " {'id': '2063971957',\n",
       "  'title': 'Self-organizing neural network that discovers surfaces in random-dot stereograms',\n",
       "  'abstract': 'THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Suzanna Becker', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Perceptual learning',\n",
       "   'Modality (human–computer interaction)',\n",
       "   'Artificial neural network',\n",
       "   'Random dot stereogram',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Backpropagation',\n",
       "   'Computer vision',\n",
       "   'Perception',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '466',\n",
       "  'reference_count': '3',\n",
       "  'references': ['1498436455', '2797583072', '1944592753']},\n",
       " {'id': '2079782346',\n",
       "  'title': 'Statistical theory of learning curves under entropic loss criterion',\n",
       "  'abstract': 'The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Shun-Ichi Amari', 'Noboru Murata'],\n",
       "  'related_topics': ['Early stopping',\n",
       "   'Artificial neural network',\n",
       "   'Generalization',\n",
       "   'Entropy (information theory)',\n",
       "   'Conditional probability distribution',\n",
       "   'Statistical theory',\n",
       "   'Stochastic modelling',\n",
       "   'Learning curve',\n",
       "   'Applied mathematics',\n",
       "   'Mathematics',\n",
       "   'Statistics',\n",
       "   'View Less'],\n",
       "  'citation_count': '213',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2154642048',\n",
       "   '2142635246',\n",
       "   '2165758113',\n",
       "   '2019363670',\n",
       "   '2020246210',\n",
       "   '1520168181',\n",
       "   '1968908999',\n",
       "   '2098545770',\n",
       "   '3137895569',\n",
       "   '2322002063']},\n",
       " {'id': '2089419199',\n",
       "  'title': 'Asymptotic quantization error of continuous signals and the quantization dimension',\n",
       "  'abstract': \"Extensions of the limiting qnanfizafion error formula of Bennet are proved. These are of the form D_{s,k}(N,F)=N^{-\\\\beta}B , where N is the number of output levels, D_{s,k}(N,F) is the s th moment of the metric distance between quantizer input and output, \\\\beta,B>0,k=s/\\\\beta is the signal space dimension, and F is the signal distribution. If a suitably well-behaved k -dimensional signal density f(x) exists, B=b_{s,k}[\\\\int f^{\\\\rho}(x)dx]^{1/ \\\\rho},\\\\rho=k/(s+k) , and b_{s,k} does not depend on f . For k=1,s=2 this reduces to Bennett's formula. If F is the Cantor distribution on [0,1],0 and this k equals the fractal dimension of the Cantor set [12,13] . Random quantization, optimal quantization in the presence of an output information constraint, and quantization noise in high dimensional spaces are also investigated.\",\n",
       "  'date': '1982',\n",
       "  'authors': ['P.'],\n",
       "  'related_topics': ['Cantor distribution',\n",
       "   'Quantization (signal processing)',\n",
       "   'Cantor set',\n",
       "   'Fractal',\n",
       "   'Combinatorics',\n",
       "   'Discrete mathematics',\n",
       "   'Fractal dimension',\n",
       "   'Mathematics',\n",
       "   'High dimensional',\n",
       "   'Limiting',\n",
       "   'Space dimension',\n",
       "   'View Less'],\n",
       "  'citation_count': '631',\n",
       "  'reference_count': '4',\n",
       "  'references': ['2142228262', '1973387369', '1976356564', '2001968606']},\n",
       " {'id': '2162604518',\n",
       "  'title': 'Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Laplacian sources',\n",
       "  'abstract': 'Lattice vector quantizer design procedures for nonuniform sources are presented. The procedures yield lattice vector quantizers with excellent performance and retaining the structure required for fast quantization. Analytical methods for truncating and scaling lattices to be used in vector quantizations are given, and their utility is demonstrated for independent and identically distributed (i.i.d.) Gaussian and Laplacian sources. An analytical technique for piecewise linear multidimensional compandor designs is evaluated for i.i.d. Gaussian and Laplacian sources by comparing its performance to that of the other vector quantizers. >',\n",
       "  'date': '1993',\n",
       "  'authors': ['D.G. Jeong', 'J.D.'],\n",
       "  'related_topics': ['Vector Laplacian',\n",
       "   'Vector quantization',\n",
       "   'Quantization (signal processing)',\n",
       "   'Gaussian',\n",
       "   'Piecewise linear function',\n",
       "   'Piecewise',\n",
       "   'Multidimensional systems',\n",
       "   'Laplace operator',\n",
       "   'Applied mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '132',\n",
       "  'reference_count': '39',\n",
       "  'references': ['1634005169',\n",
       "   '2134383396',\n",
       "   '2186435531',\n",
       "   '1565930783',\n",
       "   '2142228262',\n",
       "   '2801840425',\n",
       "   '2119352491',\n",
       "   '2063678710',\n",
       "   '2089419199',\n",
       "   '2029495080']},\n",
       " {'id': '2155487652',\n",
       "  'title': 'On Edge Detection',\n",
       "  'abstract': 'Edge detection is the process that attempts to characterize the intensity changes in the image in terms of the physical processes that have originated them. A critical, intermediate goal of edge detection is the detection and characterization of significant intensity changes. This paper discusses this part of the edge detection problem. To characterize the types of intensity changes derivatives of different types, and possibly different scales, are needed. Thus, we consider this part of edge detection as a problem in numerical differentiation. We show that numerical differentiation of images is an ill-posed problem in the sense of Hadamard. Differentiation needs to be regularized by a regularizing filtering operation before differentiation. This shows that this part of edge detection consists of two steps, a filtering step and a differentiation step. Following this perspective, the paper discusses in detail the following theoretical aspects of edge detection. 1) The properties of different types of filters-with minimal uncertainty, with a bandpass spectrum, and with limited support-are derived. Minimal uncertainty filters optimize a tradeoff between computational efficiency and regularizing properties. 2) Relationships among several 2-D differential operators are established. In particular, we characterize the relation between the Laplacian and the second directional derivative along the gradient. Zero crossings of the Laplacian are not the only features computed in early vision. 3) Geometrical and topological properties of the zero crossings of differential operators are studied in terms of transversality and Morse theory.',\n",
       "  'date': '1986',\n",
       "  'authors': ['Vincent Torre', 'Tomaso A. Poggio'],\n",
       "  'related_topics': ['Edge detection',\n",
       "   'Numerical differentiation',\n",
       "   'Laplace operator',\n",
       "   'Directional derivative',\n",
       "   'Differential operator',\n",
       "   'Hadamard transform',\n",
       "   'Morse theory',\n",
       "   'Regularization (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Mathematical analysis',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,324',\n",
       "  'reference_count': '27',\n",
       "  'references': ['2740373864',\n",
       "   '2109863423',\n",
       "   '2003370853',\n",
       "   '2006500012',\n",
       "   '1995756857',\n",
       "   '2133155955',\n",
       "   '2007057443',\n",
       "   '2121203842',\n",
       "   '2038584908',\n",
       "   '2073974819']},\n",
       " {'id': '1995169133',\n",
       "  'title': 'Boltzmann machines for speech recognition',\n",
       "  'abstract': 'Boltzmann machines offer a new and exciting approach to automatic speech recognition, and provide a rigorous mathematical formalism for parallel computing arrays. In this paper we briefly summarize Boltzmann machine theory, and present results showing their ability to recognize both static and time-varying speech patterns. A machine with 2000 units was able to distinguish between the 11 steady-state vowels in English with an accuracy of 85%. The stability of the learning algorithm and methods of preprocessing and coding speech data before feeding it to the machine are also discussed. A new type of unit called a carry input unit, which involves a type of state-feedback, was developed for the processing of time-varying patterns and this was tested on a few short sentences. Use is made of the implications of recent work into associative memory, and the modelling of neural arrays to suggest a good configuration of Boltzmann machines for this sort of pattern recognition.',\n",
       "  'date': '1986',\n",
       "  'authors': ['R. W. Prager', 'T. D. Harrison', 'F. Fallside'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Content-addressable memory',\n",
       "   'sort',\n",
       "   'Speech recognition',\n",
       "   'Preprocessor',\n",
       "   'Coding (social sciences)',\n",
       "   'Boltzmann constant',\n",
       "   'Computer science',\n",
       "   'Speech patterns',\n",
       "   'View Less'],\n",
       "  'citation_count': '101',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2581275558',\n",
       "   '1652505363',\n",
       "   '1997063559',\n",
       "   '2293063825',\n",
       "   '1507849272',\n",
       "   '2171850596',\n",
       "   '2112325651',\n",
       "   '2056760934',\n",
       "   '2157629899',\n",
       "   '1981025738']},\n",
       " {'id': '2591802459',\n",
       "  'title': 'G‐maximization: an unsupervised learning procedure for discovering regularities',\n",
       "  'abstract': 'Hill climbing is used to maximize an information theoretic measure of the difference betwen the actual behavior of a unit and the behavior that would be predicted by a statistician who knew the first order statistics of the inputs but believed them to be independent. This causes the unit to detect higher order correlations among its inputs. Initial simulations are presented, and seem encouraging. We describe an extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Barak A.', 'Geoffrey E.'],\n",
       "  'related_topics': ['Competitive learning',\n",
       "   'Unsupervised learning',\n",
       "   'Population',\n",
       "   'Hill climbing',\n",
       "   'Maximization',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Measure (mathematics)',\n",
       "   'Statistician',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '32',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2010581677',\n",
       "  'title': 'A Theory of Adaptive Pattern Classifiers',\n",
       "  'abstract': 'This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions.',\n",
       "  'date': '1967',\n",
       "  'authors': ['Shunichi Amari'],\n",
       "  'related_topics': ['Probability distribution',\n",
       "   'Decision boundary',\n",
       "   'Distribution (mathematics)',\n",
       "   'Discriminant',\n",
       "   'Weight',\n",
       "   'Convergence (routing)',\n",
       "   'Function (mathematics)',\n",
       "   'Adaptive system',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '706',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2161278885',\n",
       "   '2160133692',\n",
       "   '2065973527',\n",
       "   '2041273609',\n",
       "   '2079724156']},\n",
       " {'id': '2121947440',\n",
       "  'title': 'Normalized cuts and image segmentation',\n",
       "  'abstract': 'We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Jianbo Shi', 'J. Malik'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Spectral clustering',\n",
       "   'Minimum spanning tree-based segmentation',\n",
       "   'Graph partition',\n",
       "   'Graph (abstract data type)',\n",
       "   'Spectral graph theory',\n",
       "   'Graph theory',\n",
       "   'Cluster analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '19,408',\n",
       "  'reference_count': '25',\n",
       "  'references': ['2121947440',\n",
       "   '2798909945',\n",
       "   '1578099820',\n",
       "   '1997063559',\n",
       "   '1971784203',\n",
       "   '2114487471',\n",
       "   '2913192828',\n",
       "   '2114030927',\n",
       "   '2132603077',\n",
       "   '100944330']},\n",
       " {'id': '1578099820',\n",
       "  'title': 'Spectral Graph Theory',\n",
       "  'abstract': 'Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Fan R K Chung'],\n",
       "  'related_topics': ['Integral graph',\n",
       "   'Spectral graph theory',\n",
       "   'Laplacian matrix',\n",
       "   'Algebraic connectivity',\n",
       "   'Isoperimetric inequality',\n",
       "   'Sobolev inequality',\n",
       "   'Random walk',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Combinatorics',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,967',\n",
       "  'reference_count': '3',\n",
       "  'references': ['2901284226', '2053631808', '2027808858']},\n",
       " {'id': '108654854',\n",
       "  'title': 'Higher eigenvalues and isoperimetric inequalities on Riemannian manifolds and graphs',\n",
       "  'abstract': '5 Analysis on weighted graphs 23 5.1 Measures on graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.2 Discrete Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.3 Green’s formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.4 Integration versus Summation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.5 Eigenvalues of Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.6 Heat kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.7 Co-area formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Research supported in part by NSF Grant No. DMS 98-01446 Supported by EPSRC Fellowship B/94/AF/1782 Supported in part by NSF Grant No. DMS 95-04834',\n",
       "  'date': '2000',\n",
       "  'authors': ['Fan', 'Alexander', 'Shing-Tung'],\n",
       "  'related_topics': ['Isoperimetric inequality',\n",
       "   'Isoperimetric dimension',\n",
       "   'Laplacian matrix',\n",
       "   'Heat kernel',\n",
       "   'Laplace operator',\n",
       "   'Ricci-flat manifold',\n",
       "   'Riemannian geometry',\n",
       "   'Exponential map (Riemannian geometry)',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'Mathematical analysis',\n",
       "   'View Less'],\n",
       "  'citation_count': '114',\n",
       "  'reference_count': '31',\n",
       "  'references': ['1578099820',\n",
       "   '100944330',\n",
       "   '2131183115',\n",
       "   '181601562',\n",
       "   '1522020530',\n",
       "   '1567771969',\n",
       "   '2237678354',\n",
       "   '638069330',\n",
       "   '2051616415',\n",
       "   '2139320601']},\n",
       " {'id': '3029645440',\n",
       "  'title': 'Numerical Optimization',\n",
       "  'abstract': 'Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems.  For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Jorge Nocedal', 'Stephen J. Wright'],\n",
       "  'related_topics': ['Continuous optimization',\n",
       "   'Nonlinear programming',\n",
       "   'Field (computer science)',\n",
       "   'Management science',\n",
       "   'CUTEr',\n",
       "   'Broyden–Fletcher–Goldfarb–Shanno algorithm',\n",
       "   'Trust region',\n",
       "   'Quadratic programming',\n",
       "   'Focus (computing)',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,223',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2104095591',\n",
       "  'title': 'Snakes : Active Contour Models',\n",
       "  'abstract': 'A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Michael Kass', 'Andrew P. Witkin', 'Demetri Terzopoulos'],\n",
       "  'related_topics': ['Active contour model',\n",
       "   'Active shape model',\n",
       "   'Match moving',\n",
       "   'Mumford–Shah functional',\n",
       "   'Spline (mathematics)',\n",
       "   'Level set method',\n",
       "   'Computer vision',\n",
       "   'Vector flow',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Stereo matching',\n",
       "   'View Less'],\n",
       "  'citation_count': '25,185',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2109863423',\n",
       "   '2003370853',\n",
       "   '1995756857',\n",
       "   '1531060698',\n",
       "   '2139762693',\n",
       "   '2107198582',\n",
       "   '2582614493',\n",
       "   '2045798786',\n",
       "   '1631253743',\n",
       "   '1977699267']},\n",
       " {'id': '2150134853',\n",
       "  'title': 'Scale-space and edge detection using anisotropic diffusion',\n",
       "  'abstract': \"A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image. >\",\n",
       "  'date': '1990',\n",
       "  'authors': ['P. Perona', 'J. Malik'],\n",
       "  'related_topics': ['Anisotropic diffusion',\n",
       "   'Edge-preserving smoothing',\n",
       "   'Smoothing',\n",
       "   'Scale space',\n",
       "   'Edge detection',\n",
       "   'Scale-space axioms',\n",
       "   'Structure tensor',\n",
       "   'Diffusion filter',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,978',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2145023731',\n",
       "   '1997063559',\n",
       "   '2109863423',\n",
       "   '2114487471',\n",
       "   '2913192828',\n",
       "   '2022735534',\n",
       "   '2133155955',\n",
       "   '2002312729',\n",
       "   '1968245656',\n",
       "   '1973976434']},\n",
       " {'id': '2165874743',\n",
       "  'title': 'On Spectral Clustering: Analysis and an algorithm',\n",
       "  'abstract': 'Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Andrew Y. Ng', 'Michael I. Jordan', 'Yair Weiss'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Correlation clustering',\n",
       "   'Fuzzy clustering',\n",
       "   'Canopy clustering algorithm',\n",
       "   'Constrained clustering',\n",
       "   'CURE data clustering algorithm',\n",
       "   'Clustering high-dimensional data',\n",
       "   'Biclustering',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,813',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2140095548',\n",
       "   '1578099820',\n",
       "   '2141376824',\n",
       "   '2160167256',\n",
       "   '658559791',\n",
       "   '2130891992',\n",
       "   '2067976091',\n",
       "   '2171009857',\n",
       "   '2123320529',\n",
       "   '1981193610']},\n",
       " {'id': '2154579312',\n",
       "  'title': 'Handwritten Digit Recognition with a Back-Propagation Network',\n",
       "  'abstract': 'We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Yann LeCun',\n",
       "   'Bernhard E. Boser',\n",
       "   'John S. Denker',\n",
       "   '',\n",
       "   'Donnie Henderson',\n",
       "   'R. E. Howard',\n",
       "   'Wayne E. Hubbard',\n",
       "   'Lawrence D. Jackel'],\n",
       "  'related_topics': ['Word error rate',\n",
       "   'Task (project management)',\n",
       "   'Backpropagation',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Normalization (statistics)',\n",
       "   'Artificial intelligence',\n",
       "   'Digit recognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,071',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2154642048',\n",
       "   '2147800946',\n",
       "   '2114766824',\n",
       "   '169539560',\n",
       "   '56903235',\n",
       "   '2157475639',\n",
       "   '1965770722',\n",
       "   '2091987367',\n",
       "   '2153988646',\n",
       "   '2058841211']},\n",
       " {'id': '2122837498',\n",
       "  'title': 'Partially labeled classification with Markov random walks',\n",
       "  'abstract': 'To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Martin Szummer', 'Tommi Jaakkola'],\n",
       "  'related_topics': ['Random walk',\n",
       "   'Markov chain',\n",
       "   'Margin (machine learning)',\n",
       "   'Probabilistic logic',\n",
       "   'Representation (mathematics)',\n",
       "   'Regularization (mathematics)',\n",
       "   'Scale (ratio)',\n",
       "   'Set (abstract data type)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '739',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2001141328',\n",
       "   '1585385982',\n",
       "   '2017753243',\n",
       "   '2161813919',\n",
       "   '2127086485',\n",
       "   '2120720283']},\n",
       " {'id': '1511160855',\n",
       "  'title': 'Diffusion Kernels on Graphs and Other Discrete Input Spaces',\n",
       "  'abstract': 'The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Risi Imre Kondor', 'John D. Lafferty'],\n",
       "  'related_topics': ['Kernel (category theory)',\n",
       "   'Discretization',\n",
       "   'Heat kernel',\n",
       "   'Euclidean distance matrix',\n",
       "   'Gaussian function',\n",
       "   'Euclidean space',\n",
       "   'Diffusion equation',\n",
       "   'Matrix exponential',\n",
       "   'Algebra',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,079',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2124637492',\n",
       "   '2139212933',\n",
       "   '3023786531',\n",
       "   '2149684865',\n",
       "   '2097308346',\n",
       "   '1578099820',\n",
       "   '2009570821',\n",
       "   '2122837498',\n",
       "   '1576213419',\n",
       "   '1979711143']},\n",
       " {'id': '1585385982',\n",
       "  'title': 'Learning from Labeled and Unlabeled Data using Graph Mincuts',\n",
       "  'abstract': 'Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Avrim Blum', 'Shuchi'],\n",
       "  'related_topics': ['Pairwise comparison',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Graph',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,338',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1979711143',\n",
       "  'title': 'Large margin classification using the perceptron algorithm',\n",
       "  'abstract': 'We introduce and analyze a new algorithm for linear classification which combines Rosenblatt‘s perceptron algorithm with Helmbold and Warmuth‘s leave-one-out method. Like Vapnik‘s maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik‘s algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Yoav Freund', 'Robert E. Schapire'],\n",
       "  'related_topics': ['Perceptron',\n",
       "   'Linear classifier',\n",
       "   'Linear separability',\n",
       "   'Computation',\n",
       "   'Classifier (UML)',\n",
       "   'Pattern recognition',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'High dimensional',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,618',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2119821739',\n",
       "   '2087347434',\n",
       "   '1530699444',\n",
       "   '2069317438',\n",
       "   '1979675141',\n",
       "   '1496612019',\n",
       "   '1667072054',\n",
       "   '2011395874']},\n",
       " {'id': '2113592823',\n",
       "  'title': 'Cluster Kernels for Semi-Supervised Learning',\n",
       "  'abstract': 'We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Olivier Chapelle', 'Jason Weston', 'Bernhard Schölkopf'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Kernel (linear algebra)',\n",
       "   'Classifier (UML)',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '642',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2165874743',\n",
       "   '2140095548',\n",
       "   '2048679005',\n",
       "   '2158001550',\n",
       "   '2107008379',\n",
       "   '2160167256',\n",
       "   '2166473218',\n",
       "   '2122837498',\n",
       "   '2139578439',\n",
       "   '1574877594']},\n",
       " {'id': '200434350',\n",
       "  'title': 'A Random Walks View of Spectral Segmentation.',\n",
       "  'abstract': \"We present a new view of clustering and segmentation by pairwise similarities. We interpret the similarities as edge ows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This view shows that spectral methods for clustering and segmentation have a probabilistic foundation. We prove that the Normalized Cut method arises naturally from our framework and we provide a complete characterization of the cases when the Normalized Cut algorithm is exact. Then we discuss other spectral segmentation and clustering methods showing that several of them are essentially the same as NCut.\",\n",
       "  'date': '2001',\n",
       "  'authors': ['Marina Meila', 'Jianbo Shi'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Random walk',\n",
       "   'Heterogeneous random walk in one dimension',\n",
       "   'Loop-erased random walk',\n",
       "   'Segmentation',\n",
       "   'Quantum walk',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Stochastic matrix',\n",
       "   'Pattern recognition',\n",
       "   'Discrete mathematics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '839',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2121947440',\n",
       "   '2138621811',\n",
       "   '2147152072',\n",
       "   '1578099820',\n",
       "   '2160167256',\n",
       "   '2130891992',\n",
       "   '2171009857',\n",
       "   '1640070940',\n",
       "   '2065060195',\n",
       "   '2323009482']},\n",
       " {'id': '1497256448',\n",
       "  'title': 'Adaptation in natural and artificial systems',\n",
       "  'abstract': '',\n",
       "  'date': '1992',\n",
       "  'authors': ['John H.'],\n",
       "  'related_topics': ['Artificial development',\n",
       "   'Artificial creation',\n",
       "   'Adaptation (computer science)',\n",
       "   'Evolutionary acquisition of neural topologies',\n",
       "   'Genetic fuzzy systems',\n",
       "   'Computer science',\n",
       "   'Effective fitness',\n",
       "   'Evolutionary programming',\n",
       "   'Natural (archaeology)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '61,021',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2148694408',\n",
       "  'title': 'Principal Component Analysis',\n",
       "  'abstract': 'Introduction * Properties of Population Principal Components * Properties of Sample Principal Components * Interpreting Principal Components: Examples * Graphical Representation of Data Using Principal Components * Choosing a Subset of Principal Components or Variables * Principal Component Analysis and Factor Analysis * Principal Components in Regression Analysis * Principal Components Used with Other Multivariate Techniques * Outlier Detection, Influential Observations and Robust Estimation * Rotation and Interpretation of Principal Components * Principal Component Analysis for Time Series and Other Non-Independent Data * Principal Component Analysis for Special Types of Data * Generalizations and Adaptations of Principal Component Analysis',\n",
       "  'date': '1986',\n",
       "  'authors': ['Ian Jolliffe'],\n",
       "  'related_topics': ['Principal component analysis',\n",
       "   'Multilinear principal component analysis',\n",
       "   'Kernel principal component analysis',\n",
       "   'Relationship square',\n",
       "   'Correspondence analysis',\n",
       "   'Multiple correspondence analysis',\n",
       "   'Dimensionality reduction',\n",
       "   'Population',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '55,709',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '23758216',\n",
       "  'title': 'Self-organization and associative memory: 3rd edition',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['T. Kohonen'],\n",
       "  'related_topics': ['Content-addressable memory',\n",
       "   'Computer science',\n",
       "   'Cognitive science',\n",
       "   'Self-organization',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,014',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2100659887',\n",
       "  'title': 'A database for handwritten text recognition research',\n",
       "  'abstract': 'An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >',\n",
       "  'date': '1994',\n",
       "  'authors': ['J.J. Hull'],\n",
       "  'related_topics': ['Alphanumeric',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Digital image',\n",
       "   'Grayscale',\n",
       "   'Pixel',\n",
       "   'Database',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Text recognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,770',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2092642599',\n",
       "   '2164371886',\n",
       "   '2093439427',\n",
       "   '1579840964',\n",
       "   '2156113848']},\n",
       " {'id': '2159174312',\n",
       "  'title': 'Mapping a Manifold of Perceptual Observations',\n",
       "  'abstract': 'Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-space embedding of a set of observations that preserves as closely as possible their intrinsic metric structure - the distances between points on the observation manifold as measured along geodesic paths. Our isometric feature mapping procedure, or isomap, is able to reliably recover low-dimensional nonlinear structure in realistic perceptual data sets, such as a manifold of face images, where conventional global mapping methods find only local minima. The recovered map provides a canonical set of globally meaningful features, which allows perceptual transformations such as interpolation, extrapolation, and analogy - highly nonlinear transformations in the original observation space - to be computed with simple linear operations in feature space.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Joshua B. Tenenbaum'],\n",
       "  'related_topics': ['Isomap',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Diffusion map',\n",
       "   'Dimensionality reduction',\n",
       "   'Intrinsic metric',\n",
       "   'Geodesic',\n",
       "   'Manifold',\n",
       "   'Feature vector',\n",
       "   'Algorithm',\n",
       "   'Topology',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '335',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2124776405',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '2143956139',\n",
       "   '2047870719',\n",
       "   '23758216',\n",
       "   '1580684925',\n",
       "   '2114309103',\n",
       "   '2123421115',\n",
       "   '2151391352']},\n",
       " {'id': '2106346128',\n",
       "  'title': 'Learning distributed representations of concepts using linear relational embedding',\n",
       "  'abstract': 'We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.',\n",
       "  'date': '2001',\n",
       "  'authors': ['A. Paccanaro', 'G.E.'],\n",
       "  'related_topics': ['Relational algebra',\n",
       "   'Feature learning',\n",
       "   'Concept learning',\n",
       "   'Binary relation',\n",
       "   'Representation (mathematics)',\n",
       "   'Generalization',\n",
       "   'Matrix multiplication',\n",
       "   'Relation (database)',\n",
       "   'Embedding',\n",
       "   'Discriminative model',\n",
       "   'Theoretical computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '117',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2154642048',\n",
       "   '2147152072',\n",
       "   '2110485445',\n",
       "   '1983578042',\n",
       "   '2051812123',\n",
       "   '183625566',\n",
       "   '145476170',\n",
       "   '1971844566',\n",
       "   '2121553911',\n",
       "   '1982370770']},\n",
       " {'id': '1746680969',\n",
       "  'title': 'Learning in graphical models',\n",
       "  'abstract': 'Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Michael I.'],\n",
       "  'related_topics': ['Graphical model',\n",
       "   'Bayesian network',\n",
       "   'Markov chain Monte Carlo',\n",
       "   'Inference',\n",
       "   'Artificial neural network',\n",
       "   'Cluster analysis',\n",
       "   'Latent variable',\n",
       "   'Gaussian process',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,681',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2083380015',\n",
       "  'title': 'Connectionist learning of belief networks',\n",
       "  'abstract': 'Abstract   Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Radford M. Neal'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'Boltzmann machine',\n",
       "   'Computational learning theory',\n",
       "   'Feature learning',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Deep belief network',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '682',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2159080219',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '2049633694',\n",
       "   '2083875149',\n",
       "   '1593793857',\n",
       "   '1507849272',\n",
       "   '2166698530',\n",
       "   '1992880122',\n",
       "   '1547224907']},\n",
       " {'id': '2114153178',\n",
       "  'title': 'Rate-coded Restricted Boltzmann Machines for Face Recognition',\n",
       "  'abstract': 'We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Yee Whye Teh', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Feature (computer vision)',\n",
       "   'Generative model',\n",
       "   'Posterior probability',\n",
       "   'Standard test image',\n",
       "   'Boltzmann machine',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '161',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2116064496',\n",
       "   '2138451337',\n",
       "   '1902027874',\n",
       "   '2121647436',\n",
       "   '2159080219',\n",
       "   '2113341759',\n",
       "   '2125027820',\n",
       "   '2128716185',\n",
       "   '1547224907',\n",
       "   '1813659000']},\n",
       " {'id': '1547224907',\n",
       "  'title': 'Learning and relearning in Boltzmann machines',\n",
       "  'abstract': 'This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References',\n",
       "  'date': '1986',\n",
       "  'authors': ['G. E.', 'T. J.'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Computation',\n",
       "   'Relaxation (approximation)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Boltzmann constant',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,055',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2101706260',\n",
       "  'title': 'Recognizing handwritten digits using hierarchical products of experts',\n",
       "  'abstract': 'The product of experts learning procedure can discover a set of stochastic binary features that constitute a nonlinear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, a hierarchy of separate models can be learned, for each digit class. Each model in the hierarchy learns a layer of binary feature detectors that model the probability distribution of vectors of activity of feature detectors in the layer below. The models in the hierarchy are trained sequentially and each model uses a layer of binary feature detectors to learn a generative model of the patterns of feature activities in the preceding layer. After training, each layer of feature detectors produces a separate, unnormalized log probability score. With three layers of feature detectors for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data.',\n",
       "  'date': '2002',\n",
       "  'authors': ['G. Mayraz', 'G.E. Hinton'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Feature (computer vision)',\n",
       "   'Discriminative model',\n",
       "   'Feature extraction',\n",
       "   'Product of experts',\n",
       "   'Standard test image',\n",
       "   'Artificial neural network',\n",
       "   'Handwriting recognition',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '84',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2912934387',\n",
       "   '2116064496',\n",
       "   '2159080219',\n",
       "   '2150884987',\n",
       "   '28412257',\n",
       "   '1547224907',\n",
       "   '2104867159',\n",
       "   '1667072054',\n",
       "   '1813659000',\n",
       "   '2100559472']},\n",
       " {'id': '2124351082',\n",
       "  'title': 'Training support vector machines: an application to face detection',\n",
       "  'abstract': \"We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.\",\n",
       "  'date': '1997',\n",
       "  'authors': ['E. Osuna', 'R. Freund', 'F. Girosit'],\n",
       "  'related_topics': ['Sequential minimal optimization',\n",
       "   'Least squares support vector machine',\n",
       "   'Support vector machine',\n",
       "   'Quadratic programming',\n",
       "   'Optimization problem',\n",
       "   'Artificial neural network',\n",
       "   'Face detection',\n",
       "   'Statistical classification',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,885',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2156909104',\n",
       "   '2119821739',\n",
       "   '2087347434',\n",
       "   '2159686933',\n",
       "   '26816478',\n",
       "   '2159173611',\n",
       "   '2137346077',\n",
       "   '2084844503',\n",
       "   '2056695679',\n",
       "   '2125713050']},\n",
       " {'id': '2155511848',\n",
       "  'title': 'A statistical method for 3D object detection applied to faces and cars',\n",
       "  'abstract': 'In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.',\n",
       "  'date': '2000',\n",
       "  'authors': ['H. Schneiderman', 'T. Kanade'],\n",
       "  'related_topics': ['Viola–Jones object detection framework',\n",
       "   'Object detection',\n",
       "   'Face detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Histogram',\n",
       "   'Object (computer science)',\n",
       "   'Wavelet',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Rotation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,889',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2156909104',\n",
       "   '3124955340',\n",
       "   '2117812871',\n",
       "   '2217896605',\n",
       "   '1658679052',\n",
       "   '2159686933',\n",
       "   '2140785063',\n",
       "   '2166713160',\n",
       "   '2138560582',\n",
       "   '2151777012']},\n",
       " {'id': '2160225842',\n",
       "  'title': 'Learning a Sparse Representation for Object Detection',\n",
       "  'abstract': 'We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Shivani Agarwal', 'Dan Roth'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Object detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Object model',\n",
       "   'Spatial relation',\n",
       "   'Implicit Shape Model',\n",
       "   'Sparse approximation',\n",
       "   'Test set',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '766',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2164598857',\n",
       "   '2217896605',\n",
       "   '2152473410',\n",
       "   '2124087378',\n",
       "   '2124351082',\n",
       "   '2155511848',\n",
       "   '1949116567',\n",
       "   '1564419782',\n",
       "   '2156406284',\n",
       "   '2124722975']},\n",
       " {'id': '2295106276',\n",
       "  'title': 'Matching shapes',\n",
       "  'abstract': 'We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solving for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. Dis-similarity between two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset.',\n",
       "  'date': '2001',\n",
       "  'authors': ['S. Belongie', 'J.', 'J.'],\n",
       "  'related_topics': ['Shape analysis (digital geometry)',\n",
       "   'Shape context',\n",
       "   'Similarity (geometry)',\n",
       "   'Correspondence problem',\n",
       "   'Discriminative model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Assignment problem',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Image matching',\n",
       "   'View Less'],\n",
       "  'citation_count': '511',\n",
       "  'reference_count': '22',\n",
       "  'references': ['2310919327',\n",
       "   '2123977795',\n",
       "   '2101522199',\n",
       "   '2095757522',\n",
       "   '2089181482',\n",
       "   '2062104878',\n",
       "   '2106404777',\n",
       "   '2108444897',\n",
       "   '2100318434',\n",
       "   '2096840836']},\n",
       " {'id': '2141376824',\n",
       "  'title': 'Contour and Texture Analysis for Image Segmentation',\n",
       "  'abstract': 'This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Jitendra Malik',\n",
       "   'Serge Belongie',\n",
       "   'Thomas Leung',\n",
       "   'Jianbo Shi'],\n",
       "  'related_topics': ['Image texture',\n",
       "   'Texture filtering',\n",
       "   'Texture compression',\n",
       "   'Image segmentation',\n",
       "   'Grayscale',\n",
       "   'Pixel',\n",
       "   'Texton',\n",
       "   'Texture (geology)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,595',\n",
       "  'reference_count': '38',\n",
       "  'references': ['2121947440',\n",
       "   '2145023731',\n",
       "   '1578099820',\n",
       "   '1997063559',\n",
       "   '2121927366',\n",
       "   '1634005169',\n",
       "   '3017143921',\n",
       "   '2114487471',\n",
       "   '2160167256',\n",
       "   '1490632837']},\n",
       " {'id': '1612003148',\n",
       "  'title': 'Probabilistic latent semantic analysis',\n",
       "  'abstract': 'Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent semantic analysis',\n",
       "   'Document-term matrix',\n",
       "   'Latent class model',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Explicit semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Non-negative matrix factorization',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,337',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2147152072',\n",
       "   '2107743791',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '1983578042',\n",
       "   '2134731454',\n",
       "   '2127314673',\n",
       "   '2056029990',\n",
       "   '2143144851',\n",
       "   '2140842551']},\n",
       " {'id': '2122090912',\n",
       "  'title': 'Maximum-Margin Matrix Factorization',\n",
       "  'abstract': 'We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Nathan Srebro', 'Jason Rennie', 'Tommi S. Jaakkola'],\n",
       "  'related_topics': ['Matrix decomposition',\n",
       "   'Margin (machine learning)',\n",
       "   'Algebra',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'Generalization error',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,246',\n",
       "  'reference_count': '14',\n",
       "  'references': ['1902027874',\n",
       "   '2145295623',\n",
       "   '2134731454',\n",
       "   '2049455633',\n",
       "   '2165395308',\n",
       "   '1966096622',\n",
       "   '2151052953',\n",
       "   '2118079529',\n",
       "   '2135001774',\n",
       "   '1999613943']},\n",
       " {'id': '205159212',\n",
       "  'title': 'Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure',\n",
       "  'abstract': '',\n",
       "  'date': '2007',\n",
       "  'authors': ['Ruslan', 'Geoffrey E.'],\n",
       "  'related_topics': ['k-nearest neighbors algorithm',\n",
       "   'Neighbourhood (mathematics)',\n",
       "   'Theoretical computer science',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear embedding',\n",
       "   'View Less'],\n",
       "  'citation_count': '385',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2117154949',\n",
       "   '2130556178',\n",
       "   '2157364932',\n",
       "   '2144935315',\n",
       "   '2159737176',\n",
       "   '2124914669',\n",
       "   '2157444450']},\n",
       " {'id': '2165395308',\n",
       "  'title': 'Weighted low-rank approximations',\n",
       "  'abstract': 'We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Nathan Srebro', 'Tommi Jaakkola'],\n",
       "  'related_topics': ['Rank (linear algebra)',\n",
       "   'Matrix (mathematics)',\n",
       "   'Context (language use)',\n",
       "   'Representation (mathematics)',\n",
       "   'Collaborative filtering',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical optimization',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Noise',\n",
       "   'Mathematics',\n",
       "   'Task (computing)',\n",
       "   'View Less'],\n",
       "  'citation_count': '920',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2117354486',\n",
       "   '1673941785',\n",
       "   '2170653751',\n",
       "   '2021680564',\n",
       "   '2135001774',\n",
       "   '1496451467',\n",
       "   '1516172206',\n",
       "   '1568698519',\n",
       "   '2139451327',\n",
       "   '2252194958']},\n",
       " {'id': '1989702938',\n",
       "  'title': 'Face recognition: A literature survey',\n",
       "  'abstract': 'As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.',\n",
       "  'date': '2003',\n",
       "  'authors': ['W. Zhao', 'R. Chellappa', 'P. J. Phillips', 'A. Rosenfeld'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Literature survey',\n",
       "   'Eigenface',\n",
       "   'Facial recognition system',\n",
       "   'Face hallucination',\n",
       "   'FERET database',\n",
       "   'Perception',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,005',\n",
       "  'reference_count': '162',\n",
       "  'references': ['2156909104',\n",
       "   '2164598857',\n",
       "   '2138451337',\n",
       "   '2217896605',\n",
       "   '2121647436',\n",
       "   '2152826865',\n",
       "   '2108384452',\n",
       "   '2033419168',\n",
       "   '2121601095',\n",
       "   '2038952578']},\n",
       " {'id': '2098947662',\n",
       "  'title': 'View-based and modular eigenspaces for face recognition',\n",
       "  'abstract': 'We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated. >',\n",
       "  'date': '1994',\n",
       "  'authors': ['Pentland', 'Moghaddam', 'Starner'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   '3D single-object recognition',\n",
       "   'Feature (machine learning)',\n",
       "   'Eigenface',\n",
       "   'Facial recognition system',\n",
       "   'Feature extraction',\n",
       "   'Face (geometry)',\n",
       "   'Orientation (computer vision)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Salient',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,971',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2138451337',\n",
       "   '2113341759',\n",
       "   '2135463994',\n",
       "   '2138313032',\n",
       "   '2130506643',\n",
       "   '2157418942',\n",
       "   '1993867646',\n",
       "   '2112684592',\n",
       "   '2121863133',\n",
       "   '2030234875']},\n",
       " {'id': '2905573712',\n",
       "  'title': 'Face recognition: A Literature Survey',\n",
       "  'abstract': '',\n",
       "  'date': '2008',\n",
       "  'authors': ['W.', 'R.', 'R.'],\n",
       "  'related_topics': ['Literature survey',\n",
       "   'Business intelligence',\n",
       "   'Facial recognition system',\n",
       "   'Knowledge management',\n",
       "   'Computer science',\n",
       "   'Process improvement',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,625',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2155759509',\n",
       "  'title': 'The CMU Pose, Illumination, and Expression (PIE) database',\n",
       "  'abstract': 'Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.',\n",
       "  'date': '2002',\n",
       "  'authors': ['T. Sim', 'S. Baker', 'M. Bsat'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Computer vision',\n",
       "   'Computer graphics (images)',\n",
       "   'Expression (mathematics)',\n",
       "   'Database',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Image storage',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,888',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2125127226',\n",
       "   '2118774738',\n",
       "   '2102760078',\n",
       "   '2120420721',\n",
       "   '2110822444',\n",
       "   '2121114545',\n",
       "   '2106143125',\n",
       "   '2141503314',\n",
       "   '2144855601']},\n",
       " {'id': '2095757522',\n",
       "  'title': 'Distortion invariant object recognition in the dynamic link architecture',\n",
       "  'abstract': 'An object recognition system based on the dynamic link architecture, an extension to classical artificial neural networks (ANNs), is presented. The dynamic link architecture exploits correlations in the fine-scale temporal structure of cellular signals to group neurons dynamically into higher-order entities. These entities represent a rich structure and can code for high-level objects. To demonstrate the capabilities of the dynamic link architecture, a program was implemented that can recognize human faces and other objects from video images. Memorized objects are represented by sparse graphs, whose vertices are labeled by a multiresolution description in terms of a local power spectrum, and whose edges are labeled by geometrical distance vectors. Object recognition can be formulated as elastic graph matching, which is performed here by stochastic optimization of a matching cost function. The implementation on a transputer network achieved recognition of human faces and office objects from gray-level camera images. The performance of the program is evaluated by a statistical analysis of recognition results from a portrait gallery comprising images of 87 persons. >',\n",
       "  'date': '1993',\n",
       "  'authors': ['M. Lades',\n",
       "   'J.C. Vorbruggen',\n",
       "   'J. Buhmann',\n",
       "   'J. Lange',\n",
       "   'C. von der Malsburg',\n",
       "   'R.P.',\n",
       "   'W. Konen'],\n",
       "  'related_topics': ['Dynamic link matching',\n",
       "   '3D single-object recognition',\n",
       "   'Facial recognition system',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Matching (graph theory)',\n",
       "   'Artificial neural network',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,688',\n",
       "  'reference_count': '31',\n",
       "  'references': ['2011039300',\n",
       "   '3144368627',\n",
       "   '1991848143',\n",
       "   '2135463994',\n",
       "   '2130259898',\n",
       "   '2079948225',\n",
       "   '2167034998',\n",
       "   '23758216',\n",
       "   '2051719061',\n",
       "   '1914401667']},\n",
       " {'id': '2144354855',\n",
       "  'title': 'Face recognition: a convolutional neural-network approach',\n",
       "  'abstract': 'We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.',\n",
       "  'date': '1997',\n",
       "  'authors': ['S. Lawrence', 'C.L. Giles', 'Ah Chung Tsoi', 'A.D. Back'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Multilayer perceptron',\n",
       "   'Self-organizing map',\n",
       "   'Artificial neural network',\n",
       "   'Facial recognition system',\n",
       "   'Feature extraction',\n",
       "   'Quantization (image processing)',\n",
       "   'Dimensionality reduction',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,297',\n",
       "  'reference_count': '43',\n",
       "  'references': ['2124776405',\n",
       "   '1679913846',\n",
       "   '2138451337',\n",
       "   '2046079134',\n",
       "   '2115689562',\n",
       "   '2098947662',\n",
       "   '2113341759',\n",
       "   '1770825568',\n",
       "   '2095757522',\n",
       "   '2012352340']},\n",
       " {'id': '2107369107',\n",
       "  'title': 'Recognizing imprecisely localized, partially occluded, and expression variant faces from a single sample per class',\n",
       "  'abstract': 'The classical way of attempting to solve the face (or object) recognition problem is by using large and representative data sets. In many applications, though, only one sample per class is available to the system. In this contribution, we describe a probabilistic approach that is able to compensate for imprecisely localized, partially occluded, and expression-variant faces even when only one single training sample per class is available to the system. To solve the localization problem, we find the subspace (within the feature space, e.g., eigenspace) that represents this error for each of the training images. To resolve the occlusion problem, each face is divided into k local regions which are analyzed in isolation. In contrast with other approaches where a simple voting space is used, we present a probabilistic method that analyzes how \"good\" a local match is. To make the recognition system less sensitive to the differences between the facial expression displayed on the training and the testing images, we weight the results obtained on each local area on the basis of how much of this local area is affected by the expression displayed on the current test image.',\n",
       "  'date': '2002',\n",
       "  'authors': ['A.M. Martinez'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Feature vector',\n",
       "   'Standard test image',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Face (geometry)',\n",
       "   'Probabilistic logic',\n",
       "   'Subspace topology',\n",
       "   'Expression (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,001',\n",
       "  'reference_count': '79',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2138451337',\n",
       "   '2217896605',\n",
       "   '2121647436',\n",
       "   '2033419168',\n",
       "   '2132549764',\n",
       "   '2121601095',\n",
       "   '2049633694',\n",
       "   '2914885528']},\n",
       " {'id': '10021998',\n",
       "  'title': 'Loss Functions for Discriminative Training of Energy-Based Models.',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': ['Yann', 'Fu Jie Huang'],\n",
       "  'related_topics': ['Discriminative model',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Training (meteorology)',\n",
       "   'Artificial intelligence',\n",
       "   'Energy based',\n",
       "   'View Less'],\n",
       "  'citation_count': '104',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2310919327',\n",
       "   '2147880316',\n",
       "   '2132339004',\n",
       "   '2137813581',\n",
       "   '2134557905',\n",
       "   '2008652694',\n",
       "   '2105644991',\n",
       "   '1802356529',\n",
       "   '2148099973',\n",
       "   '2175582831']},\n",
       " {'id': '2145096794',\n",
       "  'title': 'Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information',\n",
       "  'abstract': 'This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f/spl isin/C/sup N/ and a randomly chosen set of frequencies /spl Omega/. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set /spl Omega/? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=/spl sigma//sub /spl tau//spl isin/T/f(/spl tau/)/spl delta/(t-/spl tau/) obeying |T|/spl les/C/sub M//spl middot/(log N)/sup -1/ /spl middot/ |/spl Omega/| for some constant C/sub M/>0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N/sup -M/), f can be reconstructed exactly as the solution to the /spl lscr//sub 1/ minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C/sub M/ which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|/spl middot/logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N/sup -M/) would in general require a number of frequency samples at least proportional to |T|/spl middot/logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.',\n",
       "  'date': '2006',\n",
       "  'authors': ['E.J. Candes', 'J. Romberg', 'T. Tao'],\n",
       "  'related_topics': ['Convex optimization',\n",
       "   'Free probability',\n",
       "   'Trigonometric polynomial',\n",
       "   'Fourier series',\n",
       "   'Binary logarithm',\n",
       "   'Omega',\n",
       "   'Sigma',\n",
       "   'Piecewise',\n",
       "   'Combinatorics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '17,448',\n",
       "  'reference_count': '31',\n",
       "  'references': ['3029645440',\n",
       "   '2078204800',\n",
       "   '2798909945',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2103559027',\n",
       "   '2154332973',\n",
       "   '2136235822',\n",
       "   '2012365979',\n",
       "   '2158537680']},\n",
       " {'id': '2078204800',\n",
       "  'title': 'Atomic Decomposition by Basis Pursuit',\n",
       "  'abstract': 'The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries---stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).\\r\\nBasis pursuit (BP) is a principle for decomposing a signal into an \"optimal\"\\' superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising.\\r\\nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Scott Shaobing Chen', 'David L. Donoho', 'Michael A. Saunders'],\n",
       "  'related_topics': ['Basis pursuit',\n",
       "   'Basis pursuit denoising',\n",
       "   'Wavelet packet decomposition',\n",
       "   'Total variation denoising',\n",
       "   'Wavelet',\n",
       "   'Matching pursuit',\n",
       "   'Orthogonal basis',\n",
       "   'Quadratic programming',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '25,068',\n",
       "  'reference_count': '43',\n",
       "  'references': ['2062024414',\n",
       "   '2798909945',\n",
       "   '2146842127',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2103559027',\n",
       "   '2152328854',\n",
       "   '2156447271',\n",
       "   '2611147814',\n",
       "   '2128659236']},\n",
       " {'id': '2116148865',\n",
       "  'title': 'Greed is good: algorithmic results for sparse approximation',\n",
       "  'abstract': \"This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho's basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms.\",\n",
       "  'date': '2004',\n",
       "  'authors': ['J.A. Tropp'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Matching pursuit',\n",
       "   'Basis pursuit',\n",
       "   'Approximation algorithm',\n",
       "   'Sparse matrix',\n",
       "   'Restricted isometry property',\n",
       "   'Greedy algorithm',\n",
       "   'Approximation theory',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,028',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2078204800',\n",
       "   '2610857016',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2154332973',\n",
       "   '2136235822',\n",
       "   '2156447271',\n",
       "   '391578156',\n",
       "   '1605417594',\n",
       "   '2167839759']},\n",
       " {'id': '2099641086',\n",
       "  'title': 'Uncertainty principles and ideal atomic decomposition',\n",
       "  'abstract': 'Suppose a discrete-time signal S(t), 0/spl les/t<N, is a superposition of atoms taken from a combined time-frequency dictionary made of spike sequences 1/sub {t=/spl tau/}/ and sinusoids exp{2/spl pi/iwt/N}//spl radic/N. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l/sup 1/ norm of the coefficients among all decompositions. Here \"highly sparse\" means that N/sub t/+N/sub w/</spl radic/N/2 where N/sub t/ is the number of time atoms, N/sub w/ is the number of frequency atoms, and N is the length of the discrete-time signal. Underlying this result is a general l/sup 1/ uncertainty principle which says that if two bases are mutually incoherent, no nonzero signal can have a sparse representation in both bases simultaneously. For the above setting, the bases are sinusoids and spikes, and mutual incoherence is measured in terms of the largest inner product between different basis elements. The uncertainty principle holds for a variety of interesting basis pairs, not just sinusoids and spikes. The results have idealized applications to band-limited approximation with gross errors, to error-correcting encryption, and to separation of uncoordinated sources. Related phenomena hold for functions of a real variable, with basis pairs such as sinusoids and wavelets, and for functions of two variables, with basis pairs such as wavelets and ridgelets. In these settings, if a function f is representable by a sufficiently sparse superposition of terms taken from both bases, then there is only one such sparse representation; it may be obtained by minimum l/sup 1/ norm atomic decomposition. The condition \"sufficiently sparse\" becomes a multiscale condition; for example, that the number of wavelets at level j plus the number of sinusoids in the jth dyadic frequency band are together less than a constant times 2/sup j/2/.',\n",
       "  'date': '2001',\n",
       "  'authors': ['D.L. Donoho', 'X. Huo'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Superposition principle',\n",
       "   'Norm (mathematics)',\n",
       "   'Wavelet',\n",
       "   'Basis pursuit',\n",
       "   'Uncertainty principle',\n",
       "   'Convex optimization',\n",
       "   'Time–frequency analysis',\n",
       "   'Combinatorics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,363',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2115755118',\n",
       "   '2062024414',\n",
       "   '2078204800',\n",
       "   '2151693816',\n",
       "   '1916685473',\n",
       "   '1604810369',\n",
       "   '2066462711',\n",
       "   '2125455772',\n",
       "   '1997149618',\n",
       "   '2033367330']},\n",
       " {'id': '2097323375',\n",
       "  'title': 'Stable recovery of sparse overcomplete representations in the presence of noise',\n",
       "  'abstract': 'Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.',\n",
       "  'date': '2004',\n",
       "  'authors': ['D.L. Donoho', 'M. Elad', 'V.N. Temlyakov'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'K-SVD',\n",
       "   'Matching pursuit',\n",
       "   'Basis pursuit',\n",
       "   'Approximation algorithm',\n",
       "   'Noise (signal processing)',\n",
       "   'Signal processing',\n",
       "   'Stability (learning theory)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,616',\n",
       "  'reference_count': '46',\n",
       "  'references': ['2135046866',\n",
       "   '2078204800',\n",
       "   '2610857016',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2154332973',\n",
       "   '2132680427',\n",
       "   '2136235822',\n",
       "   '2069912449']},\n",
       " {'id': '2136235822',\n",
       "  'title': 'Sparse representations in unions of bases',\n",
       "  'abstract': 'The purpose of this correspondence is to generalize a result by Donoho and Huo and Elad and Bruckstein on sparse representations of signals in a union of two orthonormal bases for R/sup N/. We consider general (redundant) dictionaries for R/sup N/, and derive sufficient conditions for having unique sparse representations of signals in such dictionaries. The special case where the dictionary is given by the union of L/spl ges/2 orthonormal bases for R/sup N/ is studied in more detail. In particular, it is proved that the result of Donoho and Huo, concerning the replacement of the /spl lscr//sup 0/ optimization problem with a linear programming problem when searching for sparse representations, has an analog for dictionaries that may be highly redundant.',\n",
       "  'date': '2003',\n",
       "  'authors': ['R. Gribonval', 'M. Nielsen'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Orthonormal basis',\n",
       "   'Optimization problem',\n",
       "   'Sparse matrix',\n",
       "   'Linear programming',\n",
       "   'Discrete mathematics',\n",
       "   'Special case',\n",
       "   'Mathematics',\n",
       "   'Nonlinear approximation',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,023',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2099641086',\n",
       "   '2154332973',\n",
       "   '2167839759',\n",
       "   '2086869478',\n",
       "   '2133866430',\n",
       "   '2115090644']},\n",
       " {'id': '2147656689',\n",
       "  'title': 'Just relax: convex programming methods for identifying sparse signals in noise',\n",
       "  'abstract': 'This paper studies a difficult and fundamental problem that arises throughout electrical engineering, applied mathematics, and statistics. Suppose that one forms a short linear combination of elementary signals drawn from a large, fixed collection. Given an observation of the linear combination that has been contaminated with additive noise, the goal is to identify which elementary signals participated and to approximate their coefficients. Although many algorithms have been proposed, there is little theory which guarantees that these algorithms can accurately and efficiently solve the problem. This paper studies a method called convex relaxation, which attempts to recover the ideal sparse signal by solving a convex program. This approach is powerful because the optimization can be completed in polynomial time with standard scientific software. The paper provides general conditions which ensure that convex relaxation succeeds. As evidence of the broad impact of these results, the paper describes how convex relaxation can be used for several concrete signal recovery problems. It also describes applications to channel coding, linear regression, and numerical analysis',\n",
       "  'date': '2006',\n",
       "  'authors': ['J.A. Tropp'],\n",
       "  'related_topics': ['Convex optimization',\n",
       "   'Proper convex function',\n",
       "   'Relaxation (iterative method)',\n",
       "   'Linear combination',\n",
       "   'Sparse approximation',\n",
       "   'Iterative method',\n",
       "   'Time complexity',\n",
       "   'Noise (signal processing)',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,268',\n",
       "  'reference_count': '64',\n",
       "  'references': ['2296319761',\n",
       "   '2145096794',\n",
       "   '2099111195',\n",
       "   '2115755118',\n",
       "   '2129638195',\n",
       "   '2135046866',\n",
       "   '2122825543',\n",
       "   '2078204800',\n",
       "   '2610857016',\n",
       "   '2116148865']},\n",
       " {'id': '2012365979',\n",
       "  'title': 'Near-optimal sparse fourier representations via sampling',\n",
       "  'abstract': '(MATH) We give an algorithm for finding a Fourier representation R of B terms for a given discrete signal signal A of length N, such that $\\\\|\\\\signal-\\\\repn\\\\|_2^2$ is within the factor (1 +e) of best possible $\\\\|\\\\signal-\\\\repn_\\\\opt\\\\|_2^2$. Our algorithm can access A by reading its values on a sample set T ⊆[0,N), chosen randomly from a (non-product) distribution of our choice, independent of A. That is, we sample non-adaptively. The total time cost of the algorithm is polynomial in B log(N)log(M)e (where M is the ratio of largest to smallest numerical quantity encountered), which implies a similar bound for the number of samples.',\n",
       "  'date': '2002',\n",
       "  'authors': ['A. C. Gilbert',\n",
       "   'S. Guha',\n",
       "   'P. Indyk',\n",
       "   'S. Muthukrishnan',\n",
       "   'M. Strauss'],\n",
       "  'related_topics': ['Polynomial',\n",
       "   'Discrete-time signal',\n",
       "   'Fourier transform',\n",
       "   'Distribution (mathematics)',\n",
       "   'Combinatorics',\n",
       "   'Sampling (statistics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Mathematics',\n",
       "   'Fourier representation',\n",
       "   'Time cost',\n",
       "   'View Less'],\n",
       "  'citation_count': '355',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2151693816',\n",
       "   '2080745194',\n",
       "   '2125455772',\n",
       "   '1979750072',\n",
       "   '2047424291',\n",
       "   '1970950689',\n",
       "   '2095546965',\n",
       "   '2042194938',\n",
       "   '2101610102',\n",
       "   '1899006432']},\n",
       " {'id': '2096613063',\n",
       "  'title': 'Data compression and harmonic analysis',\n",
       "  'abstract': 'In this paper we review some recent interactions between harmonic analysis and data compression. The story goes back of course to Shannon\\'s R(D) theory in the case of Gaussian stationary processes, which says that transforming into a Fourier basis followed by block coding gives an optimal lossy compression technique; practical developments like transform-based image compression have been inspired by this result. In this paper we also discuss connections perhaps less familiar to the information theory community, growing out of the field of harmonic analysis. Recent harmonic analysis constructions, such as wavelet transforms and Gabor transforms, are essentially optimal transforms for transform coding in certain settings. Some of these transforms are under consideration for future compression standards. We discuss some of the lessons of harmonic analysis in this century. Typically, the problems and achievements of this field have involved goals that were not obviously related to practical data compression, and have used a language not immediately accessible to outsiders. Nevertheless, through an extensive generalization of what Shannon called the \"sampling theorem\", harmonic analysis has succeeded in developing new forms of functional representation which turn out to have significant data compression interpretations. We explain why harmonic analysis has interacted with data compression, and we describe some interesting recent ideas in the field that may affect data compression in the future.',\n",
       "  'date': '1998',\n",
       "  'authors': ['D.L. Donoho', 'M. Vetterli', 'R.A. DeVore', 'I. Daubechies'],\n",
       "  'related_topics': ['Data compression',\n",
       "   'Image compression',\n",
       "   'Lossy compression',\n",
       "   'Information theory',\n",
       "   'Transform coding',\n",
       "   'Harmonic analysis',\n",
       "   'Wavelet transform',\n",
       "   'Entropy (information theory)',\n",
       "   'Algorithm',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '587',\n",
       "  'reference_count': '84',\n",
       "  'references': ['2099111195',\n",
       "   '2062024414',\n",
       "   '2132984323',\n",
       "   '2142276208',\n",
       "   '2098914003',\n",
       "   '2053691921',\n",
       "   '1634005169',\n",
       "   '2037612300',\n",
       "   '1996021349',\n",
       "   '1584610719']},\n",
       " {'id': '2050880896',\n",
       "  'title': 'Unconditional Bases Are Optimal Bases for Data Compression and for Statistical Estimation',\n",
       "  'abstract': 'Abstract   An orthogonal basis of L2 which is also an unconditional basis of a functional space    F    is an optimal basis for compressing, estimating, and recovering functions in    F   . Simple thresholding operations, applied in the unconditional basis, work essentially better for compressing, estimating, and recovering than they do in any other orthogonal basis. In fact, simple thresholding in an unconditional basis works essentially better for recovery and estimation than other methods, period. (Performance is measured in an asymptotic minimax sense.) As an application, we formalize and prove Mallat′s Heuristic, which says that wavelet bases are optimal for representing functions containing singularities, when there may be an arbitrary number of singularities, arbitrarily distributed.',\n",
       "  'date': '1993',\n",
       "  'authors': ['David L. Donoho'],\n",
       "  'related_topics': ['Orthogonal basis',\n",
       "   'Basis (linear algebra)',\n",
       "   'Thresholding',\n",
       "   'Minimax',\n",
       "   'Wavelet',\n",
       "   'Heuristic',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Data compression',\n",
       "   'Applied mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '576',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2129638195',\n",
       "  'title': 'Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?',\n",
       "  'abstract': 'Suppose we are given a vector f in a class FsubeRopfN , e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr2) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|(n)lesRmiddotn-1p/, where R>0 and p>0. Suppose that we take measurements yk=langf# ,Xkrang,k=1,...,K, where the Xk are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0<p<1 and with overwhelming probability, our reconstruction ft, defined as the solution to the constraints yk=langf# ,Xkrang with minimal lscr1 norm, obeys parf-f#parlscr2lesCp middotRmiddot(K/logN)-r, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed',\n",
       "  'date': '2006',\n",
       "  'authors': ['E.J. Candes', 'T. Tao'],\n",
       "  'related_topics': ['Norm (mathematics)',\n",
       "   'Restricted isometry property',\n",
       "   'Random projection',\n",
       "   'Random matrix',\n",
       "   'Concentration of measure',\n",
       "   'Gaussian',\n",
       "   'Fourier series',\n",
       "   'Basis pursuit',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,837',\n",
       "  'reference_count': '45',\n",
       "  'references': ['2296616510',\n",
       "   '2145096794',\n",
       "   '2115755118',\n",
       "   '2129131372',\n",
       "   '2078204800',\n",
       "   '2099641086',\n",
       "   '2103559027',\n",
       "   '2050834445',\n",
       "   '2154332973',\n",
       "   '2136235822']},\n",
       " {'id': '2050834445',\n",
       "  'title': 'For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution',\n",
       "  'abstract': \"We consider linear equations y = Φx where y is a given vector in ℝn and Φ is a given n × m matrix with n   0 so that for large n and for all Φ's except a negligible fraction, the following property holds: For every y having a representation y = Φx0by a coefficient vector x0 ∈ ℝmwith fewer than ρ · n nonzeros, the solution x1of the 1-minimization problem\\r\\n\\r\\n\\r\\n\\r\\nis unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems—greedy algorithms and thresholding—perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. © 2006 Wiley Periodicals, Inc.\",\n",
       "  'date': '2006',\n",
       "  'authors': ['David L. Donoho'],\n",
       "  'related_topics': ['Underdetermined system',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Norm (mathematics)',\n",
       "   'System of linear equations',\n",
       "   'M-matrix',\n",
       "   'Banach space',\n",
       "   'Wishart distribution',\n",
       "   'Linear equation',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,732',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2145096794',\n",
       "   '2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2097323375',\n",
       "   '2154332973',\n",
       "   '2136235822',\n",
       "   '2156447271',\n",
       "   '1573820523']},\n",
       " {'id': '2154332973',\n",
       "  'title': 'Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization',\n",
       "  'abstract': 'Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = ∑k γ(k)dk, with scalar coefficients γ(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the l1 norm of the coefficients γ. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.',\n",
       "  'date': '2003',\n",
       "  'authors': ['David L. Donoho', 'Michael'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Convex optimization',\n",
       "   'Linear combination',\n",
       "   'Basis pursuit',\n",
       "   'Combinatorial optimization',\n",
       "   'Minification',\n",
       "   'Discrete mathematics',\n",
       "   'Scalar (mathematics)',\n",
       "   'Special case',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,400',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2115755118',\n",
       "   '2078204800',\n",
       "   '2610857016',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2136235822',\n",
       "   '2167839759',\n",
       "   '1604810369',\n",
       "   '2125455772',\n",
       "   '1995963238']},\n",
       " {'id': '2087347434',\n",
       "  'title': 'A training algorithm for optimal margin classifiers',\n",
       "  'abstract': 'A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Bernhard E. Boser', 'Isabelle M. Guyon', 'Vladimir N. Vapnik'],\n",
       "  'related_topics': ['Margin (machine learning)',\n",
       "   'Decision boundary',\n",
       "   'Stability (learning theory)',\n",
       "   'Perceptron',\n",
       "   'Generalization',\n",
       "   'Linear combination',\n",
       "   'Radial basis function',\n",
       "   'Optical character recognition',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '13,540',\n",
       "  'reference_count': '20',\n",
       "  'references': ['3017143921',\n",
       "   '2171277043',\n",
       "   '2165758113',\n",
       "   '2154579312',\n",
       "   '2266946488',\n",
       "   '1530699444',\n",
       "   '2076118331',\n",
       "   '2086472796',\n",
       "   '2111494971',\n",
       "   '1965770722']},\n",
       " {'id': '1530699444',\n",
       "  'title': 'Estimation of Dependences Based on Empirical Data',\n",
       "  'abstract': 'Realism and Instrumentalism: Classical Statistics and VC Theory (1960-1980).- Falsifiability and Parsimony: VC Dimension and the Number of Entities (1980-2000).- Noninductive Methods of Inference: Direct Inference Instead of Generalization (2000-...).- The Big Picture.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Vladimir Naumovich Vapnik'],\n",
       "  'related_topics': ['Inference',\n",
       "   'VC dimension',\n",
       "   'Generalization',\n",
       "   'Falsifiability',\n",
       "   'Mathematical economics',\n",
       "   'Instrumentalism',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'Realism',\n",
       "   'Estimation',\n",
       "   'Empirical data',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,080',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2168228682',\n",
       "  'title': 'Comparison of classifier methods: a case study in handwritten digit recognition',\n",
       "  'abstract': 'This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.',\n",
       "  'date': '1994',\n",
       "  'authors': ['L. Bottou',\n",
       "   'C. Cortes',\n",
       "   '',\n",
       "   'J.S. Denker',\n",
       "   '',\n",
       "   'H. Drucker',\n",
       "   '',\n",
       "   'I. Guyon',\n",
       "   'L.D.',\n",
       "   'Y.',\n",
       "   'U.A.',\n",
       "   'E. Sackinger',\n",
       "   'P. Simard',\n",
       "   '',\n",
       "   'V.'],\n",
       "  'related_topics': ['Handwriting recognition',\n",
       "   'Classifier (UML)',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'NIST',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Digit recognition',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '937',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2087347434',\n",
       "   '2093717447',\n",
       "   '2137291015',\n",
       "   '2166501286',\n",
       "   '2056763477',\n",
       "   '2162363099',\n",
       "   '2093465006',\n",
       "   '2151328054']},\n",
       " {'id': '2504871398',\n",
       "  'title': 'Learning representations by back-propagation errors, nature',\n",
       "  'abstract': '',\n",
       "  'date': '1986',\n",
       "  'authors': ['DE', 'GE', 'RJ'],\n",
       "  'related_topics': ['Cognitive science', 'Backpropagation', 'Psychology'],\n",
       "  'citation_count': '1,464',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1568787085',\n",
       "  'title': 'Neural-Network and k-Nearest-neighbor Classifiers',\n",
       "  'abstract': 'The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor classifier and to human performance. The neural network has a clear advantage over the k-nearest-neighbor method, but at the same time does not yet reach human performance. Two methods for combining neural-network ideas and the k-nearest-neighbor algorithm are proposed. Numerical experiments for these methods show an improvement in performance.',\n",
       "  'date': '1991',\n",
       "  'authors': ['J.', 'E.'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Artificial neural network',\n",
       "   'Classifier (UML)',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Neural network classifier',\n",
       "   'View Less'],\n",
       "  'citation_count': '24',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '5594912',\n",
       "  'title': 'Estimation of Dependences Based on Empirical Data: Springer Series in Statistics (Springer Series in Statistics)',\n",
       "  'abstract': '',\n",
       "  'date': '1982',\n",
       "  'authors': ['Vladimir'],\n",
       "  'related_topics': ['Series (mathematics)',\n",
       "   'Computer science',\n",
       "   'Estimation',\n",
       "   'Statistics',\n",
       "   'Empirical data',\n",
       "   'View Less'],\n",
       "  'citation_count': '452',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2152473410',\n",
       "  'title': 'Example-based object detection in images by components',\n",
       "  'abstract': 'We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background.',\n",
       "  'date': '2001',\n",
       "  'authors': ['A. Mohan', 'C. Papageorgiou', 'T. Poggio'],\n",
       "  'related_topics': ['One-class classification',\n",
       "   'Object detection',\n",
       "   'Classifier (UML)',\n",
       "   'Data classification',\n",
       "   'Computer vision',\n",
       "   'Pattern recognition',\n",
       "   'Detector',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,431',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2156909104',\n",
       "   '2139212933',\n",
       "   '2912934387',\n",
       "   '2132984323',\n",
       "   '2217896605',\n",
       "   '2149684865',\n",
       "   '2112076978',\n",
       "   '2115763357',\n",
       "   '2152761983',\n",
       "   '2159686933']},\n",
       " {'id': '1992825118',\n",
       "  'title': 'Detecting Pedestrians Using Patterns of Motion and Appearance',\n",
       "  'abstract': 'This paper describes a pedestrian detection system that integratesimage intensity information with motion information.We use a detection style algorithm that scans a detectorover two consecutive frames of a video sequence. Thedetector is trained (using AdaBoost) to take advantage ofboth motion and appearance information to detect a walkingperson. Past approaches have built detectors based onmotion information or detectors based on appearance information,but ours is the first to combine both sources ofinformation in a single detector. The implementation describedruns at about 4 frames/second, detects pedestriansat very small scales (as small as 20x15 pixels), and has avery low false positive rate.Our approach builds on the detection work of Viola andJones. Novel contributions of this paper include: i) developmentof a representation of image motion which is extremelyefficient, and ii) implementation of a state of theart pedestrian detection system which operates on low resolutionimages under difficult conditions (such as rain andsnow).',\n",
       "  'date': '2003',\n",
       "  'authors': ['Paul Viola', 'Michael J. Jones', 'Daniel Snow'],\n",
       "  'related_topics': ['Pedestrian detection',\n",
       "   'AdaBoost',\n",
       "   'Pixel',\n",
       "   'Boosting (machine learning)',\n",
       "   'Computer vision',\n",
       "   'Detector',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,193',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2164598857',\n",
       "   '3124955340',\n",
       "   '2217896605',\n",
       "   '2115763357',\n",
       "   '2032210760',\n",
       "   '2155511848',\n",
       "   '2145073242',\n",
       "   '2162919312',\n",
       "   '2089181482',\n",
       "   '2143023146']},\n",
       " {'id': '1608462934',\n",
       "  'title': 'A Trainable System for Object Detection',\n",
       "  'abstract': 'This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Constantine Papageorgiou', 'Tomaso Poggio'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Object detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Face detection',\n",
       "   'Representation (systemics)',\n",
       "   'Haar wavelet',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Face (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,778',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2139212933',\n",
       "   '2132984323',\n",
       "   '2128272608',\n",
       "   '2217896605',\n",
       "   '2140235142',\n",
       "   '2124351082',\n",
       "   '2159686933',\n",
       "   '26816478']},\n",
       " {'id': '2156539399',\n",
       "  'title': 'Human Detection Based on a Probabilistic Assembly of Robust Part Detectors',\n",
       "  'abstract': 'We describe a novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion. Humans are modeled as flexible assemblies of parts, and robust part detection is the key to the approach. The parts are represented by co-occurrences of local features which captures the spatial layout of the partrsquos appearance. Feature selection and the part detectors are learnt from training images using AdaBoost. The detection algorithm is very efficient as (i) all part detectors use the same initial features, (ii) a coarse-to-fine cascade approach is used for part detection, (iii) a part assembly strategy reduces the number of spurious detections and the search space. The results outperform existing human detectors.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Krystian Mikolajczyk', 'Cordelia Schmid', 'Andrew Zisserman'],\n",
       "  'related_topics': ['Face detection',\n",
       "   'AdaBoost',\n",
       "   'Feature selection',\n",
       "   'Clutter',\n",
       "   'Probabilistic logic',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Detector',\n",
       "   'Key (cryptography)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '961',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2164598857',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2154422044',\n",
       "   '2152473410',\n",
       "   '1608462934',\n",
       "   '2155511848',\n",
       "   '2502277634',\n",
       "   '1555563476',\n",
       "   '2097041931']},\n",
       " {'id': '2914885528',\n",
       "  'title': 'Color indexing',\n",
       "  'abstract': \"Computer vision is moving into a new era in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, unconstrained environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the identity of an object with a known location, and determining the location of a known object. Color can be successfully used for both tasks.\\r\\nThis dissertation demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection which allows real-time indexing into a large database of stored models. It demonstrates techniques for dealing with crowded scenes and with models with similar color signatures. For solving the location problem it introduces an algorithm called Histogram Backprojection which performs this task efficiently in crowded scenes.\",\n",
       "  'date': '1991',\n",
       "  'authors': ['Michael James', 'Dana H.'],\n",
       "  'related_topics': ['Color normalization',\n",
       "   'Content-based image retrieval',\n",
       "   'Histogram',\n",
       "   'Intersection',\n",
       "   'Object (computer science)',\n",
       "   'Search engine indexing',\n",
       "   'Computer vision',\n",
       "   'Identity (object-oriented programming)',\n",
       "   'Robot',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,613',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2115738369',\n",
       "   '2913703059',\n",
       "   '3021212382',\n",
       "   '2415527960',\n",
       "   '2125756925',\n",
       "   '2119204143',\n",
       "   '2172373809',\n",
       "   '2489504689',\n",
       "   '2069266228',\n",
       "   '2136654525']},\n",
       " {'id': '2134731454',\n",
       "  'title': 'Unsupervised Learning by Probabilistic Latent Semantic Analysis',\n",
       "  'abstract': 'This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent semantic analysis',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Document-term matrix',\n",
       "   'Semantic computing',\n",
       "   'Latent class model',\n",
       "   'Mixture model',\n",
       "   'Probabilistic logic',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,175',\n",
       "  'reference_count': '23',\n",
       "  'references': ['1902027874',\n",
       "   '2798909945',\n",
       "   '2147152072',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '1983578042',\n",
       "   '2072773380',\n",
       "   '1524704912',\n",
       "   '2127314673',\n",
       "   '2064580901']},\n",
       " {'id': '2165828254',\n",
       "  'title': 'SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition',\n",
       "  'abstract': 'We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(±0.56%) at 15 training images per class, and 66.23%(±0.48%) at 30 training images.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Hao Zhang', 'A.C. Berg', 'M. Maire', 'J. Malik'],\n",
       "  'related_topics': ['k-nearest neighbors algorithm',\n",
       "   'Support vector machine',\n",
       "   'Caltech 101',\n",
       "   'MNIST database',\n",
       "   'Computational complexity theory',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Discriminative model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,532',\n",
       "  'reference_count': '40',\n",
       "  'references': ['2151103935',\n",
       "   '2310919327',\n",
       "   '2162915993',\n",
       "   '2057175746',\n",
       "   '2166049352',\n",
       "   '2104978738',\n",
       "   '1624854622',\n",
       "   '2147800946',\n",
       "   '2168002178',\n",
       "   '1484228140']},\n",
       " {'id': '2113606819',\n",
       "  'title': 'Efficient sparse coding algorithms',\n",
       "  'abstract': 'Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Honglak Lee', 'Alexis Battle', 'Rajat Raina', 'Andrew Y. Ng'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'K-SVD',\n",
       "   'Neural coding',\n",
       "   'Computational problem',\n",
       "   'Optimization problem',\n",
       "   'Convex optimization',\n",
       "   'Speedup',\n",
       "   'Least squares',\n",
       "   'Algorithm',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,221',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2063978378',\n",
       "   '2078204800',\n",
       "   '2145889472',\n",
       "   '2105464873',\n",
       "   '2140499889',\n",
       "   '2004915807',\n",
       "   '2101933716',\n",
       "   '16591383',\n",
       "   '2074376560',\n",
       "   '2146672645']},\n",
       " {'id': '2161516371',\n",
       "  'title': 'Image super-resolution as sparse representation of raw image patches',\n",
       "  'abstract': 'This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse and the recovered high-resolution image is competitive or even superior in quality to images produced by other SR methods.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Jianchao Yang', 'J. Wright', 'T. Huang', 'Yi Ma'],\n",
       "  'related_topics': ['K-SVD',\n",
       "   'Sparse approximation',\n",
       "   'Image processing',\n",
       "   'Image quality',\n",
       "   'Image resolution',\n",
       "   'Compressed sensing',\n",
       "   'Feature extraction',\n",
       "   'Iterative reconstruction',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,795',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2296616510',\n",
       "   '2053186076',\n",
       "   '2160547390',\n",
       "   '2153663612',\n",
       "   '2050834445',\n",
       "   '2118963448',\n",
       "   '2165939075',\n",
       "   '2097074225',\n",
       "   '2149760002',\n",
       "   '2105464873']},\n",
       " {'id': '1624854622',\n",
       "  'title': 'Object recognition with features inspired by visual cortex',\n",
       "  'abstract': \"We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['T. Serre', 'L. Wolf', 'T. Poggio'],\n",
       "  'related_topics': ['Cognitive neuroscience of visual object recognition',\n",
       "   '3D single-object recognition',\n",
       "   'Feature (machine learning)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Caltech 101',\n",
       "   'Object (computer science)',\n",
       "   'Face detection',\n",
       "   'Visual cortex',\n",
       "   'Edge detection',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,203',\n",
       "  'reference_count': '24',\n",
       "  'references': ['3097096317',\n",
       "   '2124386111',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2134557905',\n",
       "   '2152473410',\n",
       "   '2155511848',\n",
       "   '1949116567',\n",
       "   '2149194912',\n",
       "   '2171188998']},\n",
       " {'id': '1516111018',\n",
       "  'title': 'An introduction to variational methods for graphical models',\n",
       "  'abstract': 'This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Michael I. Jordan',\n",
       "   'Zoubin Ghahramani',\n",
       "   'Tommi S. Jaakkola',\n",
       "   'Lawrence K. Saul'],\n",
       "  'related_topics': ['Graphical model',\n",
       "   'Variational Bayesian methods',\n",
       "   'Variational message passing',\n",
       "   'Approximate inference',\n",
       "   'Bayesian network',\n",
       "   'Inference',\n",
       "   'Boltzmann machine',\n",
       "   'Markov chain',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,107',\n",
       "  'reference_count': '59',\n",
       "  'references': ['2099111195',\n",
       "   '2159080219',\n",
       "   '1573186872',\n",
       "   '2049633694',\n",
       "   '2171265988',\n",
       "   '2982720039',\n",
       "   '1746680969',\n",
       "   '2567948266',\n",
       "   '2397866408',\n",
       "   '1993845689']},\n",
       " {'id': '1699734612',\n",
       "  'title': 'Saliency, Scale and Image Description',\n",
       "  'abstract': 'Many computer vision problems can be considered to consist of two main tasks: the extraction of image content descriptions and their subsequent matching. The appropriate choice of type and level of description is of course task dependent, yet it is generally accepted that the low-level or so called early vision layers in the Human Visual System are context independent.\\r\\n\\r\\nThis paper concentrates on the use of low-level approaches for solving computer vision problems and discusses three inter-related aspects of this: saliencys scale selection and content description. In contrast to many previous approaches which separate these tasks, we argue that these three aspects are intrinsically related. Based on this observation, a multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Timor Kadir', 'Michael Brady'],\n",
       "  'related_topics': ['Automatic image annotation',\n",
       "   'Human visual system model',\n",
       "   'Visual Word',\n",
       "   'Scale space',\n",
       "   'Image retrieval',\n",
       "   'Scale-space axioms',\n",
       "   'Feature extraction',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,196',\n",
       "  'reference_count': '39',\n",
       "  'references': ['2115755118',\n",
       "   '2150134853',\n",
       "   '2914885528',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2156447271',\n",
       "   '2103504761',\n",
       "   '2109863423',\n",
       "   '2098152234',\n",
       "   '2003370853']},\n",
       " {'id': '2073257493',\n",
       "  'title': 'An interactive activation model of context effects in letter perception: I. An account of basic findings.',\n",
       "  'abstract': '',\n",
       "  'date': '1981',\n",
       "  'authors': ['James L. McClelland', 'David E.'],\n",
       "  'related_topics': ['Context effect',\n",
       "   'Visual perception',\n",
       "   'Perception',\n",
       "   'Missing letter effect',\n",
       "   'Word superiority effect',\n",
       "   'Transposed letter effect',\n",
       "   'Contextual Associations',\n",
       "   'Cognitive psychology',\n",
       "   'Cognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,664',\n",
       "  'reference_count': '45',\n",
       "  'references': ['1509703770',\n",
       "   '2007780422',\n",
       "   '2045597501',\n",
       "   '2068868410',\n",
       "   '2053127376',\n",
       "   '2147311265',\n",
       "   '2098683904',\n",
       "   '2040187703',\n",
       "   '2006769754',\n",
       "   '2154634575']},\n",
       " {'id': '2021878536',\n",
       "  'title': 'User Centered System Design: New Perspectives on Human-Computer Interaction',\n",
       "  'abstract': \"Contents: S.W. Draper, D.A. Norman, C. Lewis, Introduction. Part I:User Centered System Design. K. Hooper, Architectural Design: An Analogy. L.J. Bannon, Issues in Design: Some Notes. D.A. Norman, Cognitive Engineering. Part II:The Interface Experience. B.K. Laurel, Interface as Mimesis. E.L. Hutchins, J.D. Hollan, D.A. NormanDirect Manipulation Interfaces. A.A. diSessa, Notes on the Future of Programming: Breaking the Utility Barrier. Part III:Users' Understandings. M.S. Riley, User Understanding. C. Lewis, Understanding What's Happening in System Interactions. D. Owen, Naive Theories of Computation. A.A. diSessa, Models of Computation. W. Mark, Knowledge-Based Interface Design. Part IV:User Activities. A. Cypher, The Structure of Users' Activities. Y. Miyata, D.A. Norman, Psychological Issues in Support of Multiple Activities. R. Reichman, Communication Paradigms for a Window System. Part V:Toward a Pragmatics of Human-Machine Communication. W. Buxton, There's More to Interaction Than Meets the Eye: Some Issues in Manual Input. S.W. Draper, Display Managers as the Basis for User-Machine Communication. Part VI:Information Flow. D. Owen, Answers First, Then Questions. C.E. O'Malley, Helping Users Help Themselves. L.J. Bannon, Helping Users Help Each Other. C. Lewis, D.A. Norman, Designing for Error. L.J. Bannon, Computer-Mediated Communication. Part VII:The Context of Computing. J.S. Brown, From Cognitive to Social Ergonomics and Beyond.\",\n",
       "  'date': '1986',\n",
       "  'authors': ['Donald A.', 'Stephen W.'],\n",
       "  'related_topics': ['User experience design',\n",
       "   'User interface',\n",
       "   'Cognitive ergonomics',\n",
       "   'Context (language use)',\n",
       "   'Analogy',\n",
       "   'Pragmatics',\n",
       "   'Interface (Java)',\n",
       "   'Human–computer interaction',\n",
       "   'Information flow',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,674',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1490454746',\n",
       "  'title': 'Feature discovery by competitive learning',\n",
       "  'abstract': 'This paper reporis the results of our studies with an unsupervised learning paradigm which we have called “Competitive Learning.” We have examined competitive learning using both computer simulation and formal analysis and hove found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide o way to discover the salient, general features which can be used to classify o set of patterns. We show how o very simply competitive mechanism con discover a set of feature detectors which capture important aspects of the set of stimulus input patterns. We 0150 show how these feature detectors con form the basis of o multilayer system that con serve to learn categorizations of stimulus sets which ore not linearly separable. We show how the use of correlated stimuli con serve IX o kind of “teaching” input to the system to allow the development of feature detectors which would not develop otherwise. Although we find the competitive learning mechanism o very interesting and powerful learning principle, we do not, of course, imagine thot it is the only learning principle. Competitive learning is cm essentially nonassociative stotisticol learning scheme. We certainly imagine that other kinds of learning mechanisms will be involved in the building of associations among patterns of activation in o more complete neural network. We offer this analysis of these competitive learning mechanisms to further our understanding of how simple adaptive networks can discover features importont in the description of the stimulus environment in which the system finds itself.',\n",
       "  'date': '1988',\n",
       "  'authors': ['David E.', 'David'],\n",
       "  'related_topics': ['Competitive learning',\n",
       "   'Feature learning',\n",
       "   'Instance-based learning',\n",
       "   'Unsupervised learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Self-organizing map',\n",
       "   'Artificial neural network',\n",
       "   'Linear separability',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,784',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2073257493',\n",
       "   '1514711945',\n",
       "   '2113653296',\n",
       "   '2010315761',\n",
       "   '2103170504']},\n",
       " {'id': '2115647291',\n",
       "  'title': 'Direct manipulation interfaces',\n",
       "  'abstract': \"Direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. In this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. We identify two underlying phenomena that give rise to the feeling of directness. One deals with the information processing distance between the user's intentions and the facilities provided by the machine. Reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. The second phenomenon concerns the relation between the input and output vocabularies of the interface language. In particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. This provides the feeling of directness of manipulation.\",\n",
       "  'date': '1985',\n",
       "  'authors': ['Edwin L. Hutchins', 'James D. Hollan', 'Donald A. Norman'],\n",
       "  'related_topics': ['Direct manipulation interface',\n",
       "   'Interface (Java)',\n",
       "   'Information processing',\n",
       "   'Property (programming)',\n",
       "   'Human–computer interaction',\n",
       "   'Vocabulary',\n",
       "   'Relation (database)',\n",
       "   'Computer science',\n",
       "   'Data processing',\n",
       "   'Feeling',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,213',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2099305423',\n",
       "   '2021878536',\n",
       "   '2175030280',\n",
       "   '2005639687',\n",
       "   '1539777654',\n",
       "   '1979887415',\n",
       "   '2954951251',\n",
       "   '2037893691',\n",
       "   '121934918',\n",
       "   '2064302241']},\n",
       " {'id': '1505136099',\n",
       "  'title': 'Learning by statistical cooperation of self-interested neuron-like computing elements.',\n",
       "  'abstract': 'Since the usual approaches to cooperative computation in networks of neuron-like computating elements do not assume that network components have any \"preferences\", they do not make substantive contact with game theoretic concepts, despite their use of some of the same terminology. In the approach presented here, however, each network component, or adaptive element, is a self-interested agent that prefers some inputs over others and \"works\" toward obtaining the most highly preferred inputs. Here we describe an adaptive element that is robust enough to learn to cooperate with other elements like itself in order to further its self-interests. It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems. We then place the approach in its proper historical and theoretical perspective through comparison with a number of related algorithms. A secondary aim of this article is to suggest that beyond what is explicitly illustrated here, there is a wealth of ideas from game theory and allied disciplines such as mathematical economics that can be of use in thinking about cooperative computation in both nervous systems and man-made systems.',\n",
       "  'date': '1985',\n",
       "  'authors': ['Barto'],\n",
       "  'related_topics': ['Game theory',\n",
       "   'Terminology',\n",
       "   'Adaptation (computer science)',\n",
       "   'Element (category theory)',\n",
       "   'Computer science',\n",
       "   'Theoretical computer science',\n",
       "   'Computation',\n",
       "   'Order (exchange)',\n",
       "   'Perspective (graphical)',\n",
       "   'Developmental psychology',\n",
       "   'Game theoretic',\n",
       "   'View Less'],\n",
       "  'citation_count': '265',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1596324102',\n",
       "  'title': 'Machine Learning: An Artificial Intelligence Approach',\n",
       "  'abstract': 'This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective. Research directions covered include: learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis.',\n",
       "  'date': '2013',\n",
       "  'authors': ['R. S.', 'J. G.', 'T. M.'],\n",
       "  'related_topics': ['Robot learning',\n",
       "   'Explanation-based learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Learning sciences',\n",
       "   'Hyper-heuristic',\n",
       "   'Knowledge acquisition',\n",
       "   'Expert system',\n",
       "   'Knowledge base',\n",
       "   'Data science',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,555',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1569296262',\n",
       "  'title': 'Temporal credit assignment in reinforcement learning',\n",
       "  'abstract': '',\n",
       "  'date': '1984',\n",
       "  'authors': ['Richard Stuart'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Credit assignment',\n",
       "   'Error-driven learning',\n",
       "   'View Less'],\n",
       "  'citation_count': '957',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2075379212',\n",
       "  'title': 'Finite Markov chains',\n",
       "  'abstract': '',\n",
       "  'date': '1976',\n",
       "  'authors': ['John G.', 'J. Laurie'],\n",
       "  'related_topics': ['Examples of Markov chains',\n",
       "   'Markov chain',\n",
       "   'Markov chain mixing time',\n",
       "   'Markov kernel',\n",
       "   'Markov property',\n",
       "   'Markov renewal process',\n",
       "   'Lumpability',\n",
       "   'Markov process',\n",
       "   'Computer science',\n",
       "   'Statistical physics',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,046',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2122410182',\n",
       "  'title': 'Artificial Intelligence: A Modern Approach',\n",
       "  'abstract': 'The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.',\n",
       "  'date': '2020',\n",
       "  'authors': ['Stuart J. Russell', 'Peter Norvig'],\n",
       "  'related_topics': ['Symbolic artificial intelligence',\n",
       "   'Artificial Intelligence System',\n",
       "   'Artificial intelligence, situated approach',\n",
       "   'Marketing and artificial intelligence',\n",
       "   'Artificial psychology',\n",
       "   'AI-complete',\n",
       "   'Procedural reasoning system',\n",
       "   'Computational intelligence',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '42,343',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2168405694',\n",
       "  'title': 'Finite-time Analysis of the Multiarmed Bandit Problem',\n",
       "  'abstract': \"Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.\",\n",
       "  'date': '2002',\n",
       "  'authors': ['Peter Auer', 'Nicolò Cesa-Bianchi', 'Paul Fischer'],\n",
       "  'related_topics': ['Regret',\n",
       "   'Multi-armed bandit',\n",
       "   'Thompson sampling',\n",
       "   'Reinforcement learning',\n",
       "   'Dilemma',\n",
       "   'Monte Carlo tree search',\n",
       "   'Bounded function',\n",
       "   'Mathematical economics',\n",
       "   'General game playing',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,589',\n",
       "  'reference_count': '13',\n",
       "  'references': ['1497256448',\n",
       "   '2121863487',\n",
       "   '1515851193',\n",
       "   '2010029425',\n",
       "   '2000080679',\n",
       "   '2009551863',\n",
       "   '2317700292',\n",
       "   '1983962754',\n",
       "   '1977823770',\n",
       "   '1970097186']},\n",
       " {'id': '1714211023',\n",
       "  'title': 'Efficient selectivity and backup operators in Monte-Carlo tree search',\n",
       "  'abstract': 'A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to minmax as the number of simulations grows. This approach provides a finegrained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9 × 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Rémi Coulom'],\n",
       "  'related_topics': ['Interval tree',\n",
       "   'Search tree',\n",
       "   'Segment tree',\n",
       "   'Tree traversal',\n",
       "   'Optimal binary search tree',\n",
       "   'Fractal tree index',\n",
       "   'Incremental decision tree',\n",
       "   'Order statistic tree',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,260',\n",
       "  'reference_count': '29',\n",
       "  'references': ['2121863487',\n",
       "   '1515851193',\n",
       "   '2100677568',\n",
       "   '2312609093',\n",
       "   '1536615069',\n",
       "   '1512919909',\n",
       "   '1551466210',\n",
       "   '2033016725',\n",
       "   '2016647253',\n",
       "   '1532172966']},\n",
       " {'id': '131069610',\n",
       "  'title': 'Rapidly-exploring random trees : a new tool for path planning',\n",
       "  'abstract': '',\n",
       "  'date': '1998',\n",
       "  'authors': ['S.'],\n",
       "  'related_topics': ['Any-angle path planning',\n",
       "   'Random tree',\n",
       "   'Probabilistic roadmap',\n",
       "   'Motion planning',\n",
       "   'Kinodynamic planning',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'Rapidly exploring random tree',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,234',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2171084228',\n",
       "  'title': 'Monte-Carlo Planning in Large POMDPs',\n",
       "  'abstract': \"This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 x 10 battleship and partially observable PacMan, with approximately 1018 and 1056 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.\",\n",
       "  'date': '2010',\n",
       "  'authors': ['David Silver', 'Joel Veness'],\n",
       "  'related_topics': ['Partially observable Markov decision process',\n",
       "   'Black box',\n",
       "   'Domain knowledge',\n",
       "   'Curse of dimensionality',\n",
       "   'Benchmark (computing)',\n",
       "   'Tree (data structure)',\n",
       "   'Probability distribution',\n",
       "   'Monte Carlo method',\n",
       "   'Mathematical optimization',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Sampling (statistics)',\n",
       "   'Observable',\n",
       "   'View Less'],\n",
       "  'citation_count': '931',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2168405694',\n",
       "   '1625390266',\n",
       "   '2168359464',\n",
       "   '1714211023',\n",
       "   '2020135152',\n",
       "   '2099430963',\n",
       "   '2144913588',\n",
       "   '2134802714',\n",
       "   '2122659384',\n",
       "   '2110962519']},\n",
       " {'id': '2020135152',\n",
       "  'title': 'Combining online and offline knowledge in UCT',\n",
       "  'abstract': \"The UCT algorithm learns a value function online using sample-based search. The TD(λ) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.\",\n",
       "  'date': '2007',\n",
       "  'authors': ['Sylvain Gelly', 'David Silver'],\n",
       "  'related_topics': ['Monte Carlo tree search',\n",
       "   'Online and offline',\n",
       "   'Computer Go',\n",
       "   'Search tree',\n",
       "   'General game playing',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Bellman equation',\n",
       "   'Computer science',\n",
       "   'Sample (statistics)',\n",
       "   'Value (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '701',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2121863487',\n",
       "   '2168405694',\n",
       "   '1515851193',\n",
       "   '1625390266',\n",
       "   '1714211023',\n",
       "   '2100677568',\n",
       "   '1888434271',\n",
       "   '1491843047',\n",
       "   '2124175081',\n",
       "   '1778554682']},\n",
       " {'id': '1888434271',\n",
       "  'title': 'Modiﬁcation of UCT with Patterns in Monte-Carlo Go',\n",
       "  'abstract': 'Algorithm UCB1 for multi-armed bandit problem has already been extended to Algorithm UCT (Upper bound Confidence for Tree) which works for minimax tree search. We have developed a Monte-Carlo Go program, MoGo, which is the first computer Go program using UCT. We explain our modification of UCT for Go application and also the intelligent random simulation with patterns which has improved significantly the performance of MoGo. UCT combined with pruning techniques for large Go board is discussed, as well as parallelization of UCT. MoGo is now a top level Go program on $9\\\\times9$ and $13\\\\times13$ Go boards.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Sylvain Gelly', 'Yizao Wang', 'Rémi Munos', 'Olivier Teytaud'],\n",
       "  'related_topics': ['Computer Go',\n",
       "   'Minimax',\n",
       "   'Tree (data structure)',\n",
       "   'Pruning (decision trees)',\n",
       "   'Monte Carlo method',\n",
       "   'Upper and lower bounds',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Random simulation',\n",
       "   'View Less'],\n",
       "  'citation_count': '508',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2121863487',\n",
       "   '2168405694',\n",
       "   '1625390266',\n",
       "   '1714211023',\n",
       "   '3122363772',\n",
       "   '2033016725',\n",
       "   '2123932647',\n",
       "   '183472599',\n",
       "   '202670472',\n",
       "   '55358780']},\n",
       " {'id': '1510812122',\n",
       "  'title': 'Minimax policies for adversarial and stochastic bandits',\n",
       "  'abstract': 'We fill in a long open gap in the characterization of the minimax rate for the multi-armed bandit prob- lem. Concretely, we remove an extraneous loga- rithmic factor in the previously known upper bound and propose a new family of randomized algorithms based on an implicit normalization, as well as a new analysis. We also consider the stochastic case, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002) achieves the distribution-free optimal rate while still having a distribution-dependent rate log- arithmic in the number of plays.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Jean-Yves Audibert', 'Sébastien Bubeck'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Minimax',\n",
       "   'Upper and lower bounds',\n",
       "   'Randomized algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'Adversarial system',\n",
       "   'View Less'],\n",
       "  'citation_count': '359',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2168405694',\n",
       "   '1570963478',\n",
       "   '3122363772',\n",
       "   '2009551863',\n",
       "   '2098339418',\n",
       "   '2103012681',\n",
       "   '1998498767']},\n",
       " {'id': '1500868819',\n",
       "  'title': 'COMPUTING “ELO RATINGS” OF MOVE PATTERNS IN THE GAME OF GO',\n",
       "  'abstract': 'Move patterns are an essential method to incorporate do- main knowledge into Go-playing programs. This paper presents a new Bayesian technique for supervised learning of such patterns from game records, based on a generalization of Elo ratings. Each sample move in the training data is considered as a victory of a team of pattern features. Elo ratings of individual pattern features are computed from these victo- ries, and can be used in previously unseen positions to compute a prob- ability distribution over legal moves. In this approach, several pattern features may be combined, without an exponential cost in the number of features. Despite a very small number of training games (652), this algorithm outperforms most previous pattern-learning algorithms, both in terms of mean log-evidence ( 2.69), and prediction rate (34.9%). A 19◊ 19 Monte-Carlo program improved with these patterns reached the level of the strongest classical programs. and little domain expertise. This paper presents a new supervised pattern-learning algorithm, based on the Bradley-Terry model. The Bradley-Terry model is the theoretical basis of the Elo rating system. The principle of Elo ratings, as applied to chess, is that each player gets a numerical strength estimation, computed from the observation of past game results. From the ratings of players, it is possible to estimate a probability distribution over the outcome of future games. The same principle',\n",
       "  'date': '2007',\n",
       "  'authors': ['Rémi Coulom'],\n",
       "  'related_topics': ['Computer Go',\n",
       "   'Supervised learning',\n",
       "   'Outcome (game theory)',\n",
       "   'Probability distribution',\n",
       "   'Generalization',\n",
       "   'Bayesian probability',\n",
       "   'Sample (statistics)',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Subject-matter expert',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '440',\n",
       "  'reference_count': '17',\n",
       "  'references': ['1714211023',\n",
       "   '2153975459',\n",
       "   '1888434271',\n",
       "   '2075848246',\n",
       "   '2160637580',\n",
       "   '2135129960',\n",
       "   '2123932647',\n",
       "   '2107650339',\n",
       "   '202670472',\n",
       "   '1492505991']},\n",
       " {'id': '2077902449',\n",
       "  'title': 'The Nonstochastic Multiarmed Bandit Problem',\n",
       "  'abstract': 'In the multiarmed bandit problem, a gambler must decide which arm of K nonidentical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.\\r\\nIn this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/2). We show by a matching lower bound that this is the best possible.\\r\\nWe also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((log N1/2 T-1/2). Finally, we apply our results to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T-1/2).',\n",
       "  'date': '2003',\n",
       "  'authors': ['Peter Auer',\n",
       "   'Nicolò Cesa-Bianchi',\n",
       "   'Yoav Freund',\n",
       "   'Robert E. Schapire'],\n",
       "  'related_topics': ['Multi-armed bandit',\n",
       "   'Stochastic game',\n",
       "   'Minimax',\n",
       "   'Statistical assumption',\n",
       "   'Matching (graph theory)',\n",
       "   'Sequence',\n",
       "   'Upper and lower bounds',\n",
       "   'Stochastic process',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematical economics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,122',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2099111195',\n",
       "   '3124955340',\n",
       "   '3124873412',\n",
       "   '1979675141',\n",
       "   '2009551863',\n",
       "   '2742819845',\n",
       "   '2106887613',\n",
       "   '2611627047',\n",
       "   '3125873018',\n",
       "   '2317700292']},\n",
       " {'id': '2101861158',\n",
       "  'title': 'The challenge of poker',\n",
       "  'abstract': 'Poker is an interesting test-bed for artificial intelligence research. It is a game of imperfect information, where multiple competing agents must deal with probabilistic knowledge, risk assessment, and possible deception, not unlike decisions made in the real world. Opponent modeling is another difficult problem in decision-making applications, and it is essential to achieving high performance in poker. This paper describes the design considerations and architecture of the poker program Poki. In addition to methods for hand evaluation and betting strategy, Poki uses learning techniques to construct statistical models of each opponent, and dynamically adapts to exploit observed patterns and tendencies. The result is a program capable of playing reasonably strong poker, but there remains considerable research to be done to play at world-class level. Copyright 2001 Elsevier Science B.V.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Darse Billings',\n",
       "   'Aaron Davidson',\n",
       "   'Jonathan Schaeffer',\n",
       "   'Duane Szafron'],\n",
       "  'related_topics': [\"Texas hold 'em\",\n",
       "   'Planning poker',\n",
       "   'Perfect information',\n",
       "   'Deception',\n",
       "   'Probabilistic logic',\n",
       "   'Exploit',\n",
       "   'Construct (philosophy)',\n",
       "   'Computer science',\n",
       "   'Human–computer interaction',\n",
       "   'Machine learning',\n",
       "   'Statistical model',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '425',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2131600418',\n",
       "   '1978304080',\n",
       "   '1863869622',\n",
       "   '2083347533',\n",
       "   '2145224651',\n",
       "   '1561981064',\n",
       "   '2014932765',\n",
       "   '1532584713',\n",
       "   '2146628995',\n",
       "   '1630331242']},\n",
       " {'id': '2009551863',\n",
       "  'title': 'Asymptotically efficient adaptive allocation rules',\n",
       "  'abstract': '',\n",
       "  'date': '1985',\n",
       "  'authors': ['T.L Lai', 'Herbert Robbins'],\n",
       "  'related_topics': ['Mathematical optimization',\n",
       "   'Multi-armed bandit',\n",
       "   'Thompson sampling',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,667',\n",
       "  'reference_count': '3',\n",
       "  'references': ['2035879260', '2104912596', '1998498767']},\n",
       " {'id': '1512919909',\n",
       "  'title': 'A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes',\n",
       "  'abstract': 'A critical issue for the application of Markov decision processes (MDPs) to realistic problems is how the complexity of planning scales with the size of the MDP. In stochastic environments with very large or infinite state spaces, traditional planning and reinforcement learning algorithms may be inapplicable, since their running time typically grows linearly with the state space size in the worst case. In this paper we present a new algorithm that, given only a generative model (a natural and common type of simulator) for an arbitrary MDP, performs on-line, near-optimal planning with a per-state running time that has no dependence on the number of states. The running time is exponential in the horizon time (which depends only on the discount factor γ and the desired degree of approximation to the optimal policy). Our algorithm thus provides a different complexity trade-off than classical algorithms such as value iteration—rather than scaling linearly in both horizon time and state space size, our running time trades an exponential dependence on the former in exchange for no dependence on the latter.\\r\\n\\r\\nOur algorithm is based on the idea of sparse sampling. We prove that a randomly sampled look-ahead tree that covers only a vanishing fraction of the full look-ahead tree nevertheless suffices to compute near-optimal actions from any state of an MDP. Practical implementations of the algorithm are discussed, and we draw ties to our related recent results on finding a near-best strategy from a given class of strategies in very large partially observable MDPs (Kearns, Mansour, & Ng. Neural information processing systems 13, to appear).',\n",
       "  'date': '2002',\n",
       "  'authors': ['Michael Kearns', 'Yishay Mansour', 'Andrew Y. Ng'],\n",
       "  'related_topics': ['Partially observable Markov decision process',\n",
       "   'Markov decision process',\n",
       "   'Q-learning',\n",
       "   'Markov model',\n",
       "   'State space',\n",
       "   'Reinforcement learning',\n",
       "   'Tree (data structure)',\n",
       "   'Generative model',\n",
       "   'Mathematical optimization',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '822',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2122410182',\n",
       "   '2914656440',\n",
       "   '1655990431',\n",
       "   '2911283634',\n",
       "   '2009533501',\n",
       "   '1575388622',\n",
       "   '2021061679',\n",
       "   '3139578191',\n",
       "   '1650504995',\n",
       "   '2161521419']},\n",
       " {'id': '1551466210',\n",
       "  'title': 'MONTE-CARLO GO DEVELOPMENTS',\n",
       "  'abstract': 'We describe two Go programs, Olga and Oleg, developed by a Monte-Carlo approach that is simpler than Bruegmann’s (1993) approach. Our method is based on Abramson (1990). We performed experiments, to assess ideas on (1) progressive pruning, (2) all moves as first heuristic, (3) temperature, (4) simulated annealing, and (5) depth-two tree search within the Monte-Carlo framework. Progressive pruning and the all moves as first heuristic are good speed-up enhancements that do not deteriorate the level of the program too much. Then, using a constant temperature is an adequate and simple heuristic that is about as good as simulated annealing. The depth-two heuristic gives deceptive results at the moment. The results of our Monte-Carlo programs against knowledge-based programs on 9x9 boards are promising. Finally, the ever-increasing power of computers lead us to think that Monte-Carlo approaches are worth considering for computer Go in the future.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Bruno Bouzy', 'Bernard Helmstetter'],\n",
       "  'related_topics': ['Null-move heuristic',\n",
       "   'Computer Go',\n",
       "   'Heuristic',\n",
       "   'Simulated annealing',\n",
       "   'Pruning (decision trees)',\n",
       "   'Heuristics',\n",
       "   'Tree (data structure)',\n",
       "   'Monte Carlo method',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '216',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2581275558',\n",
       "   '1969481231',\n",
       "   '2101861158',\n",
       "   '2033016725',\n",
       "   '1863869622',\n",
       "   '2014932765',\n",
       "   '2107549951',\n",
       "   '2134440396',\n",
       "   '2787384486',\n",
       "   '2014969765']},\n",
       " {'id': '1863869622',\n",
       "  'title': 'World-championship-caliber Scrabble',\n",
       "  'abstract': 'Computer Scrabble programs have achieved a level of performance that exceeds that of the strongest human players. MAVEN was the first program to demonstrate this against human opposition. Scrabble is a game of imperfect information with a large branching factor. The techniques successfully applied in two-player games such as chess do not work here. MAVEN combines a selective move generator, simulations of likely game scenarios, and the B* algorithm to produce a world-championship-caliber Scrabble-playing program.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Brian Sheppard'],\n",
       "  'related_topics': ['World championship',\n",
       "   'Perfect information',\n",
       "   'Computer science',\n",
       "   'Generator (computer programming)',\n",
       "   'Artificial intelligence',\n",
       "   'Caliber',\n",
       "   'View Less'],\n",
       "  'citation_count': '187',\n",
       "  'reference_count': '16',\n",
       "  'references': ['1655990431',\n",
       "   '2100677568',\n",
       "   '2131600418',\n",
       "   '2101861158',\n",
       "   '3139578191',\n",
       "   '2135997697',\n",
       "   '2468925818',\n",
       "   '1532584713',\n",
       "   '1976776761',\n",
       "   '2107549951']},\n",
       " {'id': '2016647253',\n",
       "  'title': 'An Adaptive Sampling Algorithm for Solving Markov Decision Processes',\n",
       "  'abstract': 'Based on recent results for multiarmed bandit problems, we propose an adaptive sampling algorithm that approximates the optimal value of a finite-horizon Markov decision process (MDP) with finite state and action spaces. The algorithm adaptively chooses which action to sample as the sampling process proceeds and generates an asymptotically unbiased estimator, whose bias is bounded by a quantity that converges to zero at rate (lnN)/ N, whereN is the total number of samples that are used per state sampled in each stage. The worst-case running-time complexity of the algorithm isO(( |A|N) H ), independent of the size of the state space, where | A| is the size of the action space andH is the horizon length. The algorithm can be used to create an approximate receding horizon control to solve infinite-horizon MDPs. To illustrate the algorithm, computational results are reported on simple examples from inventory control.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Hyeong Soo Chang',\n",
       "   'Michael C. Fu',\n",
       "   'Jiaqiao Hu',\n",
       "   'Steven I. Marcus'],\n",
       "  'related_topics': ['Markov process',\n",
       "   'Adaptive algorithm',\n",
       "   'Markov decision process',\n",
       "   'Adaptive sampling',\n",
       "   'State space',\n",
       "   'Bias of an estimator',\n",
       "   'Sampling (statistics)',\n",
       "   'Dynamic programming',\n",
       "   'Mathematical optimization',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '143',\n",
       "  'reference_count': '18',\n",
       "  'references': ['3145128584',\n",
       "   '2752885492',\n",
       "   '2168405694',\n",
       "   '2098432798',\n",
       "   '2140481308',\n",
       "   '2000080679',\n",
       "   '2009551863',\n",
       "   '2113789941',\n",
       "   '1976625337',\n",
       "   '1512919909']},\n",
       " {'id': '1562282139',\n",
       "  'title': 'Monte Carlo Planning in RTS Games.',\n",
       "  'abstract': '',\n",
       "  'date': '2005',\n",
       "  'authors': ['Michael', 'Michael', 'Jonathan'],\n",
       "  'related_topics': ['Monte Carlo method',\n",
       "   'Monte Carlo tree search',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Simulation',\n",
       "   'View Less'],\n",
       "  'citation_count': '155',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2914656440',\n",
       "   '2911283634',\n",
       "   '2131600418',\n",
       "   '1551466210',\n",
       "   '1532584713',\n",
       "   '2153367522',\n",
       "   '2785370368',\n",
       "   '2120032275',\n",
       "   '2995879998',\n",
       "   '2173567424']},\n",
       " {'id': '2135997697',\n",
       "  'title': 'On-line Policy Improvement using Monte-Carlo Search',\n",
       "  'abstract': 'We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers.\\r\\n\\r\\nWe have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Gerald Tesauro', 'Gregory R. Galperin'],\n",
       "  'related_topics': ['Adaptive control',\n",
       "   'Monte Carlo method',\n",
       "   'Reduction (complexity)',\n",
       "   'Control theory',\n",
       "   'Word error rate',\n",
       "   'Artificial neural network',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'Line (geometry)',\n",
       "   'View Less'],\n",
       "  'citation_count': '277',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2098432798',\n",
       "   '2100677568',\n",
       "   '2131600418',\n",
       "   '2103626435',\n",
       "   '2117341272',\n",
       "   '2148173263',\n",
       "   '2151368309',\n",
       "   '2202514763']},\n",
       " {'id': '2295428206',\n",
       "  'title': 'Randomized Algorithms',\n",
       "  'abstract': 'For many applications, a randomized algorithm is either the simplest or the fastest algorithm available, and sometimes both. This book introduces the basic concepts in the design and analysis of randomized algorithms. The first part of the text presents basic tools such as probability theory and probabilistic analysis that are frequently used in algorithmic applications. Algorithmic examples are also given to illustrate the use of each tool in a concrete setting. In the second part of the book, each chapter focuses on an important area to which randomized algorithms can be applied, providing a comprehensive and representative selection of the algorithms that might be used in each of these areas. Although written primarily as a text for advanced undergraduates and graduate students, this book should also prove invaluable as a reference for professionals and researchers.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Rajeev Motwani', 'Prabhakar Raghavan'],\n",
       "  'related_topics': ['Probabilistic analysis of algorithms',\n",
       "   'Randomized algorithm',\n",
       "   'Seven Basic Tools of Quality',\n",
       "   'Computer science',\n",
       "   'Theoretical computer science',\n",
       "   'Selection (linguistics)',\n",
       "   'Closest string',\n",
       "   'Probability theory',\n",
       "   'Graduate students',\n",
       "   'Randomized algorithms as zero-sum games',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,279',\n",
       "  'reference_count': '50',\n",
       "  'references': ['2068871408',\n",
       "   '1963547452',\n",
       "   '1979740015',\n",
       "   '2052207834',\n",
       "   '2070991879',\n",
       "   '1972418517',\n",
       "   '1593563200',\n",
       "   '2148043549',\n",
       "   '2069816168',\n",
       "   '1568495775']},\n",
       " {'id': '1956559956',\n",
       "  'title': 'Introduction to Modern Information Retrieval',\n",
       "  'abstract': '',\n",
       "  'date': '1983',\n",
       "  'authors': ['Gerard', 'Michael J.'],\n",
       "  'related_topics': ['Human–computer information retrieval',\n",
       "   'Term Discrimination',\n",
       "   'Vector space model',\n",
       "   'Computer science',\n",
       "   'Extended Boolean model',\n",
       "   'Information retrieval',\n",
       "   'University level',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,116',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2125148312',\n",
       "  'title': 'Texture features for browsing and retrieval of image data',\n",
       "  'abstract': 'Image content based retrieval is emerging as an important research area with application to digital libraries and multimedia databases. The focus of this paper is on the image processing aspects and in particular using texture information for browsing and retrieval of large image data. We propose the use of Gabor wavelet features for texture analysis and provide a comprehensive experimental evaluation. Comparisons with other multiresolution texture features using the Brodatz texture database indicate that the Gabor features provide the best pattern retrieval accuracy. An application to browsing large air photos is illustrated.',\n",
       "  'date': '1996',\n",
       "  'authors': ['B.S. Manjunath', 'W.Y. Ma'],\n",
       "  'related_topics': ['Image texture',\n",
       "   'Content-based image retrieval',\n",
       "   'Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Texture Descriptor',\n",
       "   'Gabor wavelet',\n",
       "   'Image processing',\n",
       "   'Feature extraction',\n",
       "   'Wavelet',\n",
       "   'Wavelet transform',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,342',\n",
       "  'reference_count': '20',\n",
       "  'references': ['1996021349',\n",
       "   '2102796633',\n",
       "   '2095757522',\n",
       "   '2093191240',\n",
       "   '2008297189',\n",
       "   '2138313032',\n",
       "   '2168977926',\n",
       "   '2103384342',\n",
       "   '2171181782',\n",
       "   '2021751319']},\n",
       " {'id': '2160066518',\n",
       "  'title': 'Query by image and video content: the QBIC system',\n",
       "  'abstract': 'Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. >',\n",
       "  'date': '1997',\n",
       "  'authors': ['Myron',\n",
       "   'Harpreet',\n",
       "   'Wayne',\n",
       "   'Jonathan',\n",
       "   'Qian',\n",
       "   'Byron',\n",
       "   'Monika',\n",
       "   'Jim',\n",
       "   'Denis',\n",
       "   'Dragutin',\n",
       "   'David',\n",
       "   'Peter'],\n",
       "  'related_topics': ['Information retrieval',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Image (mathematics)',\n",
       "   'Motion (physics)',\n",
       "   'IBM',\n",
       "   'Content (measure theory)',\n",
       "   'Graphical query',\n",
       "   'Image content',\n",
       "   'Object motion',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,569',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2074429597',\n",
       "  'title': 'The design and analysis of spatial data structures',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['Hanan Samet'],\n",
       "  'related_topics': ['Spatial analysis', 'Computer science', 'Remote sensing'],\n",
       "  'citation_count': '4,149',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1541459201',\n",
       "  'title': 'A Quantitative Analysis and Performance Study for Similarity-Search Methods in High-Dimensional Spaces',\n",
       "  'abstract': 'For similarity search in high-dimensional vector spaces (or ‘HDVSs’), researchers have proposed a number of new methods (or adaptations of existing methods) based, in the main, on data-space partitioning. However, the performance of these methods generally degrades as dimensionality increases. Although this phenomenon-known as the ‘dimensional curse’-is well known, little or no quantitative a.nalysis of the phenomenon is available. In this paper, we provide a detailed analysis of partitioning and clustering techniques for similarity search in HDVSs. We show formally that these methods exhibit linear complexity at high dimensionality, and that existing methods are outperformed on average by a simple sequential scan if the number of dimensions exceeds around 10. Consequently, we come up with an alternative organization based on approximations to make the unavoidable sequential scan as fast as possible. We describe a simple vector approximation scheme, called VA-file, and report on an experimental evaluation of this and of two tree-based index methods (an R*-tree and an X-tree).',\n",
       "  'date': '1998',\n",
       "  'authors': ['Roger Weber', 'Hans-Jörg Schek', 'Stephen Blott'],\n",
       "  'related_topics': ['Nearest neighbor search',\n",
       "   'iDistance',\n",
       "   'Curse of dimensionality',\n",
       "   'Cluster analysis',\n",
       "   'Tree (data structure)',\n",
       "   'Vector space',\n",
       "   'Algorithm',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,343',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2160066518',\n",
       "   '2151135734',\n",
       "   '2118269922',\n",
       "   '2074429597',\n",
       "   '2238624099',\n",
       "   '1969294188',\n",
       "   '2157092487',\n",
       "   '1992810975',\n",
       "   '2145725688',\n",
       "   '2104128006']},\n",
       " {'id': '1981627423',\n",
       "  'title': 'Planning in a hierarchy of abstraction spaces',\n",
       "  'abstract': \"Abstract   A problem domain can be represented as a hierarchy of abstraction spaces in which successively finer levels of detail are introduced. The problem solver ABSTRIPS, a modification of STRIPS, can define an abstraction space hierarchy from the STRIPS representation of a problem domain, and it can utilize the hierarchy in solving problems. Examples of the system's performance are presented that demonstrate the significant increases in problem-solving power that this approach provides. Then some further implications of the hierarchical planning approach are explored.\",\n",
       "  'date': '1974',\n",
       "  'authors': ['Earl D. Sacerdoti'],\n",
       "  'related_topics': ['Analytical hierarchy',\n",
       "   'Hierarchy',\n",
       "   'Problem domain',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Representation (mathematics)',\n",
       "   'Theoretical computer science',\n",
       "   'Space (mathematics)',\n",
       "   'STRIPS',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'Planning approach',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,930',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2138162238',\n",
       "   '2402164836',\n",
       "   '2020149918',\n",
       "   '1541540802',\n",
       "   '3139316869',\n",
       "   '1512426208',\n",
       "   '145832685',\n",
       "   '1492160417']},\n",
       " {'id': '1572038152',\n",
       "  'title': 'Search reduction in hierarchical problem solving',\n",
       "  'abstract': 'It has long been recognized that hierarchical problem solving can be used to reduce search. Yet, there has been little analysis of the problem-solving method and few experimental results. This paper provides the first comprehensive analytical and empirical demonstrations of the effectiveness of hierarchical problem solving. First, the paper shows analytically that hierarchical problem solving can reduce the size of the search space from exponential to linear in the solution length and identifies a sufficient set of assumptions for such reductions in search. Second, it presents empirical results both in a domain that meets all of these assumptions as well as in domains in which these assumptions do not strictly hold. Third, the paper explores the conditions under which hierarchical problem solving will be effective in practice.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Craig A. Knoblock'],\n",
       "  'related_topics': ['General Group Problem Solving (GGPS) Model',\n",
       "   'Reduction (complexity)',\n",
       "   'Set (abstract data type)',\n",
       "   'Mathematical optimization',\n",
       "   'Exponential function',\n",
       "   'Computer science',\n",
       "   'Space (mathematics)',\n",
       "   'View Less'],\n",
       "  'citation_count': '100',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2293009092',\n",
       "   '1613651675',\n",
       "   '2021337406',\n",
       "   '2009328995',\n",
       "   '2084335986',\n",
       "   '2051567680',\n",
       "   '1981627423',\n",
       "   '125130877',\n",
       "   '1567972407',\n",
       "   '2017626051']},\n",
       " {'id': '2119409989',\n",
       "  'title': 'Programs with common sense',\n",
       "  'abstract': 'Abstract : This paper discusses programs to manipulate in a suitable formal language (most likely a part of the predicate calculus) common instrumental statements. The basic program will draw immediate conclusions from a list of premises. These conclusions will be either declarative or imperative sentences. When an imperative sentence is deduced the program takes a corresponding action. These actions may include printing sentences, moving sentences on lists, and reinitiating the basic deduction process on these lists.',\n",
       "  'date': '1960',\n",
       "  'authors': ['John McCarthy'],\n",
       "  'related_topics': ['Imperative logic',\n",
       "   'Sentence',\n",
       "   'Formal language',\n",
       "   'First-order logic',\n",
       "   'Action (philosophy)',\n",
       "   'Common sense',\n",
       "   'Computer programming',\n",
       "   'Natural language processing',\n",
       "   'Process (engineering)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,281',\n",
       "  'reference_count': '3',\n",
       "  'references': ['1970001627', '2479583503', '2112161977']},\n",
       " {'id': '1506806321',\n",
       "  'title': 'Pattern Recognition and Machine Learning (Information Science and Statistics)',\n",
       "  'abstract': '',\n",
       "  'date': '2006',\n",
       "  'authors': ['Christopher M.'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Unsupervised learning',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Information science',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,386',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2119567691',\n",
       "  'title': 'Markov Decision Processes: Discrete Stochastic Dynamic Programming',\n",
       "  'abstract': 'From the Publisher:\\r\\nThe past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman\\'s new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a \"theorem-proof\" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic',\n",
       "  'date': '1994',\n",
       "  'authors': ['Martin L.'],\n",
       "  'related_topics': ['Partially observable Markov decision process',\n",
       "   'Markov decision process',\n",
       "   'Markov model',\n",
       "   'Variable-order Bayesian network',\n",
       "   'Markov process',\n",
       "   'Markov algorithm',\n",
       "   'Stochastic programming',\n",
       "   'State space',\n",
       "   'Mathematical economics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '14,934',\n",
       "  'reference_count': '12',\n",
       "  'references': ['3038830718',\n",
       "   '2035446426',\n",
       "   '2341171179',\n",
       "   '2096630263',\n",
       "   '2130184319',\n",
       "   '1571552128',\n",
       "   '2319878520',\n",
       "   '2067733412',\n",
       "   '2036791211',\n",
       "   '154446778']},\n",
       " {'id': '2107726111',\n",
       "  'title': 'Reinforcement learning: a survey',\n",
       "  'abstract': 'This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Leslie Pack Kaelbling',\n",
       "   'Michael L. Littman',\n",
       "   'Andrew W. Moore'],\n",
       "  'related_topics': ['Learning classifier system',\n",
       "   'Reinforcement learning',\n",
       "   'Robot learning',\n",
       "   'Proactive learning',\n",
       "   'Q-learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Instance-based learning',\n",
       "   'Computational learning theory',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,631',\n",
       "  'reference_count': '110',\n",
       "  'references': ['1639032689',\n",
       "   '1497256448',\n",
       "   '1652505363',\n",
       "   '2098432798',\n",
       "   '2119567691',\n",
       "   '2100677568',\n",
       "   '2000836282',\n",
       "   '2119717200',\n",
       "   '1963547452',\n",
       "   '2019363670']},\n",
       " {'id': '1576452626',\n",
       "  'title': 'Neuro-dynamic programming',\n",
       "  'abstract': 'From the Publisher:\\r\\nThis is the first textbook that fully explains the neuro-dynamic programming/reinforcement learning methodology, which is a recent breakthrough in the practical application of neural networks and dynamic programming to complex problems of planning, optimal decision making, and intelligent control.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Dimitri P.', 'John N.'],\n",
       "  'related_topics': ['Inductive programming',\n",
       "   'Functional logic programming',\n",
       "   'Functional reactive programming',\n",
       "   'Programming domain',\n",
       "   'Reactive programming',\n",
       "   'Programming paradigm',\n",
       "   'Symbolic programming',\n",
       "   'Markov decision process',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,993',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2168359464',\n",
       "  'title': 'Planning and Acting in Partially Observable Stochastic Domains',\n",
       "  'abstract': 'In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Leslie Pack Kaelbling',\n",
       "   'Michael L. Littman',\n",
       "   'Anthony R. Cassandra'],\n",
       "  'related_topics': ['Partially observable Markov decision process',\n",
       "   'Predictive state representation',\n",
       "   'Markov decision process',\n",
       "   'Markov model',\n",
       "   'Observable',\n",
       "   'Mathematical optimization',\n",
       "   'Control theory',\n",
       "   'Mathematics',\n",
       "   'Bayesian reinforcement learning',\n",
       "   'Off line',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,355',\n",
       "  'reference_count': '70',\n",
       "  'references': ['2125838338',\n",
       "   '1576818901',\n",
       "   '2098432798',\n",
       "   '2119567691',\n",
       "   '1963547452',\n",
       "   '2025460523',\n",
       "   '2105934661',\n",
       "   '1978304080',\n",
       "   '1552169927',\n",
       "   '4789806']},\n",
       " {'id': '2109910161',\n",
       "  'title': 'Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning',\n",
       "  'abstract': 'Learning, planning, and representing knowledge at multiple levels of temporal ab- straction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforce- ment learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking ac- tion over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as mus- cle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning frame- work in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic pro- gramming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: 1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, 2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and 3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Richard S. Sutton', 'Doina Precup', 'Satinder Singh'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Markov decision process',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Set (psychology)',\n",
       "   'Hierarchy (mathematics)',\n",
       "   'Function approximation',\n",
       "   'Object (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Action (philosophy)',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,873',\n",
       "  'reference_count': '75',\n",
       "  'references': ['2121863487',\n",
       "   '2131600418',\n",
       "   '2009533501',\n",
       "   '73143588',\n",
       "   '2911432472',\n",
       "   '2017103958',\n",
       "   '1595483645',\n",
       "   '2333196491',\n",
       "   '1979071892',\n",
       "   '1598634407']},\n",
       " {'id': '2075268401',\n",
       "  'title': 'Fast gradient-descent methods for temporal-difference learning with linear function approximation',\n",
       "  'abstract': 'Sutton, Szepesvari and Maei (2009) recently introduced the first temporal-difference learning algorithm compatible with both linear function approximation and off-policy training, and whose complexity scales only linearly in the size of the function approximator. Although their gradient temporal difference (GTD) algorithm converges reliably, it can be very slow compared to conventional linear TD (on on-policy problems where TD is convergent), calling into question its practical utility. In this paper we introduce two new related algorithms with better convergence rates. The first algorithm, GTD2, is derived and proved convergent just as GTD was, but uses a different objective function and converges significantly faster (but still not as fast as conventional TD). The second new algorithm, linear TD with gradient correction, or TDC, uses the same update rule as conventional TD except for an additional term which is initially zero. In our experiments on small test problems and in a Computer Go application with a million features, the learning rate of this algorithm was comparable to that of conventional TD. This algorithm appears to extend linear TD to off-policy learning with no penalty in performance while only doubling computational requirements.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Richard S. Sutton',\n",
       "   'Hamid Reza Maei',\n",
       "   'Doina Precup',\n",
       "   'Shalabh Bhatnagar',\n",
       "   'David Silver',\n",
       "   'Csaba Szepesvári',\n",
       "   'Eric Wiewiora'],\n",
       "  'related_topics': ['Gradient descent',\n",
       "   'Temporal difference learning',\n",
       "   'Linear function',\n",
       "   'Function (mathematics)',\n",
       "   'Convergence (routing)',\n",
       "   'Algorithm',\n",
       "   'Term (time)',\n",
       "   'Computer Go',\n",
       "   'Zero (complex analysis)',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '517',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2100677568',\n",
       "   '2109910161',\n",
       "   '2139418546',\n",
       "   '1646707810',\n",
       "   '1778554682',\n",
       "   '2071983464',\n",
       "   '2072931156',\n",
       "   '2065134213',\n",
       "   '2132351269',\n",
       "   '2104753538']},\n",
       " {'id': '2062937587',\n",
       "  'title': 'Neo: learning conceptual knowledge by sensorimotor interaction with an environment',\n",
       "  'abstract': \"Recent developments in philosophy, linguistics, developmental psychology and arti cial intelligence make it possible to envision a developmental path for an arti cial agent, grounded in activity-based sensorimotor representations. This paper describes how Neo, an arti cial agent, learns concepts by interacting with its simulated environment. Relatively little prior structure is required to learn fairly accurate representations of objects, activities, locations and other aspects of Neo's experience. We show how classes (categories) can be abstracted from these representations, and discuss how our representation might be extended to express physical schemas, general, domain-independent activities that could be the building blocks of concept formation.\",\n",
       "  'date': '1997',\n",
       "  'authors': ['Paul R. Cohen', 'Marc S. Atkin', 'Tim Oates', 'Carole R. Beal'],\n",
       "  'related_topics': ['Concept learning',\n",
       "   'Cognitive science',\n",
       "   'Representation (arts)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '97',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2052417512',\n",
       "   '1993750641',\n",
       "   '266716723',\n",
       "   '2140243223',\n",
       "   '2228018563',\n",
       "   '2059039791',\n",
       "   '2137171066',\n",
       "   '1601336606',\n",
       "   '3022135529',\n",
       "   '1992764401']},\n",
       " {'id': '1491843047',\n",
       "  'title': 'Integrated architecture for learning, planning, and reacting based on approximating dynamic programming',\n",
       "  'abstract': \"This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.\",\n",
       "  'date': '1990',\n",
       "  'authors': ['Richard S. Sutton'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Temporal difference learning',\n",
       "   'Intelligent decision support system',\n",
       "   'Data structure',\n",
       "   'Trial and error',\n",
       "   'Dynamic programming',\n",
       "   'Reactive system',\n",
       "   'Class (computer programming)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Reinforcement',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,753',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2121863487',\n",
       "   '2100677568',\n",
       "   '1583833196',\n",
       "   '2021061679',\n",
       "   '1569296262',\n",
       "   '2225341423',\n",
       "   '1610678877',\n",
       "   '1969166509',\n",
       "   '1551329771',\n",
       "   '1506514354']},\n",
       " {'id': '2149390907',\n",
       "  'title': 'Learning symbolic models of stochastic domains',\n",
       "  'abstract': 'In this article, we work towards the goal of developing agents that can learn to act in complex worlds. We develop a probabilistic, relational planning rule representation that compactly models noisy, nondeterministic action effects, and show how such rules can be effectively learned. Through experiments in simple planning domains and a 3D simulated blocks world with realistic physics, we demonstrate that this learning algorithm allows agents to effectively model world dynamics.',\n",
       "  'date': '2007',\n",
       "  'authors': ['Hanna M. Pasula',\n",
       "   'Luke S. Zettlemoyer',\n",
       "   'Leslie Pack Kaelbling'],\n",
       "  'related_topics': ['Blocks world',\n",
       "   'Nondeterministic algorithm',\n",
       "   'Probabilistic logic',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Action (philosophy)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Dynamics (music)',\n",
       "   'View Less'],\n",
       "  'citation_count': '251',\n",
       "  'reference_count': '35',\n",
       "  'references': ['2798766386',\n",
       "   '2011039300',\n",
       "   '1977970897',\n",
       "   '2334782222',\n",
       "   '3022778360',\n",
       "   '1575388622',\n",
       "   '2121075864',\n",
       "   '2170400507',\n",
       "   '2225341423',\n",
       "   '2009207944']},\n",
       " {'id': '2134042548',\n",
       "  'title': 'Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approximation',\n",
       "  'abstract': \"We introduce the first temporal-difference learning algorithms that converge with smooth value function approximators, such as neural networks. Conventional temporal-difference (TD) methods, such as TD(λ), Q-learning and Sarsa have been used successfully with function approximation in many applications. However, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge). Sutton et al. (2009a, 2009b) solved the problem of off-policy learning with linear TD algorithms by introducing a new objective function, related to the Bellman error, and algorithms that perform stochastic gradient-descent on this function. These methods can be viewed as natural generalizations to previous TD methods, as they converge to the same limit points when used with linear function approximation methods. We generalize this work to nonlinear function approximation. We present a Bellman error objective function and two gradient-descent TD algorithms that optimize it. We prove the asymptotic almost-sure convergence of both algorithms, for any finite Markov decision process and any smooth value function approximator, to a locally optimal solution. The algorithms are incremental and the computational complexity per time step scales linearly with the number of parameters of the approximator. Empirical results obtained in the game of Go demonstrate the algorithms' effectiveness.\",\n",
       "  'date': '2009',\n",
       "  'authors': ['Shalabh Bhatnagar',\n",
       "   'Doina Precup',\n",
       "   'David Silver',\n",
       "   'Richard S Sutton',\n",
       "   'Hamid R. Maei',\n",
       "   'Csaba Szepesvári'],\n",
       "  'related_topics': ['Function approximation',\n",
       "   'Approximation algorithm',\n",
       "   'Bellman equation',\n",
       "   'Function (mathematics)',\n",
       "   'Linear function (calculus)',\n",
       "   'Temporal difference learning',\n",
       "   'Artificial neural network',\n",
       "   'Computational complexity theory',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '243',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2121863487',\n",
       "   '1515851193',\n",
       "   '2100677568',\n",
       "   '2531891978',\n",
       "   '2139418546',\n",
       "   '2075268401',\n",
       "   '2103626435',\n",
       "   '1646707810',\n",
       "   '2006903949',\n",
       "   '2117341272']},\n",
       " {'id': '13294968',\n",
       "  'title': 'Toward Off-Policy Learning Control with Function Approximation',\n",
       "  'abstract': 'We present the first temporal-difference learning algorithm for off-policy control with unrestricted linear function approximation whose per-time-step complexity is linear in the number of features. Our algorithm, Greedy-GQ, is an extension of recent work on gradient temporal-difference learning, which has hitherto been restricted to a prediction (policy evaluation) setting, to a control setting in which the target policy is greedy with respect to a linear approximation to the optimal action-value function. A limitation of our control setting is that we require the behavior policy to be stationary. We call this setting latent learning because the optimal policy, though learned, is not manifest in behavior. Popular off-policy algorithms such as Q-learning are known to be unstable in this setting when used with linear function approximation.',\n",
       "  'date': '2010',\n",
       "  'authors': ['Hamid R. Maei',\n",
       "   'Csaba Szepesv ri',\n",
       "   'Shalabh Bhatnagar',\n",
       "   'Richard S. Sutton'],\n",
       "  'related_topics': ['Q-learning',\n",
       "   'Function approximation',\n",
       "   'Linear approximation',\n",
       "   'Linear function (calculus)',\n",
       "   'Function (mathematics)',\n",
       "   'Latent learning',\n",
       "   'Mathematical optimization',\n",
       "   'Extension (predicate logic)',\n",
       "   'Control (management)',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '255',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2121863487',\n",
       "   '1515851193',\n",
       "   '1592648094',\n",
       "   '594357522',\n",
       "   '2130005627',\n",
       "   '2075268401',\n",
       "   '1646707810',\n",
       "   '1547105496',\n",
       "   '2134042548',\n",
       "   '2104753538']},\n",
       " {'id': '2096072088',\n",
       "  'title': 'A Joint Language Model With Fine-grain Syntactic Tags',\n",
       "  'abstract': 'We present a scalable joint language model designed to utilize fine-grain syntactic tags. We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora. We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees - a combination of properties that allows easy adoption of this model for new languages. We propose two fine-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Denis Filimonov', 'Mary Harper'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Parsing',\n",
       "   'Syntax',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Scale (map)',\n",
       "   'Joint (audio engineering)',\n",
       "   'Artificial intelligence',\n",
       "   'Fine grain',\n",
       "   'View Less'],\n",
       "  'citation_count': '35',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2158195707',\n",
       "   '2121227244',\n",
       "   '2056250865',\n",
       "   '2113691817',\n",
       "   '2099345940',\n",
       "   '1924403233',\n",
       "   '2080018251',\n",
       "   '2155280192',\n",
       "   '2143866356',\n",
       "   '2116625254']},\n",
       " {'id': '2468573742',\n",
       "  'title': 'The 2005 AMI system for the transcription of speech in meetings',\n",
       "  'abstract': 'In this paper we describe the 2005 AMI system for the transcription of speech in meetings used in the 2005 NIST RT evaluations. The system was designed for participation in the speech to text part of the evaluations, in particular for transcription of speech recorded with multiple distant microphones and independent headset microphones. System performance was tested on both conference room and lecture style meetings. Although input sources are processed using different front-ends, the recognition process is based on a unified system architecture. The system operates in multiple passes and makes use of state of the art technologies such as discriminative training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, speaker adaptation with maximum likelihood linear regression and minimum word error rate decoding. In this paper we describe the system performance on the official development and test sets for the NIST RT05s evaluations. The system was jointly developed in less than 10 months by a multi-site team and was shown to achieve competitive performance.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Thomas Hain',\n",
       "   'Lukas Burget',\n",
       "   'John Dines',\n",
       "   'Giulia Garau',\n",
       "   'Martin Karafiat',\n",
       "   'Mike Lincoln',\n",
       "   'Iain McCowan',\n",
       "   'Darren Moore',\n",
       "   'Vincent Wan',\n",
       "   'Roeland Ordelman',\n",
       "   'Steve Renals'],\n",
       "  'related_topics': ['Speech synthesis',\n",
       "   'Word error rate',\n",
       "   'Transcription (software)',\n",
       "   'NIST',\n",
       "   'Linear discriminant analysis',\n",
       "   'Speech recognition',\n",
       "   'Discriminative model',\n",
       "   'Vocal tract',\n",
       "   'Microphone',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '75',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2100969003',\n",
       "   '2150907703',\n",
       "   '1591607137',\n",
       "   '2125336414',\n",
       "   '2046317813',\n",
       "   '2093225945',\n",
       "   '1569447338',\n",
       "   '2037740282',\n",
       "   '1571931074',\n",
       "   '2094971681']},\n",
       " {'id': '2152808281',\n",
       "  'title': 'Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model',\n",
       "  'abstract': 'Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Y. Bengio', 'J.-S. Senecal'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Feedforward neural network',\n",
       "   'Language model',\n",
       "   'Artificial neural network',\n",
       "   'Importance sampling',\n",
       "   'Probabilistic logic',\n",
       "   'Context model',\n",
       "   'Conditional probability distribution',\n",
       "   'Markov chain',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Monte Carlo method',\n",
       "   'Maximum likelihood',\n",
       "   'View Less'],\n",
       "  'citation_count': '228',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2116064496',\n",
       "   '2132339004',\n",
       "   '1574901103',\n",
       "   '1985093013',\n",
       "   '2096175520',\n",
       "   '2069739265',\n",
       "   '1802356529',\n",
       "   '2134237567',\n",
       "   '1934041838',\n",
       "   '2140679639']},\n",
       " {'id': '2292896937',\n",
       "  'title': 'A guide to recurrent neural networks and backpropagation',\n",
       "  'abstract': 'This paper provides guidance to some of the concepts surrounding recurrent neural networks. Contrary to feedforward networks, recurrent networks can be sensitive, and be adapted to past inputs. Backpropagation learning is described for feedforward networks, adapted to suit our (probabilistic) modeling needs, and extended to cover recurrent networks. The aim of this brief paper is to set the scene for applying and understanding recurrent neural networks.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Mikael'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Feedforward neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Recurrent neural network',\n",
       "   'Catastrophic interference',\n",
       "   'Backpropagation',\n",
       "   'Probabilistic logic',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Feed forward',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '288',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2110485445',\n",
       "   '2107878631',\n",
       "   '2339378878',\n",
       "   '2016589492',\n",
       "   '1979684610',\n",
       "   '3036751298',\n",
       "   '2151834591',\n",
       "   '1959983357',\n",
       "   '2121553911',\n",
       "   '2468203291']},\n",
       " {'id': '2027499299',\n",
       "  'title': 'The AMI System for the Transcription of Speech in Meetings',\n",
       "  'abstract': 'This paper describes the AMI transcription system for speech in meetings developed in collaboration by five research groups. The system includes generic techniques such as discriminative and speaker adaptive training, vocal tract length normalisation, heteroscedastic linear discriminant analysis, maximum likelihood linear regression, and phone posterior based features, as well as techniques specifically designed for meeting data. These include segmentation and cross-talk suppression, beam-forming, domain adaptation, Web-data collection, and channel adaptive training. The system was improved by more than 20% relative in word error rate compared to our previous system and was used in the NIST RT106 evaluations where it was found to yield competitive performance.',\n",
       "  'date': '2007',\n",
       "  'authors': ['T. Hain',\n",
       "   'V. Wan',\n",
       "   'L. Burget',\n",
       "   'M. Karafiat',\n",
       "   'J. Dines',\n",
       "   'J. Vepa',\n",
       "   'G. Garau',\n",
       "   'M. Lincoln'],\n",
       "  'related_topics': ['Speech processing',\n",
       "   'Word error rate',\n",
       "   'Linear predictive coding',\n",
       "   'Acoustic model',\n",
       "   'Speech technology',\n",
       "   'Transcription (software)',\n",
       "   'Discriminative model',\n",
       "   'Vocal tract',\n",
       "   'Speech recognition',\n",
       "   'NIST',\n",
       "   'Phone',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '142',\n",
       "  'reference_count': '32',\n",
       "  'references': ['2100969003',\n",
       "   '2150907703',\n",
       "   '1591607137',\n",
       "   '2468573742',\n",
       "   '2125336414',\n",
       "   '2046317813',\n",
       "   '2093225945',\n",
       "   '1569447338',\n",
       "   '2037740282',\n",
       "   '1571931074']},\n",
       " {'id': '2437096199',\n",
       "  'title': 'Training Neural Network Language Models on Very Large Corpora',\n",
       "  'abstract': 'During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Holger Schwenk', 'Jean-Luc Gauvain'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Cache language model',\n",
       "   'Language model',\n",
       "   'Text corpus',\n",
       "   'Artificial neural network',\n",
       "   'Word (computer architecture)',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Contrast (statistics)',\n",
       "   'Space (commercial competition)',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '161',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2132339004',\n",
       "   '1631260214',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2134237567',\n",
       "   '2070534370',\n",
       "   '2140679639',\n",
       "   '2056590938']},\n",
       " {'id': '1984565341',\n",
       "  'title': 'The vocabulary problem in human-system communication',\n",
       "  'abstract': 'In almost all computer applications, users must enter correct words for the desired objects or actions. For success without extensive training, or in first-tries for new targets, the system must recognize terms that will be chosen spontaneously. We studied spontaneous word choice for objects in five application-related domains, and found the variability to be surprisingly large. In every case two people favored the same term with probability',\n",
       "  'date': '1987',\n",
       "  'authors': ['G. W. Furnas', 'T. K. Landauer', 'L. M. Gomez', 'S. T. Dumais'],\n",
       "  'related_topics': ['Vocabulary',\n",
       "   'Vocabulary mismatch',\n",
       "   'Term (time)',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Human system',\n",
       "   'Word choice',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,055',\n",
       "  'reference_count': '7',\n",
       "  'references': ['3090556797',\n",
       "   '50190571',\n",
       "   '1988337648',\n",
       "   '2104115112',\n",
       "   '2051279338',\n",
       "   '1997042879',\n",
       "   '2046760690']},\n",
       " {'id': '1964262399',\n",
       "  'title': 'Computer methods for mathematical computations',\n",
       "  'abstract': '',\n",
       "  'date': '1977',\n",
       "  'authors': ['George E.', 'Michael A.', 'Cleve B.'],\n",
       "  'related_topics': ['Computational resource',\n",
       "   'Computational number theory',\n",
       "   'Computational model',\n",
       "   'Computation',\n",
       "   'Computational science',\n",
       "   'Computer science',\n",
       "   'Programming method',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,256',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2000215628',\n",
       "  'title': \"Analysis of individual differences in multidimensional scaling via an n-way generalization of 'eckart-young' decomposition\",\n",
       "  'abstract': 'An individual differences model for multidimensional scaling is outlined in which individuals are assumed differentially to weight the several dimensions of a common “psychological space”. A corresponding method of analyzing similarities data is proposed, involving a generalization of “Eckart-Young analysis” to decomposition of three-way (or higher-way) tables. In the present case this decomposition is applied to a derived three-way table of scalar products between stimuli for individuals. This analysis yields a stimulus by dimensions coordinate matrix and a subjects by dimensions matrix of weights. This method is illustrated with data on auditory stimuli and on perception of nations.',\n",
       "  'date': '1970',\n",
       "  'authors': ['J. Douglas Carroll', 'Jih-Jie Chang'],\n",
       "  'related_topics': ['Multidimensional scaling',\n",
       "   'Tucker decomposition',\n",
       "   'Matricization',\n",
       "   'Statistical theory',\n",
       "   'Multilinear subspace learning',\n",
       "   'Tensor product network',\n",
       "   'Applied mathematics',\n",
       "   'Perception',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'Auditory stimuli',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,101',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2185761757',\n",
       "   '2056930330',\n",
       "   '1963826206',\n",
       "   '2024971339',\n",
       "   '2083795909',\n",
       "   '1997153468',\n",
       "   '1979317087',\n",
       "   '1977063224',\n",
       "   '2056640768',\n",
       "   '1572268863']},\n",
       " {'id': '2114804204',\n",
       "  'title': 'The cluster hypothesis revisited',\n",
       "  'abstract': 'A new means of evaluating the cluster hypothesis is introduced and the results of such an evaluation are presented for four collections. The results of retrieval experiments comparing a sequential search, a cluster-based search, and a search of the clustered collection in which individual documents are scored against the query are also presented. These results indicate that while the absolute performance of a search on a particular collection is dependent on the pairwise similarity of the relevant documents, the relative effectiveness of clustered retrieval versus sequential retrieval is independent of this factor. However, retrieval of entire clusters in response to a query usually results in a poorer performance than retrieval of individual documents from clusters.',\n",
       "  'date': '1985',\n",
       "  'authors': ['Ellen M. Vdorhees'],\n",
       "  'related_topics': ['Cluster hypothesis',\n",
       "   'Ranking (information retrieval)',\n",
       "   'Concept search',\n",
       "   'Linear search',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '265',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2068632118',\n",
       "   '120261275',\n",
       "   '2041565863',\n",
       "   '2065938637',\n",
       "   '2053256916',\n",
       "   '73128518',\n",
       "   '2045584315',\n",
       "   '2007154686',\n",
       "   '1980849928',\n",
       "   '1546473629']},\n",
       " {'id': '2151561903',\n",
       "  'title': 'Subject access in online catalogs: A design model',\n",
       "  'abstract': 'A model based on strikingly different philosophical as. sumptions from those currently popular is proposed for the design of online subject catalog access. Three design principles are presented and discussed: uncertainty (subject indexing is indeterminate and probabilistic beyond a certain point), variety (by Ashby’s law of requisite variety, variety of searcher query must equal variety of document indexing), and complexity (the search process, particularly during the entry and orientation phases, is subtler and more complex, on several grounds, than current models assume). Design features presented are an access phase, including entry and orientation, a hunting phase, and a selection phase. An end-user thesaurus and a front-end system mind are presented as examples of online catalog system components to improve searcher success during entry and orientation. The proposed model is “wrapped around” existing Library of Congress subject-heading indexing in such a way as to enhance access greatly without requiring reindexing. It is argued that both for cost reasons and in principle this is a superior approach to other design philosophies.',\n",
       "  'date': '1986',\n",
       "  'authors': ['Marcia J. Bates'],\n",
       "  'related_topics': ['Subject access',\n",
       "   'Subject indexing',\n",
       "   'Search engine indexing',\n",
       "   'Thesaurus (information retrieval)',\n",
       "   'User assistance',\n",
       "   'Relevance (information retrieval)',\n",
       "   'Process (engineering)',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '564',\n",
       "  'reference_count': '56',\n",
       "  'references': ['1969340322',\n",
       "   '1530533107',\n",
       "   '2993383518',\n",
       "   '2111030512',\n",
       "   '1964653871',\n",
       "   '1521517187',\n",
       "   '2315179971',\n",
       "   '2011386395',\n",
       "   '2006423836',\n",
       "   '2110416104']},\n",
       " {'id': '3012395598',\n",
       "  'title': 'Data preprocessing and the extended PARAFAC model',\n",
       "  'abstract': '',\n",
       "  'date': '1984',\n",
       "  'authors': ['R. A.'],\n",
       "  'related_topics': ['Data pre-processing',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '216',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1965061793',\n",
       "  'title': 'A critical analysis of vector space model for information retrieval',\n",
       "  'abstract': 'Notations and definitions necessary to identify the concepts and relationships that are important in modelling information retrieval objects and processes in the context of vector spaces are presented. Earlier work on the use of vector model is evaluated in terms of the concepts introduced and certain problems and inconsistencies are identified. More importantly, this investigation should lead to a clear understanding of the issues and problems in using the vector space model in information retrieval. © 1986 John Wiley & Sons, Inc.',\n",
       "  'date': '1986',\n",
       "  'authors': ['Vijay V. Raghavan', 'S. K. M. Wong'],\n",
       "  'related_topics': ['Vector space model',\n",
       "   'Relevance (information retrieval)',\n",
       "   'Document retrieval',\n",
       "   'Context (language use)',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Weighting',\n",
       "   'Mathematical model',\n",
       "   'View Less'],\n",
       "  'citation_count': '517',\n",
       "  'reference_count': '8',\n",
       "  'references': ['1956559956',\n",
       "   '1974406477',\n",
       "   '2024472735',\n",
       "   '2004913349',\n",
       "   '2049577316',\n",
       "   '2074876593',\n",
       "   '2052842312',\n",
       "   '1985414707']},\n",
       " {'id': '2024683548',\n",
       "  'title': 'Forsythe, G. E. / Malcolm, M. A. / Moler, C. B., Computer Methods for Mathematical Computations. Englewood Cliffs, New Jersey 07632. Prentice Hall, Inc., 1977. XI, 259 S',\n",
       "  'abstract': '',\n",
       "  'date': '1979',\n",
       "  'authors': ['F.'],\n",
       "  'related_topics': ['Mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Engineering physics',\n",
       "   'Programming method',\n",
       "   'View Less'],\n",
       "  'citation_count': '666',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2096411881',\n",
       "  'title': 'A THEORETICAL BASIS FOR THE USE OF CO‐OCCURRENCE DATA IN INFORMATION RETRIEVAL',\n",
       "  'abstract': 'This paper provides a foundation for a practical way of improving the effectiveness of an automatic retrieval system. Its main concern is with the weighting of index terms as a device for increasing retrieval effectiveness. Previously index terms have been assumed to be independent for the good reason that then a very simple weighting scheme can be used. In reality index terms are most unlikely to be independent. This paper explores one way of removing the independence assumption. Instead the extent of the dependence between index terms is measured and used to construct a non‐linear weighting function. In a practical situation the values of some of the parameters of such a function must be estimated from small samples of documents. So a number of estimation rules are discussed and one in particular is recommended. Finally the feasibility of the computations required for a non‐linear weighting scheme is examined.',\n",
       "  'date': '1977',\n",
       "  'authors': ['C.J. Van Rijsbergen'],\n",
       "  'related_topics': ['Weighting',\n",
       "   'Statistical assumption',\n",
       "   'Function (mathematics)',\n",
       "   'Basis (linear algebra)',\n",
       "   'Data mining',\n",
       "   'Index (economics)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Computation',\n",
       "   'Scheme (programming language)',\n",
       "   'View Less'],\n",
       "  'citation_count': '653',\n",
       "  'reference_count': '21',\n",
       "  'references': ['3017143921',\n",
       "   '2156273867',\n",
       "   '2043909051',\n",
       "   '2947000318',\n",
       "   '2123838014',\n",
       "   '2163166770',\n",
       "   '2098057602',\n",
       "   '1966365186',\n",
       "   '149585942',\n",
       "   '2107668593']},\n",
       " {'id': '2099247782',\n",
       "  'title': 'A stochastic parts program and noun phrase parser for unrestricted text',\n",
       "  'abstract': 'A program that tags each word in an input sentence with the most likely part of speech has been written. The program uses a linear-time dynamic programming algorithm to find an assignment of parts of speech to words that optimizes the product of (a) lexical probabilities (probability of observing part of speech i given word i) and (b) contextual probabilities (probability of observing part of speech i given n following parts of speech). Program performance is encouraging; a 400-word sample is presented and is judged to be 99.5% correct. >',\n",
       "  'date': '1989',\n",
       "  'authors': ['K.W. Church'],\n",
       "  'related_topics': ['Speech synthesis',\n",
       "   'Part of speech',\n",
       "   'Noun phrase',\n",
       "   'Sentence',\n",
       "   'Parsing',\n",
       "   'Natural language',\n",
       "   'Word (computer architecture)',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,360',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2134237567',\n",
       "   '2155818555',\n",
       "   '1571096757',\n",
       "   '2055438451',\n",
       "   '1526508180',\n",
       "   '2124479173']},\n",
       " {'id': '2439178139',\n",
       "  'title': 'INSIDE-OUTSIDE REESTIMATION FROM PARTIALLY BRACKETED CORPORA',\n",
       "  'abstract': 'The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modeling of hierarchical structure than the original one. In particular, over 90% test set bracketing accuracy was achieved for grammars inferred by our algorithm from a training set of handparsed part-of-speech strings for sentences in the Air Travel Information System spoken language corpus. Finally, the new algorithm has better time complexity than the original one when sufficient bracketing is provided.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Fernando Pereira', 'Yves Schabes'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Natural language',\n",
       "   'Rule-based machine translation',\n",
       "   'Data-oriented parsing',\n",
       "   'Spoken language',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '672',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2125838338',\n",
       "   '2087165009',\n",
       "   '2077302143',\n",
       "   '1978470410',\n",
       "   '2110190189',\n",
       "   '2012837062',\n",
       "   '1541301615',\n",
       "   '1982944197',\n",
       "   '2047706513',\n",
       "   '2047067573']},\n",
       " {'id': '2334801970',\n",
       "  'title': 'The computational analysis of English : a corpus-based approach',\n",
       "  'abstract': '',\n",
       "  'date': '1989',\n",
       "  'authors': ['Roger', 'Geoffrey N.', 'Geoffrey'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Computational analysis',\n",
       "   'Corpus based',\n",
       "   'View Less'],\n",
       "  'citation_count': '310',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '900993354',\n",
       "  'title': 'Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision)',\n",
       "  'abstract': 'This manual addresses the linguistic issues that arise in connection with annotating texts by part of speech (\"tagging\"). Section 2 is an alphabetical list of the parts of speech encoded in the annotation systems of the Penn Treebank Project, along with their corresponding abbreviations (\"tags\") and some information concerning their definition. This section allows you to find an unfamiliar tag by looking up a familiar part of speech. Section 3 recapitulates the information in Section 2, but this time the information is alphabetically ordered by tags. This is the section to consult in order to find out what an unfamiliar tag means. Since the parts of speech are probably familiar to you from high school English, you should have little difficulty in assimilating the tags themselves. However, it is often quite difficult to decide which tag is appropriate in a particular context. The two sections 4 and 5 therefore include examples and guidelines on how to tag problematic cases. If you are uncertain about whether a given tag is correct or not, refer to these sections in order to ensure a consistently annotated text. Section 4 discusses parts of speech that are easily confused and gives guidelines on how to tag such cases, while Section 5 contains an alphabetical list of specific problematic words and collocations. Finally, Section 6 discusses some general tagging conventions. One general rule, however, is so important that we state it here. Many texts are not models of good prose, and some contain outright errors and slips of the pen. Do not be tempted to correct a tag to what it would be if the text were correct; rather, it is the incorrect word that should be tagged correctly. Disciplines Computer Sciences Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-90-47. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/570 Part-of-S peech Tagging Guidelines For The Penn Treebank Project (3rd Revision) MS-CIS-90-47 LINC LAB 178',\n",
       "  'date': '1990',\n",
       "  'authors': ['Beatrice Santorini'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Section (typography)',\n",
       "   'Part of speech',\n",
       "   'Context (language use)',\n",
       "   'Information and Computer Science',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Annotation',\n",
       "   'Technical report',\n",
       "   'Computer science',\n",
       "   'Word (computer architecture)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '620',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2110190189',\n",
       "  'title': 'Parsing a natural language using mutual information statistics',\n",
       "  'abstract': 'The purpose of this paper is to characterize a constituent boundary parsing algorithm, using an information-theoretic measure called generalized mutual information, which serves as an alternative to traditional grammar-based parsing methods. This method is based on the hypothesis that constituent boundaries can be extracted from a given sentence (or word sequence) by analyzing the mutual information values of the part of speech n-grams within the sentence. This hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted English text with a relatively low error rate. This paper derives the generalized mutual information statistic, describes the parsing algorithm, and presents results and sample output from the parser.',\n",
       "  'date': '1990',\n",
       "  'authors': ['David M. Magerman', 'Mitchell P. Marcus'],\n",
       "  'related_topics': ['Top-down parsing',\n",
       "   'Bottom-up parsing',\n",
       "   'Parser combinator',\n",
       "   'S-attributed grammar',\n",
       "   'Parsing',\n",
       "   'Top-down parsing language',\n",
       "   'Pointwise mutual information',\n",
       "   'Parsing expression grammar',\n",
       "   'Word error rate',\n",
       "   'Sentence',\n",
       "   'Natural language',\n",
       "   'Part of speech',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Traditional grammar',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '222',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2099247782',\n",
       "   '1593045043',\n",
       "   '2134237567',\n",
       "   '1483126227',\n",
       "   '2017580301',\n",
       "   '2034274945',\n",
       "   '2126477387']},\n",
       " {'id': '2012837062',\n",
       "  'title': 'Deducing linguistic structure from the statistics of large corpora',\n",
       "  'abstract': 'Within the last two years, approaches using both stochastic and symbolic techniques have proved adequate to deduce lexical ambiguity resolution rules with less than 3-4% error rate, when trained on moderate sized (500K word) corpora of English text (e.g. Church, 1988; Hindle, 1989). The success of these techniques suggests that much of the grammatical structure of language may be derived automatically through distributional analysis, an approach attempted and abandoned in the 1950s.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Eric', 'David', 'Mitchell', 'Beatrice'],\n",
       "  'related_topics': ['Word error rate',\n",
       "   'Natural language processing',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Word (computer architecture)',\n",
       "   'Resolution (logic)',\n",
       "   'Speech recognition',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Grammatical structure',\n",
       "   'Lexical ambiguity',\n",
       "   'View Less'],\n",
       "  'citation_count': '193',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2099247782',\n",
       "   '1593045043',\n",
       "   '2134237567',\n",
       "   '1483126227',\n",
       "   '2110190189',\n",
       "   '3044664353',\n",
       "   '2034274945',\n",
       "   '2126477387',\n",
       "   '1528321674',\n",
       "   '2065585771']},\n",
       " {'id': '2121407024',\n",
       "  'title': 'ACQUIRING DISAMBIGUATION RULES FROM TEXT',\n",
       "  'abstract': 'An effective procedure for automatically acquiring a new set of disambiguation rules for an existing deterministic parser on the basis of tagged text is presented. Performance of the automatically acquired rules is much better than the existing hand-written disambiguation rules. The success of the acquired rules depends on using the linguistic information encoded in the parser; enhancements to various components of the parser improves the acquired rule set. This work suggests a path toward more robust and comprehensive syntactic analyzers.',\n",
       "  'date': '1989',\n",
       "  'authors': ['Donald Hindle'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Rule-based machine translation',\n",
       "   'Syntax',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Path (graph theory)',\n",
       "   'Basis (linear algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '145',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2099247782',\n",
       "   '2017580301',\n",
       "   '1571096757',\n",
       "   '1981724541',\n",
       "   '1487155516',\n",
       "   '2158652440',\n",
       "   '2124102576',\n",
       "   '1590656471',\n",
       "   '2015773474']},\n",
       " {'id': '2076526090',\n",
       "  'title': 'Studies in part of speech labelling',\n",
       "  'abstract': 'We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words. Part of speech tagging, per se, is a well understood problem. Our paper reports experiments in three important areas: handling unknown words, limiting the size of the training set, and returning a set of the most likely tags for each word rather than a single tag. We describe the algorithms that we used and the specific results of our experiments on Wall Street Journal articles and on MUC terrorist messages.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Marie', 'Richard', 'Ralph'],\n",
       "  'related_topics': ['Part of speech',\n",
       "   'Ambiguity',\n",
       "   'Set (psychology)',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Word (computer architecture)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Limiting',\n",
       "   'Part-of-speech tagging',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '23',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2099247782',\n",
       "   '2127836646',\n",
       "   '2122214306',\n",
       "   '2006002820',\n",
       "   '2064910499',\n",
       "   '2112323378',\n",
       "   '2083607646']},\n",
       " {'id': '2166675302',\n",
       "  'title': 'Discovering the Lexical Features of a Language',\n",
       "  'abstract': \"This paper examines the possibility of automatically discovering the lexieal features of a language. There is strong evidence that the set of possible lexical features which can be used in a language is unbounded, and thus not innate. Lakoff [Lakoff 87] describes a language in which the feature -I-woman-or-fire-ordangerons-thing exists. This feature is based upon ancient folklore of the society in which it is used. If the set of possible lexieal features is indeed unbounded, then it cannot be par t of the innate Universal Grammar and must be learned. Even if the set is not unbounded, the child is still left with the challenging task of determining which features are used in her language. If a child does not know a priori what lexical features are used in her language, there are two sources for acquiring this information: semantic and syntactic cues. A learner using semantic cues could recognize that words often refer to objects, actions, and properties, and from this deduce the lexical features: noun, verb and adjective. Pinker [Pinker 89] proposes that a combination of semantic cues and innate semantic primitives could account for the acquisition of verb features. He believes that the child can discover semantic properties of a verb by noticing the types of actions typically taking place when the verb is uttered. Once these properties are known, says Pinker, they can be used to reliably predict the distributional behavior of the verb. However, Gleitman [Gleitman 90] presents evidence that semantic cues axe not sufficient for a child to acquire verb features and believes that the use of this semantic information in conjunction with information about the subcategorization properties of the verb may be sufficient for learning verb features. This paper takes Glei tman's suggestion to the extreme, in hope of determining whether syntactic cues may not just aid in feature discovery, but may be all tha t is necessary. We present evidence for the sufficiency of a strictly syntax-based model for discovering\",\n",
       "  'date': '1991',\n",
       "  'authors': ['Eric Brill'],\n",
       "  'related_topics': ['Verb',\n",
       "   'Lexical item',\n",
       "   'Syntax',\n",
       "   'Semantic property',\n",
       "   'Noun',\n",
       "   'Feature (linguistics)',\n",
       "   'Lexical choice',\n",
       "   'Subcategorization',\n",
       "   'Lexical functional grammar',\n",
       "   'Adjective',\n",
       "   'Universal grammar',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '20',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2055460448',\n",
       "   '1483126227',\n",
       "   '2052262800',\n",
       "   '1541301615',\n",
       "   '3044664353']},\n",
       " {'id': '2148603752',\n",
       "  'title': 'Statistical learning theory',\n",
       "  'abstract': 'A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Vladimir Naumovich'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Statistical learning theory',\n",
       "   'Stability (learning theory)',\n",
       "   'Computational learning theory',\n",
       "   'Empirical risk minimization',\n",
       "   'Statistical theory',\n",
       "   'Generalization',\n",
       "   'VC dimension',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '67,327',\n",
       "  'reference_count': '1',\n",
       "  'references': ['2156909104']},\n",
       " {'id': '1480376833',\n",
       "  'title': 'The Elements of Statistical Learning',\n",
       "  'abstract': '',\n",
       "  'date': '2001',\n",
       "  'authors': ['Trevor', 'Robert', 'Jerome H.'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Semi-supervised learning',\n",
       "   'Ensemble learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Learning classifier system',\n",
       "   'Competitive learning',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,816',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2154455818',\n",
       "  'title': 'Learning with Local and Global Consistency',\n",
       "  'abstract': 'We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Dengyong Zhou',\n",
       "   'Olivier Bousquet',\n",
       "   'Thomas N. Lal',\n",
       "   'Jason Weston',\n",
       "   'Bernhard Schölkopf'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Function (mathematics)',\n",
       "   'Structure (category theory)',\n",
       "   'SIMPLE algorithm',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'General problem',\n",
       "   'Global consistency',\n",
       "   'Manifold ranking',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,402',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2148603752',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2165874743',\n",
       "   '1578099820',\n",
       "   '2139823104',\n",
       "   '1708874574',\n",
       "   '2159737176',\n",
       "   '1966949944',\n",
       "   '2122837498']},\n",
       " {'id': '2048679005',\n",
       "  'title': 'Combining labeled and unlabeled data with co-training',\n",
       "  'abstract': 'We consider the problem of using a large unlabeled sample to boost performance of a learning algorit,hrn when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks t,hat point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment, a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm’s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. *This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young investigator grant CCR-9357793. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. TO copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. COLT 98 Madison WI USA Copyright ACM 1998 l-58113-057--0/98/ 7...%5.00 92 Tom Mitchell School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3891 mitchell+@cs.cmu.edu',\n",
       "  'date': '1998',\n",
       "  'authors': ['Avrim Blum', 'Tom Mitchell'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Co-training',\n",
       "   'Web page',\n",
       "   'Hyperlink',\n",
       "   'Set (abstract data type)',\n",
       "   'Permission',\n",
       "   'Server',\n",
       "   'Task (project management)',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,427',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2049633694',\n",
       "   '3017143921',\n",
       "   '2101210369',\n",
       "   '2167044614',\n",
       "   '2128221272',\n",
       "   '1995897489',\n",
       "   '2103555337',\n",
       "   '2150516767',\n",
       "   '2020764470',\n",
       "   '1550302919']},\n",
       " {'id': '2097089247',\n",
       "  'title': 'Text Classification from Labeled and Unlabeled Documents using EM',\n",
       "  'abstract': 'This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.\\r\\n\\r\\nWe introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Kamal Nigam',\n",
       "   'Andrew Kachites McCallum',\n",
       "   'Sebastian Thrun',\n",
       "   'Tom Mitchell'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Co-training',\n",
       "   'Naive Bayes classifier',\n",
       "   'Unsupervised learning',\n",
       "   'Classifier (linguistics)',\n",
       "   'Bayesian inference',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Class (biology)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,923',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2099111195',\n",
       "   '2149684865',\n",
       "   '2049633694',\n",
       "   '2435251607',\n",
       "   '2048679005',\n",
       "   '2117853077',\n",
       "   '2114535528',\n",
       "   '1550206324',\n",
       "   '2138745909',\n",
       "   '2140785063']},\n",
       " {'id': '2010353172',\n",
       "  'title': 'Weak Convergence and Empirical Processes',\n",
       "  'abstract': '',\n",
       "  'date': '1996',\n",
       "  'authors': ['Thomas', 'Aad W. Van der', 'Jon A.'],\n",
       "  'related_topics': ['Weak convergence',\n",
       "   'Mathematics',\n",
       "   'Applied mathematics',\n",
       "   'Bernstein–von Mises theorem',\n",
       "   'Additive hazards',\n",
       "   'Oracle inequality',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,517',\n",
       "  'reference_count': '1',\n",
       "  'references': ['2802739963']},\n",
       " {'id': '2101210369',\n",
       "  'title': 'UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS',\n",
       "  'abstract': 'This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.',\n",
       "  'date': '1995',\n",
       "  'authors': ['David Yarowsky'],\n",
       "  'related_topics': ['SemEval',\n",
       "   'Unsupervised learning',\n",
       "   'Bootstrapping (linguistics)',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word sense',\n",
       "   'Word-sense disambiguation',\n",
       "   'Word-sense induction',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,982',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2049633694',\n",
       "   '2102381086',\n",
       "   '2099247782',\n",
       "   '2040004971',\n",
       "   '1971220772',\n",
       "   '1977182536',\n",
       "   '2129139611',\n",
       "   '2428981601',\n",
       "   '1554031433',\n",
       "   '2156202195']},\n",
       " {'id': '1535015163',\n",
       "  'title': 'A maximum-entropy-inspired parser',\n",
       "  'abstract': 'We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] \"standard\" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head\\'s pre-terminal before guessing the lexical head.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Eugene Charniak'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Statistical parsing',\n",
       "   'Treebank',\n",
       "   'Data-oriented parsing',\n",
       "   'Principle of maximum entropy',\n",
       "   'Word error rate',\n",
       "   'Head (linguistics)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,405',\n",
       "  'reference_count': '16',\n",
       "  'references': ['1632114991',\n",
       "   '2096175520',\n",
       "   '2092654472',\n",
       "   '1986543644',\n",
       "   '1567570606',\n",
       "   '2153439141',\n",
       "   '1551104980',\n",
       "   '2161204834',\n",
       "   '1953828586',\n",
       "   '2166394306']},\n",
       " {'id': '2092654472',\n",
       "  'title': 'Head-Driven Statistical Models for Natural Language Parsing',\n",
       "  'abstract': 'This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Michael Collins'],\n",
       "  'related_topics': ['Data-oriented parsing',\n",
       "   'Statistical parsing',\n",
       "   'S-attributed grammar',\n",
       "   'Bottom-up parsing',\n",
       "   'Top-down parsing',\n",
       "   'Parsing',\n",
       "   'Parser combinator',\n",
       "   'Treebank',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,528',\n",
       "  'reference_count': '61',\n",
       "  'references': ['2147880316',\n",
       "   '1574901103',\n",
       "   '1632114991',\n",
       "   '1535015163',\n",
       "   '2107890099',\n",
       "   '1773803948',\n",
       "   '2002089154',\n",
       "   '3021452258',\n",
       "   '2110882317',\n",
       "   '1986543644']},\n",
       " {'id': '2115792525',\n",
       "  'title': 'The Berkeley FrameNet Project',\n",
       "  'abstract': 'FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \"Tools for Lexicon Building\"). The project\\'s key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \"frame elements\" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project\\'s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Collin F. Baker', 'Charles J. Fillmore', 'John B. Lowe'],\n",
       "  'related_topics': ['FrameNet',\n",
       "   'Semantic role labeling',\n",
       "   'PropBank',\n",
       "   'Lexicon',\n",
       "   'VerbNet',\n",
       "   'Phrase',\n",
       "   'Frame semantics',\n",
       "   'Noun',\n",
       "   'Syntax',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Lexicography',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,737',\n",
       "  'reference_count': '2',\n",
       "  'references': ['3088277579', '2154890447']},\n",
       " {'id': '2151170651',\n",
       "  'title': 'Automatic labeling of semantic roles',\n",
       "  'abstract': 'We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC.The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.Our system achieves 82% accuracy in identifying the semantic role of presegmented constituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Daniel Gildea', 'Daniel Jurafsky'],\n",
       "  'related_topics': ['Semantic role labeling',\n",
       "   'FrameNet',\n",
       "   'Semantic similarity',\n",
       "   'Semantic property',\n",
       "   'Semantic equivalence',\n",
       "   'Syntactic predicate',\n",
       "   'Phrase',\n",
       "   'Sentence',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,457',\n",
       "  'reference_count': '41',\n",
       "  'references': ['2038721957',\n",
       "   '1632114991',\n",
       "   '2092654472',\n",
       "   '2115792525',\n",
       "   '2160842254',\n",
       "   '2038248725',\n",
       "   '1986543644',\n",
       "   '2039217078',\n",
       "   '1567570606',\n",
       "   '2127314673']},\n",
       " {'id': '3021452258',\n",
       "  'title': 'Discriminative Reranking for Natural Language Parsing',\n",
       "  'abstract': '',\n",
       "  'date': '2000',\n",
       "  'authors': ['Michael'],\n",
       "  'related_topics': ['Statistical parsing',\n",
       "   'Data-oriented parsing',\n",
       "   'Discriminative model',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Natural language parsing',\n",
       "   'View Less'],\n",
       "  'citation_count': '718',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2039217078',\n",
       "  'title': 'English Verb Classes and Alternations: A Preliminary Investigation',\n",
       "  'abstract': 'In this rich reference work, Beth Levin classifies over 3,000 English verbs according to shared meaning and behavior. Levin starts with the hypothesis that a verb\\'s meaning influences its syntactic behavior and develops it into a powerful tool for studying the English verb lexicon. She shows how identifying verbs with similar syntactic behavior provides an effective means of distinguishing semantically coherent verb classes, and isolates these classes by examining verb behavior with respect to a wide range of syntactic alternations that reflect verb meaning. The first part of the book sets out alternate ways in which verbs can express their arguments. The second presents classes of verbs that share a kernel of meaning and explores in detail the behavior of each class, drawing on the alternations in the first part. Levin\\'s discussion of each class and alternation includes lists of relevant verbs, illustrative examples, comments on noteworthy properties, and bibliographic references. The result is an original, systematic picture of the organization of the verb inventory. Easy to use, \"English Verb Classes and Alternations\" sets the stage for further explorations of the interface between lexical semantics and syntax. It will prove indispensable for theoretical and computational linguists, psycholinguists, cognitive scientists, lexicographers, and teachers of English as a second language. Beth Levin is associate professor of linguistics at Northwestern University.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Beth'],\n",
       "  'related_topics': ['Modal verb',\n",
       "   'English verbs',\n",
       "   'Verb',\n",
       "   'Reflexive verb',\n",
       "   'Verb phrase ellipsis',\n",
       "   'Causative alternation',\n",
       "   'VerbNet',\n",
       "   'Syntax',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,746',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1567277581',\n",
       "  'title': 'The syntactic process',\n",
       "  'abstract': \"In this book Mark Steedman argues that the surface syntax of natural languages maps spoken and written forms directly to a compositional semantic representation that includes predicate-argument structure, quantification and information structure without constructing any intervening structural representation. His purpose is to construct a principle theory of natural grammar that is directly compatible with both expalantory linguistic accounts of a number of problematic syntactic phenomena and a straightforward computational account of the way sentences are mapped onto represenations of meaning. The radical nature of Steedman's proposal stems from his claim that much of the apparent complexity of syntax, prosody and processing follows from the lexical specification of the grammar and from the involvement of a small number of universal rule-types for combining predicates and arguments. These syntactic operations are related to the combinators of Combinatory Logic, engendering a much freer definition of derivational constituency than is traditionally assumed. This property allows Combinatory Categorical Grammar to capture elegantly the structure and interpretation of coordination and intonation contour in English as well as some well-known interactions between word order, coordination and relativization across a number of other languages. It also allows more direct compatibility with incremental semantic interpretation during parsing. The book covers topics in formal linguistics, intonational phonology, computational linguistics and experimental psycholinguistics, presenting them as an integrated theory of the language faculty in a form accessible to readers from any of those fields.\",\n",
       "  'date': '2000',\n",
       "  'authors': ['Mark Steedman'],\n",
       "  'related_topics': ['Combinatory categorial grammar',\n",
       "   'Relational grammar',\n",
       "   'Syntax',\n",
       "   'Categorial grammar',\n",
       "   'Parsing',\n",
       "   'Grammar',\n",
       "   'Natural language',\n",
       "   'Computational linguistics',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,379',\n",
       "  'reference_count': '5',\n",
       "  'references': ['1519554123',\n",
       "   '2038141781',\n",
       "   '2075322664',\n",
       "   '2798490496',\n",
       "   '2111907245']},\n",
       " {'id': '2126851059',\n",
       "  'title': 'Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling',\n",
       "  'abstract': 'In this paper we describe the CoNLL-2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Xavier Carreras', \"Llu'is M`arquez\"],\n",
       "  'related_topics': ['Semantic computing',\n",
       "   'Semantic role labeling',\n",
       "   'Task (project management)',\n",
       "   'PropBank',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Evaluation methods',\n",
       "   'View Less'],\n",
       "  'citation_count': '953',\n",
       "  'reference_count': '37',\n",
       "  'references': ['1632114991',\n",
       "   '2158847908',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '2144578941',\n",
       "   '2151170651',\n",
       "   '2145310422',\n",
       "   '2138043057',\n",
       "   '204260652',\n",
       "   '2116786260']},\n",
       " {'id': '2154626406',\n",
       "  'title': 'The Necessity of Parsing for Predicate Argument Recognition',\n",
       "  'abstract': 'Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the effect of parser accuracy on these systems\\' performance, and examine the question of whether a flatter \"chunked\" representation of the input can be as effective for the purposes of semantic role identification.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Daniel Gildea', 'Martha Palmer'],\n",
       "  'related_topics': ['Semantic role labeling',\n",
       "   'Parsing',\n",
       "   'PropBank',\n",
       "   'Predicate (grammar)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '317',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2147880316',\n",
       "   '1934019294',\n",
       "   '2115792525',\n",
       "   '2151170651',\n",
       "   '1986543644',\n",
       "   '1567570606',\n",
       "   '2163918411',\n",
       "   '12836875',\n",
       "   '2098921539',\n",
       "   '1667964228']},\n",
       " {'id': '2166776180',\n",
       "  'title': 'Automatic Retrieval and Clustering of Similar Words',\n",
       "  'abstract': 'Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Dekang Lin'],\n",
       "  'related_topics': ['Thesaurus (information retrieval)',\n",
       "   'WordNet',\n",
       "   'Similarity measure',\n",
       "   'Bootstrapping (linguistics)',\n",
       "   'Natural language',\n",
       "   'Semantics',\n",
       "   'Cluster analysis',\n",
       "   'Natural language processing',\n",
       "   'Parsing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,437',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2102381086',\n",
       "   '1997841190',\n",
       "   '2127314673',\n",
       "   '2163953154',\n",
       "   '1940278502',\n",
       "   '2123084125',\n",
       "   '2123489126',\n",
       "   '2016001305',\n",
       "   '2109779030',\n",
       "   '2102997946']},\n",
       " {'id': '1520377376',\n",
       "  'title': 'An Algorithm that Learns What‘s in a Name',\n",
       "  'abstract': 'In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder‘s performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Daniel M. Bikel', 'Richard Schwartz', 'Ralph M. Weischedel'],\n",
       "  'related_topics': ['Hidden Markov model',\n",
       "   'Class (computer programming)',\n",
       "   'Machine learning',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Controlled experiment',\n",
       "   'Speech input',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,101',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2125838338',\n",
       "   '2117400858',\n",
       "   '2099247782',\n",
       "   '1718065290',\n",
       "   '1991133427',\n",
       "   '1971763646',\n",
       "   '1568620938',\n",
       "   '2080856991',\n",
       "   '2033209333',\n",
       "   '1568095077']},\n",
       " {'id': '1988995507',\n",
       "  'title': 'Chunking with support vector machines',\n",
       "  'abstract': 'We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Taku Kudo', 'Yuji Matsumoto'],\n",
       "  'related_topics': ['Relevance vector machine',\n",
       "   'Support vector machine',\n",
       "   'Kernel (linear algebra)',\n",
       "   'Curse of dimensionality',\n",
       "   'Chunking (division)',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '729',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2148603752',\n",
       "   '2119821739',\n",
       "   '2149684865',\n",
       "   '2112076978',\n",
       "   '2107008379',\n",
       "   '2101276256',\n",
       "   '1676820704',\n",
       "   '2117400858',\n",
       "   '2153104898',\n",
       "   '1553313034']},\n",
       " {'id': '2145310422',\n",
       "  'title': 'Using Predicate-Argument Structures for Information Extraction',\n",
       "  'abstract': 'In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Mihai Surdeanu',\n",
       "   'Sanda Harabagiu',\n",
       "   'John Williams',\n",
       "   'Paul Aarseth'],\n",
       "  'related_topics': ['Predicate (grammar)',\n",
       "   'Decision tree learning',\n",
       "   'Information extraction',\n",
       "   'PropBank',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '511',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2115792525',\n",
       "   '2151170651',\n",
       "   '2117400858',\n",
       "   '1986543644',\n",
       "   '2039217078',\n",
       "   '2124634352',\n",
       "   '2154626406',\n",
       "   '2085606725',\n",
       "   '1667964228',\n",
       "   '1971563386']},\n",
       " {'id': '2155693943',\n",
       "  'title': 'Immediate-Head Parsing for Language Models',\n",
       "  'abstract': 'We present two language models based upon an \"immediate-head\" parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model\\'s perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.',\n",
       "  'date': '2001',\n",
       "  'authors': ['Eugene Charniak'],\n",
       "  'related_topics': ['Perplexity',\n",
       "   'LR parser',\n",
       "   'Simple LR parser',\n",
       "   'Parser combinator',\n",
       "   'GLR parser',\n",
       "   'Top-down parsing',\n",
       "   'Top-down parsing language',\n",
       "   'Parsing',\n",
       "   'LL grammar',\n",
       "   'Language model',\n",
       "   'Trigram',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '456',\n",
       "  'reference_count': '18',\n",
       "  'references': ['1632114991',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '3021452258',\n",
       "   '1986543644',\n",
       "   '2153439141',\n",
       "   '2108321481',\n",
       "   '2949237929',\n",
       "   '2161204834',\n",
       "   '2123893795']},\n",
       " {'id': '2093647425',\n",
       "  'title': 'The Penn Treebank: annotating predicate argument structure',\n",
       "  'abstract': 'The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Mitchell Marcus',\n",
       "   'Grace Kim',\n",
       "   'Mary Ann Marcinkiewicz',\n",
       "   'Robert MacIntyre',\n",
       "   'Ann Bies',\n",
       "   'Mark Ferguson',\n",
       "   'Karen Katz',\n",
       "   'Britta Schasberger'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Semantic role labeling',\n",
       "   'Predicate (grammar)',\n",
       "   'Annotation',\n",
       "   'Tag system',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Syntactic annotation',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,042',\n",
       "  'reference_count': '11',\n",
       "  'references': ['1632114991',\n",
       "   '2796493717',\n",
       "   '1483126227',\n",
       "   '2062837929',\n",
       "   '1859173823',\n",
       "   '2334801970',\n",
       "   '2087165009',\n",
       "   '2102924265',\n",
       "   '2167434254',\n",
       "   '2021758792']},\n",
       " {'id': '2040909025',\n",
       "  'title': 'Use of support vector learning for chunk identification',\n",
       "  'abstract': 'In this paper, we explore the use of Support Vector Machines (SVMs) for CoNLL-2000 shared task, chunk identification. SVMs are so-called large margin classifiers and are well-known as their good generalization performance. We investigate how SVMs with a very large number of features perform with the classification task of chunk labelling.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Taku Kudoh', 'Yuji Matsumoto'],\n",
       "  'related_topics': ['Support vector machine',\n",
       "   'Margin (machine learning)',\n",
       "   'Identification (information)',\n",
       "   'Machine learning',\n",
       "   'Task (project management)',\n",
       "   'Pattern recognition',\n",
       "   'Generalization',\n",
       "   'Computer science',\n",
       "   'Large numbers',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '419',\n",
       "  'reference_count': '6',\n",
       "  'references': ['2156909104',\n",
       "   '2119821739',\n",
       "   '2149684865',\n",
       "   '1574862351',\n",
       "   '2153104898',\n",
       "   '1490175418']},\n",
       " {'id': '2150203234',\n",
       "  'title': 'Semantic role parsing: adding semantic structure to unstructured text',\n",
       "  'abstract': 'There is an ever-growing need to add structure in the form of semantic markup to the huge amounts of unstructured text data now available. We present the technique of shallow semantic parsing, the process of assigning a simple WHO did WHAT to WHOM, etc., structure to sentences in text, as a useful tool in achieving this goal. We formulate the semantic parsing problem as a classification problem using support vector machines. Using a hand-labeled training set and a set of features drawn from earlier work together with some feature enhancements, we demonstrate a system that performs better than all other published results on shallow semantic parsing.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Sameer Pradhan',\n",
       "   'K. Hacioglu',\n",
       "   'W. Ward',\n",
       "   'J.H. Martin',\n",
       "   'D. Jurafsky'],\n",
       "  'related_topics': ['Semantic computing',\n",
       "   'S-attributed grammar',\n",
       "   'Bottom-up parsing',\n",
       "   'Parsing',\n",
       "   'Semantic HTML',\n",
       "   'Rule-based machine translation',\n",
       "   'Feature (linguistics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Computational linguistics',\n",
       "   'Computer science',\n",
       "   'Text mining',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '121',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2156909104',\n",
       "   '2138745909',\n",
       "   '2115792525',\n",
       "   '2151170651',\n",
       "   '2166776180',\n",
       "   '2885050925',\n",
       "   '1520377376',\n",
       "   '2154626406',\n",
       "   '12836875',\n",
       "   '2145310422']},\n",
       " {'id': '2125838338',\n",
       "  'title': 'A tutorial on hidden Markov models and selected applications in speech recognition',\n",
       "  'abstract': 'This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described. >',\n",
       "  'date': '1989',\n",
       "  'authors': ['L.R. Rabiner'],\n",
       "  'related_topics': ['Layered hidden Markov model',\n",
       "   'Hidden semi-Markov model',\n",
       "   'Hierarchical hidden Markov model',\n",
       "   'Markov model',\n",
       "   'Hidden Markov model',\n",
       "   'Markov chain',\n",
       "   'Forward algorithm',\n",
       "   'Maximum-entropy Markov model',\n",
       "   'Speech recognition',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '32,632',\n",
       "  'reference_count': '62',\n",
       "  'references': ['2049633694',\n",
       "   '2105594594',\n",
       "   '1966812932',\n",
       "   '2142384583',\n",
       "   '2022554507',\n",
       "   '2002182716',\n",
       "   '1966264494',\n",
       "   '2171850596',\n",
       "   '1991133427',\n",
       "   '1877570817']},\n",
       " {'id': '2008652694',\n",
       "  'title': 'Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms',\n",
       "  'abstract': 'We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Michael Collins'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Hidden Markov model',\n",
       "   'Viterbi algorithm',\n",
       "   'Structured prediction',\n",
       "   'Iterative Viterbi decoding',\n",
       "   'Perceptron',\n",
       "   'Conditional random field',\n",
       "   'Discriminative model',\n",
       "   'Chunking (psychology)',\n",
       "   'Viterbi decoder',\n",
       "   'Noun phrase',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,502',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2147880316',\n",
       "   '1632114991',\n",
       "   '1934019294',\n",
       "   '1773803948',\n",
       "   '2117400858',\n",
       "   '2127713198',\n",
       "   '1979711143',\n",
       "   '2131297983',\n",
       "   '1623072288',\n",
       "   '2144087279']},\n",
       " {'id': '1934019294',\n",
       "  'title': 'Maximum Entropy Markov Models for Information Extraction and Segmentation',\n",
       "  'abstract': 'Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Andrew', 'Dayne Freitag', 'Fernando C. N. Pereira'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Variable-order Markov model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Markov chain',\n",
       "   'Hidden Markov model',\n",
       "   'Markov model',\n",
       "   'Markov property',\n",
       "   'Markov process',\n",
       "   'Markov kernel',\n",
       "   'Conditional probability',\n",
       "   'Probabilistic logic',\n",
       "   'Viterbi algorithm',\n",
       "   'Multinomial distribution',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,915',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2125838338',\n",
       "   '2049633694',\n",
       "   '2160842254',\n",
       "   '2117400858',\n",
       "   '1520377376',\n",
       "   '1528056001',\n",
       "   '1557074680',\n",
       "   '2158873310',\n",
       "   '1597379537',\n",
       "   '1592796124']},\n",
       " {'id': '1567570606',\n",
       "  'title': 'Statistical parsing with a context-free grammar and word statistics',\n",
       "  'abstract': 'We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Eugene Charniak'],\n",
       "  'related_topics': ['Statistical parsing',\n",
       "   'Top-down parsing',\n",
       "   'Bottom-up parsing',\n",
       "   'Parsing',\n",
       "   'Parser combinator',\n",
       "   'S-attributed grammar',\n",
       "   'Top-down parsing language',\n",
       "   'Data-oriented parsing',\n",
       "   'Language model',\n",
       "   'Context-free grammar',\n",
       "   'Sentence',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '760',\n",
       "  'reference_count': '7',\n",
       "  'references': ['1632114991',\n",
       "   '2110882317',\n",
       "   '2127314673',\n",
       "   '2153439141',\n",
       "   '2161204834',\n",
       "   '2787704407',\n",
       "   '170869742']},\n",
       " {'id': '2125712079',\n",
       "  'title': 'Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking',\n",
       "  'abstract': 'Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Eugene Charniak', 'Mark Johnson'],\n",
       "  'related_topics': ['Statistical parsing',\n",
       "   'Parsing',\n",
       "   'Discriminative model',\n",
       "   'Sentence',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Generative grammar',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Coarse to fine',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,348',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2097606805',\n",
       "   '1535015163',\n",
       "   '3021452258',\n",
       "   '1986543644',\n",
       "   '2102667697',\n",
       "   '2098379588',\n",
       "   '2123893795',\n",
       "   '1985754308',\n",
       "   '2037894654',\n",
       "   '2143458716']},\n",
       " {'id': '2729906263',\n",
       "  'title': 'Linguistic Data Consortium',\n",
       "  'abstract': '',\n",
       "  'date': '1999',\n",
       "  'authors': ['Treebank'],\n",
       "  'related_topics': ['Linguistic Data Consortium',\n",
       "   'Computer science',\n",
       "   'Linguistics'],\n",
       "  'citation_count': '1,024',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2098379588',\n",
       "  'title': 'Estimators for Stochastic \"Unification-Based\" Grammars',\n",
       "  'abstract': 'Log-linear models provide a statistically sound framework for Stochastic \"Unification-Based\" Grammars (SUBGs) and stochastic versions of other kinds of grammars. We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical-Functional Grammar.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Mark Johnson',\n",
       "   'Stuart Geman',\n",
       "   'Stephen Canon',\n",
       "   'Zhiyi Chi',\n",
       "   'Stefan Riezler'],\n",
       "  'related_topics': ['Tree-adjoining grammar',\n",
       "   'Phrase structure grammar',\n",
       "   'Context-sensitive grammar',\n",
       "   'L-attributed grammar',\n",
       "   'Indexed grammar',\n",
       "   'Stochastic context-free grammar',\n",
       "   'Context-free grammar',\n",
       "   'Deterministic context-free grammar',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Ambiguous grammar',\n",
       "   'Stochastic grammar',\n",
       "   'Parsing expression grammar',\n",
       "   'Extended Affix Grammar',\n",
       "   'Adaptive grammar',\n",
       "   'Mildly context-sensitive grammar formalism',\n",
       "   'Link grammar',\n",
       "   'Syntax',\n",
       "   'Rule-based machine translation',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '260',\n",
       "  'reference_count': '11',\n",
       "  'references': ['2159080219',\n",
       "   '2096175520',\n",
       "   '1508165687',\n",
       "   '2150218618',\n",
       "   '2114220616',\n",
       "   '1734853756',\n",
       "   '28766783',\n",
       "   '1976711294',\n",
       "   '160680375',\n",
       "   '1533001652']},\n",
       " {'id': '2037894654',\n",
       "  'title': 'Better k-best Parsing',\n",
       "  'abstract': \"We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel's implementation of Collins' lexicalized PCFG model, and on Chiang's CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['Liang Huang', 'David Chiang'],\n",
       "  'related_topics': ['Bottom-up parsing',\n",
       "   'Top-down parsing',\n",
       "   'S-attributed grammar',\n",
       "   'Parsing',\n",
       "   'Scalability',\n",
       "   'Phrase',\n",
       "   'Relevance (information retrieval)',\n",
       "   'Natural language processing',\n",
       "   'Hypergraph',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '402',\n",
       "  'reference_count': '42',\n",
       "  'references': ['3145128584',\n",
       "   '2159080219',\n",
       "   '1632114991',\n",
       "   '301824129',\n",
       "   '2146574666',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '2152263452',\n",
       "   '2151170651',\n",
       "   '2116410915']},\n",
       " {'id': '1904457459',\n",
       "  'title': 'A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)',\n",
       "  'abstract': 'Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems. The system implements a \"voting\" or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system (e.g. acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal-cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or \"voting\" process that selects the output sequence with the lowest score.',\n",
       "  'date': '1997',\n",
       "  'authors': ['J.G. Fiscus'],\n",
       "  'related_topics': ['Word error rate',\n",
       "   'NIST',\n",
       "   'Language model',\n",
       "   'Word (computer architecture)',\n",
       "   'Process (computing)',\n",
       "   'Speech recognition',\n",
       "   'Dynamic programming',\n",
       "   'Voting',\n",
       "   'Computer science',\n",
       "   'Sequence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,777',\n",
       "  'reference_count': '1',\n",
       "  'references': ['1586176709']},\n",
       " {'id': '2594610113',\n",
       "  'title': 'Finding consensus in speech recognition: word error minimization and other applications of confusion networks☆',\n",
       "  'abstract': 'We describe a new framework for distilling information from word lattices to improve the accuracy of the speech recognition output and obtain a more perspicuous representation of a set of alternative hypotheses. In the standard MAP decoding approach the recognizer outputs the string of words corresponding to the path with the highest posterior probability given the acoustics and a language model. However, even given optimal models, the MAP decoder does not necessarily minimize the commonly used performance metric, word error rate (WER). We describe a method for explicitly minimizing WER by extracting word hypotheses with the highest posterior probabilities from word lattices. We change the standard problem formulation by replacing global search over a large set of sentence hypotheses with local search over a small set of word candidates. In addition to improving the accuracy of the recognizer, our method produces a new representation of a set of candidate hypotheses that specifies the sequence of word-level confusions in a compact lattice format. We study the properties of confusion networks and examine their use for other tasks, such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Lidia Mangu', 'Eric Brill', 'Andreas Stolcke'],\n",
       "  'related_topics': ['Word error rate',\n",
       "   'Language model',\n",
       "   'Posterior probability',\n",
       "   'Sentence',\n",
       "   'Decoding methods',\n",
       "   'Speech recognition',\n",
       "   'Small set',\n",
       "   'Natural language processing',\n",
       "   'Minification',\n",
       "   'Performance metric',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '885',\n",
       "  'reference_count': '17',\n",
       "  'references': ['3017143921',\n",
       "   '2125529971',\n",
       "   '1966812932',\n",
       "   '1904457459',\n",
       "   '2097978681',\n",
       "   '1571931074',\n",
       "   '1528470941',\n",
       "   '173561343',\n",
       "   '2129334286',\n",
       "   '55333121']},\n",
       " {'id': '2100506586',\n",
       "  'title': 'Two decades of statistical language modeling: where do we go from here?',\n",
       "  'abstract': 'Statistical language models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies. Since the first significant model was proposed in 1980, many attempts have been made to improve the state of the art. We review them, point to a few promising directions, and argue for a Bayesian approach to integration of linguistic theories with data.',\n",
       "  'date': '2000',\n",
       "  'authors': ['R. Rosenfeld'],\n",
       "  'related_topics': ['Language identification',\n",
       "   'Language model',\n",
       "   'Computational linguistics',\n",
       "   'Natural language',\n",
       "   'Natural language processing',\n",
       "   'Bayesian probability',\n",
       "   'Computer science',\n",
       "   'Point (typography)',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Statistical analysis',\n",
       "   'View Less'],\n",
       "  'citation_count': '915',\n",
       "  'reference_count': '77',\n",
       "  'references': ['2038721957',\n",
       "   '1594031697',\n",
       "   '2147152072',\n",
       "   '1632114991',\n",
       "   '2096175520',\n",
       "   '1508165687',\n",
       "   '2093390569',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2160842254']},\n",
       " {'id': '1797288984',\n",
       "  'title': 'Entropy-based Pruning of Backoff Language Models',\n",
       "  'abstract': 'A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance.',\n",
       "  'date': '2000',\n",
       "  'authors': ['Andreas'],\n",
       "  'related_topics': ['Maximum entropy probability distribution',\n",
       "   'Entropy (energy dispersal)',\n",
       "   'Entropy (information theory)',\n",
       "   'Kullback–Leibler divergence',\n",
       "   'Binary entropy function',\n",
       "   'Entropy (classical thermodynamics)',\n",
       "   'Entropy (statistical thermodynamics)',\n",
       "   'Perplexity',\n",
       "   'Entropy (arrow of time)',\n",
       "   'Language model',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Entropy (order and disorder)',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '425',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2099111195',\n",
       "   '2611071497',\n",
       "   '2134237567',\n",
       "   '2162995740',\n",
       "   '1903115690',\n",
       "   '2157140289',\n",
       "   '1757803263',\n",
       "   '32217939',\n",
       "   '2082092506']},\n",
       " {'id': '2097978681',\n",
       "  'title': 'THE SRI MARCH 2000 HUB-5 CONVERSATIONAL SPEECH TRANSCRIPTION SYSTEM',\n",
       "  'abstract': 'We describe SRI’s large vocabulary conversational speech r ecognition system as used in the March 2000 NIST Hub-5E evaluation. The system performs four recognition passes: (1) bigram recognition with phone-loop-adapted, within-word triphone acoustic models, (2) lattice generation with transcription-mod e-adapted models, (3) trigram lattice recognition with adapted cross -word triphone models, and (4) N-best rescoring and reranking with various additional knowledge sources. The system incorporates two new kinds of acoustic model: triphone models conditioned on speaking rate, and an explicit joint model of within-word phone durations. We also obtained an unusually large improvement from modeling crossword pronunciation variants in “multiword” vocabulary items. The language model (LM) was enhanced with an “anti-LM” representing acoustically confusable word sequences. Finally, we applied a generalized ROVER algorithm to combine the N-best hypotheses from several systems based on different acoustic models.',\n",
       "  'date': '2000',\n",
       "  'authors': ['A.', 'H.', 'J.', 'H.', 'V. R. Rao', 'C.', 'E.', 'F.', 'J.'],\n",
       "  'related_topics': ['Triphone',\n",
       "   'Acoustic model',\n",
       "   'Bigram',\n",
       "   'Language model',\n",
       "   'Trigram',\n",
       "   'Vocabulary',\n",
       "   'Pronunciation',\n",
       "   'Transcription (software)',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '187',\n",
       "  'reference_count': '18',\n",
       "  'references': ['2121227244',\n",
       "   '1904457459',\n",
       "   '1797288984',\n",
       "   '2113641473',\n",
       "   '2082474452',\n",
       "   '1528470941',\n",
       "   '2121464381',\n",
       "   '173561343',\n",
       "   '2104663520',\n",
       "   '2144920829']},\n",
       " {'id': '1528470941',\n",
       "  'title': 'Explicit word error minimization in N-Best list rescoring',\n",
       "  'abstract': 'We show that the standard hypothesis scoring paradigm used in maximum-likelihood-based speech recognition systems is not optimal with regard to minimizing the word error rate, the commonly used performance metric in speech recognition. This can lead to sub-optimal performance, especially in high-error-rate environments where word error and sentence error are not necessarily monotonically related. To address this discrepancy, we developed a new algorithm that explicitly minimizes expected word error for recognition hypotheses. First, we approximate the posterior hypothesis probabilities using N-best lists. We then compute the expected word error for each hypothesis with respect to the posterior distribution, and choose the hypothesis with the lowest error. Experiments show improved recognition rates on two spontaneous speech corpora.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Andreas Stolcke', 'Yochai Konig', 'Mitchel Weintraub'],\n",
       "  'related_topics': ['Word error rate',\n",
       "   'Word (computer architecture)',\n",
       "   'Posterior probability',\n",
       "   'Sentence',\n",
       "   'Speech recognition',\n",
       "   'Minification',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '223',\n",
       "  'reference_count': '5',\n",
       "  'references': ['3017143921',\n",
       "   '1966812932',\n",
       "   '2166637769',\n",
       "   '2144920829',\n",
       "   '1990387894']},\n",
       " {'id': '2127836646',\n",
       "  'title': 'A cache-based natural language model for speech recognition',\n",
       "  'abstract': 'Speech-recognition systems must often decide between competing ways of breaking up the acoustic input into strings of words. Since the possible strings may be acoustically similar, a language model is required; given a word string, the model returns its linguistic probability. Several Markov language models are discussed. A novel kind of language model which reflects short-term patterns of word use by means of a cache component (analogous to cache memory in hardware terminology) is presented. The model also contains a 3g-gram component of the traditional type. The combined model and a pure 3g-gram model were tested on samples drawn from the Lancaster-Oslo/Bergen (LOB) corpus of English text. The relative performance of the two models is examined, and suggestions for the future improvements are made. >',\n",
       "  'date': '1990',\n",
       "  'authors': ['R. Kuhn', 'R. De Mori'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Cache',\n",
       "   'Natural language',\n",
       "   'Markov model',\n",
       "   'CPU cache',\n",
       "   'String (computer science)',\n",
       "   'Word (computer architecture)',\n",
       "   'Vocabulary',\n",
       "   'Terminology',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '756',\n",
       "  'reference_count': '10',\n",
       "  'references': ['2105594594',\n",
       "   '1966812932',\n",
       "   '2751601659',\n",
       "   '1597533204',\n",
       "   '2159782014',\n",
       "   '1521239006',\n",
       "   '2055528812',\n",
       "   '1507680813',\n",
       "   '2076639289',\n",
       "   '340893908']},\n",
       " {'id': '2099111195',\n",
       "  'title': 'Elements of information theory',\n",
       "  'abstract': \"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.\",\n",
       "  'date': '1991',\n",
       "  'authors': ['Thomas M. Cover', 'Joy A. Thomas'],\n",
       "  'related_topics': [\"Shannon's source coding theorem\",\n",
       "   'Entropy rate',\n",
       "   'Joint entropy',\n",
       "   'Differential entropy',\n",
       "   'Maximum entropy probability distribution',\n",
       "   'Entropy power inequality',\n",
       "   'Entropy (information theory)',\n",
       "   'Rényi entropy',\n",
       "   'Applied mathematics',\n",
       "   'Engineering',\n",
       "   'View Less'],\n",
       "  'citation_count': '62,092',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2160842254',\n",
       "  'title': 'Inducing features of random fields',\n",
       "  'abstract': 'We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.',\n",
       "  'date': '1997',\n",
       "  'authors': ['S. Della Pietra', 'V. Della', 'J. Lafferty'],\n",
       "  'related_topics': ['Random field',\n",
       "   'Generalized iterative scaling',\n",
       "   'Greedy algorithm',\n",
       "   'Feature extraction',\n",
       "   'Stochastic process',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Cluster analysis',\n",
       "   'Decision tree',\n",
       "   'Iterative method',\n",
       "   'Kullback–Leibler divergence',\n",
       "   'Empirical distribution function',\n",
       "   'Principle of maximum entropy',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,624',\n",
       "  'reference_count': '20',\n",
       "  'references': ['1594031697',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2096175520',\n",
       "   '2121227244',\n",
       "   '2097333193',\n",
       "   '107938046',\n",
       "   '2167837909',\n",
       "   '2089969354',\n",
       "   '2035301451']},\n",
       " {'id': '2097333193',\n",
       "  'title': 'A statistical approach to machine translation',\n",
       "  'abstract': 'In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Peter F. Brown',\n",
       "   'John Cocke',\n",
       "   'Stephen A. Della Pietra',\n",
       "   'Vincent J. Della Pietra',\n",
       "   'Fredrick Jelinek',\n",
       "   'John D. Lafferty',\n",
       "   'Robert L. Mercer',\n",
       "   'Paul S. Roossin'],\n",
       "  'related_topics': ['Example-based machine translation',\n",
       "   'Machine translation software usability',\n",
       "   'Rule-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Evaluation of machine translation',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Translation (geometry)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'ROUGE',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,545',\n",
       "  'reference_count': '8',\n",
       "  'references': ['2049633694',\n",
       "   '1966812932',\n",
       "   '2334801970',\n",
       "   '1597533204',\n",
       "   '1575431606',\n",
       "   '2029825515',\n",
       "   '2008506796',\n",
       "   '2021021293']},\n",
       " {'id': '1597533204',\n",
       "  'title': 'Interpolated estimation of Markov source parameters from sparse data',\n",
       "  'abstract': '',\n",
       "  'date': '1980',\n",
       "  'authors': ['F.'],\n",
       "  'related_topics': ['Markov model',\n",
       "   'Variable-order Markov model',\n",
       "   'Markov chain',\n",
       "   'Sparse approximation',\n",
       "   'Sparse matrix',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Estimation',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,327',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2099345940',\n",
       "  'title': 'A tree-based statistical language model for natural language speech recognition',\n",
       "  'abstract': 'The problem of predicting the next word a speaker will say, given the words already spoken; is discussed. Specifically, the problem is to estimate the probability that a given word will be the next word uttered. Algorithms are presented for automatically constructing a binary decision tree designed to estimate these probabilities. At each node of the tree there is a yes/no question relating to the words already spoken, and at each leaf there is a probability distribution over the allowable vocabulary. Ideally, these nodal questions can take the form of arbitrarily complex Boolean expressions, but computationally cheaper alternatives are also discussed. Some results obtained on a 5000-word vocabulary with a tree designed to predict the next word spoken from the preceding 20 words are included. The tree is compared to an equivalent trigram model and shown to be superior. >',\n",
       "  'date': '1989',\n",
       "  'authors': ['L.R. Bahl', 'P.F. Brown', 'P.V. de Souza', 'R.L. Mercer'],\n",
       "  'related_topics': ['Incremental decision tree',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Tree (data structure)',\n",
       "   'Trigram',\n",
       "   'Word (computer architecture)',\n",
       "   'Decision tree',\n",
       "   'Binary decision diagram',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '479',\n",
       "  'reference_count': '9',\n",
       "  'references': ['2581275558',\n",
       "   '1966812932',\n",
       "   '1990005915',\n",
       "   '1990822176',\n",
       "   '1521239006',\n",
       "   '193579291',\n",
       "   '2120234416',\n",
       "   '1995564620',\n",
       "   '2079145130']},\n",
       " {'id': '2167434254',\n",
       "  'title': 'Towards History-based Grammars: Using Richer Models for Probabilistic Parsing',\n",
       "  'abstract': 'We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Ezra Black',\n",
       "   'Fred Jelinek',\n",
       "   'John Lafrerty',\n",
       "   'David M. Magerman',\n",
       "   'Robert Mercer',\n",
       "   'Salim Roukos'],\n",
       "  'related_topics': ['Parse tree',\n",
       "   'Top-down parsing',\n",
       "   'Parsing',\n",
       "   'Treebank',\n",
       "   'Rule-based machine translation',\n",
       "   'Generative grammar',\n",
       "   'Probabilistic logic',\n",
       "   'Syntax',\n",
       "   'Natural language',\n",
       "   'Sentence',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '226',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2121227244',\n",
       "   '2099247782',\n",
       "   '1859173823',\n",
       "   '2012837062',\n",
       "   '1535681052',\n",
       "   '2021758792',\n",
       "   '2029825515',\n",
       "   '2127009519',\n",
       "   '1973021928',\n",
       "   '2008506796']},\n",
       " {'id': '1976241232',\n",
       "  'title': 'Tagging text with a probabilistic model',\n",
       "  'abstract': 'Experiments on the use of a probabilistic model to tag English text, that is, to assign to each word the correct tag (part of speech) in the context of the sentence, are presented. A simple triclass Markov model is used, and the best way to estimate the parameters of this model, depending on the kind and amount of training data that is provided, is found. Two approaches are compared: the use of text that has been tagged by hand and comparing relative frequency counts; and use text without tags and training the model as a hidden Markov process, according to a maximum likelihood principle. Experiments show that the best training is obtained by using as much tagged text as is available, a maximum likelihood training may improve the accuracy of the tagging. >',\n",
       "  'date': '1991',\n",
       "  'authors': ['B. Merialdo'],\n",
       "  'related_topics': ['Markov model',\n",
       "   'Hidden Markov model',\n",
       "   'Markov process',\n",
       "   'Statistical model',\n",
       "   'Estimation theory',\n",
       "   'Viterbi algorithm',\n",
       "   'Context model',\n",
       "   'Context (language use)',\n",
       "   'Sentence',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Maximum likelihood',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '118',\n",
       "  'reference_count': '6',\n",
       "  'references': ['1571096757',\n",
       "   '2055528812',\n",
       "   '2093099215',\n",
       "   '2152428909',\n",
       "   '62444100',\n",
       "   '2109621219']},\n",
       " {'id': '3036751298',\n",
       "  'title': 'Parallel Networks that Learn to Pronounce English Text',\n",
       "  'abstract': 'This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations.',\n",
       "  'date': '1989',\n",
       "  'authors': ['T. J.'],\n",
       "  'related_topics': ['NETtalk',\n",
       "   'Speech synthesis',\n",
       "   'Network model',\n",
       "   'Invariant (computer science)',\n",
       "   'Task (computing)',\n",
       "   'Speech recognition',\n",
       "   'Class (computer programming)',\n",
       "   'Hierarchical clustering',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,671',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2118373646',\n",
       "  'title': 'Connectionism and cognitive architecture: a critical analysis',\n",
       "  'abstract': 'Abstract   This paper explores differences between Connectionist proposals for cognitive architecture and the sorts of models that have traditionally been assumed in cognitive science. We claim that the major distinction is that, while both Connectionist and Classical architectures postulate representational mental states, the latter but not the former are committed to a symbol-level of representation, or to a ‘language of thought’: i.e., to representational states that have combinatorial syntactic and semantic structure. Several arguments for combinatorial structure in mental representations are then reviewed. These include arguments based on the ‘systematicity’ of mental representation: i.e., on the fact that cognitive capacities always exhibit certain symmetries, so that the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents. We claim that such arguments make a powerful case that mind/brain architecture is not Connectionist at the cognitive level. We then consider the possibility that Connectionism may provide an account of the neural (or ‘abstract neurological’) structures in which Classical cognitive architecture is implemented. We survey a number of the standard arguments that have been offered in favor of Connectionism, and conclude that they are coherent only on this interpretation.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Jerry A. Fodor', 'Zenon W. Pylyshyn'],\n",
       "  'related_topics': ['Cognitive architecture',\n",
       "   'Language of thought hypothesis',\n",
       "   'Cognitive model',\n",
       "   'Connectionism',\n",
       "   'Mental representation',\n",
       "   'Dynamicism',\n",
       "   'Cognition',\n",
       "   'Cognitive science',\n",
       "   'Representation (arts)',\n",
       "   'Cognitive psychology',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,402',\n",
       "  'reference_count': '50',\n",
       "  'references': ['1652505363',\n",
       "   '2158365276',\n",
       "   '2083137466',\n",
       "   '2112325651',\n",
       "   '2122988375',\n",
       "   '2170716495',\n",
       "   '2094249282',\n",
       "   '2912225506',\n",
       "   '2144862731',\n",
       "   '1529681538']},\n",
       " {'id': '2046432185',\n",
       "  'title': 'Learning the hidden structure of speech',\n",
       "  'abstract': 'In the work described here, the backpropagation neural network learning procedure is applied to the analysis and recognition of speech. This procedure takes a set of input/output pattern pairs and attempts to learn their functional relationship; it develops the necessary representational features during the course of learning. A series of computer simulation studies was carried out to assess the ability of these networks to accurately label sounds, to learn to recognize sounds without labels, and to learn feature representations of continuous speech. These studies demonstrated that the networks can learn to label presegmented test tokens with accuracies of up to 95%. Networks trained on segmented sounds using a strategy that requires no external labels were able to recognize and delineate sounds in continuous speech. These networks developed rich internal representations that included units which corresponded to such traditional distinctions as vowels and consonants, as well as units that were sensitive to novel and nonstandard features. Networks trained on a large corpus of unsegmented, continuous speech without labels also developed interesting feature representations, which may be useful in both segmentation and label learning. The results of these studies, while preliminary, demonstrate that backpropagation learning can be used with complex, natural data to identify a feature structure that can serve as the basis for both analysis and nontrivial pattern recognition.',\n",
       "  'date': '1988',\n",
       "  'authors': ['Jeffery Locke Elman', 'David'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Phonetics',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Backpropagation',\n",
       "   'Set (psychology)',\n",
       "   'Speech recognition',\n",
       "   'Segmentation',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '428',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2122988375',\n",
       "  'title': 'On the proper treatment of connectionism',\n",
       "  'abstract': 'A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Paul Smolensky'],\n",
       "  'related_topics': ['Cognitive model',\n",
       "   'Connectionism',\n",
       "   'Dynamicism',\n",
       "   'Dynamical systems theory',\n",
       "   'Language of thought hypothesis',\n",
       "   'Information processing',\n",
       "   'Massively parallel',\n",
       "   'Set (psychology)',\n",
       "   'Cognitive science',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,082',\n",
       "  'reference_count': '150',\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1997063559',\n",
       "   '2293063825',\n",
       "   '1708874574',\n",
       "   '1991848143',\n",
       "   '2177721432',\n",
       "   '22297218',\n",
       "   '2002089154',\n",
       "   '2266946488']},\n",
       " {'id': '2170716495',\n",
       "  'title': 'Aspects of the Theory of Syntax',\n",
       "  'abstract': 'Abstract : Contents: Methodological preliminaries: Generative grammars as theories of linguistic competence; theory of performance; organization of a generative grammar; justification of grammars; formal and substantive grammars; descriptive and explanatory theories; evaluation procedures; linguistic theory and language learning; generative capacity and its linguistic relevance Categories and relations in syntactic theory: Scope of the base; aspects of deep structure; illustrative fragment of the base component; types of base rules Deep structures and grammatical transformations Residual problems: Boundaries of syntax and semantics; structure of the lexicon',\n",
       "  'date': '1965',\n",
       "  'authors': ['Noam'],\n",
       "  'related_topics': ['Generative grammar',\n",
       "   'c-command',\n",
       "   'Phrase structure grammar',\n",
       "   'Context-free grammar',\n",
       "   'Generative second-language acquisition',\n",
       "   'Linguistic competence',\n",
       "   'Transformational grammar',\n",
       "   'Principles and parameters',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '42,244',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2094249282',\n",
       "  'title': 'Language learnability and language development',\n",
       "  'abstract': 'Language learnability and language devlopment revisited the acquisition theory - assumptions and postulates phrase structure rules phrase stucture rules - developmental considerations inflection complementation and control auxiliaries lexical entries and lexical rules.',\n",
       "  'date': '1984',\n",
       "  'authors': ['Steven Pinker'],\n",
       "  'related_topics': ['Learnability',\n",
       "   'Phrase structure rules',\n",
       "   'Object language',\n",
       "   'Language construct',\n",
       "   'Developmental linguistics',\n",
       "   'Phrase',\n",
       "   'Universal Networking Language',\n",
       "   'Language identification',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,930',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1480928214',\n",
       "  'title': \"Lapack Users' Guide\",\n",
       "  'abstract': 'Preface to the third edition Preface to the secondedition Part 1. Guide. 1. Essentials 2. Contents of LAPACK 3. Performance of LAPACK 4. Accuracy and Stability 5. Documentation and Software Conventions 6. Installing LAPACK Routines 7. Troubleshooting Appendix A. Index of Driver and Computational Routines Appendix B. Index of Auxiliary Routines Appendix C. Quick Reference Guide to the BLAS Appendix D. Converting from LINPACK or EISPACK Appendix E. LAPACK Working Notes Part 2. Specifications of Routines. Bibliography Index by Keyword Index by Routine Name.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Ed'],\n",
       "  'related_topics': ['ScaLAPACK',\n",
       "   'EISPACK',\n",
       "   'Index (publishing)',\n",
       "   'Troubleshooting',\n",
       "   'Software',\n",
       "   'Documentation',\n",
       "   'Programming language',\n",
       "   'Computer science',\n",
       "   'Operations research',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,593',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1521571223',\n",
       "  'title': 'The High Performance Fortran Handbook',\n",
       "  'abstract': \"From the Publisher:\\r\\nHigh Performance Fortran (HPF) is a set of extensions to Fortran expressing parallel execution at a relatively high level. For the thousands of scientists, engineers, and others who wish to take advantage of the power of both vector and parallel supercomputers, five of the principal authors of HPF have teamed up here to write a tutorial for the language.\\r\\nThere is an increasing need for a common parallel Fortran that can serve as a programming interface with the new parallel machines that are appearing on the market. While HPF does not solve all the problems of parallel programming, it does provide a portable, high-level expression for data- parallel algorithms that brings the convenience of sequential Fortran a step closer to today's complex parallel machines.\",\n",
       "  'date': '1993',\n",
       "  'authors': ['Charles H. Koelbel',\n",
       "   'David B. Loveman',\n",
       "   'Robert S. Schreiber',\n",
       "   'Guy L. Steele',\n",
       "   'Mary E. Zosel'],\n",
       "  'related_topics': ['High Performance Fortran',\n",
       "   'Parallel programming model',\n",
       "   'Fortran',\n",
       "   'Parallel algorithm',\n",
       "   'Interface (Java)',\n",
       "   'Programming language',\n",
       "   'Parallel computing',\n",
       "   'Expression (computer science)',\n",
       "   'Set (abstract data type)',\n",
       "   'Principal (computer security)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,068',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1964564149',\n",
       "  'title': 'Monitors, messages, and clusters: the p4 parallel programming system',\n",
       "  'abstract': 'p4 is a portable library of C and Fortran subroutines for programming parallel computers. It is the current version of a system that has been in use since 1984. It includes features for explicit parallel programming of shared-memory machines, distributed-memory machines (including heterogeneous networks of workstations), and clusters, by which we mean shared-memory multiprocessors communicating via message passing. We discuss here the design goals, history, and system architecture of p4 and describe briefly a diverse collection of applications that have demonstrated the utility of p4.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Ralph M. Butler', 'Ewing L. Lusk'],\n",
       "  'related_topics': ['Procedural programming',\n",
       "   'Subroutine',\n",
       "   'Message passing',\n",
       "   'Fortran',\n",
       "   'Systems architecture',\n",
       "   'Workstation',\n",
       "   'Heterogeneous network',\n",
       "   'Operating system',\n",
       "   'Parallel computing',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '381',\n",
       "  'reference_count': '17',\n",
       "  'references': ['1966285605',\n",
       "   '2127218465',\n",
       "   '1537293292',\n",
       "   '351767304',\n",
       "   '2018802931',\n",
       "   '2026896213',\n",
       "   '2106879254',\n",
       "   '2110256578',\n",
       "   '1600264885',\n",
       "   '2137902979']},\n",
       " {'id': '1978513924',\n",
       "  'title': 'Disk-directed I/O for MIMD multiprocessors',\n",
       "  'abstract': \"Many scientific applications that run on today's multiprocessors, such as weather forecasting and seismic analysis, are bottlenecked by their file-I/O needs. Even if the multiprocessor is configured with sufficient I/O hardware, the file system software often fails to provide the available bandwidth to the application. Although libraries and enhanced file system interfaces can make a significant improvement, we believe that fundamental changes are needed in the file server software. We propose a new technique, disk-directed I/O, to allow the disk servers to determine the flow of data for maximum performance. Our simulations show that tremendous performance gains are possible both for simple reads and writes and for an out-of-core application. Indeed, our disk-directed I/O technique provided consistent high performance that was largely independent of data distribution and obtained up to 93% of peak disk bandwidth. It was as much as 18 times faster than either a typical parallel file system or a two-phase-I/O library.\",\n",
       "  'date': '1997',\n",
       "  'authors': ['David Kotz'],\n",
       "  'related_topics': ['Parallel I/O',\n",
       "   'Device file',\n",
       "   'File system',\n",
       "   'File server',\n",
       "   'Server',\n",
       "   'MIMD',\n",
       "   'Bandwidth (computing)',\n",
       "   'Multiprocessing',\n",
       "   'Parallel computing',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '710',\n",
       "  'reference_count': '73',\n",
       "  'references': ['2147504831',\n",
       "   '1496940124',\n",
       "   '2168395296',\n",
       "   '1967141605',\n",
       "   '2150864656',\n",
       "   '2098815550',\n",
       "   '2114167330',\n",
       "   '1978513924',\n",
       "   '2157219191',\n",
       "   '2041330544']},\n",
       " {'id': '2083200599',\n",
       "  'title': 'Improved parallel I/O via a two-phase run-time access strategy',\n",
       "  'abstract': 'As scientists expand their models to describe physical phenomena of increasingly large extent, I/O becomes crucial and a system with limited I/O capacity can severely constrain the performance of the entire program.We provide experimental results, performed on an lntel Touchtone Delta and nCUBE 2 I/O system, to show that the performance of existing parallel I/O systems can vary by several orders of magnitude as a function of the data access pattern of the parallel program. We then propose a two-phase access strategy, to be implemented in a runtime system, in which the data distribution on computational nodes is decoupled from storage distribution. Our experimental results show that performance improvements of several orders of magnitude over direct access based data distribution methods can be obtained, and that performance for most data access patterns can be improved to within a factor of 2 of the best performance. Further, the cost of redistribution is a very small fraction of the overall access cost.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Juan Miguel del', 'Rajesh', 'Alok'],\n",
       "  'related_topics': ['Parallel I/O',\n",
       "   'Data access',\n",
       "   'Runtime system',\n",
       "   'Orders of magnitude (bit rate)',\n",
       "   'Parallel computing',\n",
       "   'Function (mathematics)',\n",
       "   'Computer science',\n",
       "   'Fraction (mathematics)',\n",
       "   'Factor (programming language)',\n",
       "   'Phase (waves)',\n",
       "   'View Less'],\n",
       "  'citation_count': '380',\n",
       "  'reference_count': '5',\n",
       "  'references': ['1998657709',\n",
       "   '2037535386',\n",
       "   '2108000325',\n",
       "   '3161155143',\n",
       "   '607187418']},\n",
       " {'id': '2090683636',\n",
       "  'title': 'Server-Directed Collective I/O in Panda',\n",
       "  'abstract': \"We present the architecture and implementation results for Panda 2.0, a library for input and output of multidimensional arrays on parallel and sequential platforms. Panda achieves remarkable performance levels on the IBM SP2, showing excellent scalability as data size increases and as the number of nodes increases, and provides throughputs close to the full capacity of the AIX file system on the SP2 we used. We argue that this good performance can be traced to Panda's use of server-directed i/o (a logical-level version of disk-directed i/o [Kotz94b]) to perform array i/o using sequential disk reads and writes, a very high level interface for collective i/o requests, and built-in facilities for arbitrary rearrangements of arrays during i/o. Other advantages of Panda's approach are ease of use, easy application portability, and a reliance on commodity system software.\",\n",
       "  'date': '1995',\n",
       "  'authors': ['K. E. Seamons',\n",
       "   'Y. Chen',\n",
       "   'P. Jones',\n",
       "   'J. Jozwiak',\n",
       "   'M. Winslett'],\n",
       "  'related_topics': ['File system',\n",
       "   'Interface (computing)',\n",
       "   'Scalability',\n",
       "   'Supercomputer',\n",
       "   'Application software',\n",
       "   'System software',\n",
       "   'Concurrent computing',\n",
       "   'Operating system',\n",
       "   'Parallel computing',\n",
       "   'Throughput',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '325',\n",
       "  'reference_count': '16',\n",
       "  'references': ['1978513924',\n",
       "   '3141437129',\n",
       "   '2144413105',\n",
       "   '2156359540',\n",
       "   '2001948415',\n",
       "   '2131610641',\n",
       "   '2080413856',\n",
       "   '2115394931',\n",
       "   '1567735309',\n",
       "   '2117645881']},\n",
       " {'id': '2010542899',\n",
       "  'title': 'Integrated Pvm Framework Supports Heterogeneous Network Computing',\n",
       "  'abstract': '',\n",
       "  'date': '1993',\n",
       "  'authors': ['Jack', 'G. A.', 'Robert', 'V. S.'],\n",
       "  'related_topics': ['Grid computing',\n",
       "   'Network architecture',\n",
       "   'Intelligent computer network',\n",
       "   'Network simulation',\n",
       "   'Heterogeneous network',\n",
       "   'Data diffusion machine',\n",
       "   'Distributed computing',\n",
       "   'Computer architecture',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '273',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2294265735',\n",
       "  'title': 'Visualization and debugging in a heterogeneous environment',\n",
       "  'abstract': \"The authors' experiences with visualization and debugging of parallel virtual machine (PVM) applications and two of the tools they have devised to facilitate these tasks are described. One of the tools is a graphical monitoring package called Xab that can visually display PVM activities inside an application running across a network. The other is a graphical programming environment called Hence, which helps the user write, compile, execute, and trace heterogeneous distributed programs. The authors discuss their early work, the present research, and the future directions of these experimental projects. >\",\n",
       "  'date': '1995',\n",
       "  'authors': ['Adam', 'Jack', 'Al', 'Vaidy'],\n",
       "  'related_topics': ['Debugging',\n",
       "   'TRACE (psycholinguistics)',\n",
       "   'Visualization',\n",
       "   'Virtual machine',\n",
       "   'Visual programming language',\n",
       "   'Compiler',\n",
       "   'Programming language',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '210',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1843937266',\n",
       "  'title': 'A Proposal for a User-Level, Message-Passing Interface in a Distributed Memory Environment',\n",
       "  'abstract': 'This paper describes Message Passing Interface 1 (MPI1), a proposed library interface standard for supporting point-to-point message passing. The intended standard will be provided with Fortran 77 and C interfaces, and will form the basis of a standard high level communication environment featuring collective communication and data distribution transformations. The standard proposed here provides blocking and nonblocking message passing between pairs of processes, with message selectivity by source process and message type. Provision is made for noncontiguous messages. Context control provides a convenient means of avoiding message selectivity conflicts between different phases of an application. The ability to form and manipulate process groups permit task parallelism to be exploited, and is a useful abstraction in controlling certain types of collective communication.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Jack J.', 'Rolf', 'Anthony J.G.', 'David W.'],\n",
       "  'related_topics': ['Message broker',\n",
       "   'Message passing',\n",
       "   'Message Passing Interface',\n",
       "   'Interface standard',\n",
       "   'Task parallelism',\n",
       "   'Distributed memory',\n",
       "   'Context (language use)',\n",
       "   'Blocking (computing)',\n",
       "   'Distributed computing',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '135',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2108167744',\n",
       "   '2161307885',\n",
       "   '2006774019',\n",
       "   '2111820660',\n",
       "   '2099159950',\n",
       "   '2116524044',\n",
       "   '1538115971',\n",
       "   '90802191',\n",
       "   '1532915129',\n",
       "   '2142471664']},\n",
       " {'id': '2010269868',\n",
       "  'title': 'The design and evolution of Zipcode',\n",
       "  'abstract': 'Abstract   Zipcode is a message-passing and process-management system that was designed for multicomputers and homogeneous networks of computers in order to support libraries and large-scale multicomputer software. The system has evolved significantly over the last five years, based on our experiences and identified needs. Features of Zipcode that were originally unique to it, were its simulataneous support of static process groups, communication contexts, and virtual topologies, forming the ‘mailer’ data structure. Point-to-point and collective operations reference the underlying group, and use contexts to avoid mixing up messages. Recently, we have added ‘gather-send’ and ‘receive-scatter’ semantics, based on persistent Zipcode ‘invoices’, both as a means to simplify message passing, and as a means to reveal more potential runtime optimizations. Key features in Zipcode appear in the forthcoming MPI standard.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Anthony Skjellum',\n",
       "   'Steven G. Smith',\n",
       "   'Nathan E. Doss',\n",
       "   'Alvin P. Leung',\n",
       "   'Manfred Morari'],\n",
       "  'related_topics': ['Message passing',\n",
       "   'Context (language use)',\n",
       "   'Software portability',\n",
       "   'Data structure',\n",
       "   'Data type',\n",
       "   'Interface (Java)',\n",
       "   'Semantics (computer science)',\n",
       "   'Computer science',\n",
       "   'Programming language',\n",
       "   'Distributed computing',\n",
       "   'Network topology',\n",
       "   'View Less'],\n",
       "  'citation_count': '75',\n",
       "  'reference_count': '21',\n",
       "  'references': ['2155066383',\n",
       "   '2051975225',\n",
       "   '2111820660',\n",
       "   '2001946794',\n",
       "   '2099159950',\n",
       "   '2026896213',\n",
       "   '1971077560',\n",
       "   '90802191',\n",
       "   '2086507055',\n",
       "   '2106918918']},\n",
       " {'id': '2170120409',\n",
       "  'title': 'Numerical recipes in C',\n",
       "  'abstract': \"Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08\",\n",
       "  'date': '1994',\n",
       "  'authors': ['William H.', 'Saul A.', 'William T.', 'Brian P.'],\n",
       "  'related_topics': ['IBM PC compatible',\n",
       "   'Computer graphics (images)',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'View Less'],\n",
       "  'citation_count': '16,006',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2611071497',\n",
       "  'title': 'Text Compression',\n",
       "  'abstract': '',\n",
       "  'date': '1990',\n",
       "  'authors': ['Timothy C.', 'John G.', 'Ian H.'],\n",
       "  'related_topics': ['Prediction by partial matching',\n",
       "   'Computer science',\n",
       "   'Compressed pattern matching',\n",
       "   'Speech recognition',\n",
       "   'Text compression',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,223',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2166637769',\n",
       "  'title': 'SWITCHBOARD: telephone speech corpus for research and development',\n",
       "  'abstract': 'SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording. >',\n",
       "  'date': '1992',\n",
       "  'authors': ['J.J. Godfrey', 'E.C. Holliman', 'J. McDaniel'],\n",
       "  'related_topics': ['Speech corpus',\n",
       "   'Speech processing',\n",
       "   'VoxForge',\n",
       "   'Speaker recognition',\n",
       "   'Speech technology',\n",
       "   'Transcription (software)',\n",
       "   'Dialog act',\n",
       "   'Vocabulary',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,440',\n",
       "  'reference_count': '3',\n",
       "  'references': ['1643320849', '2077302143', '1976349544']},\n",
       " {'id': '2134237567',\n",
       "  'title': 'Estimation of probabilities from sparse data for the language model component of a speech recognizer',\n",
       "  'abstract': 'The description of a novel type of m-gram language model is given. The model offers, via a nonlinear recursive procedure, a computation and space efficient solution to the problem of estimating probabilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and successfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of estimating probabilities from sparse data arises.',\n",
       "  'date': '1987',\n",
       "  'authors': ['S. Katz'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Speech processing',\n",
       "   'Language model',\n",
       "   'Sparse matrix',\n",
       "   'Natural language',\n",
       "   'Component (UML)',\n",
       "   'Data-oriented parsing',\n",
       "   'Computation',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,737',\n",
       "  'reference_count': '7',\n",
       "  'references': ['1597533204',\n",
       "   '2159782014',\n",
       "   '1507770639',\n",
       "   '2168938909',\n",
       "   '2170967986',\n",
       "   '2082092506',\n",
       "   '1590636096']},\n",
       " {'id': '2075201173',\n",
       "  'title': 'On structuring probabilistic dependences in stochastic language modelling',\n",
       "  'abstract': 'Abstract   In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.',\n",
       "  'date': '1994',\n",
       "  'authors': ['Hermann Ney', 'Ute Essen', 'Reinhard Kneser'],\n",
       "  'related_topics': ['Bigram',\n",
       "   'Word (computer architecture)',\n",
       "   'Linear interpolation',\n",
       "   'Conditional probability',\n",
       "   'Trigram',\n",
       "   'Probabilistic logic',\n",
       "   'Cluster analysis',\n",
       "   'Optimal estimation',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '848',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2142901448',\n",
       "  'title': 'Information Theory and Reliable Communication',\n",
       "  'abstract': 'Communication Systems and Information Theory. A Measure of Information. Coding for Discrete Sources. Discrete Memoryless Channels and Capacity. The Noisy-Channel Coding Theorem. Techniques for Coding and Decoding. Memoryless Channels with Discrete Time. Waveform Channels. Source Coding with a Fidelity Criterion. Index.',\n",
       "  'date': '1968',\n",
       "  'authors': ['Robert G. Gallager'],\n",
       "  'related_topics': ['Variable-length code',\n",
       "   'Shannon–Fano coding',\n",
       "   'Information theory',\n",
       "   'Decoding methods',\n",
       "   'Binary symmetric channel',\n",
       "   'Discrete time and continuous time',\n",
       "   'Coding (social sciences)',\n",
       "   'Communications system',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,928',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2751862591',\n",
       "  'title': 'An introduction to probability theory and its applications',\n",
       "  'abstract': '',\n",
       "  'date': '1950',\n",
       "  'authors': ['William'],\n",
       "  'related_topics': ['Probability and statistics',\n",
       "   'Imprecise probability',\n",
       "   'Tree diagram',\n",
       "   'Inverse probability',\n",
       "   'Law of total probability',\n",
       "   'Empirical probability',\n",
       "   'Conditional mutual information',\n",
       "   'Frequentist probability',\n",
       "   'Mathematics',\n",
       "   'Statistics',\n",
       "   'View Less'],\n",
       "  'citation_count': '57,767',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1575431606',\n",
       "  'title': 'An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process',\n",
       "  'abstract': '',\n",
       "  'date': '1972',\n",
       "  'authors': ['L.'],\n",
       "  'related_topics': ['Markov process',\n",
       "   'Markov chain',\n",
       "   'Markov renewal process',\n",
       "   'Markov model',\n",
       "   'Variable-order Markov model',\n",
       "   'Markov property',\n",
       "   'Markov kernel',\n",
       "   'Partially observable Markov decision process',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'Econometrics',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,350',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2007780422',\n",
       "  'title': 'Computational analysis of present-day American English',\n",
       "  'abstract': '',\n",
       "  'date': '1967',\n",
       "  'authors': ['Henry',\n",
       "   'W. Nelson',\n",
       "   'W. F.',\n",
       "   'Mary Lois',\n",
       "   'Laura M.',\n",
       "   'John Bissell'],\n",
       "  'related_topics': ['American English',\n",
       "   'Psychology',\n",
       "   'Brown Corpus',\n",
       "   'Present day',\n",
       "   'Linguistics',\n",
       "   'Library science',\n",
       "   'Computational analysis',\n",
       "   'Visual word recognition',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,294',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2016871293',\n",
       "  'title': 'Context based spelling correction',\n",
       "  'abstract': 'Abstract   Some mistakes in spelling and typing produce correct words, such as typing “fig” when “fog” was intended. These errors are undetectable by traditional spelling correction techniques. In this paper we present a statistical technique capable of detecting and correcting some of these errors when they occur in sentences. Experimental results show that this technique is capable of detecting 76% of simple spelling errors and correcting 73%.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Eric Mays', 'Fred J. Damerau', 'Robert L. Mercer'],\n",
       "  'related_topics': ['Spelling',\n",
       "   'Context (language use)',\n",
       "   'Orthography',\n",
       "   'Error detection and correction',\n",
       "   'Sentence',\n",
       "   'Phrase',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Syntax',\n",
       "   'Computational linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '401',\n",
       "  'reference_count': '5',\n",
       "  'references': ['1966812932',\n",
       "   '2109469864',\n",
       "   '2066792529',\n",
       "   '2002938383',\n",
       "   '2065533756']},\n",
       " {'id': '1628850721',\n",
       "  'title': 'Experiments with the Tangora 20,000 word speech recognizer',\n",
       "  'abstract': 'The Speech Recognition Group at IBM Research in Yorktown Heights has developed a real-time, isolated-utterance speech recognizer for natural language based on the IBM Personal Computer AT and IBM Signal Processors. The system has recently been enhanced by expanding the vocabulary from 5,000 words to 20,000 words and by the addition of a speech workstation to support usability studies on document creation by voice. The system supports spelling and interactive personalization to augment the vocabularies. This paper describes the implementation, user interface, and comparative performance of the recognizer.',\n",
       "  'date': '1987',\n",
       "  'authors': ['A. Averbuch',\n",
       "   'L. Bahl',\n",
       "   'R. Bakis',\n",
       "   'P. Brown',\n",
       "   'G. Daggett',\n",
       "   'S. Das',\n",
       "   'K. Davies',\n",
       "   'S. De Gennaro',\n",
       "   'P. de Souza',\n",
       "   'E. Epstein',\n",
       "   'D. Fraleigh',\n",
       "   'F. Jelinek',\n",
       "   'B. Lewis',\n",
       "   'R. Mercer',\n",
       "   'J. Moorhead',\n",
       "   'A. Nadas',\n",
       "   'D. Nahamoo',\n",
       "   'M. Picheny',\n",
       "   'G. Shichman',\n",
       "   'P. Spinelli',\n",
       "   'D. Van Compernolle',\n",
       "   'H. Wilkens'],\n",
       "  'related_topics': ['Speech synthesis',\n",
       "   'Speech processing',\n",
       "   'Speech corpus',\n",
       "   'Speech technology',\n",
       "   'Speech analytics',\n",
       "   'Speech enhancement',\n",
       "   'Word (computer architecture)',\n",
       "   'Audio mining',\n",
       "   'Personal computer',\n",
       "   'Vocabulary',\n",
       "   'Natural language',\n",
       "   'User interface',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '146',\n",
       "  'reference_count': '10',\n",
       "  'references': ['1966812932',\n",
       "   '2134237567',\n",
       "   '1990005915',\n",
       "   '1521239006',\n",
       "   '1507770639',\n",
       "   '2134587001',\n",
       "   '2170967986',\n",
       "   '2056809970',\n",
       "   '1901281023',\n",
       "   '2089661020']},\n",
       " {'id': '2107743791',\n",
       "  'title': 'Probabilistic latent semantic indexing',\n",
       "  'abstract': 'Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Document-term matrix',\n",
       "   'Latent semantic analysis',\n",
       "   'Latent class model',\n",
       "   'Topic model',\n",
       "   'Search engine indexing',\n",
       "   'Probabilistic logic',\n",
       "   'Dynamic topic model',\n",
       "   'Pachinko allocation',\n",
       "   'Natural language processing',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'Synonym',\n",
       "   'Artificial intelligence',\n",
       "   'Latent semantic indexing',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,777',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2147152072',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '1612003148',\n",
       "   '2567948266',\n",
       "   '2127314673',\n",
       "   '1718512272',\n",
       "   '2143144851',\n",
       "   '2140842551',\n",
       "   '2063089147']},\n",
       " {'id': '2020842694',\n",
       "  'title': 'Modeling annotated data',\n",
       "  'abstract': 'We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval.',\n",
       "  'date': '2003',\n",
       "  'authors': ['David M. Blei', 'Michael I. Jordan'],\n",
       "  'related_topics': ['Latent Dirichlet allocation',\n",
       "   'Image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Latent variable model',\n",
       "   'Mixture model',\n",
       "   'Graphical model',\n",
       "   'Conditional probability distribution',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,462',\n",
       "  'reference_count': '17',\n",
       "  'references': ['1880262756',\n",
       "   '2121947440',\n",
       "   '1516111018',\n",
       "   '2137471889',\n",
       "   '2093390569',\n",
       "   '2109868644',\n",
       "   '2137918516',\n",
       "   '2172085063',\n",
       "   '3016843226',\n",
       "   '2108346334']},\n",
       " {'id': '2063392856',\n",
       "  'title': 'Latent semantic indexing: a probabilistic analysis',\n",
       "  'abstract': '',\n",
       "  'date': '1998',\n",
       "  'authors': ['Christos H. Papadimitriou',\n",
       "   'Hisao Tamaki',\n",
       "   'Prabhakar Raghavan',\n",
       "   'Santosh Vempala'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Document-term matrix',\n",
       "   'Probabilistic analysis of algorithms',\n",
       "   'Computer science',\n",
       "   'Information retrieval',\n",
       "   'Latent semantic indexing',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,587',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2138621811',\n",
       "   '2798909945',\n",
       "   '2147152072',\n",
       "   '1956559956',\n",
       "   '2072773380',\n",
       "   '1979750072',\n",
       "   '2013737143',\n",
       "   '2983896310',\n",
       "   '2058616517',\n",
       "   '2106285343']},\n",
       " {'id': '2001082470',\n",
       "  'title': 'Finding scientific topics',\n",
       "  'abstract': 'A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. & Jordan, M. I. (2003)  J. Machine Learn. Res.  3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.',\n",
       "  'date': '2004',\n",
       "  'authors': ['Thomas L. Griffiths', 'Mark Steyvers'],\n",
       "  'related_topics': ['Topic model',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Dynamic topic model',\n",
       "   'Generative model',\n",
       "   'Pachinko allocation',\n",
       "   'Bayesian inference',\n",
       "   'Inference',\n",
       "   'Information retrieval',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '6,832',\n",
       "  'reference_count': '10',\n",
       "  'references': ['1574901103',\n",
       "   '2130416410',\n",
       "   '1997063559',\n",
       "   '2069739265',\n",
       "   '2134731454',\n",
       "   '2104924585',\n",
       "   '2753533763',\n",
       "   '1666636243',\n",
       "   '578760377',\n",
       "   '2165554837']},\n",
       " {'id': '2158266063',\n",
       "  'title': 'Hierarchical Dirichlet Processes',\n",
       "  'abstract': 'We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes ...',\n",
       "  'date': '2006',\n",
       "  'authors': ['Yee Whye', 'Michael I.', 'Matthew J.', 'David M.'],\n",
       "  'related_topics': ['Hierarchical Dirichlet process',\n",
       "   'Dirichlet process',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Dirichlet distribution',\n",
       "   'Mixture model',\n",
       "   'Pitman–Yor process',\n",
       "   'Chinese restaurant process',\n",
       "   'Dynamic topic model',\n",
       "   'Pure mathematics',\n",
       "   'Mathematics',\n",
       "   'Econometrics',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,522',\n",
       "  'reference_count': '50',\n",
       "  'references': ['1880262756',\n",
       "   '2098126593',\n",
       "   '2125838338',\n",
       "   '2152664025',\n",
       "   '2110755408',\n",
       "   '2009570821',\n",
       "   '1956559956',\n",
       "   '2011832962',\n",
       "   '2890040444',\n",
       "   '2115979064']},\n",
       " {'id': '2047804403',\n",
       "  'title': 'Modular toolkit for Data Processing (MDP): a Python data processing framework',\n",
       "  'abstract': \"Modular toolkit for Data Processing (MDP) is a data processing framework written in Python. From the user's perspective, MDP is a collection of supervised and unsupervised learning algorithms and other data processing units that can be combined into data processing sequences and more complex feed-forward network architectures. Computations are performed efficiently in terms of speed and memory requirements. From the scientific developer's perspective, MDP is a modular framework, which can easily be expanded. The implementation of new algorithms is easy and intuitive. The new implemented units are then automatically integrated with the rest of the library. MDP has been written in the context of theoretical research in neuroscience, but it has been designed to be helpful in any context where trainable data processing algorithms are used. Its simplicity on the user's side, the variety of readily available algorithms, and the reusability of the implemented units make it also a useful educational tool.\",\n",
       "  'date': '2008',\n",
       "  'authors': ['Tiziano Zito',\n",
       "   'Niko Wilbert',\n",
       "   'Laurenz Wiskott',\n",
       "   'Pietro Berkes'],\n",
       "  'related_topics': ['Python (programming language)',\n",
       "   'Modular design',\n",
       "   'Data processing',\n",
       "   'Unsupervised learning',\n",
       "   'Network architecture',\n",
       "   'Programming language',\n",
       "   'Computer science',\n",
       "   'Reusability',\n",
       "   'Computational neuroscience',\n",
       "   'Computation',\n",
       "   'View Less'],\n",
       "  'citation_count': '143',\n",
       "  'reference_count': '11',\n",
       "  'references': ['1663973292',\n",
       "   '2136922672',\n",
       "   '1554663460',\n",
       "   '2156838815',\n",
       "   '2167217202',\n",
       "   '2138754805',\n",
       "   '2148856562',\n",
       "   '1511812886',\n",
       "   '2102724320',\n",
       "   '2156141384']},\n",
       " {'id': '2143017621',\n",
       "  'title': 'NLTK: The Natural Language Toolkit',\n",
       "  'abstract': 'The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simplified toolkit and explains how it is used in teaching NLP.',\n",
       "  'date': '2006',\n",
       "  'authors': ['Steven Bird'],\n",
       "  'related_topics': ['Natural language programming',\n",
       "   'Language technology',\n",
       "   'Natural language user interface',\n",
       "   'Natural language',\n",
       "   'Python (programming language)',\n",
       "   'Computational linguistics',\n",
       "   'Temporal annotation',\n",
       "   'Question answering',\n",
       "   'Programming language',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,567',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2038248725',\n",
       "   '1549285799',\n",
       "   '2953320089',\n",
       "   '2161160885',\n",
       "   '1548991402',\n",
       "   '1625780798',\n",
       "   '2045784978',\n",
       "   '2042978285',\n",
       "   '2136248378',\n",
       "   '118140219']},\n",
       " {'id': '2159426623',\n",
       "  'title': 'Reading Tea Leaves: How Humans Interpret Topic Models',\n",
       "  'abstract': 'Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.',\n",
       "  'date': '2009',\n",
       "  'authors': ['Jonathan Chang',\n",
       "   'Sean Gerrish',\n",
       "   'Chong Wang',\n",
       "   'Jordan L. Boyd-graber',\n",
       "   'David M. Blei'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Topic model',\n",
       "   'Probabilistic logic',\n",
       "   'Natural language processing',\n",
       "   'Reading (process)',\n",
       "   'Space (commercial competition)',\n",
       "   'Meaning (linguistics)',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,155',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2108598243',\n",
       "   '1880262756',\n",
       "   '2147152072',\n",
       "   '2049633694',\n",
       "   '1970381522',\n",
       "   '1983578042',\n",
       "   '2143017621',\n",
       "   '1612003148',\n",
       "   '2100935296',\n",
       "   '2334889010']},\n",
       " {'id': '2334889010',\n",
       "  'title': 'Probabilistic Topic Models',\n",
       "  'abstract': '',\n",
       "  'date': '2007',\n",
       "  'authors': ['Mark Steyvers', 'Tom Griffiths'],\n",
       "  'related_topics': ['Probabilistic logic',\n",
       "   'Topic model',\n",
       "   'Semantics (computer science)',\n",
       "   'Frequentist probability',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Cognition',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,174',\n",
       "  'reference_count': '23',\n",
       "  'references': ['1880262756',\n",
       "   '2098126593',\n",
       "   '2001082470',\n",
       "   '2158266063',\n",
       "   '2130416410',\n",
       "   '3143596294',\n",
       "   '1983578042',\n",
       "   '2158997610',\n",
       "   '2134731454',\n",
       "   '1612003148']},\n",
       " {'id': '1983578042',\n",
       "  'title': \"A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.\",\n",
       "  'abstract': 'How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched.',\n",
       "  'date': '1997',\n",
       "  'authors': ['Thomas K. Landauer', 'Susan T.'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Similarity (psychology)',\n",
       "   'Latent semantic analysis',\n",
       "   'Knowledge representation and reasoning',\n",
       "   'Vocabulary',\n",
       "   'Verbal learning',\n",
       "   'Distributional semantics',\n",
       "   'Statistical semantics',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,790',\n",
       "  'reference_count': '106',\n",
       "  'references': ['2147152072',\n",
       "   '2293063825',\n",
       "   '1898014694',\n",
       "   '1593045043',\n",
       "   '2001467963',\n",
       "   '1540136915',\n",
       "   '2059975159',\n",
       "   '2152632951',\n",
       "   '1594369375',\n",
       "   '2163953154']},\n",
       " {'id': '1674947250',\n",
       "  'title': 'Comprehension: A Paradigm for Cognition',\n",
       "  'abstract': 'Preface Acknowledgements 1. Introduction Part I. The Theory: 2. Cognition and representation 3. Propositional representations 4. Modeling comprehension processes: the construction-integration model Part II. Models of Comprehension: 5. Word identification in discourse 6. Textbases and situation models 7. The role of working memory in comprehension 8. Memory for text 9. Learning from text 10. Word problems 11. Beyond text References name index Subject index.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Walter Kintsch'],\n",
       "  'related_topics': ['Comprehension',\n",
       "   'Cognition',\n",
       "   'Coh-Metrix',\n",
       "   'Working memory',\n",
       "   'Representation (arts)',\n",
       "   'Cognitive psychology',\n",
       "   'Index (publishing)',\n",
       "   'Metacomprehension',\n",
       "   'Subject (grammar)',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,003',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2072773380',\n",
       "  'title': 'Using linear algebra for intelligent information retrieval',\n",
       "  'abstract': 'Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users’ requests and those in or assigned to documents in a database. ...',\n",
       "  'date': '1995',\n",
       "  'authors': ['Michael W.', 'Susan T.', 'Gavin W.'],\n",
       "  'related_topics': ['Document retrieval',\n",
       "   'Latent semantic analysis',\n",
       "   'Automatic indexing',\n",
       "   'Search engine indexing',\n",
       "   'Semantic analysis (machine learning)',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Linear algebra',\n",
       "   'Systems design',\n",
       "   'Latent semantic indexing',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,401',\n",
       "  'reference_count': '12',\n",
       "  'references': ['2147152072',\n",
       "   '2000672666',\n",
       "   '2106365165',\n",
       "   '2149671658',\n",
       "   '2032840958',\n",
       "   '2093432797',\n",
       "   '2074965064',\n",
       "   '2099708568',\n",
       "   '30999966',\n",
       "   '1990215053']},\n",
       " {'id': '2056029990',\n",
       "  'title': 'Personalized information delivery: an analysis of information filtering methods',\n",
       "  'abstract': '',\n",
       "  'date': '1992',\n",
       "  'authors': ['Peter W.', 'Susan T.'],\n",
       "  'related_topics': ['Information filtering system',\n",
       "   'Information system',\n",
       "   'Information source',\n",
       "   'Information retrieval',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'Information delivery',\n",
       "   'Latent semantic indexing',\n",
       "   'View Less'],\n",
       "  'citation_count': '953',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2147152072',\n",
       "   '1956559956',\n",
       "   '2000672666',\n",
       "   '2106365165',\n",
       "   '2078875869',\n",
       "   '2058616517',\n",
       "   '2083605078',\n",
       "   '2165978089',\n",
       "   '2032840958',\n",
       "   '2025172185']},\n",
       " {'id': '1981617416',\n",
       "  'title': 'Producing high-dimensional semantic spaces from lexical co-occurrence',\n",
       "  'abstract': 'A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).',\n",
       "  'date': '1996',\n",
       "  'authors': ['Kevin Lund', 'Curt Burgess'],\n",
       "  'related_topics': ['Explicit semantic analysis',\n",
       "   'Semantic similarity',\n",
       "   'SemEval',\n",
       "   'Latent semantic analysis',\n",
       "   'Semantic computing',\n",
       "   'Semantic compression',\n",
       "   'Random indexing',\n",
       "   'Basis (linear algebra)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,077',\n",
       "  'reference_count': '13',\n",
       "  'references': ['2116411927',\n",
       "   '2069736034',\n",
       "   '1986010321',\n",
       "   '2086220442',\n",
       "   '2074439471',\n",
       "   '2074657344',\n",
       "   '1550028225',\n",
       "   '1568757443',\n",
       "   '1974351977',\n",
       "   '3149550307']},\n",
       " {'id': '2059086756',\n",
       "  'title': 'Strategies of discourse comprehension',\n",
       "  'abstract': 'rhetorical schemata to be discussed in what follows. Finally, schemata are descriptions, not definitions. The ‘bus’ schema contains information that is nor-',\n",
       "  'date': '1983',\n",
       "  'authors': ['Teun Adrianus van', 'Walter'],\n",
       "  'related_topics': ['Schema (psychology)',\n",
       "   'Rhetorical question',\n",
       "   'Comprehension',\n",
       "   'Coh-Metrix',\n",
       "   'Linguistics',\n",
       "   'Psychology',\n",
       "   'Discourse processing',\n",
       "   'Narrative comprehension',\n",
       "   'Situation model',\n",
       "   'Text comprehension',\n",
       "   'View Less'],\n",
       "  'citation_count': '11,592',\n",
       "  'reference_count': '110',\n",
       "  'references': ['2264742718',\n",
       "   '1581218697',\n",
       "   '2121773050',\n",
       "   '2073257493',\n",
       "   '2153076044',\n",
       "   '2091785129',\n",
       "   '2157368609',\n",
       "   '2021625970',\n",
       "   '2134199742',\n",
       "   '1582594879']},\n",
       " {'id': '2092919341',\n",
       "  'title': 'The Adaptive Character of Thought',\n",
       "  'abstract': 'Contents: Part I:Introduction. Preliminaries. Levels of a Cognitive Theory. Current Formulation of the Levels Issues. The New Theoretical Framework. Is Human Cognition Rational? The Rest of This Book. Appendix: Non-Identifiability and Response Time. Part II:Memory. Preliminaries. A Rational Analysis of Human Memory. The History Factor. The Contextual Factor. Relationship of Need and Probability to Probability and Latency of Recall. Combining Information From Cues. Implementation in the ACT Framework. Effects of Subject Strategy. Conclusions. Part III:Categorization. Preliminaries. The Goal of Categorization. The Structure of the Environment. Recapitulation of Goals and Environment. The Optimal Solution. An Iterative Algorithm for Categorization. Application of the Algorithm. Survey of the Experimental Literature. Conclusion. Appendix: The Ideal Algorithm. Part IV:Causal Inference. Preliminaries. Basic Formulation of the Causal Inference Problem. Causal Estimation. Cues for Causal Inference. Integration of Statistical and Temporal Cues. Discrimination. Abstraction of Causal Laws. Implementation in a Production System. Conclusion. Appendix. Part V:Problem Solving. Preliminaries. Making a Choice Among Simple Actions. Combining Steps. Studies of Hill Climbing. Means-Ends Analysis. Instantiation of Indefinite Objects. Conclusions on Rational Analysis of Problem Solving. Implementation in ACT. Appendix: Problem Solving and Clotheslines. Part VI:Retrospective. Preliminaries. Twelve Questions About Rational Analysis.',\n",
       "  'date': '1990',\n",
       "  'authors': ['John R. Anderson'],\n",
       "  'related_topics': ['Causal inference',\n",
       "   'Rational analysis',\n",
       "   'Inference',\n",
       "   'Categorization',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Hill climbing',\n",
       "   'Cognition',\n",
       "   'Artificial intelligence',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,548',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2928502135',\n",
       "  'title': 'The role of knowledge in discourse comprehension : a construction-integration model',\n",
       "  'abstract': 'Publisher Summary   This chapter discusses data concerning the time course of word identification in a discourse context. A simulation of arithmetic word-problem understanding provides a plausible account for some well-known phenomena. The current theories use representations with several mutually constraining layers. There is typically a linguistic level of representation, conceptual levels to represent both the local and global meaning and structure of a text, and a level at which the text itself has lost its individuality and its information content. Knowledge provides part of the context within which a discourse interpreted. The integration phase is the price the model pays for the necessary flexibility in the construction process.',\n",
       "  'date': '1991',\n",
       "  'authors': ['Walter'],\n",
       "  'related_topics': ['Context (language use)',\n",
       "   'Meaning (linguistics)',\n",
       "   'Process (engineering)',\n",
       "   'Representation (arts)',\n",
       "   'Flexibility (engineering)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Comprehension',\n",
       "   'Cognitive science',\n",
       "   'Psychology',\n",
       "   'Content (Freudian dream analysis)',\n",
       "   'View Less'],\n",
       "  'citation_count': '5,730',\n",
       "  'reference_count': '26',\n",
       "  'references': ['2042276900',\n",
       "   '2039107287',\n",
       "   '2153076044',\n",
       "   '2409145188',\n",
       "   '2090639711',\n",
       "   '2002654918',\n",
       "   '1982914430',\n",
       "   '1990948551',\n",
       "   '2040187703',\n",
       "   '2092771413']},\n",
       " {'id': '1996650435',\n",
       "  'title': 'Are Good Texts Always Better? Interactions of Text Coherence, Background Knowledge, and Levels of Understanding in Learning From Text',\n",
       "  'abstract': \"Two experiments, theoretically motivated by the construction-integration model of text comprehension (W. Kintsch, 1988), investigated the role of text coherence in the comprehension of science texts. In Experiment 1, junior high school students' comprehension of one of three versions of a biology text was examined via free recall, written questions, and a key-word sorting task. This study demonstrates advantages for globally coherent text and for more explanatory text. In Experiment 2, interactions among local and global text coherence, readers' background knowledge, and levels of understanding were examined. Using the same methods as in Experiment 1, we examined students' comprehension of one of four versions of a text, orthogonally varying local and global coherence. We found that readers who know little about the domain of the text benefit from a coherent text, whereas high-knowledge readers benefit from a minimally coherent text. We argue that the poorly written text forces the knowledgeable readers t...\",\n",
       "  'date': '1996',\n",
       "  'authors': ['Danielle S. McNamara',\n",
       "   '',\n",
       "   'Eileen Kintsch',\n",
       "   'Nancy Butler Songer',\n",
       "   'Walter Kintsch'],\n",
       "  'related_topics': ['Coherence (linguistics)',\n",
       "   'Reading comprehension',\n",
       "   'Coh-Metrix',\n",
       "   'Comprehension',\n",
       "   'Free recall',\n",
       "   'Expertise reversal effect',\n",
       "   'Knowledge level',\n",
       "   'Cognitive psychology',\n",
       "   'Communication',\n",
       "   'Task (project management)',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,997',\n",
       "  'reference_count': '33',\n",
       "  'references': ['2059086756',\n",
       "   '2928502135',\n",
       "   '1752512628',\n",
       "   '1813659000',\n",
       "   '2134907585',\n",
       "   '2024710858',\n",
       "   '118516307',\n",
       "   '2004017050',\n",
       "   '2004805246',\n",
       "   '2032811864']},\n",
       " {'id': '2096765155',\n",
       "  'title': 'Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling',\n",
       "  'abstract': 'Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.',\n",
       "  'date': '2005',\n",
       "  'authors': ['Jenny Rose Finkel', 'Trond Grenager', 'Christopher Manning'],\n",
       "  'related_topics': ['Approximate inference',\n",
       "   'Information extraction',\n",
       "   'Gibbs sampling',\n",
       "   'Inference',\n",
       "   'Simulated annealing',\n",
       "   'Probabilistic logic',\n",
       "   'Consistency (database systems)',\n",
       "   'Monte Carlo method',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,700',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2147880316',\n",
       "   '2125838338',\n",
       "   '2581275558',\n",
       "   '1997063559',\n",
       "   '2160842254',\n",
       "   '2135194391',\n",
       "   '2962735828',\n",
       "   '1513861746',\n",
       "   '2171776966',\n",
       "   '2129712609']},\n",
       " {'id': '1996430422',\n",
       "  'title': 'Feature-rich part-of-speech tagging with a cyclic dependency network',\n",
       "  'abstract': 'We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Kristina Toutanova',\n",
       "   'Dan Klein',\n",
       "   'Christopher D. Manning',\n",
       "   'Yoram Singer'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Dependency network',\n",
       "   'Feature (machine learning)',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Part-of-speech tagging',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,930',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2147880316',\n",
       "   '1632114991',\n",
       "   '2008652694',\n",
       "   '2135843243',\n",
       "   '1773803948',\n",
       "   '2117400858',\n",
       "   '2099247782',\n",
       "   '1513861746',\n",
       "   '1860991815',\n",
       "   '1529196404']},\n",
       " {'id': '2148540243',\n",
       "  'title': 'Unsupervised named-entity extraction from the Web: An experimental study',\n",
       "  'abstract': \"The KnowItAll system aims to automate the tedious process of extracting large collections of facts (e.g., names of scientists or politicians) from the Web in an unsupervised, domain-independent, and scalable manner. The paper presents an overview of KnowItAll's novel architecture and design principles, emphasizing its distinctive ability to extract information without any hand-labeled training examples. In its first major run, KnowItAll extracted over 50,000 class instances, but suggested a challenge: How can we improve KnowItAll's recall and extraction rate without sacrificing precision? This paper presents three distinct ways to address this challenge and evaluates their performance. Pattern Learning learns domain-specific extraction rules, which enable additional extractions. Subclass Extraction automatically identifies sub-classes in order to boost recall (e.g., ''chemist'' and ''biologist'' are identified as sub-classes of ''scientist''). List Extraction locates lists of class instances, learns a ''wrapper'' for each list, and extracts elements of each list. Since each method bootstraps from KnowItAll's domain-independent methods, the methods also obviate hand-labeled training examples. The paper reports on experiments, focused on building lists of named entities, that measure the relative efficacy of each method and demonstrate their synergy. In concert, our methods gave KnowItAll a 4-fold to 8-fold increase in recall at precision of 0.90, and discovered over 10,000 cities missing from the Tipster Gazetteer.\",\n",
       "  'date': '2005',\n",
       "  'authors': ['Oren Etzioni',\n",
       "   'Michael Cafarella',\n",
       "   'Doug Downey',\n",
       "   'Ana-Maria Popescu',\n",
       "   'Tal Shaked',\n",
       "   'Stephen Soderland',\n",
       "   'Daniel S. Weld',\n",
       "   'Alexander Yates'],\n",
       "  'related_topics': ['Information extraction',\n",
       "   'Unsupervised learning',\n",
       "   'Pointwise mutual information',\n",
       "   'Question answering',\n",
       "   'Class (computer programming)',\n",
       "   'Scalability',\n",
       "   'World Wide Web',\n",
       "   'Computer science',\n",
       "   'Bootstrapping (linguistics)',\n",
       "   'The Internet',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,450',\n",
       "  'reference_count': '54',\n",
       "  'references': ['3146306708',\n",
       "   '2048679005',\n",
       "   '2097089247',\n",
       "   '2140785063',\n",
       "   '2155328222',\n",
       "   '2168625136',\n",
       "   '1567365482',\n",
       "   '2068737686',\n",
       "   '2103931177',\n",
       "   '202303397']},\n",
       " {'id': '2144578941',\n",
       "  'title': 'Introduction to the CoNLL-2003 shared task: language-independent named entity recognition',\n",
       "  'abstract': 'We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.',\n",
       "  'date': '2003',\n",
       "  'authors': ['Erik F. Tjong Kim Sang', 'Fien De Meulder'],\n",
       "  'related_topics': ['Entity linking',\n",
       "   'Named-entity recognition',\n",
       "   'Task (project management)',\n",
       "   'Sequence labeling',\n",
       "   'German',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Background information',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,104',\n",
       "  'reference_count': '23',\n",
       "  'references': ['2141099517',\n",
       "   '1623072288',\n",
       "   '2056451646',\n",
       "   '1505083828',\n",
       "   '1522263329',\n",
       "   '2075635421',\n",
       "   '2915429162',\n",
       "   '2041614298',\n",
       "   '2004384146',\n",
       "   '1553244859']},\n",
       " {'id': '2128634885',\n",
       "  'title': 'Simple Semi-supervised Dependency Parsing',\n",
       "  'abstract': 'We present a simple and effective semisupervised method for training dependency parsers. We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus. We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions. For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuracy of 92.02% to 93.16%, and in the case of Czech unlabeled second-order parsing, we improve from a baseline accuracy of 86.13% to 87.13%. In addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.',\n",
       "  'date': '2008',\n",
       "  'authors': ['Terry Koo', 'Xavier Carreras', 'Michael Collins'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Parsing',\n",
       "   'Dependency grammar',\n",
       "   'Dependency (UML)',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Range (mathematics)',\n",
       "   'Computer science',\n",
       "   'Baseline (configuration management)',\n",
       "   'Focus (optics)',\n",
       "   'Czech',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '560',\n",
       "  'reference_count': '30',\n",
       "  'references': ['2147880316',\n",
       "   '1632114991',\n",
       "   '2008652694',\n",
       "   '1773803948',\n",
       "   '2121227244',\n",
       "   '2139621418',\n",
       "   '2116410915',\n",
       "   '2027979924',\n",
       "   '2122922578',\n",
       "   '194033037']},\n",
       " {'id': '1995945562',\n",
       "  'title': 'An introduction to the bootstrap',\n",
       "  'abstract': 'This article presents bootstrap methods for estimation, using simple arguments. Minitab macros for implementing these methods are given.',\n",
       "  'date': '1993',\n",
       "  'authors': ['Bradley Efron', 'Robert J'],\n",
       "  'related_topics': ['Bootstrap aggregating',\n",
       "   'Macro',\n",
       "   'Computer science',\n",
       "   'Data mining',\n",
       "   'Inference',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Simar',\n",
       "   'Bootstrap confidence interval',\n",
       "   'Nonparametric bootstrap',\n",
       "   'Sampling theory',\n",
       "   'View Less'],\n",
       "  'citation_count': '48,254',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1773803948',\n",
       "  'title': 'A Maximum Entropy Model for Part-Of-Speech Tagging',\n",
       "  'abstract': 'This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems',\n",
       "  'date': '1996',\n",
       "  'authors': ['Adwait'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Statistical model',\n",
       "   'Part of speech',\n",
       "   'Trigram tagger',\n",
       "   'Natural language processing',\n",
       "   'Consistency (database systems)',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Part-of-speech tagging',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,387',\n",
       "  'reference_count': '12',\n",
       "  'references': ['1632114991',\n",
       "   '2096175520',\n",
       "   '2121227244',\n",
       "   '2160842254',\n",
       "   '2153439141',\n",
       "   '2170381724',\n",
       "   '1718065290',\n",
       "   '2112861996',\n",
       "   '2015042937',\n",
       "   '2069912724']},\n",
       " {'id': '2962735828',\n",
       "  'title': 'Discriminative probabilistic models for relational data',\n",
       "  'abstract': 'In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.',\n",
       "  'date': '2002',\n",
       "  'authors': ['Ben Taskar', 'Pieter Abbeel', 'Daphne Koller'],\n",
       "  'related_topics': ['Bayesian network',\n",
       "   'Discriminative model',\n",
       "   'Relational database',\n",
       "   'Probabilistic logic',\n",
       "   'Supervised learning',\n",
       "   'Statistical model',\n",
       "   'Markov chain',\n",
       "   'Representation (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '829',\n",
       "  'reference_count': '19',\n",
       "  'references': ['2156909104',\n",
       "   '2147880316',\n",
       "   '2138621811',\n",
       "   '2159080219',\n",
       "   '2107008379',\n",
       "   '2160842254',\n",
       "   '2167044614',\n",
       "   '2076008912',\n",
       "   '2098678088',\n",
       "   '2126185296']},\n",
       " {'id': '2117400858',\n",
       "  'title': 'Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging',\n",
       "  'abstract': 'Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging.',\n",
       "  'date': '1995',\n",
       "  'authors': ['Eric Brill'],\n",
       "  'related_topics': ['Deep linguistic processing',\n",
       "   'Language identification',\n",
       "   'Brill tagger',\n",
       "   'Rule-based machine translation',\n",
       "   'Coh-Metrix',\n",
       "   'Natural language processing',\n",
       "   'Phrase chunking',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Encoding (memory)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,318',\n",
       "  'reference_count': '40',\n",
       "  'references': ['1594031697',\n",
       "   '2149706766',\n",
       "   '1632114991',\n",
       "   '2102381086',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '2081687495',\n",
       "   '1489181569',\n",
       "   '2046224275',\n",
       "   '2170381724']},\n",
       " {'id': '2140235142',\n",
       "  'title': 'Pfinder: real-time tracking of the human body',\n",
       "  'abstract': 'Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10 Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding.',\n",
       "  'date': '1997',\n",
       "  'authors': ['C.R. Wren', 'A. Azarbayejani', 'T. Darrell', 'A.P. Pentland'],\n",
       "  'related_topics': ['Gesture recognition',\n",
       "   'Foreground detection',\n",
       "   'Background subtraction',\n",
       "   'Data compression',\n",
       "   'Image segmentation',\n",
       "   'Statistical model',\n",
       "   'Computer vision',\n",
       "   'Real-time operating system',\n",
       "   'Segmentation',\n",
       "   'Virtual reality',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,660',\n",
       "  'reference_count': '14',\n",
       "  'references': ['2157548127',\n",
       "   '2069356045',\n",
       "   '2144573889',\n",
       "   '1635989058',\n",
       "   '2119444142',\n",
       "   '2131938593',\n",
       "   '2042850474',\n",
       "   '2178278515',\n",
       "   '2140487300',\n",
       "   '2034836133']},\n",
       " {'id': '2099244020',\n",
       "  'title': 'Bilateral filtering for gray and color images',\n",
       "  'abstract': 'Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.',\n",
       "  'date': '1998',\n",
       "  'authors': ['C. Tomasi', 'R. Manduchi'],\n",
       "  'related_topics': ['Color histogram',\n",
       "   'Color balance',\n",
       "   'Color quantization',\n",
       "   'Color space',\n",
       "   'RGB color model',\n",
       "   'Color image',\n",
       "   'Bilateral filter',\n",
       "   'Edge-preserving smoothing',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '10,272',\n",
       "  'reference_count': '15',\n",
       "  'references': ['2150134853',\n",
       "   '1667165204',\n",
       "   '2101248405',\n",
       "   '2104763670',\n",
       "   '2064347832',\n",
       "   '2067681708',\n",
       "   '2008014451',\n",
       "   '1526351017',\n",
       "   '2051826135',\n",
       "   '1602550945']},\n",
       " {'id': '2132549764',\n",
       "  'title': 'Statistical pattern recognition: a review',\n",
       "  'abstract': 'The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field.',\n",
       "  'date': '2000',\n",
       "  'authors': ['A.K. Jain', 'R.P.W. Duin', 'Jianchang Mao'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Intelligent character recognition',\n",
       "   'Feature extraction',\n",
       "   'Statistical learning theory',\n",
       "   'Facial recognition system',\n",
       "   'Cluster analysis',\n",
       "   'Artificial neural network',\n",
       "   'Feature selection',\n",
       "   'Classifier (UML)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '8,673',\n",
       "  'reference_count': '178',\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2124776405',\n",
       "   '1554663460',\n",
       "   '2139212933',\n",
       "   '2912934387',\n",
       "   '2117812871',\n",
       "   '1548802052',\n",
       "   '2125838338',\n",
       "   '1679913846']},\n",
       " {'id': '2159128898',\n",
       "  'title': 'Real-time tracking of non-rigid objects using mean shift',\n",
       "  'abstract': 'A new method for real time tracking of non-rigid objects seen from a moving camera is proposed. The central computational module is based on the mean shift iterations and finds the most probable target position in the current frame. The dissimilarity between the target model (its color distribution) and the target candidates is expressed by a metric derived from the Bhattacharyya coefficient. The theoretical analysis of the approach shows that it relates to the Bayesian framework while providing a practical, fast and efficient solution. The capability of the tracker to handle in real time partial occlusions, significant clutter, and target scale variations, is demonstrated for several image sequences.',\n",
       "  'date': '2000',\n",
       "  'authors': ['D. Comaniciu', 'V. Ramesh', 'P.'],\n",
       "  'related_topics': ['Bhattacharyya distance',\n",
       "   'Mean-shift',\n",
       "   'Condensation algorithm',\n",
       "   'Metric (mathematics)',\n",
       "   'Clutter',\n",
       "   'Kernel (image processing)',\n",
       "   'Position (vector)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,826',\n",
       "  'reference_count': '24',\n",
       "  'references': ['2140235142',\n",
       "   '2161406034',\n",
       "   '2914885528',\n",
       "   '1874027545',\n",
       "   '1964443764',\n",
       "   '1687797484',\n",
       "   '204885769',\n",
       "   '2168682262',\n",
       "   '2135346934',\n",
       "   '2033009866']},\n",
       " {'id': '1971784203',\n",
       "  'title': 'Algorithms for clustering data',\n",
       "  'abstract': '',\n",
       "  'date': '1988',\n",
       "  'authors': ['Anil K. Jain', 'Richard C. Dubes'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Correlation clustering',\n",
       "   'CURE data clustering algorithm',\n",
       "   'Fuzzy clustering',\n",
       "   'Biclustering',\n",
       "   'Single-linkage clustering',\n",
       "   'Canopy clustering algorithm',\n",
       "   'Determining the number of clusters in a data set',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '15,516',\n",
       "  'reference_count': '16',\n",
       "  'references': ['2913066018',\n",
       "   '1975152892',\n",
       "   '2118587067',\n",
       "   '1572134371',\n",
       "   '1576534100',\n",
       "   '1533790012',\n",
       "   '2018388286',\n",
       "   '2086943813',\n",
       "   '1978616828',\n",
       "   '2120636855']},\n",
       " {'id': '2129905273',\n",
       "  'title': 'Density estimation for statistics and data analysis',\n",
       "  'abstract': 'Introduction. Survey of Existing Methods. The Kernel Method for Univariate Data. The Kernel Method for Multivariate Data. Three Important Methods. Density Estimation in Action.',\n",
       "  'date': '1986',\n",
       "  'authors': ['Bernard. W.'],\n",
       "  'related_topics': ['Multivariate kernel density estimation',\n",
       "   'Variable kernel density estimation',\n",
       "   'Kernel (statistics)',\n",
       "   'Kernel density estimation',\n",
       "   'Density estimation',\n",
       "   'Univariate',\n",
       "   'Kernel method',\n",
       "   'Kernel Bandwidth',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'View Less'],\n",
       "  'citation_count': '27,525',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2999729612',\n",
       "  'title': 'Finding Groups in Data: An Introduction to Cluster Analysis',\n",
       "  'abstract': '1. Introduction. 2. Partitioning Around Medoids (Program PAM). 3. Clustering large Applications (Program CLARA). 4. Fuzzy Analysis. 5. Agglomerative Nesting (Program AGNES). 6. Divisive Analysis (Program DIANA). 7. Monothetic Analysis (Program MONA). Appendix 1. Implementation and Structure of the Programs. Appendix 2. Running the Programs. Appendix 3. Adapting the Programs to Your Needs. Appendix 4. The Program CLUSPLOT. References. Author Index. Subject Index.',\n",
       "  'date': '1990',\n",
       "  'authors': ['Leonard Kaufman', 'Peter J. Rousseeuw'],\n",
       "  'related_topics': ['Medoid',\n",
       "   'Dunn index',\n",
       "   'Cluster analysis',\n",
       "   'Nesting (computing)',\n",
       "   'Hierarchical clustering',\n",
       "   'OPTICS algorithm',\n",
       "   'Index (publishing)',\n",
       "   'k-medoids',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'View Less'],\n",
       "  'citation_count': '20,559',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2482402870',\n",
       "  'title': 'Statistical Pattern Recognition',\n",
       "  'abstract': 'The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical ap...',\n",
       "  'date': '2000',\n",
       "  'authors': ['K', 'P W', ''],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Feature extraction',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Cluster analysis',\n",
       "   'Artificial neural network',\n",
       "   'Feature selection',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Statistical pattern',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,328',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '182831726',\n",
       "  'title': 'Speech and Language Processing',\n",
       "  'abstract': 'is one of the most recognizablecharacters in 20th century cinema. HAL is an artiﬁcial agent capable of such advancedlanguage behavior as speaking and understanding English, and at a crucial moment inthe plot, even reading lips. It is now clear that HAL’s creator, Arthur C. Clarke, wasa little optimistic in predicting when an artiﬁcial agent such as HAL would be avail-able. But just how far off was he? What would it take to create at least the language-relatedpartsofHAL?WecallprogramslikeHALthatconversewithhumansinnatural',\n",
       "  'date': '1999',\n",
       "  'authors': ['Dan', 'James H.'],\n",
       "  'related_topics': ['Cued speech',\n",
       "   'Speech processing',\n",
       "   'Language technology',\n",
       "   'Speech technology',\n",
       "   'Telegraphic speech',\n",
       "   'Universal Networking Language',\n",
       "   'Reading (process)',\n",
       "   'Natural language',\n",
       "   'Linguistics',\n",
       "   'Psychology',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,393',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1994851566',\n",
       "  'title': 'Statistical Language Learning',\n",
       "  'abstract': 'From the Publisher:\\r\\nEugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background.\\r\\nNew, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises.\\r\\nCharniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: \"one simply gathers statistics.\"\\r\\nLanguage, Speech, and Communication',\n",
       "  'date': '1994',\n",
       "  'authors': ['Eugene'],\n",
       "  'related_topics': ['Language identification',\n",
       "   'Cache language model',\n",
       "   'Computational linguistics',\n",
       "   'Machine translation',\n",
       "   'Language technology',\n",
       "   'Grammar induction',\n",
       "   'Parsing',\n",
       "   'Knowledge representation and reasoning',\n",
       "   'Natural language processing',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,726',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2108321481',\n",
       "  'title': 'Exploiting Syntactic Structure for Language Modeling',\n",
       "  'abstract': 'The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Ciprian Chelba', 'Frederick Jelinek'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Trigram',\n",
       "   'Headword',\n",
       "   'Probabilistic logic',\n",
       "   'Word (computer architecture)',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Sequence',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Syntactic structure',\n",
       "   'View Less'],\n",
       "  'citation_count': '254',\n",
       "  'reference_count': '7',\n",
       "  'references': ['2049633694',\n",
       "   '1632114991',\n",
       "   '2110882317',\n",
       "   '1597533204',\n",
       "   '2069912724',\n",
       "   '1607229519',\n",
       "   '1606548921']},\n",
       " {'id': '1549026077',\n",
       "  'title': 'Natural Language Understanding',\n",
       "  'abstract': 'From the Publisher:\\r\\nIn addition, this title offers coverage of two entirely new subject areas. First, the text features a new chapter on statistically-based methods using large corpora. Second, it includes an appendix on speech recognition and spoken language understanding. Also, the information on semantics that was covered in the first edition has been largely expanded in this edition to include an emphasis on compositional interpretation.',\n",
       "  'date': '1987',\n",
       "  'authors': ['James Allen'],\n",
       "  'related_topics': ['Spoken language',\n",
       "   'Language identification',\n",
       "   'Comprehension approach',\n",
       "   'Natural language understanding',\n",
       "   'Semantics',\n",
       "   'Linguistics',\n",
       "   'Interpretation (philosophy)',\n",
       "   'Natural language processing',\n",
       "   'Emphasis (typography)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Subject areas',\n",
       "   'View Less'],\n",
       "  'citation_count': '4,069',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2949237929',\n",
       "  'title': 'Expoiting Syntactic Structure for Language Modeling',\n",
       "  'abstract': 'The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words--binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.',\n",
       "  'date': '1998',\n",
       "  'authors': ['Ciprian', 'Frederick Jelinek'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Trigram',\n",
       "   'Headword',\n",
       "   'Probabilistic logic',\n",
       "   'Word (computer architecture)',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Sequence',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Syntactic structure',\n",
       "   'View Less'],\n",
       "  'citation_count': '203',\n",
       "  'reference_count': '5',\n",
       "  'references': ['2049633694',\n",
       "   '1632114991',\n",
       "   '2110882317',\n",
       "   '1607229519',\n",
       "   '1606548921']},\n",
       " {'id': '1795234945',\n",
       "  'title': 'Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning',\n",
       "  'abstract': \"This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word ``line'' using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.\",\n",
       "  'date': '1996',\n",
       "  'authors': ['Raymond J.'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Instance-based learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Online machine learning',\n",
       "   'Learning classifier system',\n",
       "   'Semi-supervised learning',\n",
       "   'Computational learning theory',\n",
       "   'Stability (learning theory)',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '315',\n",
       "  'reference_count': '47',\n",
       "  'references': ['2154642048',\n",
       "   '1632114991',\n",
       "   '3017143921',\n",
       "   '2147169507',\n",
       "   '2099247782',\n",
       "   '1994851566',\n",
       "   '2136000097',\n",
       "   '2132166479',\n",
       "   '1999138184',\n",
       "   '2132513611']},\n",
       " {'id': '1746620543',\n",
       "  'title': 'Text Information Retrieval Systems',\n",
       "  'abstract': 'From the Publisher:\\r\\nThis book\\'s purpose is to teach people who will be searching or designing text retrieval systems how the systems work. For designers, it covers problems they will face and reviews currently available solutions to provide a basis for more advanced study. For the searcher its purpose is to describe why such systems work as they do. The book is primarily about computer-based retrieval systems, but the principles apply to nonmechanized ones as well.. \"The book covers the nature of information, how it is organized for use by a computer, how search functions are carried out, and some of the theory underlying these functions. As well, it discusses the interaction between user and system and how retrieved items, users, and complete systems are evaluated. A limited knowledge of mathematics and of computing is assumed.',\n",
       "  'date': '1992',\n",
       "  'authors': ['Charles T.', 'Donald H.', 'Bert R.'],\n",
       "  'related_topics': ['Information retrieval',\n",
       "   'Computer science',\n",
       "   'Face (sociological concept)',\n",
       "   'Work (electrical)',\n",
       "   'Basis (linear algebra)',\n",
       "   'Text retrieval',\n",
       "   'View Less'],\n",
       "  'citation_count': '684',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '1736036918',\n",
       "  'title': 'Information Retrieval: A Health Care Perspective',\n",
       "  'abstract': 'As the health care industry becomes increasingly dependent on electronic information, the need for sophisticated information retrieval systems and for knowledgeable people to design, purchase and use them also increases. This book provides an overview of the theory, practical applications, evaluation and research directions of these systems. In addition to bibliographic and full-text literature retrieval, the author discusses clinical records, multimedia and networked applications.',\n",
       "  'date': '2013',\n",
       "  'authors': ['William R. Hersh'],\n",
       "  'related_topics': ['Human–computer information retrieval',\n",
       "   'Cognitive models of information retrieval',\n",
       "   'Relevance (information retrieval)',\n",
       "   'Health care',\n",
       "   'Perspective (graphical)',\n",
       "   'Information retrieval',\n",
       "   'Medicine',\n",
       "   'Clinical record',\n",
       "   'Electronic information',\n",
       "   'View Less'],\n",
       "  'citation_count': '126',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " {'id': '2117812871',\n",
       "  'title': 'Pattern recognition and neural networks',\n",
       "  'abstract': 'From the Publisher:\\r\\nPattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.',\n",
       "  'date': '1996',\n",
       "  'authors': ['Brian D. Ripley', 'N. L.'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Intelligent character recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Neocognitron',\n",
       "   'Cellular neural network',\n",
       "   'Artificial neural network',\n",
       "   'Relation (database)',\n",
       "   'Computer science',\n",
       "   'Pattern recognition',\n",
       "   'Subject (documents)',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '9,210',\n",
       "  'reference_count': '102',\n",
       "  'references': ['2156909104',\n",
       "   '1554663460',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '1679913846',\n",
       "   '2112076978',\n",
       "   '2046079134',\n",
       "   '2147800946',\n",
       "   '1536929369']},\n",
       " {'id': '2128716185',\n",
       "  'title': 'Probabilistic visual learning for object representation',\n",
       "  'abstract': 'We present an unsupervised technique for visual learning, which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a mixture-of-Gaussians model (for multimodal distributions). Those probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and nonrigid objects, such as hands.',\n",
       "  'date': '1997',\n",
       "  'authors': ['B. Moghaddam', 'A. Pentland'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Visual modeling',\n",
       "   'Visual learning',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Face detection',\n",
       "   'Object detection',\n",
       "   'Density estimation',\n",
       "   'Visual search',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,338',\n",
       "  'reference_count': '38',\n",
       "  'references': ['2099111195',\n",
       "   '2138451337',\n",
       "   '2798909945',\n",
       "   '2104095591',\n",
       "   '2148694408',\n",
       "   '2049633694',\n",
       "   '2159686933',\n",
       "   '2123977795',\n",
       "   '2098947662',\n",
       "   '2113341759']},\n",
       " {'id': '2012352340',\n",
       "  'title': 'Using discriminant eigenfeatures for image retrieval',\n",
       "  'abstract': 'This paper describes the automatic selection of features from an image training set using the theories of multidimensional discriminant analysis and the associated optimal linear projection. We demonstrate the effectiveness of these most discriminating features for view-based class retrieval from a large database of widely varying real-world objects presented as \"well-framed\" views, and compare it with that of the principal component analysis.',\n",
       "  'date': '1996',\n",
       "  'authors': ['D.L. Swets', 'J.J. Weng'],\n",
       "  'related_topics': ['Linear discriminant analysis',\n",
       "   'Optimal discriminant analysis',\n",
       "   'Content-based image retrieval',\n",
       "   'Image retrieval',\n",
       "   'Facial recognition system',\n",
       "   'Principal component analysis',\n",
       "   'Feature selection',\n",
       "   'Discriminant',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set',\n",
       "   'View Less'],\n",
       "  'citation_count': '2,329',\n",
       "  'reference_count': '20',\n",
       "  'references': ['2138451337',\n",
       "   '2148694408',\n",
       "   '2098947662',\n",
       "   '1770825568',\n",
       "   '2135463994',\n",
       "   '2159173611',\n",
       "   '2135346934',\n",
       "   '2101055828',\n",
       "   '2122111042',\n",
       "   '2798461040']},\n",
       " {'id': '2130259898',\n",
       "  'title': 'Low-dimensional procedure for the characterization of human faces',\n",
       "  'abstract': 'A method is presented for the representation of (pictures of) faces. Within a specified framework the representation is ideal. This results in the characterization of a face, to within an error bound, by a relatively low-dimensional vector. The method is illustrated in detail by the use of an ensemble of pictures taken for this purpose.',\n",
       "  'date': '1987',\n",
       "  'authors': ['L. Sirovich', 'M. Kirby'],\n",
       "  'related_topics': ['Face (geometry)',\n",
       "   'Eigenface',\n",
       "   'Representation (mathematics)',\n",
       "   'Ideal (set theory)',\n",
       "   'Facial recognition system',\n",
       "   'Image processing',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Algorithm',\n",
       "   'Fourier transform',\n",
       "   'Computer science',\n",
       "   'Optics',\n",
       "   'View Less'],\n",
       "  'citation_count': '3,427',\n",
       "  'reference_count': '4',\n",
       "  'references': ['2135346934', '1587863748', '1535031115', '3040267042']},\n",
       " {'id': '2156406284',\n",
       "  'title': 'Recognition-by-Components: A Theory of Human Image Understanding.',\n",
       "  'abstract': \"The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components, such as blocks, cylinders, wedges, and cones. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons (N £ 36), can be derived from contrasts of five readily detectable properties of edges in a two-dimensiona l image: curvature, collinearity, symmetry, parallelism, and cotermination. The detection of these properties is generally invariant over viewing position an$ image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition: The constraints toward regularization (Pragnanz) characterize not the complete object but the object's components. Representational power derives from an allowance of free combinations of the geons. A Principle of Componential Recovery can account for the major phenomena of object recognition: If an arrangement of two or three geons can be recovered from the input, objects can be quickly recognized even when they are occluded, novel, rotated in depth, or extensively degraded. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. Any single object can project an infinity of image configurations to the retina. The orientation of the object to the viewer can vary continuously, each giving rise to a different two-dimensional projection. The object can be occluded by other objects or texture fields, as when viewed behind foliage. The object need not be presented as a full-colored textured image but instead can be a simplified line drawing. Moreover, the object can even be missing some of its parts or be a novel exemplar of its particular category. But it is only with rare exceptions that an image fails to be rapidly and readily classified, either as an instance of a familiar object category or as an instance that cannot be so classified (itself a form of classification).\",\n",
       "  'date': '1987',\n",
       "  'authors': ['Irving Biederman'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Form perception',\n",
       "   'Object model',\n",
       "   'Structural information theory',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Object permanence',\n",
       "   'Image quality',\n",
       "   'Perceptual Closure',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '7,598',\n",
       "  'reference_count': '73',\n",
       "  'references': ['2740373864',\n",
       "   '2149095485',\n",
       "   '2059975159',\n",
       "   '2073257493',\n",
       "   '1513966746',\n",
       "   '2059799772',\n",
       "   '2032533296',\n",
       "   '2125756925',\n",
       "   '1501418839',\n",
       "   '2081519360']},\n",
       " {'id': '1524408959',\n",
       "  'title': 'Blobworld: A System for Region-Based Image Indexing and Retrieval',\n",
       "  'abstract': 'Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (\"blobs\") with associated color and texture descriptors. Queryingi s based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions usinga tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both queryinga nd indexing.',\n",
       "  'date': '1999',\n",
       "  'authors': ['Chad Carson',\n",
       "   'Megan Thomas',\n",
       "   'Serge Belongie',\n",
       "   'Joseph M. Hellerstein',\n",
       "   'Jitendra Malik'],\n",
       "  'related_topics': ['Image texture',\n",
       "   'Visual Word',\n",
       "   'Automatic image annotation',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image retrieval',\n",
       "   'Image processing',\n",
       "   'Color histogram',\n",
       "   'Search engine indexing',\n",
       "   'Feature vector',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,246',\n",
       "  'reference_count': '17',\n",
       "  'references': ['2049633694',\n",
       "   '2160066518',\n",
       "   '2151135734',\n",
       "   '2008297189',\n",
       "   '2238624099',\n",
       "   '1533169541',\n",
       "   '2134814621',\n",
       "   '2118783153',\n",
       "   '2068272887',\n",
       "   '2102475035']},\n",
       " {'id': '2180838288',\n",
       "  'title': 'What is the goal of sensory coding',\n",
       "  'abstract': 'A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a \"compact\" coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal compact code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of \"sparse distributed\" coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling \"wavelet transforms\" are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented.',\n",
       "  'date': '1999',\n",
       "  'authors': ['David J.'],\n",
       "  'related_topics': ['Neural coding',\n",
       "   'Code (cryptography)',\n",
       "   'Wavelet',\n",
       "   'Redundancy (engineering)',\n",
       "   'Wavelet transform',\n",
       "   'Histogram',\n",
       "   'Coding (social sciences)',\n",
       "   'Kurtosis',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'View Less'],\n",
       "  'citation_count': '1,664',\n",
       "  'reference_count': '0',\n",
       "  'references': []},\n",
       " ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "peripheral-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Greg Yang': 0,\n",
       " 'Linda Wang': 1,\n",
       " 'Zhong Qiu Lin': 2,\n",
       " 'Alexander Wong': 3,\n",
       " 'Han Zhang': 4,\n",
       " 'Ian Goodfellow': 5,\n",
       " 'Dimitris Metaxas': 6,\n",
       " 'Augustus Odena': 7,\n",
       " 'Alexey Dosovitskiy': 8,\n",
       " 'Lucas Beyer': 9,\n",
       " 'Alexander Kolesnikov': 10,\n",
       " 'Dirk Weissenborn': 11,\n",
       " 'Xiaohua Zhai': 12,\n",
       " 'Thomas Unterthiner': 13,\n",
       " 'Mostafa Dehghani': 14,\n",
       " 'Matthias Minderer': 15,\n",
       " 'Georg Heigold': 16,\n",
       " 'Sylvain Gelly': 17,\n",
       " 'Jakob Uszkoreit': 18,\n",
       " 'Neil Houlsby': 19,\n",
       " 'Volodymyr Mnih': 20,\n",
       " 'Koray Kavukcuoglu': 21,\n",
       " 'David Silver': 22,\n",
       " 'Andrei A. Rusu': 23,\n",
       " 'Joel Veness': 24,\n",
       " 'Marc G. Bellemare': 25,\n",
       " 'Alex Graves': 26,\n",
       " 'Martin Riedmiller': 27,\n",
       " 'Andreas K. Fidjeland': 28,\n",
       " 'Georg Ostrovski': 29,\n",
       " 'Stig Petersen': 30,\n",
       " 'Charles Beattie': 31,\n",
       " 'Amir Sadik': 32,\n",
       " 'Ioannis Antonoglou': 33,\n",
       " 'Helen King': 34,\n",
       " 'Dharshan Kumaran': 35,\n",
       " 'Daan Wierstra': 36,\n",
       " 'Shane Legg': 37,\n",
       " 'Demis Hassabis': 38,\n",
       " 'Tomas Mikolov': 39,\n",
       " 'Ilya Sutskever': 40,\n",
       " 'Kai Chen': 41,\n",
       " 'Greg S Corrado': 42,\n",
       " 'Jeff Dean': 43,\n",
       " 'Kaiming He': 44,\n",
       " 'Xiangyu Zhang': 45,\n",
       " 'Shaoqing Ren': 46,\n",
       " 'Jian Sun': 47,\n",
       " 'Ashish Vaswani': 48,\n",
       " 'Noam Shazeer': 49,\n",
       " 'Niki Parmar': 50,\n",
       " 'Llion Jones': 51,\n",
       " 'Aidan N. Gomez': 52,\n",
       " 'Lukasz Kaiser': 53,\n",
       " 'Illia Polosukhin': 54,\n",
       " 'Sergey Ioffe': 55,\n",
       " 'Christian Szegedy': 56,\n",
       " 'Dzmitry Bahdanau': 57,\n",
       " 'Kyunghyun Cho': 58,\n",
       " 'Yoshua Bengio': 59,\n",
       " 'Gao Huang': 60,\n",
       " 'Zhuang Liu': 61,\n",
       " 'Laurens van der Maaten': 62,\n",
       " 'Kilian Q. Weinberger': 63,\n",
       " 'Bart van Merrienboer': 64,\n",
       " 'Caglar Gulcehre': 65,\n",
       " 'Fethi Bougares': 66,\n",
       " 'Holger Schwenk': 67,\n",
       " '': 68,\n",
       " 'Yann Lecun': 69,\n",
       " 'Leon Bottou': 70,\n",
       " 'Patrick Haffner': 71,\n",
       " 'Sepp Hochreiter': 72,\n",
       " 'Jürgen Schmidhuber': 73,\n",
       " 'Xavier Glorot': 74,\n",
       " 'Chaolin Huang': 75,\n",
       " 'Yeming Wang': 76,\n",
       " 'Xingwang Li': 77,\n",
       " 'Lili Ren': 78,\n",
       " 'Jianping Zhao': 79,\n",
       " 'Yi Hu': 80,\n",
       " 'Li Zhang': 81,\n",
       " 'Guohui Fan': 82,\n",
       " 'Jiuyang Xu': 83,\n",
       " 'Xiaoying Gu': 84,\n",
       " 'Zhenshun Cheng': 85,\n",
       " 'Ting Yu': 86,\n",
       " 'Jiaan Xia': 87,\n",
       " 'Yuan Wei': 88,\n",
       " 'Wenjuan Wu': 89,\n",
       " 'Xuelei Xie': 90,\n",
       " 'Wen Yin': 91,\n",
       " 'Hui Li': 92,\n",
       " 'Min Liu': 93,\n",
       " 'Yan Xiao': 94,\n",
       " 'Hong Gao': 95,\n",
       " 'Li Guo': 96,\n",
       " 'Jungang Xie': 97,\n",
       " 'Guangfa Wang': 98,\n",
       " 'Rongmeng Jiang': 99,\n",
       " 'Zhancheng Gao': 100,\n",
       " 'Qi Jin': 101,\n",
       " 'Jianwei Wang': 102,\n",
       " 'Bin Cao': 103,\n",
       " 'Karen Simonyan': 104,\n",
       " 'Andrew Zisserman': 105,\n",
       " 'Wei-jie Guan': 106,\n",
       " 'Zheng-yi Ni': 107,\n",
       " 'Yu Hu': 108,\n",
       " 'Wenhua Liang': 109,\n",
       " 'Chun-quan Ou': 110,\n",
       " 'Jianxing He': 111,\n",
       " 'Lei Liu': 112,\n",
       " 'Hong Shan': 113,\n",
       " 'Chunliang Lei': 114,\n",
       " 'David S.C. Hui': 115,\n",
       " 'Bin Du': 116,\n",
       " 'Lan-juan Li': 117,\n",
       " 'Guang Zeng': 118,\n",
       " 'Kwok-Yung Yuen': 119,\n",
       " 'Ruchong Chen': 120,\n",
       " 'Chun-Li Tang': 121,\n",
       " 'Tao Wang': 122,\n",
       " 'Ping-yan Chen': 123,\n",
       " 'Jie Xiang': 124,\n",
       " 'Shiyue Li': 125,\n",
       " 'Jinlin Wang': 126,\n",
       " 'Zi Jing Liang': 127,\n",
       " 'Yi-xiang Peng': 128,\n",
       " 'Li Wei': 129,\n",
       " 'Yong Liu': 130,\n",
       " 'Ya-hua Hu': 131,\n",
       " 'Peng Peng': 132,\n",
       " 'Jian-ming Wang': 133,\n",
       " 'Ji-yang Liu': 134,\n",
       " 'Zhong Chen': 135,\n",
       " 'Gang Li': 136,\n",
       " 'Zhi-jian Zheng': 137,\n",
       " 'Shao-qin Qiu': 138,\n",
       " 'Jie Luo': 139,\n",
       " 'Chang-jiang Ye': 140,\n",
       " 'Shao-yong Zhu': 141,\n",
       " 'Nanshan Zhong': 142,\n",
       " 'Yann LeCun': 143,\n",
       " 'Geoffrey Hinton': 144,\n",
       " 'Jia Deng': 145,\n",
       " 'Wei Dong': 146,\n",
       " 'Richard Socher': 147,\n",
       " 'Li-Jia Li': 148,\n",
       " 'Kai Li': 149,\n",
       " 'Li Fei-Fei': 150,\n",
       " 'Tao Ai': 151,\n",
       " 'Zhenlu Yang': 152,\n",
       " 'Hongyan Hou': 153,\n",
       " 'Chenao Zhan': 154,\n",
       " 'Chong Chen': 155,\n",
       " 'Wenzhi Lv': 156,\n",
       " 'Qian Tao': 157,\n",
       " 'Ziyong Sun': 158,\n",
       " 'Liming Xia': 159,\n",
       " 'Wenling Wang': 160,\n",
       " 'Yanli Xu': 161,\n",
       " 'Ruqin Gao': 162,\n",
       " 'Roujian Lu': 163,\n",
       " 'Kai Han': 164,\n",
       " 'Guizhen Wu': 165,\n",
       " 'Wenjie Tan': 166,\n",
       " 'Yicheng Fang': 167,\n",
       " 'Huangqi Zhang': 168,\n",
       " 'Jicheng Xie': 169,\n",
       " 'Minjie Lin': 170,\n",
       " 'Lingjun Ying': 171,\n",
       " 'Peipei Pang': 172,\n",
       " 'Wenbin Ji': 173,\n",
       " 'Diederik P. Kingma': 174,\n",
       " 'Jimmy Lei Ba': 175,\n",
       " 'Olga Russakovsky': 176,\n",
       " 'Hao Su': 177,\n",
       " 'Jonathan Krause': 178,\n",
       " 'Sanjeev Satheesh': 179,\n",
       " 'Sean Ma': 180,\n",
       " 'Zhiheng Huang': 181,\n",
       " 'Andrej Karpathy': 182,\n",
       " 'Aditya Khosla': 183,\n",
       " 'Michael Bernstein': 184,\n",
       " 'Alexander C. Berg': 185,\n",
       " 'Jean Pouget-Abadie': 186,\n",
       " 'Mehdi Mirza': 187,\n",
       " 'Bing Xu': 188,\n",
       " 'David Warde-Farley': 189,\n",
       " 'Sherjil Ozair': 190,\n",
       " 'Aaron Courville': 191,\n",
       " 'Phillip Isola': 192,\n",
       " 'Jun-Yan Zhu': 193,\n",
       " 'Tinghui Zhou': 194,\n",
       " 'Alexei A. Efros': 195,\n",
       " 'Taesung Park': 196,\n",
       " 'Alec Radford': 197,\n",
       " 'Luke Metz': 198,\n",
       " 'Soumith Chintala': 199,\n",
       " 'Tim Salimans': 200,\n",
       " 'Wojciech Zaremba': 201,\n",
       " 'Vicki': 202,\n",
       " 'Xi Chen': 203,\n",
       " 'Christian Ledig': 204,\n",
       " 'Lucas Theis': 205,\n",
       " 'Ferenc Huszar': 206,\n",
       " 'Jose Caballero': 207,\n",
       " 'Andrew': 208,\n",
       " 'Alejandro Acosta': 209,\n",
       " 'Andrew Aitken': 210,\n",
       " 'Alykhan Tejani': 211,\n",
       " 'Johannes Totz': 212,\n",
       " 'Zehan Wang': 213,\n",
       " 'Wenzhe Shi': 214,\n",
       " 'Alex Krizhevsky': 215,\n",
       " 'Geoffrey E. Hinton': 216,\n",
       " 'Jacob Devlin': 217,\n",
       " 'Ming-Wei Chang': 218,\n",
       " 'Kenton Lee': 219,\n",
       " 'Kristina N. Toutanova': 220,\n",
       " 'Xiaolong Wang': 221,\n",
       " 'Ross Girshick': 222,\n",
       " 'Abhinav Gupta': 223,\n",
       " 'Ting Chen': 224,\n",
       " 'Simon Kornblith': 225,\n",
       " 'Mohammad Norouzi': 226,\n",
       " 'G. E. Hinton': 227,\n",
       " 'R. R. Salakhutdinov': 228,\n",
       " 'Laurens van der': 229,\n",
       " 'Geoffrey': 230,\n",
       " 'Vinod Nair': 231,\n",
       " 'Kevin Jarrett': 232,\n",
       " \"Marc'Aurelio Ranzato\": 233,\n",
       " 'R.S. Sutton': 234,\n",
       " 'A.G.': 235,\n",
       " 'Yavar Naddaf': 236,\n",
       " 'Michael Bowling': 237,\n",
       " 'David E. Rumelhart': 238,\n",
       " 'James L. McClelland': 239,\n",
       " 'Greg S. Corrado': 240,\n",
       " 'Jeffrey Dean': 241,\n",
       " 'Wen-tau Yih': 242,\n",
       " 'Geoffrey Zweig': 243,\n",
       " 'Ronan Collobert': 244,\n",
       " 'Jason Weston': 245,\n",
       " 'Réjean Ducharme': 246,\n",
       " 'Pascal Vincent': 247,\n",
       " 'Christian Janvin': 248,\n",
       " 'Joseph Turian': 249,\n",
       " 'Lev-Arie Ratinov': 250,\n",
       " 'Cliff C. Lin': 251,\n",
       " 'Chris Manning': 252,\n",
       " 'Andrew Y. Ng': 253,\n",
       " 'Ronald J. Williams': 254,\n",
       " 'Peter D. Turney': 255,\n",
       " 'Patrick Pantel': 256,\n",
       " 'Brody Huval': 257,\n",
       " 'Christopher D. Manning': 258,\n",
       " 'Andriy Mnih': 259,\n",
       " 'Wei Liu': 260,\n",
       " 'Yangqing Jia': 261,\n",
       " 'Pierre Sermanet': 262,\n",
       " 'Scott Reed': 263,\n",
       " 'Dragomir Anguelov': 264,\n",
       " 'Dumitru Erhan': 265,\n",
       " 'Vincent Vanhoucke': 266,\n",
       " 'Andrew Rabinovich': 267,\n",
       " 'Jeff Donahue': 268,\n",
       " 'Trevor Darrell': 269,\n",
       " 'Jitendra Malik': 270,\n",
       " 'Jonathan Long': 271,\n",
       " 'Evan Shelhamer': 272,\n",
       " 'Sergey Karayev': 273,\n",
       " 'Sergio Guadarrama': 274,\n",
       " 'Nitish Srivastava': 275,\n",
       " 'Ruslan Salakhutdinov': 276,\n",
       " 'John Duchi': 277,\n",
       " 'Elad Hazan': 278,\n",
       " 'Yoram Singer': 279,\n",
       " 'Greg Corrado': 280,\n",
       " 'Rajat Monga': 281,\n",
       " 'Matthieu Devin': 282,\n",
       " 'Mark Mao': 283,\n",
       " \"Marc'aurelio Ranzato\": 284,\n",
       " 'Andrew Senior': 285,\n",
       " 'Paul Tucker': 286,\n",
       " 'Ke Yang': 287,\n",
       " 'Quoc V. Le': 288,\n",
       " 'James Martens': 289,\n",
       " 'George Dahl': 290,\n",
       " 'Matthew D.': 291,\n",
       " 'Nal Kalchbrenner': 292,\n",
       " 'Phil Blunsom': 293,\n",
       " 'Razvan Pascanu': 294,\n",
       " 'Philipp Koehn': 295,\n",
       " 'Franz Josef Och': 296,\n",
       " 'Daniel Marcu': 297,\n",
       " 'Jon Shlens': 298,\n",
       " 'Zbigniew Wojna': 299,\n",
       " 'G. E. Dahl': 300,\n",
       " 'Dong Yu': 301,\n",
       " 'Li Deng': 302,\n",
       " 'A. Acero': 303,\n",
       " 'Antoine Bordes': 304,\n",
       " 'Andrew M. Saxe': 305,\n",
       " 'Surya Ganguli': 306,\n",
       " 'Y. Bengio': 307,\n",
       " 'P. Simard': 308,\n",
       " 'P. Frasconi': 309,\n",
       " 'Michael C Mozer': 310,\n",
       " 'Fernando J. Pineda': 311,\n",
       " 'Sepp': 312,\n",
       " 'G.V. Puskorius': 313,\n",
       " 'L.A. Feldkamp': 314,\n",
       " 'Barak A. Pearlmutter': 315,\n",
       " 'B.A. Pearlmutter': 316,\n",
       " 'Tsungnan Lin': 317,\n",
       " 'B.G. Horne': 318,\n",
       " 'P. Tino': 319,\n",
       " 'C.L. Giles': 320,\n",
       " 'Kevin J. Lang': 321,\n",
       " 'Alex H. Waibel': 322,\n",
       " 'David': 323,\n",
       " 'Simon Osindero': 324,\n",
       " 'Yee-Whye Teh': 325,\n",
       " 'Hugo Larochelle': 326,\n",
       " 'Pierre-Antoine Manzagol': 327,\n",
       " 'Pascal Lamblin': 328,\n",
       " 'Dan Popovici': 329,\n",
       " 'James Bergstra': 330,\n",
       " 'Christopher Poultney': 331,\n",
       " 'Sumit Chopra': 332,\n",
       " 'Yann L. Cun': 333,\n",
       " 'Jie Cui': 334,\n",
       " 'Fang Li': 335,\n",
       " 'Zheng Li Shi': 336,\n",
       " 'Ali Moh Zaki': 337,\n",
       " 'Sander Van Boheemen': 338,\n",
       " 'Theo M. Bestebroer': 339,\n",
       " 'Albert D.M.E.': 340,\n",
       " 'Ron A.M.': 341,\n",
       " 'Timothy P. Sheahan': 342,\n",
       " 'Amy C. Sims': 343,\n",
       " 'Sarah R. Leist': 344,\n",
       " 'Alexandra Schäfer': 345,\n",
       " 'John Won': 346,\n",
       " 'Ariane J. Brown': 347,\n",
       " 'Stephanie A. Montgomery': 348,\n",
       " 'Alison Hogg': 349,\n",
       " 'Darius Babusis': 350,\n",
       " 'Michael O. Clarke': 351,\n",
       " 'Jamie E. Spahn': 352,\n",
       " 'Laura Bauer': 353,\n",
       " 'Scott Sellers': 354,\n",
       " 'Danielle Porter': 355,\n",
       " 'Joy Y. Feng': 356,\n",
       " 'Tomas Cihlar': 357,\n",
       " 'Robert Jordan': 358,\n",
       " 'Mark R. Denison': 359,\n",
       " 'Ralph S. Baric': 360,\n",
       " 'Christian Drosten': 361,\n",
       " 'Stephan Günther': 362,\n",
       " 'Wolfgang Preiser': 363,\n",
       " 'Sylvie van der Werf': 364,\n",
       " 'Hans-Reinhard Brodt': 365,\n",
       " 'Stephan Becker': 366,\n",
       " 'Holger Rabenau': 367,\n",
       " 'Marcus Panning': 368,\n",
       " 'Larissa Kolesnikova': 369,\n",
       " 'Ron A.M. Fouchier': 370,\n",
       " 'Annemarie Berger': 371,\n",
       " 'Ana-Maria Burguière': 372,\n",
       " 'Jindrich Cinatl': 373,\n",
       " 'Markus Eickmann': 374,\n",
       " 'Nicolas Escriou': 375,\n",
       " 'Klaus Grywna': 376,\n",
       " 'Stefanie Kramme': 377,\n",
       " 'Jean-Claude Manuguerra': 378,\n",
       " 'Stefanie Müller': 379,\n",
       " 'Volker Rickerts': 380,\n",
       " 'Martin Stürmer': 381,\n",
       " 'Simon Vieth': 382,\n",
       " 'Hans-Dieter Klenk': 383,\n",
       " 'Albert D.M.E. Osterhaus': 384,\n",
       " 'Herbert Schmitz': 385,\n",
       " 'Hans Wilhelm Doerr': 386,\n",
       " 'Arif Khwaja': 387,\n",
       " 'Ksiazek Tg': 388,\n",
       " 'Erdman D': 389,\n",
       " 'Goldsmith Cs': 390,\n",
       " 'Zaki': 391,\n",
       " 'Peret T': 392,\n",
       " 'Emery S': 393,\n",
       " 'Tong S': 394,\n",
       " 'Urbani C': 395,\n",
       " 'Comer Ja': 396,\n",
       " 'Lim W': 397,\n",
       " 'Rollin Pe': 398,\n",
       " 'Dowell Sf': 399,\n",
       " 'Ling Ae': 400,\n",
       " 'Humphrey Cd': 401,\n",
       " 'Shieh Wj': 402,\n",
       " 'Guarner J': 403,\n",
       " 'Paddock Cd': 404,\n",
       " 'Rota P': 405,\n",
       " 'Fields B': 406,\n",
       " 'DeRisi J': 407,\n",
       " 'Yang Jy': 408,\n",
       " 'Cox N': 409,\n",
       " 'Hughes Jm': 410,\n",
       " 'LeDuc Jw': 411,\n",
       " 'Bellini Wj': 412,\n",
       " 'Anderson Lj': 413,\n",
       " 'Nelson Lee': 414,\n",
       " 'David Hui': 415,\n",
       " 'Alan Wu': 416,\n",
       " 'Paul Chan': 417,\n",
       " 'Peter Cameron': 418,\n",
       " 'Gavin M Joynt': 419,\n",
       " 'Anil Ahuja': 420,\n",
       " 'Man Yee Yung': 421,\n",
       " 'C B Leung': 422,\n",
       " 'K F To': 423,\n",
       " 'S F Lui': 424,\n",
       " 'C C Szeto': 425,\n",
       " 'Sydney Chung': 426,\n",
       " 'Joseph J Y Sung': 427,\n",
       " 'Abdullah Assiri': 428,\n",
       " 'Jaffar A Al-Tawfiq': 429,\n",
       " 'Abdullah A Al-Rabeeah': 430,\n",
       " 'Fahad A Al-Rabiah': 431,\n",
       " 'Sami Al-Hajjar': 432,\n",
       " 'Ali Al-Barrak': 433,\n",
       " 'Hesham Flemban': 434,\n",
       " 'Wafa N Al-Nassir': 435,\n",
       " 'Hanan H Balkhy': 436,\n",
       " 'Rafat F Al-Hakeem': 437,\n",
       " 'Hatem Q Makhdoom': 438,\n",
       " 'Alimuddin I Zumla': 439,\n",
       " 'Ziad A Memish': 440,\n",
       " 'Wenjie': 441,\n",
       " 'Xiang': 442,\n",
       " 'Xuejun': 443,\n",
       " 'Wenling': 444,\n",
       " 'Peihua': 445,\n",
       " 'Wenbo': 446,\n",
       " 'George F.': 447,\n",
       " 'Guizhen': 448,\n",
       " 'Rachel L. Graham': 449,\n",
       " 'Vineet D. Menachery': 450,\n",
       " 'Lisa E. Gralinski': 451,\n",
       " 'James B. Case': 452,\n",
       " 'Krzysztof Pyrc': 453,\n",
       " 'Iva Trantcheva': 454,\n",
       " 'Roy Bannister': 455,\n",
       " 'Yeojin Park': 456,\n",
       " 'Richard L. Mackman': 457,\n",
       " 'Christopher A. Palmiotti': 458,\n",
       " 'Dustin Siegel': 459,\n",
       " 'Adrian S. Ray': 460,\n",
       " 'Na Zhu': 461,\n",
       " 'Dingyu Zhang': 462,\n",
       " 'Bo Yang': 463,\n",
       " 'Jingdong Song': 464,\n",
       " 'Xiang Zhao': 465,\n",
       " 'Baoying Huang': 466,\n",
       " 'Weifeng Shi': 467,\n",
       " 'Peihua Niu': 468,\n",
       " 'Faxian Zhan': 469,\n",
       " 'Xuejun Ma': 470,\n",
       " 'Dayan Wang': 471,\n",
       " 'Wenbo Xu': 472,\n",
       " 'George F. Gao': 473,\n",
       " 'Dawei Wang': 474,\n",
       " 'Bo Hu': 475,\n",
       " 'Chang Hu': 476,\n",
       " 'Fangfang Zhu': 477,\n",
       " 'Xing Liu': 478,\n",
       " 'Jing Zhang': 479,\n",
       " 'Binbin Wang': 480,\n",
       " 'Hui Xiang': 481,\n",
       " 'Yong Xiong': 482,\n",
       " 'Yan Zhao': 483,\n",
       " 'Yirong Li': 484,\n",
       " 'Xinghuan Wang': 485,\n",
       " 'Zhiyong Peng': 486,\n",
       " 'Nanshan Chen': 487,\n",
       " 'Min Zhou': 488,\n",
       " 'Xuan Dong': 489,\n",
       " 'Jieming Qu': 490,\n",
       " 'Fengyun Gong': 491,\n",
       " 'Yang Han': 492,\n",
       " 'Yang Qiu': 493,\n",
       " 'Jingli Wang': 494,\n",
       " 'Ying Liu': 495,\n",
       " \"Jia'an Xia\": 496,\n",
       " 'Xinxin Zhang': 497,\n",
       " 'Qun Li': 498,\n",
       " 'Xuhua Guan': 499,\n",
       " 'Peng Wu': 500,\n",
       " 'Xiaoye Wang': 501,\n",
       " 'Lei Zhou': 502,\n",
       " 'Yeqing Tong': 503,\n",
       " 'Ruiqi Ren': 504,\n",
       " 'Kathy S.M. Leung': 505,\n",
       " 'Eric H.Y. Lau': 506,\n",
       " 'Jessica Y. Wong': 507,\n",
       " 'Xuesen Xing': 508,\n",
       " 'Nijuan Xiang': 509,\n",
       " 'Yang Wu': 510,\n",
       " 'Chao Li': 511,\n",
       " 'Qi Chen': 512,\n",
       " 'Dan Li': 513,\n",
       " 'Tian Liu': 514,\n",
       " 'Jing Zhao': 515,\n",
       " 'Man Liu': 516,\n",
       " 'Wenxiao Tu': 517,\n",
       " 'Chuding Chen': 518,\n",
       " 'Lianmei Jin': 519,\n",
       " 'Rui Yang': 520,\n",
       " 'Qi Wang': 521,\n",
       " 'Suhua Zhou': 522,\n",
       " 'Rui Wang': 523,\n",
       " 'Hui Liu': 524,\n",
       " 'Yingbo Luo': 525,\n",
       " 'Yuan Liu': 526,\n",
       " 'Ge Shao': 527,\n",
       " 'Huan Li': 528,\n",
       " 'Zhongfa Tao': 529,\n",
       " 'Yang Yang': 530,\n",
       " 'Zhiqiang Deng': 531,\n",
       " 'Boxi Liu': 532,\n",
       " 'Zhitao Ma': 533,\n",
       " 'Yanping Zhang': 534,\n",
       " 'Guoqing Shi': 535,\n",
       " 'Tommy T.Y. Lam': 536,\n",
       " 'Joseph T. Wu': 537,\n",
       " 'Benjamin J. Cowling': 538,\n",
       " 'Gabriel M. Leung': 539,\n",
       " 'Zijian Feng': 540,\n",
       " 'Jasper Fuk Woo Chan': 541,\n",
       " 'Shuofeng Yuan': 542,\n",
       " 'Kin Hang Kok': 543,\n",
       " 'Kelvin Kai Wang To': 544,\n",
       " 'Hin Chu': 545,\n",
       " 'Jin Yang': 546,\n",
       " 'Fanfan Xing': 547,\n",
       " 'Jieling Liu': 548,\n",
       " 'Cyril Chik Yan Yip': 549,\n",
       " 'Rosana Wing Shan Poon': 550,\n",
       " 'Hoi Wah Tsoi': 551,\n",
       " 'Simon Kam Fai Lo': 552,\n",
       " 'Kwok Hung Chan': 553,\n",
       " 'Vincent Kwok Man Poon': 554,\n",
       " 'Wan Mui Chan': 555,\n",
       " 'Jonathan Daniel Ip': 556,\n",
       " 'Jian Piao Cai': 557,\n",
       " 'Vincent Chi Chung Cheng': 558,\n",
       " 'Honglin Chen': 559,\n",
       " 'Christopher Kim Ming Hui': 560,\n",
       " 'Kwok Yung Yuen': 561,\n",
       " 'Juan Li': 562,\n",
       " 'Honglong Wu': 563,\n",
       " 'Hao Song': 564,\n",
       " 'Yuhai Bi': 565,\n",
       " 'Liang Wang': 566,\n",
       " 'Tao Hu': 567,\n",
       " 'Hong Zhou': 568,\n",
       " 'Zhenhong Hu': 569,\n",
       " 'Weimin Zhou': 570,\n",
       " 'Li Zhao': 571,\n",
       " 'Jing Chen': 572,\n",
       " 'Yao Meng': 573,\n",
       " 'Ji Wang': 574,\n",
       " 'Yang Lin': 575,\n",
       " 'Jianying Yuan': 576,\n",
       " 'Zhihao Xie': 577,\n",
       " 'Jinmin Ma': 578,\n",
       " 'William J Liu': 579,\n",
       " 'Edward C Holmes': 580,\n",
       " 'George F Gao': 581,\n",
       " 'Weijun Chen': 582,\n",
       " 'Michelle L Holshue': 583,\n",
       " 'Chas DeBolt': 584,\n",
       " 'Scott': 585,\n",
       " 'Kathy H': 586,\n",
       " 'John': 587,\n",
       " 'Hollianne': 588,\n",
       " 'Christopher': 589,\n",
       " 'Keith': 590,\n",
       " 'Sara': 591,\n",
       " 'Ahmet': 592,\n",
       " 'George': 593,\n",
       " 'Amanda': 594,\n",
       " 'LeAnne': 595,\n",
       " 'Anita': 596,\n",
       " 'Susan I': 597,\n",
       " 'Lindsay': 598,\n",
       " 'Suxiang': 599,\n",
       " 'Xiaoyan': 600,\n",
       " 'Steve': 601,\n",
       " 'Mark A': 602,\n",
       " 'William C': 603,\n",
       " 'Holly M': 604,\n",
       " 'Timothy M': 605,\n",
       " 'Satish K': 606,\n",
       " 'Camilla Rothe': 607,\n",
       " 'Mirjam Schunk': 608,\n",
       " 'Peter Sothmann': 609,\n",
       " 'Gisela Bretzel': 610,\n",
       " 'Guenter Froeschl': 611,\n",
       " 'Claudia Wallrauch': 612,\n",
       " 'Thorbjörn Zimmer': 613,\n",
       " 'Verena Thiel': 614,\n",
       " 'Christian Janke': 615,\n",
       " 'Wolfgang Guggemos': 616,\n",
       " 'Michael Seilmaier': 617,\n",
       " 'Patrick Vollmar': 618,\n",
       " 'Katrin Zwirglmaier': 619,\n",
       " 'Sabine Zange': 620,\n",
       " 'Roman Wölfel': 621,\n",
       " 'Michael Hoelscher': 622,\n",
       " 'Joseph T Wu': 623,\n",
       " 'Kathy Leung': 624,\n",
       " 'Gabriel M Leung': 625,\n",
       " 'A. Courville': 626,\n",
       " 'P. Vincent': 627,\n",
       " 'G. Hinton': 628,\n",
       " 'A. Mohamed': 629,\n",
       " 'N. Jaitly': 630,\n",
       " 'V. Vanhoucke': 631,\n",
       " 'P. Nguyen': 632,\n",
       " 'T. N. Sainath': 633,\n",
       " 'B. Kingsbury': 634,\n",
       " 'C. Farabet': 635,\n",
       " 'C. Couprie': 636,\n",
       " 'L. Najman': 637,\n",
       " 'Y. LeCun': 638,\n",
       " 'David G. Lowe': 639,\n",
       " 'Christiane': 640,\n",
       " 'D. Nister': 641,\n",
       " 'H. Stewenius': 642,\n",
       " 'Bryan C. Russell': 643,\n",
       " 'Antonio Torralba': 644,\n",
       " 'Kevin P. Murphy': 645,\n",
       " 'William T. Freeman': 646,\n",
       " 'Gary B. Huang': 647,\n",
       " 'Marwan Mattar': 648,\n",
       " 'Tamara Berg': 649,\n",
       " 'Eric Learned-Miller': 650,\n",
       " 'Gregory': 651,\n",
       " 'Alex': 652,\n",
       " 'Pietro': 653,\n",
       " 'A. Torralba': 654,\n",
       " 'R. Fergus': 655,\n",
       " 'W.T. Freeman': 656,\n",
       " 'Luis von Ahn': 657,\n",
       " 'Laura Dabbish': 658,\n",
       " 'P. Perona': 659,\n",
       " 'Jamie Shotton': 660,\n",
       " 'John Winn': 661,\n",
       " 'Carsten Rother': 662,\n",
       " 'Antonio Criminisi': 663,\n",
       " 'Novel Coronavirus Pneumonia Emergency Response Epidemiology Team': 664,\n",
       " 'Michael Chung': 665,\n",
       " 'Adam Bernheim': 666,\n",
       " 'Xueyan Mei': 667,\n",
       " 'Ning Zhang': 668,\n",
       " 'Mingqian Huang': 669,\n",
       " 'Xianjun Zeng': 670,\n",
       " 'Jiufa Cui': 671,\n",
       " 'Wenjian Xu': 672,\n",
       " 'Zahi A. Fayad': 673,\n",
       " 'Adam Jacobi': 674,\n",
       " 'Kunwei Li': 675,\n",
       " 'Shaolin Li': 676,\n",
       " 'Feng': 677,\n",
       " 'Tianhe': 678,\n",
       " 'Peng': 679,\n",
       " 'Shan': 680,\n",
       " 'Bo': 681,\n",
       " 'Lingli': 682,\n",
       " 'Dandan': 683,\n",
       " 'Jiazheng': 684,\n",
       " 'Richard L': 685,\n",
       " 'Lian': 686,\n",
       " 'Chuansheng': 687,\n",
       " 'Xingzhi Xie': 688,\n",
       " 'Zheng Zhong': 689,\n",
       " 'Wei Zhao': 690,\n",
       " 'Chao Zheng': 691,\n",
       " 'Fei Wang': 692,\n",
       " 'Jun Liu': 693,\n",
       " 'Yueying Pan': 694,\n",
       " 'Hanxiong Guan': 695,\n",
       " 'Shuchang Zhou': 696,\n",
       " 'Yujin Wang': 697,\n",
       " 'Qian Li': 698,\n",
       " 'Tingting Zhu': 699,\n",
       " 'Qiongjie Hu': 700,\n",
       " 'Junqiang Lei': 701,\n",
       " 'Junfeng Li': 702,\n",
       " 'Xun Li': 703,\n",
       " 'Xiaolong Qi': 704,\n",
       " 'Peikai Huang': 705,\n",
       " 'Tianzhu Liu': 706,\n",
       " 'Lesheng Huang': 707,\n",
       " 'Hailong Liu': 708,\n",
       " 'Ming Lei': 709,\n",
       " 'Wangdong Xu': 710,\n",
       " 'Xiaolu Hu': 711,\n",
       " 'Jun Chen': 712,\n",
       " 'Bo Liu': 713,\n",
       " 'Heshui Shi': 714,\n",
       " 'Xiaoyu Han': 715,\n",
       " 'Wei Zhang': 716,\n",
       " 'Rong-Hui Du': 717,\n",
       " 'Bei Li': 718,\n",
       " 'Xiao-Shuang Zheng': 719,\n",
       " 'Xing-Lou Yang': 720,\n",
       " 'Ben Hu': 721,\n",
       " 'Yan-Yi Wang': 722,\n",
       " 'Geng-Fu Xiao': 723,\n",
       " 'Bing Yan': 724,\n",
       " 'Zheng-Li Shi': 725,\n",
       " 'Peng Zhou': 726,\n",
       " 'Yang Pan': 727,\n",
       " 'Daitao Zhang': 728,\n",
       " 'Peng Yang': 729,\n",
       " 'Leo L M Poon': 730,\n",
       " 'Quanyi Wang': 731,\n",
       " 'Jie Yu': 732,\n",
       " 'Peiwei Chai': 733,\n",
       " 'Shengfang Ge': 734,\n",
       " 'Xianqun Fan': 735,\n",
       " 'Axel Kramer': 736,\n",
       " 'Rüdiger Külpmann': 737,\n",
       " 'Arnold Brunner': 738,\n",
       " 'Michael Müller': 739,\n",
       " 'Georgi Wassilew': 740,\n",
       " 'Niamh Cahill': 741,\n",
       " 'Dearbháile Morris': 742,\n",
       " 'Lionel Roques': 743,\n",
       " 'Etienne K. Klein': 744,\n",
       " 'Julien Papaïx': 745,\n",
       " 'Antoine Sar': 746,\n",
       " 'Samuel Soubeyrand': 747,\n",
       " 'Yee Ling Lau': 748,\n",
       " 'Ilyiana Ismail': 749,\n",
       " 'Nur Izati Mustapa': 750,\n",
       " 'Meng Yee Lai': 751,\n",
       " 'Tuan Suhaila Tuan Soh': 752,\n",
       " 'Afifah Hassan': 753,\n",
       " 'Kalaiarasu M Peariasamy': 754,\n",
       " 'Yee Leng Lee': 755,\n",
       " 'Yoong Min Chong': 756,\n",
       " 'I-Ching Sam': 757,\n",
       " 'Pik Pin Goh': 758,\n",
       " 'Song Su': 759,\n",
       " 'Jun Shen': 760,\n",
       " 'Liangru Zhu': 761,\n",
       " 'Yun Qiu': 762,\n",
       " 'Jin-Shen He': 763,\n",
       " 'Jin-Yu Tan': 764,\n",
       " 'Marietta Iacucci': 765,\n",
       " 'Siew C Ng': 766,\n",
       " 'Subrata Ghosh': 767,\n",
       " 'Ren Mao': 768,\n",
       " 'Jie Liang': 769,\n",
       " 'Ali Reza Rahmani': 770,\n",
       " 'Mostafa Leili': 771,\n",
       " 'Ghasem Azarian': 772,\n",
       " 'Ali Poormohammadi': 773,\n",
       " 'Raymund R. Razonable': 774,\n",
       " 'Kelly M. Pennington': 775,\n",
       " 'Anne M. Meehan': 776,\n",
       " 'John W. Wilson': 777,\n",
       " 'Adam T. Froemming': 778,\n",
       " 'Courtney E. Bennett': 779,\n",
       " 'Ariela L. Marshall': 780,\n",
       " 'Abinash Virk': 781,\n",
       " 'Eva M. Carmona': 782,\n",
       " 'Kamal Kant Sahu': 783,\n",
       " 'Ajay Kumar Mishra': 784,\n",
       " 'Amos Lal': 785,\n",
       " 'Daniel Shyu': 786,\n",
       " 'James Dorroh': 787,\n",
       " 'Caleb Holtmeyer': 788,\n",
       " 'Detlef Ritter': 789,\n",
       " 'Anandhi Upendran': 790,\n",
       " 'Raghuraman Kannan': 791,\n",
       " 'Dima Dandachi': 792,\n",
       " 'Christian Rojas-Moreno': 793,\n",
       " 'Stevan P Whitt': 794,\n",
       " 'Hariharan Regunath': 795,\n",
       " 'Ioana M Ciuca': 796,\n",
       " 'P F Felzenszwalb': 797,\n",
       " 'R B Girshick': 798,\n",
       " 'D McAllester': 799,\n",
       " 'D Ramanan': 800,\n",
       " 'Matthew D. Zeiler': 801,\n",
       " 'Rob Fergus': 802,\n",
       " 'Diederik P Kingma': 803,\n",
       " 'Max Welling': 804,\n",
       " 'Nitish': 805,\n",
       " 'Ilya': 806,\n",
       " 'Ruslan R. Salakhutdinov': 807,\n",
       " 'Joan Bruna': 808,\n",
       " 'Olaf Ronneberger': 809,\n",
       " 'Philipp Fischer': 810,\n",
       " 'Thomas Brox': 811,\n",
       " 'Zhou Wang': 812,\n",
       " 'A.C. Bovik': 813,\n",
       " 'H.R. Sheikh': 814,\n",
       " 'E.P. Simoncelli': 815,\n",
       " 'Marius Cordts': 816,\n",
       " 'Mohamed Omran': 817,\n",
       " 'Sebastian Ramos': 818,\n",
       " 'Timo Rehfeld': 819,\n",
       " 'Markus Enzweiler': 820,\n",
       " 'Rodrigo Benenson': 821,\n",
       " 'Uwe Franke': 822,\n",
       " 'Stefan Roth': 823,\n",
       " 'Bernt Schiele': 824,\n",
       " 'Martín': 825,\n",
       " 'Ashish': 826,\n",
       " 'Paul': 827,\n",
       " 'Eugene': 828,\n",
       " 'Zhifeng': 829,\n",
       " 'Craig': 830,\n",
       " 'Gregory S.': 831,\n",
       " 'Andy': 832,\n",
       " 'Jeffrey': 833,\n",
       " 'Matthieu': 834,\n",
       " 'Sanjay': 835,\n",
       " 'Ian J.': 836,\n",
       " 'Michael': 837,\n",
       " 'Yangqing': 838,\n",
       " 'Rafal': 839,\n",
       " 'Lukasz': 840,\n",
       " 'Manjunath': 841,\n",
       " 'Josh': 842,\n",
       " 'Dan': 843,\n",
       " 'Rajat': 844,\n",
       " 'Sherry': 845,\n",
       " 'Derek Gordon': 846,\n",
       " 'Chris': 847,\n",
       " 'Mike': 848,\n",
       " 'Jonathon': 849,\n",
       " 'Benoit': 850,\n",
       " 'Kunal': 851,\n",
       " 'Paul A.': 852,\n",
       " 'Vincent': 853,\n",
       " 'Vijay': 854,\n",
       " 'Fernanda B.': 855,\n",
       " 'Oriol': 856,\n",
       " 'Pete': 857,\n",
       " 'Martin': 858,\n",
       " 'Yuan': 859,\n",
       " 'Xiaoqiang': 860,\n",
       " 'Emily Denton': 861,\n",
       " 'Arthur Szlam': 862,\n",
       " 'Diederik P.': 863,\n",
       " 'Danilo J.': 864,\n",
       " 'Shakir': 865,\n",
       " 'Max': 866,\n",
       " 'Antti Rasmus': 867,\n",
       " 'Harri Valpola': 868,\n",
       " 'Mikko Honkala': 869,\n",
       " 'Mathias Berglund': 870,\n",
       " 'Tapani Raiko': 871,\n",
       " 'Yujia Li': 872,\n",
       " 'Kevin Swersky': 873,\n",
       " 'Rich Zemel': 874,\n",
       " 'Leo Breiman': 875,\n",
       " 'Honglak Lee': 876,\n",
       " 'Roger Grosse': 877,\n",
       " 'Rajesh Ranganath': 878,\n",
       " 'Jeffrey Pennington': 879,\n",
       " 'Christopher Manning': 880,\n",
       " 'Matthew E. Peters': 881,\n",
       " 'Mark Neumann': 882,\n",
       " 'Mohit Iyyer': 883,\n",
       " 'Matt Gardner': 884,\n",
       " 'Christopher Clark': 885,\n",
       " 'Luke Zettlemoyer': 886,\n",
       " 'Quoc Le': 887,\n",
       " 'Jean Wu': 888,\n",
       " 'Jason Chuang': 889,\n",
       " 'Andrew Ng': 890,\n",
       " 'Christopher Potts': 891,\n",
       " 'Pranav Rajpurkar': 892,\n",
       " 'Jian Zhang': 893,\n",
       " 'Konstantin Lopyrev': 894,\n",
       " 'Percy Liang': 895,\n",
       " 'Iain Murray': 896,\n",
       " 'George A. Miller': 897,\n",
       " 'Yoav Freund': 898,\n",
       " 'David Haussler': 899,\n",
       " 'Georgia Gkioxari': 900,\n",
       " 'Piotr Dollar': 901,\n",
       " 'Tsung-Yi Lin': 902,\n",
       " 'Michael Maire': 903,\n",
       " 'Serge J. Belongie': 904,\n",
       " 'James Hays': 905,\n",
       " 'Pietro Perona': 906,\n",
       " 'Deva Ramanan': 907,\n",
       " 'Piotr Dollár': 908,\n",
       " 'C. Lawrence Zitnick': 909,\n",
       " 'Bharath Hariharan': 910,\n",
       " 'Serge Belongie': 911,\n",
       " 'Sam T. Roweis': 912,\n",
       " 'Lawrence K. Saul': 913,\n",
       " 'J. B. Tenenbaum': 914,\n",
       " 'V. de Silva': 915,\n",
       " 'J. C. Langford': 916,\n",
       " 'John J.': 917,\n",
       " 'Nandakishore Kambhatla': 918,\n",
       " 'Todd K. Leen': 919,\n",
       " 'Robert': 920,\n",
       " 'David C. Plaut': 921,\n",
       " 'Mikhail Belkin': 922,\n",
       " 'Partha Niyogi': 923,\n",
       " 'L. Grady': 924,\n",
       " 'Xiaojin Zhu': 925,\n",
       " 'Zoubin Ghahramani': 926,\n",
       " 'John Lafferty': 927,\n",
       " 'Eric Postma': 928,\n",
       " 'Jaap van den Herik': 929,\n",
       " 'John A.': 930,\n",
       " 'Michel': 931,\n",
       " 'Fu Jie Huang': 932,\n",
       " 'L. Bottou': 933,\n",
       " 'Neeraj Kumar': 934,\n",
       " 'Peter N. Belhumeur': 935,\n",
       " 'Shree K. Nayar': 936,\n",
       " 'S. Chopra': 937,\n",
       " 'R. Hadsell': 938,\n",
       " 'Vladimir N. Vapnik': 939,\n",
       " 'D.L. Donoho': 940,\n",
       " 'E.J. Candes': 941,\n",
       " 'T. Tao': 942,\n",
       " 'Corinna Cortes': 943,\n",
       " 'Vladimir Vapnik': 944,\n",
       " 'N. Dalal': 945,\n",
       " 'B. Triggs': 946,\n",
       " 'S. Lazebnik': 947,\n",
       " 'C. Schmid': 948,\n",
       " 'J. Ponce': 949,\n",
       " 'Jianchao Yang': 950,\n",
       " 'Kai Yu': 951,\n",
       " 'Yihong Gong': 952,\n",
       " 'Thomas Huang': 953,\n",
       " 'David E.': 954,\n",
       " 'D. E.': 955,\n",
       " 'G. E.': 956,\n",
       " 'R. J.': 957,\n",
       " 'Richard O.': 958,\n",
       " 'Peter E.': 959,\n",
       " 'Richard S. Sutton': 960,\n",
       " 'Bernard': 961,\n",
       " 'Marcian E.': 962,\n",
       " 'Dimitri P. Bertsekas': 963,\n",
       " 'John N. Tsitsiklis': 964,\n",
       " 'C. J. C. H.': 965,\n",
       " 'Graham C.': 966,\n",
       " 'Kwai Sang': 967,\n",
       " 'David S.': 968,\n",
       " 'Arthur L.': 969,\n",
       " 'C. B. Browne': 970,\n",
       " 'E. Powley': 971,\n",
       " 'D. Whitehouse': 972,\n",
       " 'S. M. Lucas': 973,\n",
       " 'P. I. Cowling': 974,\n",
       " 'P. Rohlfshagen': 975,\n",
       " 'S. Tavener': 976,\n",
       " 'D. Perez': 977,\n",
       " 'S. Samothrakis': 978,\n",
       " 'S. Colton': 979,\n",
       " 'Richard S.': 980,\n",
       " 'Andrew G.': 981,\n",
       " 'Levente Kocsis': 982,\n",
       " 'Csaba Szepesvári': 983,\n",
       " 'Aristides Gionis': 984,\n",
       " 'Piotr Indyk': 985,\n",
       " 'Rajeev Motwani': 986,\n",
       " 'Michael R.': 987,\n",
       " 'Nathaniel Love': 988,\n",
       " 'Barney': 989,\n",
       " 'Marcus': 990,\n",
       " 'Carlos Diuk': 991,\n",
       " 'Andre Cohen': 992,\n",
       " 'Michael L. Littman': 993,\n",
       " 'Joseph Modayil': 994,\n",
       " 'Michael Delp': 995,\n",
       " 'Thomas Degris': 996,\n",
       " 'Patrick M. Pilarski': 997,\n",
       " 'Adam White': 998,\n",
       " 'Doina Precup': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_to_index = {}\n",
    "for paper in data:\n",
    "    for author in paper[\"authors\"]:\n",
    "        if author not in authors_to_index:\n",
    "            authors_to_index[author] = len(authors_to_index)\n",
    "authors_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bacterial-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_ref_matrix = np.zeros((len(authors_to_index), len(authors_to_index)))\n",
    "for i in range(len(data)):\n",
    "    authors = data[i][\"authors\"]\n",
    "    refs = data[i][\"references\"]\n",
    "    for j in range(len(data)):\n",
    "        if i != j:\n",
    "            if data[j][\"id\"] in refs:\n",
    "                target_authors = data[j][\"authors\"]\n",
    "                for a in authors:\n",
    "                    for b in target_authors:\n",
    "                        authors_ref_matrix[authors_to_index[a], authors_to_index[b]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "local-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.ones(len(authors_ref_matrix))\n",
    "a = np.ones(len(authors_ref_matrix))\n",
    "for rep in range(5):\n",
    "    for i in range(len(authors_ref_matrix)):\n",
    "        h[i] += a.reshape((1, len(a))).dot(authors_ref_matrix[i].reshape((len(authors_ref_matrix), 1)))\n",
    "        \n",
    "    for i in range(len(authors_ref_matrix)):\n",
    "        a[i] += h.reshape((1, len(h))).dot(authors_ref_matrix[:, i])\n",
    "    a = a / sum(a)\n",
    "    h = h / sum(h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "creative-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Christian Drosten', 0.003954036439205177),\n",
       " ('', 0.003726857425419863),\n",
       " ('Stephan Günther', 0.0033668895591079493),\n",
       " ('Ksiazek Tg', 0.003045150427858884),\n",
       " ('Erdman D', 0.003045150427858884),\n",
       " ('Goldsmith Cs', 0.003045150427858884),\n",
       " ('Zaki', 0.003045150427858884),\n",
       " ('Peret T', 0.003045150427858884),\n",
       " ('Emery S', 0.003045150427858884),\n",
       " ('Tong S', 0.003045150427858884),\n",
       " ('Urbani C', 0.003045150427858884),\n",
       " ('Comer Ja', 0.003045150427858884),\n",
       " ('Lim W', 0.003045150427858884),\n",
       " ('Rollin Pe', 0.003045150427858884),\n",
       " ('Dowell Sf', 0.003045150427858884),\n",
       " ('Ling Ae', 0.003045150427858884),\n",
       " ('Humphrey Cd', 0.003045150427858884),\n",
       " ('Shieh Wj', 0.003045150427858884),\n",
       " ('Guarner J', 0.003045150427858884),\n",
       " ('Paddock Cd', 0.003045150427858884),\n",
       " ('Rota P', 0.003045150427858884),\n",
       " ('Fields B', 0.003045150427858884),\n",
       " ('DeRisi J', 0.003045150427858884),\n",
       " ('Yang Jy', 0.003045150427858884),\n",
       " ('Cox N', 0.003045150427858884),\n",
       " ('Hughes Jm', 0.003045150427858884),\n",
       " ('LeDuc Jw', 0.003045150427858884),\n",
       " ('Bellini Wj', 0.003045150427858884),\n",
       " ('Anderson Lj', 0.003045150427858884),\n",
       " ('Albert D.M.E. Osterhaus', 0.002926702927730334),\n",
       " ('Herbert Schmitz', 0.002886390780057456),\n",
       " ('Marcus Panning', 0.0028859186458954408),\n",
       " ('Sylvie van der Werf', 0.002868570634242909),\n",
       " ('Wolfgang Preiser', 0.0028677066125521204),\n",
       " ('Stefanie Kramme', 0.0028677066125521204),\n",
       " ('Stephan Becker', 0.0028638696932732155),\n",
       " ('Larissa Kolesnikova', 0.0028638696932732155),\n",
       " ('Ron A.M. Fouchier', 0.002860637616266489),\n",
       " ('Hans-Reinhard Brodt', 0.00283927036951103),\n",
       " ('Holger Rabenau', 0.00283927036951103),\n",
       " ('Annemarie Berger', 0.00283927036951103),\n",
       " ('Ana-Maria Burguière', 0.00283927036951103),\n",
       " ('Jindrich Cinatl', 0.00283927036951103),\n",
       " ('Markus Eickmann', 0.00283927036951103),\n",
       " ('Nicolas Escriou', 0.00283927036951103),\n",
       " ('Klaus Grywna', 0.00283927036951103),\n",
       " ('Jean-Claude Manuguerra', 0.00283927036951103),\n",
       " ('Stefanie Müller', 0.00283927036951103),\n",
       " ('Volker Rickerts', 0.00283927036951103),\n",
       " ('Martin Stürmer', 0.00283927036951103),\n",
       " ('Simon Vieth', 0.00283927036951103),\n",
       " ('Hans-Dieter Klenk', 0.00283927036951103),\n",
       " ('Hans Wilhelm Doerr', 0.00283927036951103),\n",
       " ('Qi Chen', 0.0026137384624729774),\n",
       " ('Heinz Feldmann', 0.002517075613020322),\n",
       " ('Y Guan', 0.0025066442971621496),\n",
       " ('Ron Fouchier', 0.002390695221696298),\n",
       " ('Lisa Fernando', 0.0023814912119660726),\n",
       " ('Yan Li', 0.0023485936295933336),\n",
       " ('Marco A. Marra', 0.0023193003170160317),\n",
       " ('Steven J. M. Jones', 0.0023193003170160317),\n",
       " ('Caroline R. Astell', 0.0023193003170160317),\n",
       " ('Robert A. Holt', 0.0023193003170160317),\n",
       " ('Angela Brooks-Wilson', 0.0023193003170160317),\n",
       " ('Yaron S. N. Butterfield', 0.0023193003170160317),\n",
       " ('Jaswinder Khattra', 0.0023193003170160317),\n",
       " ('Jennifer K. Asano', 0.0023193003170160317),\n",
       " ('Sarah A. Barber', 0.0023193003170160317),\n",
       " ('Susanna Y. Chan', 0.0023193003170160317),\n",
       " ('Alison Cloutier', 0.0023193003170160317),\n",
       " ('Shaun M. Coughlin', 0.0023193003170160317),\n",
       " ('Doug Freeman', 0.0023193003170160317),\n",
       " ('Noreen Girn', 0.0023193003170160317),\n",
       " ('Obi L.', 0.0023193003170160317),\n",
       " ('Stephen R. Leach', 0.0023193003170160317),\n",
       " ('Michael Mayo', 0.0023193003170160317),\n",
       " ('Helen McDonald', 0.0023193003170160317),\n",
       " ('Stephen B. Montgomery', 0.0023193003170160317),\n",
       " ('Pawan K. Pandoh', 0.0023193003170160317),\n",
       " ('Anca S. Petrescu', 0.0023193003170160317),\n",
       " ('A. Gordon Robertson', 0.0023193003170160317),\n",
       " ('Jacqueline E. Schein', 0.0023193003170160317),\n",
       " ('Asim Siddiqui', 0.0023193003170160317),\n",
       " ('Duane E. Smailus', 0.0023193003170160317),\n",
       " ('Jeff M. Stott', 0.0023193003170160317),\n",
       " ('George S. Yang', 0.0023193003170160317),\n",
       " ('Francis Plummer', 0.0023193003170160317),\n",
       " ('Anton Andonov', 0.0023193003170160317),\n",
       " ('Harvey Artsob', 0.0023193003170160317),\n",
       " ('Nathalie Bastien', 0.0023193003170160317),\n",
       " ('Kathy Bernard', 0.0023193003170160317),\n",
       " ('Timothy F. Booth', 0.0023193003170160317),\n",
       " ('Donnie Bowness', 0.0023193003170160317),\n",
       " ('Martin Czub', 0.0023193003170160317),\n",
       " ('Michael Drebot', 0.0023193003170160317),\n",
       " ('Ramon Flick', 0.0023193003170160317),\n",
       " ('Michael Garbutt', 0.0023193003170160317),\n",
       " ('Michael Gray', 0.0023193003170160317),\n",
       " ('Allen Grolla', 0.0023193003170160317),\n",
       " ('Steven Jones', 0.0023193003170160317),\n",
       " ('Adrienne Meyers', 0.0023193003170160317),\n",
       " ('Amin Kabani', 0.0023193003170160317),\n",
       " ('Susan Normand', 0.0023193003170160317),\n",
       " ('Ute Stroher', 0.0023193003170160317),\n",
       " ('Graham A. Tipples', 0.0023193003170160317),\n",
       " ('Shaun Tyler 3', 0.0023193003170160317),\n",
       " ('Dean D. Erdman', 0.0022616462685638014),\n",
       " ('Larry J. Anderson', 0.0022616462685638014),\n",
       " ('Paul A. Rota', 0.0022459064054301376),\n",
       " ('William J. Bellini', 0.0022459064054301376),\n",
       " ('Joseph L. DeRisi', 0.0022453291238657695),\n",
       " ('David Wang', 0.0022453291238657695),\n",
       " ('Bettina Bankamp', 0.002242358559392961),\n",
       " ('Pierre E. Rollin', 0.0022173332114513167),\n",
       " ('Thomas G. Ksiazek', 0.0022140785384018604),\n",
       " ('Anthony Sanchez', 0.0022140785384018604),\n",
       " ('M. Steven Oberste', 0.002211864760572631),\n",
       " ('Stephan S. Monroe', 0.002211864760572631),\n",
       " ('W. Allan Nix', 0.002211864760572631),\n",
       " ('Ray Campagnoli', 0.002211864760572631),\n",
       " ('Joseph P. Icenogle', 0.002211864760572631),\n",
       " ('Silvia Peñaranda', 0.002211864760572631),\n",
       " ('Kaija Maher', 0.002211864760572631),\n",
       " ('Min hsin Chen', 0.002211864760572631),\n",
       " ('Suxiong Tong', 0.002211864760572631),\n",
       " ('Azaibi Tamin', 0.002211864760572631),\n",
       " ('Luis Lowe', 0.002211864760572631),\n",
       " ('Michael Frace', 0.002211864760572631),\n",
       " ('Teresa C.T. Peret', 0.002211864760572631),\n",
       " ('Cara Burns', 0.002211864760572631),\n",
       " ('Stephanie Liffick', 0.002211864760572631),\n",
       " ('Brian Holloway', 0.002211864760572631),\n",
       " ('Josef Limor', 0.002211864760572631),\n",
       " ('Karen McCaustland', 0.002211864760572631),\n",
       " ('Mellissa Olsen-Rasmussen', 0.002211864760572631),\n",
       " ('Albert D.H.E. Osterhaus', 0.002211864760572631),\n",
       " ('Mark A. Pallansch', 0.002211864760572631),\n",
       " ('Nelson Lee', 0.0019270537444025298),\n",
       " ('Y. Guan', 0.001774653557053103),\n",
       " ('C. L.', 0.0017228816848910624),\n",
       " ('W.', 0.0017155109545189122),\n",
       " ('B. J.', 0.0017112897087597652),\n",
       " ('Y. Q.', 0.0017112897087597652),\n",
       " ('X. L.', 0.0017112897087597652),\n",
       " ('Z. X.', 0.0017112897087597652),\n",
       " ('S. W.', 0.0017112897087597652),\n",
       " ('P. H.', 0.0017112897087597652),\n",
       " ('L. J.', 0.0017112897087597652),\n",
       " ('Y. J.', 0.0017112897087597652),\n",
       " ('K. M.', 0.0017112897087597652),\n",
       " ('K. L.', 0.0017112897087597652),\n",
       " ('K. W.', 0.0017112897087597652),\n",
       " ('K. F.', 0.0017112897087597652),\n",
       " ('K. Y.', 0.0017112897087597652),\n",
       " ('J. S. M.', 0.0017112897087597652),\n",
       " ('L. L. M.', 0.0017112897087597652),\n",
       " ('Kwok-Yung Yuen', 0.0017036394627487773),\n",
       " ('Peter Cameron', 0.0016654648436493142),\n",
       " ('Jsm Peiris', 0.0016621203550222568),\n",
       " ('ST Lai', 0.0016621203550222568),\n",
       " ('Llm Poon', 0.0016621203550222568),\n",
       " ('Lyc Yam', 0.0016621203550222568),\n",
       " ('W Lim', 0.0016621203550222568),\n",
       " ('J Nicholls', 0.0016621203550222568),\n",
       " ('Wks Yee', 0.0016621203550222568),\n",
       " ('WW Yan', 0.0016621203550222568),\n",
       " ('MT Cheung', 0.0016621203550222568),\n",
       " ('Vcc Cheng', 0.0016621203550222568),\n",
       " ('KH Chan', 0.0016621203550222568),\n",
       " ('Dnc Tsang', 0.0016621203550222568),\n",
       " ('Rwh Yung', 0.0016621203550222568),\n",
       " ('TK Ng', 0.0016621203550222568),\n",
       " ('KY Yuen', 0.0016621203550222568),\n",
       " ('David Hui', 0.0016612956005113853),\n",
       " ('Alan Wu', 0.0016612956005113853),\n",
       " ('Paul Chan', 0.0016612956005113853),\n",
       " ('Gavin M Joynt', 0.0016612956005113853),\n",
       " ('Anil Ahuja', 0.0016612956005113853),\n",
       " ('Man Yee Yung', 0.0016612956005113853),\n",
       " ('C B Leung', 0.0016612956005113853),\n",
       " ('K F To', 0.0016612956005113853),\n",
       " ('S F Lui', 0.0016612956005113853),\n",
       " ('C C Szeto', 0.0016612956005113853),\n",
       " ('Sydney Chung', 0.0016612956005113853),\n",
       " ('Joseph J Y Sung', 0.0016612956005113853),\n",
       " ('J S M Peiris', 0.0016199466468194176),\n",
       " ('L L M Poon', 0.0014621280905011335),\n",
       " ('C M Chu', 0.0014590676277202613),\n",
       " ('V C C Cheng', 0.0014590676277202613),\n",
       " ('K S Chan', 0.0014590676277202613),\n",
       " ('I F N Hung', 0.0014590676277202613),\n",
       " ('K H Chan', 0.0014590676277202613),\n",
       " ('Theo M. Bestebroer', 0.001359779150466974),\n",
       " ('Ali Moh Zaki', 0.0013378675600507139),\n",
       " ('Sander Van Boheemen', 0.0013378675600507139),\n",
       " ('Albert D.M.E.', 0.0013378675600507139),\n",
       " ('Ron A.M.', 0.0013378675600507139),\n",
       " ('K I Law', 0.0013284422530998132),\n",
       " ('B S F Tang', 0.0013284422530998132),\n",
       " ('T Y W Hon', 0.0013284422530998132),\n",
       " ('C S Chan', 0.0013284422530998132),\n",
       " ('J S C Ng', 0.0013284422530998132),\n",
       " ('B J Zheng', 0.0013284422530998132),\n",
       " ('W L Ng', 0.0013284422530998132),\n",
       " ('R W M Lai', 0.0013284422530998132),\n",
       " ('Susan M. Poutanen', 0.0013252277335605786),\n",
       " ('Wenhui Li', 0.0012463377580411085),\n",
       " ('Michael Farzan', 0.0012463377580411085),\n",
       " ('Hui Li', 0.0011459814201232636),\n",
       " ('Bing', 0.0011444957252540948),\n",
       " ('Thomas M.', 0.001121720944504548),\n",
       " ('Kenneth W. Tsang', 0.0011199151510584754),\n",
       " ('Pak L.', 0.0011199151510584754),\n",
       " ('Gaik C.', 0.0011199151510584754),\n",
       " ('Wilson K.', 0.0011199151510584754),\n",
       " ('Teresa', 0.0011199151510584754),\n",
       " ('Moira', 0.0011199151510584754),\n",
       " ('Wah K.', 0.0011199151510584754),\n",
       " ('Wing H.', 0.0011199151510584754),\n",
       " ('Loretta Y.', 0.0011199151510584754),\n",
       " ('Poon C.', 0.0011199151510584754),\n",
       " ('Mary S.', 0.0011199151510584754),\n",
       " ('Jane', 0.0011199151510584754),\n",
       " ('Kwok Y.', 0.0011199151510584754),\n",
       " ('Kar N.', 0.0011199151510584754),\n",
       " ('Michael J. Moore', 0.0011194678073729936),\n",
       " ('Natalya Vasilieva', 0.0011194678073729936),\n",
       " ('Jianhua Sui', 0.0011194678073729936),\n",
       " ('Swee Kee Wong', 0.0011194678073729936),\n",
       " ('Michael A. Berne', 0.0011194678073729936),\n",
       " ('Mohan Somasundaran', 0.0011194678073729936),\n",
       " ('John L. Sullivan', 0.0011194678073729936),\n",
       " ('Katherine Luzuriaga', 0.0011194678073729936),\n",
       " ('Thomas C. Greenough', 0.0011194678073729936),\n",
       " ('Hyeryun Choe', 0.0011194678073729936),\n",
       " ('Arthur S. Slutsky', 0.001112676107997392),\n",
       " ('Donald E. Low', 0.0010703327054165908),\n",
       " ('Bonnie Henry', 0.0010703327054165908),\n",
       " ('Sandy Finkelstein', 0.0010703327054165908),\n",
       " ('David Rose', 0.0010703327054165908),\n",
       " ('Karen Green', 0.0010703327054165908),\n",
       " ('Raymond Tellier', 0.0010703327054165908),\n",
       " ('Ryan Draker', 0.0010703327054165908),\n",
       " ('Dena Adachi', 0.0010703327054165908),\n",
       " ('Melissa Ayers', 0.0010703327054165908),\n",
       " ('Adrienne K. Chan', 0.0010703327054165908),\n",
       " ('Danuta M. Skowronski', 0.0010703327054165908),\n",
       " ('Irving Salit', 0.0010703327054165908),\n",
       " ('Andrew E. Simor', 0.0010703327054165908),\n",
       " ('Patrick W. Doyle', 0.0010703327054165908),\n",
       " ('Mel Krajden', 0.0010703327054165908),\n",
       " ('Martin Petric', 0.0010703327054165908),\n",
       " ('Robert C. Brunham', 0.0010703327054165908),\n",
       " ('Allison J. McGeer', 0.0010703327054165908),\n",
       " ('Bart L. Haagmans', 0.0010636567538101088),\n",
       " ('Zheng Li Shi', 0.0010618203766629866),\n",
       " ('Volker Thiel', 0.0009264865181565604),\n",
       " ('Xingwang Li', 0.000908603464305599),\n",
       " ('Wei Zhang', 0.0009026458643601961),\n",
       " ('Ben Hu', 0.0009026458643601961),\n",
       " ('Xing Lou Yang', 0.0009019987156911346),\n",
       " ('Peter', 0.000900854798660031),\n",
       " ('Kwok Yung Yuen', 0.000884377530082326),\n",
       " ('Susanna K. P. Lau', 0.0008837645629697651),\n",
       " ('Albert D. M. E. Osterhaus', 0.0008816224468937684),\n",
       " ('Hong Gao', 0.0008804844903486188),\n",
       " ('Jonathan H.', 0.000874198506458739),\n",
       " ('Yan Zhu', 0.0008646202800960547),\n",
       " ('Ron A. M. Fouchier', 0.0008523291343164668),\n",
       " ('L. L. M. Poon', 0.0008446545775419935),\n",
       " ('Ting Yu', 0.0008432372879577886),\n",
       " ('Yuan Wei', 0.00084157124214358),\n",
       " ('Gary', 0.0008200825775051357),\n",
       " ('Meng', 0.0008191850649852944),\n",
       " ('Bryan T.', 0.0008191850649852944),\n",
       " ('Marcel A. Müller', 0.000818030283287821),\n",
       " ('Craig', 0.0008169098299398133),\n",
       " ('Wendong Li', 0.0008154104780267597),\n",
       " ('Zhengli', 0.0008154104780267597),\n",
       " ('Wuze', 0.0008154104780267597),\n",
       " ('Hanzhong', 0.0008154104780267597),\n",
       " ('Zhihong', 0.0008154104780267597),\n",
       " ('Huajun', 0.0008154104780267597),\n",
       " ('Jianhong', 0.0008154104780267597),\n",
       " ('Jennifer', 0.0008154104780267597),\n",
       " ('Hume', 0.0008154104780267597),\n",
       " ('Shuyi', 0.0008154104780267597),\n",
       " ('Lin-Fa', 0.0008154104780267597),\n",
       " ('George F. Gao', 0.0008125566474392372),\n",
       " ('V. Stalin Raj', 0.0008081246800219252),\n",
       " ('Doreen Muth', 0.0008063998315627259),\n",
       " ('Li Zhang', 0.0007945737707859146),\n",
       " ('Hin Chu', 0.0007879989864362282),\n",
       " ('Yeming Wang', 0.0007868776758768923),\n",
       " ('Guohui Fan', 0.0007868776758768923),\n",
       " ('Jiuyang Xu', 0.0007868776758768923),\n",
       " ('Xiaoying Gu', 0.0007868776758768923),\n",
       " ('Bo Yang', 0.0007722492961366273),\n",
       " ('Bin Cao', 0.0007686690507947125),\n",
       " ('Zhancheng Gao', 0.0007567319923140562),\n",
       " ('Ziad A. Memish', 0.0007563115983917621),\n",
       " ('Zhenshun Cheng', 0.0007520229185126575),\n",
       " ('Rongmeng Jiang', 0.000745846724853015),\n",
       " ('Yi Guan', 0.0007429840352212669),\n",
       " ('Chaolin Huang', 0.000739880204519227),\n",
       " ('Lili Ren', 0.000739880204519227),\n",
       " ('Jianping Zhao', 0.000739880204519227),\n",
       " ('Yi Hu', 0.000739880204519227),\n",
       " ('Jiaan Xia', 0.000739880204519227),\n",
       " ('Wenjuan Wu', 0.000739880204519227),\n",
       " ('Xuelei Xie', 0.000739880204519227),\n",
       " ('Wen Yin', 0.000739880204519227),\n",
       " ('Min Liu', 0.000739880204519227),\n",
       " ('Yan Xiao', 0.000739880204519227),\n",
       " ('Li Guo', 0.000739880204519227),\n",
       " ('Jungang Xie', 0.000739880204519227),\n",
       " ('Guangfa Wang', 0.000739880204519227),\n",
       " ('Qi Jin', 0.000739880204519227),\n",
       " ('Jianwei Wang', 0.000739880204519227),\n",
       " ('Victor M. Corman', 0.0007397515862330393),\n",
       " ('Eric J. Snijder', 0.0007227758730736561),\n",
       " ('Alexander E. Gorbalenya', 0.0007222748316441442),\n",
       " ('Roujian Lu', 0.0007115333935821727),\n",
       " ('Wenjie Tan', 0.0007115333935821727),\n",
       " ('Wenling Wang', 0.0007105813142348523),\n",
       " ('Guizhen Wu', 0.0007105813142348523),\n",
       " ('Peter Daszak', 0.0007099454486813782),\n",
       " ('Abdullah Assiri', 0.0007095818164153149),\n",
       " ('Honglin Chen', 0.0006984642461302748),\n",
       " ('M.', 0.0006970263192418095),\n",
       " ('A.', 0.0006946596308938099),\n",
       " ('T.', 0.0006945737367702351),\n",
       " ('S.', 0.0006919019548549527),\n",
       " ('K. Y. Yuen', 0.0006881662503095291),\n",
       " ('P.', 0.0006879091142600948),\n",
       " ('L.', 0.0006872733375182904),\n",
       " ('Suxiang', 0.0006866233121976655),\n",
       " ('K.', 0.0006864419913932717),\n",
       " ('J.-S.', 0.0006822875049498218),\n",
       " ('Y.-C.', 0.0006822875049498218),\n",
       " ('I.-H.', 0.0006822875049498218),\n",
       " ('K.-T.', 0.0006822875049498218),\n",
       " ('K.-H.', 0.0006822875049498218),\n",
       " ('T.-J.', 0.0006822875049498218),\n",
       " ('H.-T.', 0.0006822875049498218),\n",
       " ('S.-J.', 0.0006822875049498218),\n",
       " ('Xing Yi Ge', 0.0006676908075856874),\n",
       " ('Jonathan H. Epstein', 0.000666813485902861),\n",
       " ('Weifeng Shi', 0.000658984015876841),\n",
       " ('Saskia L. Smits', 0.0006478995338940271),\n",
       " ('Berend Jan Bosch', 0.0006478995338940271),\n",
       " ('Ning Wang', 0.0006457441874052535),\n",
       " ('Fang Li', 0.0006415903168160364),\n",
       " ('Lin Fa Wang', 0.0006399906602218163),\n",
       " ('J. S. M. Peiris', 0.0006397457201842322),\n",
       " ('V M Corman', 0.0006381782307570437),\n",
       " ('I Eckerle', 0.0006381782307570437),\n",
       " ('T Bleicker', 0.0006381782307570437),\n",
       " ('A Zaki', 0.0006381782307570437),\n",
       " ('O Landt', 0.0006381782307570437),\n",
       " ('M Eschbach-Bludau', 0.0006381782307570437),\n",
       " ('S van Boheemen', 0.0006381782307570437),\n",
       " ('R Gopal', 0.0006381782307570437),\n",
       " ('M Ballhause', 0.0006381782307570437),\n",
       " ('T M Bestebroer', 0.0006381782307570437),\n",
       " ('D Muth', 0.0006381782307570437),\n",
       " ('M A Müller', 0.0006381782307570437),\n",
       " ('J F Drexler', 0.0006381782307570437),\n",
       " ('M Zambon', 0.0006381782307570437),\n",
       " ('A D Osterhaus', 0.0006381782307570437),\n",
       " ('R M Fouchier', 0.0006381782307570437),\n",
       " ('C Drosten', 0.0006381782307570437),\n",
       " ('Yi', 0.0006347885439075494),\n",
       " ('Jan Felix Drexler', 0.0006347250592080772),\n",
       " ('Jasper Fuk Woo Chan', 0.0006336964392105886),\n",
       " ('Jian Piao Cai', 0.0006300091889415991),\n",
       " ('Kwok Hung Chan', 0.0006298389414105311),\n",
       " ('Kwok Y Yuen', 0.0006225662536917038),\n",
       " ('Na Zhu', 0.0006138584313061256),\n",
       " ('Peihua Niu', 0.0006138584313061256),\n",
       " ('Xiang Zhao', 0.0006129063519588051),\n",
       " ('Baoying Huang', 0.0006129063519588051),\n",
       " ('Faxian Zhan', 0.0006129063519588051),\n",
       " ('Xuejun Ma', 0.0006129063519588051),\n",
       " ('Dayan Wang', 0.0006129063519588051),\n",
       " ('Wenbo Xu', 0.0006129063519588051),\n",
       " ('Alimuddin I. Zumla', 0.0006101128682895803),\n",
       " ('Aleksei A. Chmura', 0.0006083657518101736),\n",
       " ('Guangjian Zhu', 0.0006083657518101736),\n",
       " ('Ronald Dijkman', 0.0006079554312259295),\n",
       " ('Jia Lu Li', 0.0006026122246267362),\n",
       " ('Jonna A Mazet', 0.0006026122246267362),\n",
       " ('Cheng Peng', 0.0006026122246267362),\n",
       " ('Yu Ji Zhang', 0.0006026122246267362),\n",
       " ('Chu Ming Luo', 0.0006026122246267362),\n",
       " ('Bing Tan', 0.0006026122246267362),\n",
       " ('Gary Crameri', 0.0006026122246267362),\n",
       " ('Shu Yi Zhang', 0.0006026122246267362),\n",
       " ('Patrick C. Y.', 0.0005890408165240812),\n",
       " ('Kenneth S. M.', 0.0005890408165240812),\n",
       " ('Hoi-Wah', 0.0005890408165240812),\n",
       " ('Beatrice H. L.', 0.0005890408165240812),\n",
       " ('Samson S. Y.', 0.0005890408165240812),\n",
       " ('Suet-Yi', 0.0005890408165240812),\n",
       " ('Kwok-Hung', 0.0005890408165240812),\n",
       " ('Kwok-Yung', 0.0005890408165240812),\n",
       " ('K. H. Chan', 0.0005796866947762894),\n",
       " ('Stanley Perlman', 0.0005772959131183658),\n",
       " ('Shuofeng Yuan', 0.0005721627335992774),\n",
       " ('Kin Hang Kok', 0.0005721627335992774),\n",
       " ('Kelvin Kai Wang To', 0.0005721627335992774),\n",
       " ('Jin Yang', 0.0005684754833302879),\n",
       " ('Fanfan Xing', 0.0005684754833302879),\n",
       " ('Jieling Liu', 0.0005684754833302879),\n",
       " ('Cyril Chik Yan Yip', 0.0005684754833302879),\n",
       " ('Rosana Wing Shan Poon', 0.0005684754833302879),\n",
       " ('Hoi Wah Tsoi', 0.0005684754833302879),\n",
       " ('Simon Kam Fai Lo', 0.0005684754833302879),\n",
       " ('Vincent Kwok Man Poon', 0.0005684754833302879),\n",
       " ('Wan Mui Chan', 0.0005684754833302879),\n",
       " ('Jonathan Daniel Ip', 0.0005684754833302879),\n",
       " ('Vincent Chi Chung Cheng', 0.0005684754833302879),\n",
       " ('Christopher Kim Ming Hui', 0.0005684754833302879),\n",
       " ('Trish M. Perl', 0.0005641162160870216),\n",
       " ('Dingyu Zhang', 0.0005621375778657908),\n",
       " ('Jingdong Song', 0.0005621375778657908),\n",
       " ('N. S. Zhong', 0.0005599292183493261),\n",
       " ('B. J. Zheng', 0.0005599292183493261),\n",
       " ('Y. M. Li', 0.0005599292183493261),\n",
       " ('Z. H. Xie', 0.0005599292183493261),\n",
       " ('P. H. H. Li', 0.0005599292183493261),\n",
       " ('S. Y. Tan', 0.0005599292183493261),\n",
       " ('Q. Chang', 0.0005599292183493261),\n",
       " ('J. P. Xie', 0.0005599292183493261),\n",
       " ('X. Q. Liu', 0.0005599292183493261),\n",
       " ('J. Xu', 0.0005599292183493261),\n",
       " ('D. X. Li', 0.0005599292183493261),\n",
       " ('Jaffar A. Al-Tawfiq', 0.0005459330748097583),\n",
       " ('Hanan H Balkhy', 0.0005411548997445437),\n",
       " ('Huihui Mou', 0.000538544889328665),\n",
       " ('Dick H. W. Dekkers', 0.000538544889328665),\n",
       " ('Jeroen A. A. Demmers', 0.000538544889328665),\n",
       " ('Ali Zaki', 0.000538544889328665),\n",
       " ('Peter J. M. Rottier', 0.000538544889328665),\n",
       " ('Emmie de Wit', 0.0005326056157249409),\n",
       " ('Albert Osterhaus', 0.0005273055868320032),\n",
       " ('Leo L.M. Poon', 0.0005263759214888331),\n",
       " ('Benjamin Meyer', 0.0005229891063866682),\n",
       " ('Gabriel M. Leung', 0.0005195808981745564),\n",
       " ('Krzysztof Pyrc', 0.00051493200905021),\n",
       " ('Matthew Cotten', 0.0005115884737814458),\n",
       " ('Ralph S. Baric', 0.0005095584545790423),\n",
       " ('Bei Li', 0.0005042016025473885),\n",
       " ('Xu Rui Shen', 0.000503554453878327),\n",
       " ('Xiao Shuang Zheng', 0.000503554453878327),\n",
       " ('Erik Lattwein', 0.0005013420147629823),\n",
       " ('Andrea', 0.0005002393749594258),\n",
       " ('Peng Wu', 0.0004969485075156383),\n",
       " ('Allison McGeer', 0.0004925792320838696),\n",
       " ('Connie S. Price', 0.0004925792320838696),\n",
       " ('Abdullah A. Al Rabeeah', 0.0004925792320838696),\n",
       " ('Derek A.T. Cummings', 0.0004925792320838696),\n",
       " ('Zaki N. Alabdullatif', 0.0004925792320838696),\n",
       " ('Maher Assad', 0.0004925792320838696),\n",
       " ('Abdulmohsen Almulhim', 0.0004925792320838696),\n",
       " ('Hatem Makhdoom', 0.0004925792320838696),\n",
       " ('Hossam Madani', 0.0004925792320838696),\n",
       " ('Rafat Alhakeem', 0.0004925792320838696),\n",
       " ('Simon J. Watson', 0.0004925792320838696),\n",
       " ('Paul Kellam', 0.0004925792320838696),\n",
       " ('Jie Cui', 0.0004903454523852839),\n",
       " ('Marion P.G. Koopmans', 0.0004787446397039188),\n",
       " ('Ziad A Memish', 0.0004776968363563955),\n",
       " ('Vineet D. Menachery', 0.00047392256038768834),\n",
       " ('Lisa E. Gralinski', 0.00047392256038768834),\n",
       " ('Pei', 0.0004672796190457452),\n",
       " ('Jaffar A Al-Tawfiq', 0.000467123366299813),\n",
       " ('Abdullah A Al-Rabeeah', 0.000467123366299813),\n",
       " ('Fahad A Al-Rabiah', 0.000467123366299813),\n",
       " ('Sami Al-Hajjar', 0.000467123366299813),\n",
       " ('Ali Al-Barrak', 0.000467123366299813),\n",
       " ('Hesham Flemban', 0.000467123366299813),\n",
       " ('Wafa N Al-Nassir', 0.000467123366299813),\n",
       " ('Rafat F Al-Hakeem', 0.000467123366299813),\n",
       " ('Hatem Q Makhdoom', 0.000467123366299813),\n",
       " ('Alimuddin I Zumla', 0.000467123366299813),\n",
       " ('Ying Liu', 0.00046008275034052783),\n",
       " ('Patrick C. Y. Woo', 0.00045542947838914604),\n",
       " ('Ming', 0.0004527336569532717),\n",
       " ('Yang', 0.0004527336569532717),\n",
       " ('Hui', 0.0004527336569532717),\n",
       " ('Yang Yang', 0.00045093098405808116),\n",
       " ('Chao Li', 0.00044869175669944874),\n",
       " ('Yuan Liu', 0.00044869175669944874),\n",
       " ('Qun Li', 0.00044296813254765034),\n",
       " ('Zijian Feng', 0.00044296813254765034),\n",
       " ('Xuhua Guan', 0.0004427341525664454),\n",
       " ('Xiaoye Wang', 0.0004427341525664454),\n",
       " ('Lei Zhou', 0.0004427341525664454),\n",
       " ('Yeqing Tong', 0.0004427341525664454),\n",
       " ('Ruiqi Ren', 0.0004427341525664454),\n",
       " ('Kathy S.M. Leung', 0.0004427341525664454),\n",
       " ('Eric H.Y. Lau', 0.0004427341525664454),\n",
       " ('Jessica Y. Wong', 0.0004427341525664454),\n",
       " ('Xuesen Xing', 0.0004427341525664454),\n",
       " ('Nijuan Xiang', 0.0004427341525664454),\n",
       " ('Yang Wu', 0.0004427341525664454),\n",
       " ('Dan Li', 0.0004427341525664454),\n",
       " ('Tian Liu', 0.0004427341525664454),\n",
       " ('Jing Zhao', 0.0004427341525664454),\n",
       " ('Man Liu', 0.0004427341525664454),\n",
       " ('Wenxiao Tu', 0.0004427341525664454),\n",
       " ('Chuding Chen', 0.0004427341525664454),\n",
       " ('Lianmei Jin', 0.0004427341525664454),\n",
       " ('Rui Yang', 0.0004427341525664454),\n",
       " ('Qi Wang', 0.0004427341525664454),\n",
       " ('Suhua Zhou', 0.0004427341525664454),\n",
       " ('Rui Wang', 0.0004427341525664454),\n",
       " ('Hui Liu', 0.0004427341525664454),\n",
       " ('Yingbo Luo', 0.0004427341525664454),\n",
       " ('Ge Shao', 0.0004427341525664454),\n",
       " ('Huan Li', 0.0004427341525664454),\n",
       " ('Zhongfa Tao', 0.0004427341525664454),\n",
       " ('Zhiqiang Deng', 0.0004427341525664454),\n",
       " ('Boxi Liu', 0.0004427341525664454),\n",
       " ('Zhitao Ma', 0.0004427341525664454),\n",
       " ('Yanping Zhang', 0.0004427341525664454),\n",
       " ('Guoqing Shi', 0.0004427341525664454),\n",
       " ('Tommy T.Y. Lam', 0.0004427341525664454),\n",
       " ('Joseph T. Wu', 0.0004427341525664454),\n",
       " ('Benjamin J. Cowling', 0.0004427341525664454),\n",
       " ('Darryl Falzarano', 0.00044154714507785905),\n",
       " ('K Y Yuen', 0.00044120289843935906),\n",
       " ('Tabea Binger', 0.00043980830915167114),\n",
       " ('Jing Chen', 0.00043489474715740296),\n",
       " ('Alimuddin Zumla', 0.00042833300597765623),\n",
       " ('Wolfgang Guggemos', 0.0004278278744460994),\n",
       " ('Michael Seilmaier', 0.0004278278744460994),\n",
       " ('Feng', 0.0004277570175715497),\n",
       " ('Rachel L. Graham', 0.0004267753063956526),\n",
       " ('Larry', 0.00042326734311744956),\n",
       " ('Huai-Dong Song', 0.0004228357974143092),\n",
       " ('Chang-Chun', 0.0004228357974143092),\n",
       " ('Guo-Wei', 0.0004228357974143092),\n",
       " ('Sheng-Yue', 0.0004228357974143092),\n",
       " ('Kui', 0.0004228357974143092),\n",
       " ('Lian-Cheng', 0.0004228357974143092),\n",
       " ('Qiu-Xia', 0.0004228357974143092),\n",
       " ('Yu-Wei', 0.0004228357974143092),\n",
       " ('Hui-Qiong', 0.0004228357974143092),\n",
       " ('Hua', 0.0004228357974143092),\n",
       " ('Hua-Jun', 0.0004228357974143092),\n",
       " ('Shur-Wern Wang', 0.0004228357974143092),\n",
       " ('Chun-Ming', 0.0004228357974143092),\n",
       " ('Sai-Juan', 0.0004228357974143092),\n",
       " ('Hui-Ming', 0.0004228357974143092),\n",
       " ('Duan-Hua', 0.0004228357974143092),\n",
       " ('Yu-Fei', 0.0004228357974143092),\n",
       " ('Jian-Feng', 0.0004228357974143092),\n",
       " ('Peng-Zhe', 0.0004228357974143092),\n",
       " ('Ling-Hui', 0.0004228357974143092),\n",
       " ('Yu-Qi', 0.0004228357974143092),\n",
       " ('Wen-Jia', 0.0004228357974143092),\n",
       " ('Ye-Dong', 0.0004228357974143092),\n",
       " ('Rui-Heng', 0.0004228357974143092),\n",
       " ('Xin-Wei', 0.0004228357974143092),\n",
       " ('Huan-Ying', 0.0004228357974143092),\n",
       " ('Jin-Ding', 0.0004228357974143092),\n",
       " ('Guodong', 0.0004228357974143092),\n",
       " ('Ling', 0.0004228357974143092),\n",
       " ('Li-Yun', 0.0004228357974143092),\n",
       " ('Fang', 0.0004228357974143092),\n",
       " ('Biao', 0.0004228357974143092),\n",
       " ('Li-Juan', 0.0004228357974143092),\n",
       " ('Jin-Yan', 0.0004228357974143092),\n",
       " ('Xiangang', 0.0004228357974143092),\n",
       " ('Lin', 0.0004228357974143092),\n",
       " ('Xiao-Jing', 0.0004228357974143092),\n",
       " ('Ottavia', 0.0004228357974143092),\n",
       " ('Zong-Ming Guo', 0.0004228357974143092),\n",
       " ('Peter J. Bredenbeek', 0.00041426821534730674),\n",
       " ('Jessika C. Dobbe', 0.00041426821534730674),\n",
       " ('John Ziebuhr', 0.00041426821534730674),\n",
       " ('Mikhail Rozanov', 0.00041426821534730674),\n",
       " ('Willy J.M. Spaan', 0.00041426821534730674),\n",
       " ('Peng Zhou', 0.0004110666071710924),\n",
       " ('Kai Zhao', 0.00041041945850203086),\n",
       " ('Chantal B.E.M. Reusken', 0.00040841893514037607),\n",
       " ('Sudhakar S. Agnihothram', 0.0003995211452394306),\n",
       " ('Maria Zambon', 0.00039153333446893205),\n",
       " ('Min Zhou', 0.00039151489803976986),\n",
       " ('Jieming Qu', 0.00039151489803976986),\n",
       " ('Xinxin Zhang', 0.00039151489803976986),\n",
       " ('Gwen M. Stephens', 0.0003890838099703795),\n",
       " ('Qi', 0.0003890204108188018),\n",
       " ('Wai K Leung', 0.00038835044846666016),\n",
       " ('Ka-fai To', 0.00038835044846666016),\n",
       " ('Paul K.S Chan', 0.00038835044846666016),\n",
       " ('Henry L.Y Chan', 0.00038835044846666016),\n",
       " ('Alan K.L Wu', 0.00038835044846666016),\n",
       " ('Joseph J.Y Sung', 0.00038835044846666016),\n",
       " ('Rafat F. Al-Hakeem', 0.0003866262859785959),\n",
       " ('Abdullah A. Al-Rabeeah', 0.0003866262859785959),\n",
       " ('Ralph S Baric', 0.00038591740608492344),\n",
       " (\"Jia'an Xia\", 0.0003854618213179116),\n",
       " ('Antonio Lanzavecchia', 0.0003844735702501307),\n",
       " ('Naruya Saitou', 0.000378567190518703),\n",
       " ('Masatoshi', 0.000378567190518703),\n",
       " ('Richard Myers', 0.00037586806280531977),\n",
       " ('Christopher M. Booth', 0.0003753057778214554),\n",
       " ('Larissa M. Matukas', 0.0003753057778214554),\n",
       " ('George A. Tomlinson', 0.0003753057778214554),\n",
       " ('Anita R. Rachlis', 0.0003753057778214554),\n",
       " ('David B. Rose', 0.0003753057778214554),\n",
       " ('Hy A. Dwosh', 0.0003753057778214554),\n",
       " ('Sharon L. Walmsley', 0.0003753057778214554),\n",
       " ('Tony Mazzulli', 0.0003753057778214554),\n",
       " ('Monica Avendano', 0.0003753057778214554),\n",
       " ('Peter Derkach', 0.0003753057778214554),\n",
       " ('Issa E. Ephtimios', 0.0003753057778214554),\n",
       " ('Ian Kitai', 0.0003753057778214554),\n",
       " ('Barbara D. Mederski', 0.0003753057778214554),\n",
       " ('Steven B. Shadowitz', 0.0003753057778214554),\n",
       " ('Wayne L. Gold', 0.0003753057778214554),\n",
       " ('Laura A. Hawryluck', 0.0003753057778214554),\n",
       " ('Elizabeth Rea', 0.0003753057778214554),\n",
       " ('Jordan S. Chenkin', 0.0003753057778214554),\n",
       " ('David W. Cescon', 0.0003753057778214554),\n",
       " ('Allan S. Detsky', 0.0003753057778214554),\n",
       " ('Andrew Rambaut', 0.00037488858323684134),\n",
       " ('Timothy Patrick Sheahan', 0.00036991255956130123),\n",
       " ('Victor Corman', 0.000368959128531989),\n",
       " ('Marcel Müller', 0.000368959128531989),\n",
       " ('U. Costabel', 0.000368959128531989),\n",
       " ('J. Timm', 0.000368959128531989),\n",
       " ('Bernhard Meyer', 0.000368959128531989),\n",
       " ('P. Kreher', 0.000368959128531989),\n",
       " ('Monika Eschbach-Bludau', 0.000368959128531989),\n",
       " ('A. Nitsche', 0.000368959128531989),\n",
       " ('T. Bleicker', 0.000368959128531989),\n",
       " ('O. Landt', 0.000368959128531989),\n",
       " ('Brunhilde Schweiger', 0.000368959128531989),\n",
       " ('Jan-Felix Drexler', 0.000368959128531989),\n",
       " ('Bart Haagmans', 0.000368959128531989),\n",
       " ('U. Dittmer', 0.000368959128531989),\n",
       " ('F. Bonin', 0.000368959128531989),\n",
       " ('Thorsten Wolff', 0.000368959128531989),\n",
       " ('Jian Feng He', 0.00036812222597295896),\n",
       " ('Guo Wen Peng', 0.00036812222597295896),\n",
       " ('Jun Min', 0.00036812222597295896),\n",
       " ('De Wen Yu', 0.00036812222597295896),\n",
       " ('Wen Jia Liang', 0.00036812222597295896),\n",
       " ('Shu Yi', 0.00036812222597295896),\n",
       " ('Rui Heng Xu', 0.00036812222597295896),\n",
       " ('Huan Ying Zheng', 0.00036812222597295896),\n",
       " ('Xin Wei Wu', 0.00036812222597295896),\n",
       " ('Jun Xu', 0.00036812222597295896),\n",
       " ('Zhan Hui', 0.00036812222597295896),\n",
       " ('Ling Fang', 0.00036812222597295896),\n",
       " ('Xin Zhang', 0.00036812222597295896),\n",
       " ('Xin Ge Yan', 0.00036812222597295896),\n",
       " ('Jia Hai Lu', 0.00036812222597295896),\n",
       " ('Zhi Hong', 0.00036812222597295896),\n",
       " ('Ji Cheng Huang', 0.00036812222597295896),\n",
       " ('Zhuo Yue Wan', 0.00036812222597295896),\n",
       " ('Jin Lin', 0.00036812222597295896),\n",
       " ('Jin Yan Lin', 0.00036812222597295896),\n",
       " ('Huai Dong Song', 0.00036812222597295896),\n",
       " ('Sheng Yue Wang', 0.00036812222597295896),\n",
       " ('Xiang Jun Zhou', 0.00036812222597295896),\n",
       " ('Guo Wei Zhang', 0.00036812222597295896),\n",
       " ('Bo Wei Gu', 0.00036812222597295896),\n",
       " ('Hua Jun Zheng', 0.00036812222597295896),\n",
       " ('Xiang Lin Zhang', 0.00036812222597295896),\n",
       " ('Mei He', 0.00036812222597295896),\n",
       " ('Kui Zheng', 0.00036812222597295896),\n",
       " ('Bo Fei Wang', 0.00036812222597295896),\n",
       " ('Gang Fu', 0.00036812222597295896),\n",
       " ('Xiao Ning', 0.00036812222597295896),\n",
       " ('Sai Juan Chen', 0.00036812222597295896),\n",
       " ('Zhu Chen', 0.00036812222597295896),\n",
       " ('Hua Tang', 0.00036812222597295896),\n",
       " ('Shuang Xi Ren', 0.00036812222597295896),\n",
       " ('Yang Zhong', 0.00036812222597295896),\n",
       " ('Zong Ming', 0.00036812222597295896),\n",
       " ('You Gang', 0.00036812222597295896),\n",
       " ('Xiang Yin', 0.00036812222597295896),\n",
       " ('Wei Zhong', 0.00036812222597295896),\n",
       " ('Yi Xue', 0.00036812222597295896),\n",
       " ('Chung I. Wu', 0.00036812222597295896),\n",
       " ('Guo Ping Zhao', 0.00036812222597295896),\n",
       " ('Rossa W.K. Chiu', 0.00036812222597295896),\n",
       " ('Stephen S.C. Chim', 0.00036812222597295896),\n",
       " ('Yu Kwan Tong 8', 0.00036812222597295896),\n",
       " ('Alison Bermingham', 0.0003664800664034336),\n",
       " ('Monica Galiano', 0.0003664800664034336),\n",
       " ('Boyd L. Yount', 0.00036534751323368377),\n",
       " ('Kari Debbink', 0.00036534751323368377),\n",
       " ('Jessica A. Plante', 0.00036534751323368377),\n",
       " ('Trevor Scobey', 0.00036534751323368377),\n",
       " ('Wayne A. Marasco', 0.00036534751323368377),\n",
       " ('Florian Gloza-Rausch', 0.0003653291406657231),\n",
       " ('Victor Max Corman', 0.0003653291406657231),\n",
       " ('Antje Seebens', 0.0003653291406657231),\n",
       " ('Peter Vallo', 0.0003653291406657231),\n",
       " ('Chuan Qin', 0.00036527404419893814),\n",
       " ('Nanshan Chen', 0.00036483358694003196),\n",
       " ('Xuan Dong', 0.00036483358694003196),\n",
       " ('Fengyun Gong', 0.00036483358694003196),\n",
       " ('Yang Han', 0.00036483358694003196),\n",
       " ('Yang Qiu', 0.00036483358694003196),\n",
       " ('Jingli Wang', 0.00036483358694003196),\n",
       " ('Jasper F. W. Chan', 0.0003636391765011863),\n",
       " ('Yuhai Bi', 0.0003591361104355242),\n",
       " ('Raymond J Pickles', 0.00035394711175466516),\n",
       " ('Davide Corti', 0.00035394711175466516),\n",
       " ('Gert Jan Godeke', 0.00035316871810371376),\n",
       " ('Lia van der Hoek', 0.00034431625496659114),\n",
       " ('Maarten F Jebbink', 0.00034431625496659114),\n",
       " ('Wilma Vermeulen-Oost', 0.00034431625496659114),\n",
       " ('Ron J M Berkhout', 0.00034431625496659114),\n",
       " ('Katja C Wolthers', 0.00034431625496659114),\n",
       " ('Pauline M E Wertheim-van Dillen', 0.00034431625496659114),\n",
       " ('Jos Kaandorp', 0.00034431625496659114),\n",
       " ('Joke Spaargaren', 0.00034431625496659114),\n",
       " ('Ben Berkhout', 0.00034431625496659114),\n",
       " ('Amy C Sims', 0.00034308845415757447),\n",
       " ('Rachel Lauren Graham', 0.00034308845415757447),\n",
       " ('Carlos Gutierrez', 0.00034308190154904057),\n",
       " ('Laura Smits De Vries', 0.00034308190154904057),\n",
       " ('Yasmin E. El Tahir', 0.00034308190154904057),\n",
       " ('Rita De Sousa', 0.00034308190154904057),\n",
       " ('Janko van Beek', 0.00034308190154904057),\n",
       " ('Norbert Nowotny', 0.00034308190154904057),\n",
       " ('Kees van Maanen', 0.00034308190154904057),\n",
       " ('Ezequiel Hidalgo-Hermoso', 0.00034308190154904057),\n",
       " ('Peter Rottier', 0.00034308190154904057),\n",
       " ('Christian Gortázar-Schmidt', 0.00034308190154904057),\n",
       " ('Bo Hu', 0.0003427993024855682),\n",
       " ('Yong Xiong', 0.0003427993024855682),\n",
       " ('Yan Zhao', 0.0003427993024855682),\n",
       " ('Joseph S M Peiris', 0.00034271551246345156),\n",
       " ('Albert D M E Osterhaus', 0.00034271551246345156),\n",
       " ('Klaus', 0.00034271551246345156),\n",
       " ('Dawei Wang', 0.000341427668347657),\n",
       " ('Chang Hu', 0.000341427668347657),\n",
       " ('Fangfang Zhu', 0.000341427668347657),\n",
       " ('Xing Liu', 0.000341427668347657),\n",
       " ('Jing Zhang', 0.000341427668347657),\n",
       " ('Binbin Wang', 0.000341427668347657),\n",
       " ('Hui Xiang', 0.000341427668347657),\n",
       " ('Yirong Li', 0.000341427668347657),\n",
       " ('Xinghuan Wang', 0.000341427668347657),\n",
       " ('Zhiyong Peng', 0.000341427668347657),\n",
       " ('Scott R. Royal', 0.0003384694253867636),\n",
       " ('Jesica Swanstrom', 0.0003384694253867636),\n",
       " ('Scott H Randell', 0.0003384694253867636),\n",
       " ('Hua Guo', 0.0003365742543559623),\n",
       " ('Bing Yan', 0.0003321679117364279),\n",
       " ('Xian Guang Wang', 0.0003315207630673663),\n",
       " ('Lei Zhang', 0.0003315207630673663),\n",
       " ('Hao Rui Si', 0.0003315207630673663),\n",
       " ('Chao Lin Huang', 0.0003315207630673663),\n",
       " ('Hui Dong Chen', 0.0003315207630673663),\n",
       " ('Yun Luo', 0.0003315207630673663),\n",
       " ('Ren Di Jiang', 0.0003315207630673663),\n",
       " ('Mei Qin Liu', 0.0003315207630673663),\n",
       " ('Ying Chen', 0.0003315207630673663),\n",
       " ('Xi Wang', 0.0003315207630673663),\n",
       " ('Quan Jiao Chen', 0.0003315207630673663),\n",
       " ('Fei Deng', 0.0003315207630673663),\n",
       " ('Lin Lin Liu', 0.0003315207630673663),\n",
       " ('Fa Xian Zhan', 0.0003315207630673663),\n",
       " ('Yan Yi Wang', 0.0003315207630673663),\n",
       " ('Geng Fu Xiao', 0.0003315207630673663),\n",
       " ('Jie Zhou', 0.00033045451536568653),\n",
       " ('Suok Kai Chew', 0.00033004801158347414),\n",
       " ('Marc Lipsitch', 0.00032607515201805705),\n",
       " ('Ted Cohen', 0.00032607515201805705),\n",
       " ('Ben Cooper', 0.00032607515201805705),\n",
       " ('James M. Robins', 0.00032607515201805705),\n",
       " ('Stefan Ma', 0.00032607515201805705),\n",
       " ('Lyn James', 0.00032607515201805705),\n",
       " ('Gowri Gopalakrishna', 0.00032607515201805705),\n",
       " ('Chorh Chuan Tan', 0.00032607515201805705),\n",
       " ('Matthew H. Samore', 0.00032607515201805705),\n",
       " ('David Fisman', 0.00032607515201805705),\n",
       " ('Megan Murray', 0.00032607515201805705),\n",
       " ('Sander van Boheemen', 0.0003242492872718978),\n",
       " ('Miranda de Graaf', 0.0003242492872718978),\n",
       " ('Chris Lauber', 0.0003242492872718978),\n",
       " ('Kelvin K. W. To', 0.0003226226542018835),\n",
       " ('Vincent C. C. Cheng', 0.0003226226542018835),\n",
       " ('Vincent J. Munster', 0.0003211699029263936),\n",
       " ('Keith', 0.000318935828417304),\n",
       " ('Steve', 0.000318935828417304),\n",
       " ('Gabriel M Leung', 0.0003186194286187754),\n",
       " ('Eric F. Donaldson', 0.000318200259241648),\n",
       " ('J. S. Malik Peiris', 0.0003026584694796831),\n",
       " ('Sudhakar Agnihothram', 0.00030004001134487454),\n",
       " ('Joseph T Wu', 0.00029924660910032755),\n",
       " ('Gary Wong', 0.00029683741082352916),\n",
       " ('George', 0.00029447456286772023),\n",
       " ('Chantal Reusken', 0.000293297164654461),\n",
       " ('Stéphane Guindon', 0.00029048436899776185),\n",
       " ('Olivier Gascuel', 0.00029048436899776185),\n",
       " ('John', 0.0002873934300881484),\n",
       " ('Amanda', 0.0002860534030437865),\n",
       " ('Camilla Rothe', 0.00028285025200058616),\n",
       " ('Patrick Vollmar', 0.00028285025200058616),\n",
       " ('Katrin Zwirglmaier', 0.00028285025200058616),\n",
       " ('Sabine Zange', 0.00028285025200058616),\n",
       " ('Roman Wölfel', 0.00028285025200058616),\n",
       " ('Michael Hoelscher', 0.00028285025200058616),\n",
       " ('Esam I. Azhar', 0.00028245631626730693),\n",
       " ('Scott', 0.00027822547959071203),\n",
       " ('Michelle L Holshue', 0.00027793720022270416),\n",
       " ('Chas DeBolt', 0.00027793720022270416),\n",
       " ('Kathy H', 0.00027793720022270416),\n",
       " ('Hollianne', 0.00027793720022270416),\n",
       " ('Christopher', 0.00027793720022270416),\n",
       " ('Sara', 0.00027793720022270416),\n",
       " ('Ahmet', 0.00027793720022270416),\n",
       " ('LeAnne', 0.00027793720022270416),\n",
       " ('Anita', 0.00027793720022270416),\n",
       " ('Susan I', 0.00027793720022270416),\n",
       " ('Lindsay', 0.00027793720022270416),\n",
       " ('Xiaoyan', 0.00027793720022270416),\n",
       " ('Mark A', 0.00027793720022270416),\n",
       " ('William C', 0.00027793720022270416),\n",
       " ('Holly M', 0.00027793720022270416),\n",
       " ('Timothy M', 0.00027793720022270416),\n",
       " ('Satish K', 0.00027793720022270416),\n",
       " ('Jie Xiang', 0.0002778951322535226),\n",
       " ('Scott H. Randell', 0.0002732159059411479),\n",
       " ('Zhengli Li Shi', 0.0002732159059411479),\n",
       " ('Jörg Glende', 0.0002726133043153674),\n",
       " ('Matthias Goettsche', 0.0002726133043153674),\n",
       " ('Matthias Niedrig', 0.0002726133043153674),\n",
       " ('Susanne Pfefferle', 0.0002726133043153674),\n",
       " ('Stoian Yordanov', 0.0002726133043153674),\n",
       " ('Lyubomir Zhelyazkov', 0.0002726133043153674),\n",
       " ('Uwe Hermanns', 0.0002726133043153674),\n",
       " ('Alexander Lukashev', 0.0002726133043153674),\n",
       " ('Marcel Alexander Müller', 0.0002726133043153674),\n",
       " ('Hongkui Deng', 0.0002726133043153674),\n",
       " ('Georg Herrler', 0.0002726133043153674),\n",
       " ('M.A. Chand', 0.0002711734376823281),\n",
       " ('C.S. Brown', 0.0002711734376823281),\n",
       " ('E. Aarons', 0.0002711734376823281),\n",
       " ('C. Tong', 0.0002711734376823281),\n",
       " ('C. Langrish', 0.0002711734376823281),\n",
       " ('K. Hoschler', 0.0002711734376823281),\n",
       " ('Kevin Brown', 0.0002711734376823281),\n",
       " ('R.G. Pebody', 0.0002711734376823281),\n",
       " ('H.K. Green', 0.0002711734376823281),\n",
       " ('N.L. Boddington', 0.0002711734376823281),\n",
       " ('Robin Gopal', 0.0002711734376823281),\n",
       " ('N. Price', 0.0002711734376823281),\n",
       " ('W. Newsholme', 0.0002711734376823281),\n",
       " ('Arnaud Fontanet', 0.0002674406106966422),\n",
       " ('Benoit Guery', 0.00026482698619572796),\n",
       " ('Julien Poissy', 0.00026482698619572796),\n",
       " ('Anne Goffard', 0.00026482698619572796),\n",
       " ('Daniel Mathieu', 0.00026482698619572796),\n",
       " ('Yanfeng Yao', 0.00026089587405750766),\n",
       " ('Souhaib Aldabbagh', 0.0002595154913134526),\n",
       " ('Neeltje van Doremalen', 0.00025867987176021517),\n",
       " ('Friederike Feldmann', 0.0002583163566790475),\n",
       " ('Atsushi Okumura', 0.0002583163566790475),\n",
       " ('Tariq A. Madani', 0.00025640330372252916),\n",
       " ('Linlin Bao', 0.0002560247418318369),\n",
       " ('Vincent J Munster', 0.00025582215454844555),\n",
       " ('R. J. de Groot', 0.00025544940617482114),\n",
       " ('S. C. Baker', 0.00025544940617482114),\n",
       " ('R. S. Baric', 0.00025544940617482114),\n",
       " ('C. S. Brown', 0.00025544940617482114),\n",
       " ('C. Drosten', 0.00025544940617482114),\n",
       " ('L. Enjuanes', 0.00025544940617482114),\n",
       " ('R. A. M. Fouchier', 0.00025544940617482114),\n",
       " ('M. Galiano', 0.00025544940617482114),\n",
       " ('A. E. Gorbalenya', 0.00025544940617482114),\n",
       " ('Z. A. Memish', 0.00025544940617482114),\n",
       " ('S. Perlman', 0.00025544940617482114),\n",
       " ('E. J. Snijder', 0.00025544940617482114),\n",
       " ('G. M. Stephens', 0.00025544940617482114),\n",
       " ('P. C. Y. Woo', 0.00025544940617482114),\n",
       " ('A. M. Zaki', 0.00025544940617482114),\n",
       " ('M. Zambon', 0.00025544940617482114),\n",
       " ('J. Ziebuhr', 0.00025544940617482114),\n",
       " ('Yi Huang', 0.0002535079849589741),\n",
       " ('Darren P. Martin', 0.0002531130216000774),\n",
       " ('Philippe Lemey', 0.0002531130216000774),\n",
       " ('Martin Lott', 0.0002531130216000774),\n",
       " ('Vincent Moulton', 0.0002531130216000774),\n",
       " ('David Posada', 0.0002531130216000774),\n",
       " ('Pierre Lefeuvre', 0.0002531130216000774),\n",
       " ('Mark R. Denison', 0.0002461869458740853),\n",
       " ('Ziad A.', 0.00024264910283813295),\n",
       " ('Angela L Rasmussen', 0.00024231151015542508),\n",
       " ('Dana P Scott', 0.00024231151015542508),\n",
       " ('Doug Brining', 0.00024231151015542508),\n",
       " ('Trenton Bushmaker', 0.00024231151015542508),\n",
       " ('Cynthia Martellaro', 0.00024231151015542508),\n",
       " ('Laura Baseler', 0.00024231151015542508),\n",
       " ('Arndt G Benecke', 0.00024231151015542508),\n",
       " ('Michael G Katze', 0.00024231151015542508),\n",
       " ('Maria Anisimova', 0.00023277105898012552),\n",
       " ('Jean-François Dufayard', 0.00023234696152172598),\n",
       " ('Vincent Lefort', 0.00023234696152172598),\n",
       " ('Wim Hordijk', 0.00023234696152172598),\n",
       " ('Hong Shan', 0.00023152123861078708),\n",
       " ('Thijs Kuiken', 0.00023043426606340707),\n",
       " ('David S.C. Hui', 0.0002287305859346334),\n",
       " ('David S Hui', 0.00022776898740045572),\n",
       " ('Samson S. Y. Wong', 0.0002271111622457403),\n",
       " ('Steven Riley', 0.0002242586604623457),\n",
       " ('Wei Deng', 0.0002221481480031306),\n",
       " ('Stephen C. Harrison', 0.00022119103489958502),\n",
       " ('Christophe Fraser', 0.00022080408698489984),\n",
       " ('George F Gao', 0.00021986386138943148),\n",
       " ('A', 0.00021856995058688688),\n",
       " ('P', 0.00021711820513264913),\n",
       " ('Valérie Caro', 0.0002167828786167876),\n",
       " ('H', 0.00021617457147458865),\n",
       " ('Timothy P. Sheahan', 0.00021372355605137235),\n",
       " ('Amy C. Sims', 0.00021372355605137235),\n",
       " ('Sarah R. Leist', 0.00021372355605137235),\n",
       " ('Darius Babusis', 0.00021372355605137235),\n",
       " ('Michael O. Clarke', 0.00021372355605137235),\n",
       " ('Jamie E. Spahn', 0.00021372355605137235),\n",
       " ('Joy Y. Feng', 0.00021372355605137235),\n",
       " ('Tomas Cihlar', 0.00021372355605137235),\n",
       " ('Robert Jordan', 0.00021372355605137235),\n",
       " ('C. J. Peters', 0.00021219045609613686),\n",
       " ('Lei Ping Zeng', 0.00021209924976712922),\n",
       " ('Jia Zheng Xie', 0.00021209924976712922),\n",
       " ('Yun Zhi Zhang', 0.00021209924976712922),\n",
       " ('Dong Sheng Luo', 0.00021209924976712922),\n",
       " ('Mei Niang Wang', 0.00021209924976712922),\n",
       " ('Wenjie', 0.00021101082090497963),\n",
       " ('Xiang', 0.00021101082090497963),\n",
       " ('Xuejun', 0.00021101082090497963),\n",
       " ('Wenling', 0.00021101082090497963),\n",
       " ('Peihua', 0.00021101082090497963),\n",
       " ('Wenbo', 0.00021101082090497963),\n",
       " ('George F.', 0.00021101082090497963),\n",
       " ('Guizhen', 0.00021101082090497963),\n",
       " ('Abdullah M. Assiri', 0.00021002017594388422),\n",
       " ('Tao Wang', 0.00020640333024065928),\n",
       " ('Vincent Enouf', 0.00020609027657826975),\n",
       " ('Wenjun Liu', 0.0002048557630579747),\n",
       " ('Wei-jie Guan', 0.00020379252508446666),\n",
       " ('Zheng-yi Ni', 0.00020379252508446666),\n",
       " ('Yu Hu', 0.00020379252508446666),\n",
       " ('Wenhua Liang', 0.00020379252508446666),\n",
       " ('Chun-quan Ou', 0.00020379252508446666),\n",
       " ('Jianxing He', 0.00020379252508446666),\n",
       " ('Lei Liu', 0.00020379252508446666),\n",
       " ('Chunliang Lei', 0.00020379252508446666),\n",
       " ('Bin Du', 0.00020379252508446666),\n",
       " ('Lan-juan Li', 0.00020379252508446666),\n",
       " ('Guang Zeng', 0.00020379252508446666),\n",
       " ('Ruchong Chen', 0.00020379252508446666),\n",
       " ('Chun-Li Tang', 0.00020379252508446666),\n",
       " ('Ping-yan Chen', 0.00020379252508446666),\n",
       " ('Shiyue Li', 0.00020379252508446666),\n",
       " ('Jinlin Wang', 0.00020379252508446666),\n",
       " ('Zi Jing Liang', 0.00020379252508446666),\n",
       " ('Yi-xiang Peng', 0.00020379252508446666),\n",
       " ('Li Wei', 0.00020379252508446666),\n",
       " ('Yong Liu', 0.00020379252508446666),\n",
       " ('Ya-hua Hu', 0.00020379252508446666),\n",
       " ('Peng Peng', 0.00020379252508446666),\n",
       " ('Jian-ming Wang', 0.00020379252508446666),\n",
       " ('Ji-yang Liu', 0.00020379252508446666),\n",
       " ('Zhong Chen', 0.00020379252508446666),\n",
       " ('Gang Li', 0.00020379252508446666),\n",
       " ('Zhi-jian Zheng', 0.00020379252508446666),\n",
       " ('Shao-qin Qiu', 0.00020379252508446666),\n",
       " ('Jie Luo', 0.00020379252508446666),\n",
       " ('Chang-jiang Ye', 0.00020379252508446666),\n",
       " ('Shao-yong Zhu', 0.00020379252508446666),\n",
       " ('Nanshan Zhong', 0.00020379252508446666),\n",
       " ('Bin', 0.00020343295764554706),\n",
       " ('Cun Li', 0.00020152663301998462),\n",
       " ('Dong Wang', 0.00020152663301998462),\n",
       " ('Bosco Ho-Yin Wong', 0.00020152663301998462),\n",
       " ('Dong Yang', 0.00020152663301998462),\n",
       " ('Ivy Hau-Yee Chan', 0.00020152663301998462),\n",
       " ('Jasper Fuk-Woo Chan', 0.00020152663301998462),\n",
       " ('Kelvin Kai-Wang To', 0.00020152663301998462),\n",
       " ('Yi Zhang', 0.00019911558679697918),\n",
       " ('Chung-ming Chu', 0.0001979800912984344),\n",
       " ('Kwok-hung Chan', 0.0001979800912984344),\n",
       " ('Hoi-wah Tsoi', 0.0001979800912984344),\n",
       " ('Beatrice H. L. Wong', 0.0001979800912984344),\n",
       " ('Rosana W. S. Poon', 0.0001979800912984344),\n",
       " ('James J. Cai', 0.0001979800912984344),\n",
       " ('Wei-kwang Luk', 0.0001979800912984344),\n",
       " ...]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorities = dict([(i, a[authors_to_index[i]]) for i in authors_to_index])\n",
    "sorted(authorities.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "measured-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kyunghyun Cho', 'Bart van Merrienboer', 'Caglar Gulcehre', 'Dzmitry Bahdanau', 'Fethi Bougares', 'Holger Schwenk', 'Yoshua Bengio', '', '']\n",
      "['Yann Lecun', 'Leon Bottou', '', 'Yoshua Bengio', '', '', 'Patrick Haffner', '']\n",
      "['Yann LeCun', '', 'Yoshua Bengio', 'Geoffrey Hinton', '']\n",
      "['Kyunghyun Cho', 'Bart van Merrienboer', 'Dzmitry Bahdanau', 'Yoshua Bengio', '', '']\n",
      "['Abdullah Assiri', 'Jaffar A Al-Tawfiq', 'Abdullah A Al-Rabeeah', 'Fahad A Al-Rabiah', 'Sami Al-Hajjar', 'Ali Al-Barrak', 'Hesham Flemban', 'Wafa N Al-Nassir', 'Hanan H Balkhy', 'Rafat F Al-Hakeem', 'Hatem Q Makhdoom', 'Alimuddin I Zumla', '', 'Ziad A Memish']\n",
      "['Na Zhu', 'Dingyu Zhang', 'Wenling Wang', 'Xingwang Li', 'Bo Yang', 'Jingdong Song', 'Xiang Zhao', 'Baoying Huang', 'Weifeng Shi', 'Roujian Lu', 'Peihua Niu', 'Faxian Zhan', 'Xuejun Ma', '', 'Dayan Wang', 'Wenbo Xu', 'Guizhen Wu', 'George F. Gao', 'Wenjie Tan']\n",
      "['Jasper Fuk Woo Chan', 'Shuofeng Yuan', 'Kin Hang Kok', 'Kelvin Kai Wang To', '', 'Hin Chu', 'Jin Yang', 'Fanfan Xing', 'Jieling Liu', 'Cyril Chik Yan Yip', 'Rosana Wing Shan Poon', 'Hoi Wah Tsoi', 'Simon Kam Fai Lo', 'Kwok Hung Chan', 'Vincent Kwok Man Poon', 'Wan Mui Chan', 'Jonathan Daniel Ip', 'Jian Piao Cai', 'Vincent Chi Chung Cheng', 'Honglin Chen', '', 'Christopher Kim Ming Hui', 'Kwok Yung Yuen']\n",
      "['Roujian Lu', 'Xiang Zhao', 'Juan Li', 'Peihua Niu', 'Bo Yang', 'Honglong Wu', 'Wenling Wang', 'Hao Song', 'Baoying Huang', 'Na Zhu', 'Yuhai Bi', 'Xuejun Ma', 'Faxian Zhan', 'Liang Wang', 'Tao Hu', 'Hong Zhou', 'Zhenhong Hu', 'Weimin Zhou', 'Li Zhao', 'Jing Chen', 'Yao Meng', 'Ji Wang', 'Yang Lin', 'Jianying Yuan', 'Zhihao Xie', 'Jinmin Ma', 'William J Liu', 'Dayan Wang', 'Wenbo Xu', 'Edward C Holmes', 'George F Gao', '', 'Guizhen Wu', 'Weijun Chen', 'Weifeng Shi', 'Wenjie Tan', '']\n",
      "['Yang Pan', '', 'Daitao Zhang', 'Peng Yang', '', 'Leo L M Poon', 'Quanyi Wang']\n",
      "['Christian Szegedy', 'Wojciech Zaremba', 'Ilya Sutskever', 'Joan Bruna', 'Dumitru Erhan', 'Ian Goodfellow', 'Rob Fergus', '']\n",
      "['Yujia Li', 'Kevin Swersky', 'Rich Zemel', '']\n",
      "['Koby Crammer', '', 'Ofer Dekel', 'Joseph Keshet', 'Shai Shalev-Shwartz', 'Yoram Singer']\n",
      "['Yoshua Bengio', '', '', 'Yann']\n",
      "['Daniel D. Lee', 'H. Sebastian Seung', '']\n",
      "['Shuo Su', 'Gary Wong', 'Weifeng Shi', 'Jun Liu', '', 'Alexander C.K. Lai', 'Jiyong Zhou', 'Wenjun Liu', 'Yuhai Bi', 'George F. Gao']\n",
      "['Xing Yi Ge', 'Jia Lu Li', 'Xing Lou Yang', 'Aleksei A. Chmura', 'Guangjian Zhu', 'Jonathan H. Epstein', 'Jonna A Mazet', 'Ben Hu', 'Wei Zhang', 'Cheng Peng', 'Yu Ji Zhang', 'Chu Ming Luo', 'Bing Tan', 'Ning Wang', 'Yan Zhu', 'Gary Crameri', 'Shu Yi Zhang', 'Lin Fa Wang', '', 'Peter Daszak', 'Zheng Li Shi']\n",
      "['V. Stalin Raj', 'Huihui Mou', 'Saskia L. Smits', 'Dick H. W. Dekkers', 'Marcel A. Müller', 'Ronald Dijkman', 'Doreen Muth', 'Jeroen A. A. Demmers', 'Ali Zaki', 'Ron A. M. Fouchier', 'Volker Thiel', '', 'Christian Drosten', 'Peter J. M. Rottier', 'Albert D. M. E. Osterhaus', 'Berend Jan Bosch', 'Bart L. Haagmans']\n",
      "['Yaseen M. Arabi', '', '', 'Adel Alothman', '', 'Hanan H. Balkhy', '', 'Abdulaziz Al-Dawood', '', 'Sameera AlJohani', '', 'Shmeylan Al Harbi', '', 'Suleiman Kojan', '', 'Majed Al Jeraisy', '', 'Ahmad M. Deeb', '', 'Abdullah M. Assiri', 'Fahad Al-Hameed', '', 'Asim AlSaedi', '', 'Yasser Mandourah', 'Ghaleb A. Almekhlafi', 'Nisreen Murad Sherbeeni', 'Fatehi Elnour Elzein', 'Javed Memon', 'Yusri Taha', 'Abdullah Almotairi', 'Khalid A. Maghrabi', 'Ismael Qushmaq', 'Ali Al Bshabshe', 'Ayman Kharaba', 'Sarah Shalhoub', 'Jesna Jose', 'Robert A. Fowler', '', 'Frederick G. Hayden', 'Mohamed A. Hussein']\n",
      "['Maria L. Agostini', 'Erica L. Andres', 'Amy C Sims', 'Rachel Lauren Graham', 'Timothy Patrick Sheahan', 'Xiaotao Lu', 'Everett Clinton Smith', '', 'James Brett Case', 'Joy Y. Feng', 'Robert Jordan', 'Adrian S. Ray', 'Tomas Cihlar', 'Dustin Siegel', 'Richard L. Mackman', 'Michael O. Clarke', 'Ralph S Baric', 'Mark R. Denison']\n",
      "['Susan M. Poutanen', '', 'Donald E. Low', 'Bonnie Henry', 'Sandy Finkelstein', 'David Rose', 'Karen Green', 'Raymond Tellier', '', 'Ryan Draker', 'Dena Adachi', 'Melissa Ayers', 'Adrienne K. Chan', 'Danuta M. Skowronski', 'Irving Salit', 'Andrew E. Simor', 'Arthur S. Slutsky', 'Patrick W. Doyle', 'Mel Krajden', 'Martin Petric', 'Robert C. Brunham', 'Allison J. McGeer']\n",
      "['Steven G. Coca', 'Bushra Yusuf', '', 'Michael G. Shlipak', '', 'Amit X. Garg', 'Chirag R. Parikh', '']\n",
      "['Abdullah Assiri', 'Allison McGeer', 'Trish M. Perl', 'Connie S. Price', 'Abdullah A. Al Rabeeah', 'Derek A.T. Cummings', 'Zaki N. Alabdullatif', 'Maher Assad', 'Abdulmohsen Almulhim', 'Hatem Makhdoom', 'Hossam Madani', 'Rafat Alhakeem', 'Jaffar A. Al-Tawfiq', 'Matthew Cotten', 'Simon J. Watson', 'Paul Kellam', '', 'Alimuddin I. Zumla', '', 'Ziad A. Memish']\n",
      "['Christopher M. Booth', 'Larissa M. Matukas', 'George A. Tomlinson', 'Anita R. Rachlis', 'David B. Rose', 'Hy A. Dwosh', 'Sharon L. Walmsley', 'Tony Mazzulli', '', 'Monica Avendano', 'Peter Derkach', 'Issa E. Ephtimios', 'Ian Kitai', 'Barbara D. Mederski', 'Steven B. Shadowitz', 'Wayne L. Gold', '', 'Laura A. Hawryluck', 'Elizabeth Rea', 'Jordan S. Chenkin', 'David W. Cescon', 'Susan M. Poutanen', '', '', 'Allan S. Detsky']\n",
      "['Christian Drosten', 'Michael Seilmaier', 'Victor M. Corman', 'Wulf Hartmann', 'Gregor Scheible', 'Stefan Sack', 'Wolfgang Guggemos', 'Rene Kallies', 'Doreen Muth', 'Sandra Junglen', 'Marcel A. Müller', 'Walter Haas', 'Hana Guberina', 'Tim Röhnisch', 'Monika Schmid-Wendtner', 'Souhaib Aldabbagh', 'Ulf Dittmer', 'Hermann Gold', 'Petra Graf', 'Frank Bonin', 'Andrew Rambaut', '', 'Clemens Martin Wendtner']\n",
      "['Benoit Guery', 'Julien Poissy', 'Loubna El Mansouf', 'Caroline Séjourné', 'Nicolas Ettahar', 'Xavier Lemaire', 'Fanny Vuotto', 'Anne Goffard', 'Sylvie Behillil', '', '', 'Vincent Enouf', '', '', 'Valérie Caro', 'Alexandra Mailles', 'Didier Che', 'Jean Claude Manuguerra', 'Daniel Mathieu', 'Arnaud Fontanet', '', 'Sylvie Van Der Werf', '', '']\n",
      "['Vineet D. Menachery', 'Boyd L. Yount', 'Amy C Sims', 'Kari Debbink', 'Sudhakar S. Agnihothram', 'Lisa E. Gralinski', 'Rachel Lauren Graham', 'Trevor Scobey', 'Jessica A. Plante', 'Scott R. Royal', 'Jesica Swanstrom', 'Timothy Patrick Sheahan', 'Raymond J Pickles', '', 'Davide Corti', 'Scott H Randell', 'Antonio Lanzavecchia', 'Wayne A. Marasco', 'Ralph S Baric', '']\n",
      "['Marcel A. Müller', 'Benjamin Meyer', 'Victor M. Corman', 'Malak Al-Masri', 'Abdulhafeez Turkestani', 'Daniel Ritz', 'Andrea Sieberg', 'Souhaib Aldabbagh', 'Berend J. Bosch', 'Erik Lattwein', 'Raafat F. Alhakeem', 'Abdullah M. Assiri', 'Ali M. Albarrak', 'Ali M. Al-Shangiti', 'Jaffar A. Al-Tawfiq', '', 'Paul Wikramaratna', 'Abdullah A. Alrabeeah', 'Christian Drosten', 'Ziad A. Memish']\n",
      "['George F. Gao', '']\n",
      "['Hulda Run Jonsdottir', '', 'Ronald Dijkman', '']\n",
      "['Ards Definition Task Force', 'V Marco Ranieri', 'Gordon D Rubenfeld', 'B Taylor Thompson', 'Niall D Ferguson', 'Ellen Caldwell', 'Eddy Fan', 'Luigi Camporota', '', 'Arthur S Slutsky']\n",
      "['K.K.W. To', 'C.C.Y. Yip', '', 'C.Y.W. Lai', '', 'C.K.H. Wong', 'D.T.Y. Ho', 'P.K.P. Pang', 'A.C.K. Ng', 'K.-H. Leung', 'R.W.S. Poon', '', 'K.-H. Chan', 'V.C.C. Cheng', '', 'I.F.N. Hung', '', 'K.-Y. Yuen']\n",
      "['Jie Zhou', 'Cun Li', 'Guangyu Zhao', 'Hin Chu', 'Dong Wang', 'Helen Hoi-Ning Yan', 'Vincent Kwok-Man Poon', 'Lei Wen', 'Bosco Ho-Yin Wong', 'Xiaoyu Zhao', 'Man Chun Chiu', 'Dong Yang', 'Yixin Wang', 'Rex K. H. Au-Yeung', 'Ivy Hau-Yee Chan', 'Shihui Sun', 'Jasper Fuk-Woo Chan', 'Kelvin Kai-Wang To', 'Ziad Ahmed Memish', '', 'Victor M. Corman', 'Christian Drosten', 'Ivan Fan-Ngai Hung', 'Yusen Zhou', 'Suet Yi Leung', 'Kwok-Yung Yuen']\n",
      "['Andrew Waterhouse', '', 'Martino Bertoni', '', 'Stefan Bienert', '', 'Gabriel Studer', '', 'Gerardo Tauriello', '', 'Rafal Gumienny', '', 'Florian T Heer', '', 'Tjaart A P de Beer', '', 'Christine Rempfer', '', 'Lorenza Bordoli', '', 'Rosalba Lepore', '', 'Torsten Schwede', '']\n",
      "['Gerardo Chowell', '', 'Fatima Abdirizak', 'Sunmi Lee', 'Jonggul Lee', 'Eunok Jung', 'Hiroshi Nishiura', 'Cécile Viboud']\n",
      "['Markus Weber', 'Max Welling', 'Pietro Perona', '']\n",
      "['Xiaotian Ni', '', 'Changqing Sun', 'Yaping Tian', 'Yanjie Huang', 'Tongqing Gong', 'Lan Song', 'Xing Yang', 'Kai Li', 'Nairen Zheng', 'Jianping Wang', 'Hongxing Wu', 'Ruoxian Zhang', 'Yi Wang', 'Guangshun Wang', 'Jun Qin', '']\n",
      "['Narinder S. Paul', 'Heidi Roberts', 'Jagdish Butany', 'Tae Bong Chung', 'Wayne Gold', '', 'Sangeeta Mehta', 'Eli Konen', 'Anuradha Rao', 'Yves Provost', 'Harry H. Hong', 'Leon Zelovitsky', 'Gordon L. Weisbrod']\n",
      "['Ícaro Raony', 'Camila Saggioro de Figueiredo', 'Pablo Pandolfo', 'Elizabeth Giestal-de-Araujo', '', 'Priscilla Oliveira-Silva Bomfim', '', '', 'Wilson Savino', '']\n",
      "['Daniel K W Chu', 'Yang Pan', '', 'Samuel M S Cheng', 'Kenrie P Y Hui', 'Pavithra Krishnan', 'Yingzhi Liu', 'Daisy Y M Ng', 'Carrie K C Wan', 'Peng Yang', '', 'Quanyi Wang', 'Malik Peiris', 'Leo L M Poon']\n",
      "['Fei Zhou', 'Ting Yu', 'Ronghui Du', 'Guohui Fan', 'Ying Liu', 'Zhibo Liu', 'Jie Xiang', 'Yeming Wang', 'Bin Song', 'Xiaoying Gu', '', 'Lulu Guan', 'Yuan Wei', 'Hui Li', 'Xudong Wu', 'Jiuyang Xu', 'Shengjin Tu', 'Yi Zhang', 'Hua Chen', 'Bin']\n",
      "['Roman Wölfel', 'Victor M. Corman', 'Wolfgang Guggemos', 'Michael Seilmaier', 'Sabine Zange', 'Marcel A. Müller', 'Daniela Niemeyer', 'Terry C. Jones', '', 'Patrick Vollmar', 'Camilla Rothe', 'Michael Hoelscher', 'Tobias Bleicker', 'Sebastian Brünink', 'Julia Schneider', 'Rosina Ehmann', 'Katrin Zwirglmaier', 'Christian Drosten', 'Clemens Wendtner']\n",
      "['Ying-Hui Jin', 'Lin Cai', 'Zhen-Shun Cheng', 'Hong Cheng', 'Tong Deng', '', 'Yi-Pin Fan', 'Cheng Fang', 'Di Huang', 'Lu-Qi Huang', 'Qiao Huang', 'Yong Han', 'Bo Hu', 'Fen Hu', 'Bing-Hui Li', '', 'Yi-Rong Li', 'Ke Liang', 'Li-Kai Lin', 'Li-Sha Luo', 'Jing Ma', 'Lin-Lu Ma', 'Zhi-Yong Peng', 'Yun-Bao Pan', 'Zhen-Yu Pan', 'Xue-Qun Ren', 'Hui-Min Sun', 'Ying Wang', 'Yun-Yun Wang', 'Hong Weng', 'Chao-Jie Wei', 'Dong-Fang Wu', 'Jian Xia', 'Yong Xiong', 'Hai-Bo Xu', 'Xiao-Mei Yao', 'Yu-Feng Yuan', 'Tai-Sheng Ye', 'Xiao-Chun Zhang', 'Ying-Wen Zhang', 'Yin-Gao Zhang', 'Hua-Min Zhang', 'Yan Zhao', 'Ming-Juan Zhao', 'Hao Zi', '', 'Xian-Tao Zeng', 'Yong-Yan Wang', 'Xing-Huan Wang']\n",
      "['Jasper Fuk Woo Chan', 'Kin Hang Kok', '', 'Zheng Zhu', 'Hin Chu', '', 'Kelvin Kai Wang To', 'Shuofeng Yuan', '', 'Kwok Yung Yuen']\n",
      "['Robert Verity', 'Lucy C Okell', 'Ilaria Dorigatti', 'Peter Winskill', 'Charles Whittaker', 'Natsuko Imai', 'Gina Cuomo-Dannenburg', 'Hayley Thompson', 'Patrick G T Walker', 'Han Fu', 'Amy Dighe', 'Jamie T Griffin', 'Marc Baguelin', 'Sangeeta Bhatia', 'Adhiratha Boonyasiri', 'Anne Cori', 'Zulma Cucunubá', 'Rich FitzJohn', 'Katy Gaythorpe', 'Will Green', 'Arran Hamlet', 'Wes Hinsley', 'Daniel Laydon', 'Gemma Nedjati-Gilani', 'Steven Riley', 'Sabine van Elsland', 'Erik Volz', 'Haowei Wang', 'Yuanrong Wang', 'Xiaoyue Xi', 'Christl A Donnelly', '', 'Azra C Ghani', 'Neil M Ferguson']\n",
      "['Ying Liu', 'Albert A Gayle', 'Annelies Wilder-Smith', '', 'Joacim Rocklöv']\n",
      "['Moritz U.G. Kraemer', '', 'Chia Hung Yang', 'Bernardo Gutierrez', 'Chieh Hsi Wu', 'Brennan Klein', 'David M. Pigott', 'Louis du Plessis', 'Nuno R. Faria', 'Ruoran Li', 'William P. Hanage', 'John S. Brownstein', 'Maylis Layan', 'Alessandro Vespignani', 'Huaiyu Tian', 'Christopher Dye', 'Oliver G. Pybus', 'Samuel V. Scarpino']\n",
      "['Sheng-feng Hu', 'Miao Li', 'Lan-lan Zhong', 'Shi-miao Lu', 'Ze-xia Liu', 'Jie-ying Pu', 'Jin-sheng Wen', 'Xi Huang', '']\n",
      "['Xiaobo Yang', 'Yuan Yu', 'Jiqian Xu', 'Huaqing Shu', \"Jia'an Xia\", 'Hong Liu', '', 'Yongran Wu', 'Lu Zhang', 'Zhui Yu', 'Minghao Fang', 'Ting Yu', 'Yaxin Wang', 'Shangwen Pan', 'Xiaojing Zou', 'Shiying Yuan', 'You Shang', '']\n",
      "['Zhen Dong Guo', 'Zhong Yi Wang', 'Shou Feng Zhang', 'Xiao Li', 'Lin Li', 'Chao Li', 'Yan Cui', 'Rui Bin Fu', 'Yun Zhu Dong', 'Xiang Yang Chi', 'Meng Yao Zhang', 'Kun Liu', 'Cheng Cao', 'Bin Liu', 'Ke Zhang', 'Yu Wei Gao', 'Bing Lu', 'Wei Chen', '']\n",
      "['Po Ying Chia', '', 'Kristen Kelli Coleman', 'Yian Kim Tan', 'Sean Wei Xiang Ong', 'Marcus Gum', 'Sok Kiang Lau', 'Xiao Fang Lim', 'Ai Sim Lim', 'Stephanie Sutjipto', 'Pei Hua Lee', 'Barnaby Edward Young', '', 'Donald K Milton', 'Gregory C Gray', '', 'Stephan Schuster', 'Timothy Barkham', '', 'Partha Pratim De', '', 'Shawn Vasoo', '', 'Monica Chan', 'Brenda Sze Peng Ang', 'Boon Huan Tan', 'Yee-Sin Leo', 'Oon-Tek Ng', '', 'Michelle Su Yen Wong', 'Kalisvar Marimuthu', '']\n",
      "['Giacomo Grasselli', '', 'Alberto Zangrillo', 'Alberto Zanella', '', 'Massimo Antonelli', '', 'Luca Cabrini', 'Antonio Castelli', 'Danilo Cereda', 'Antonio Coluccello', 'Giuseppe Foti', 'Roberto Fumagalli', 'Giorgio Iotti', 'Nicola Latronico', 'Luca Lorini', 'Stefano Merler', 'Giuseppe Natalini', 'Alessandra Piatti', 'Marco Vito Ranieri', 'Anna Mara Scandroglio', 'Enrico Storti', 'Maurizio Cecconi', 'Antonio Pesenti', '']\n",
      "['Maxime Oquab', '', 'Leon Bottou', 'Ivan Laptev', 'Josef Sivic']\n",
      "['Bruno A. Olshausen', '', 'David J. Field']\n",
      "['Thomas Martinetz', '', 'Klaus Schulten']\n",
      "['', '', '']\n",
      "['Yann LeCun', 'Bernhard E. Boser', 'John S. Denker', '', 'Donnie Henderson', 'R. E. Howard', 'Wayne E. Hubbard', 'Lawrence D. Jackel']\n",
      "['L. Bottou', 'C. Cortes', '', 'J.S. Denker', '', 'H. Drucker', '', 'I. Guyon', 'L.D.', 'Y.', 'U.A.', 'E. Sackinger', 'P. Simard', '', 'V.']\n",
      "['Danielle S. McNamara', '', 'Eileen Kintsch', 'Nancy Butler Songer', 'Walter Kintsch']\n",
      "['K', 'P W', '']\n",
      "['', '']\n",
      "['Leon Bottou', '', 'Yann']\n",
      "['Nicolas Pinto', '', 'David Doukhan', '', 'James J. DiCarlo', '', 'David Daniel Cox', '', '']\n",
      "['Scott Kirkpatrick', 'David Sherrington', '']\n",
      "['Scott Kirkpatrick', '']\n",
      "['J. D. Pettigrew', '', 'T. Nikara', '', 'P. O. Bishop', '']\n",
      "['Andrew Fire', 'SiQun Xu', 'Mary K. Montgomery', 'Steven A. Kostas', '', 'Samuel E. Driver', 'Craig C. Mello']\n",
      "['Morgan N. Price', 'Paramvir S. Dehal', 'Adam P. Arkin', '']\n",
      "['Chantal B.E.M. Reusken', 'Bart L. Haagmans', 'Marcel A. Müller', 'Carlos Gutierrez', 'Gert Jan Godeke', 'Benjamin Meyer', 'Doreen Muth', 'V. Stalin Raj', 'Laura Smits De Vries', 'Victor M. Corman', 'Jan Felix Drexler', 'Saskia L. Smits', 'Yasmin E. El Tahir', 'Rita De Sousa', 'Janko van Beek', 'Norbert Nowotny', '', 'Kees van Maanen', 'Ezequiel Hidalgo-Hermoso', 'Berend Jan Bosch', 'Peter Rottier', 'Albert Osterhaus', 'Christian Gortázar-Schmidt', 'Christian Drosten', 'Marion P.G. Koopmans']\n",
      "['Sander van Boheemen', 'Miranda de Graaf', 'Chris Lauber', 'Theo M. Bestebroer', 'V. Stalin Raj', 'Ali Moh Zaki', 'Albert D. M. E. Osterhaus', 'Bart L. Haagmans', 'Alexander E. Gorbalenya', '', 'Eric J. Snijder', 'Ron A. M. Fouchier']\n",
      "['Alison Bermingham', 'M.A. Chand', 'C.S. Brown', '', 'E. Aarons', 'C. Tong', 'C. Langrish', 'K. Hoschler', 'Kevin Brown', 'Monica Galiano', 'Richard Myers', 'R.G. Pebody', 'H.K. Green', 'N.L. Boddington', 'Robin Gopal', 'N. Price', 'W. Newsholme', 'Christian Drosten', 'Ron Fouchier', 'Maria Zambon']\n",
      "['Darryl Falzarano', 'Emmie de Wit', 'Angela L Rasmussen', 'Friederike Feldmann', 'Atsushi Okumura', 'Dana P Scott', 'Doug Brining', 'Trenton Bushmaker', 'Cynthia Martellaro', 'Laura Baseler', 'Arndt G Benecke', '', 'Michael G Katze', 'Vincent J Munster', 'Heinz Feldmann', '']\n",
      "['Xiangguo Qiu', 'Gary Wong', '', 'Jonathan Audet', '', 'Alexander Bello', '', 'Lisa Fernando', 'Judie B. Alimonti', 'Hugues Fausther-Bovendo', '', 'Haiyan Wei', '', 'Jenna Aviles', 'Ernie Hiatt', 'Ashley Johnson', 'Josh Morton', 'Kelsi Swope', 'Ognian Bohorov', 'Natasha Bohorova', 'Charles Goodman', 'Do Kim', 'Michael H. Pauly', 'Jesus Velasco', 'James Pettitt', '', 'Gene G. Olinger', '', 'Kevin Whaley', 'Bianli Xu', 'James E. Strong', '', 'Larry Zeitlin', 'Gary P. Kobinger']\n",
      "['Luke Hunt', 'Ankur Gupta-Wright', 'Victoria Simms', 'Fayia Tamba', 'Victoria Knott', 'Kongoneh Tamba', 'Saidu Heisenberg-Mansaray', 'Emmanuel Tamba', 'Alpha Sheriff', 'Sulaiman Conteh', 'Tom Smith', 'Shelagh Tobin', 'Tim Brooks', 'Catherine Houlihan', 'Rachael Cummings', 'Tom Fletcher', '']\n",
      "['Joseph B. McCormick', 'Isabel J. King', 'Patricia A. Webb', 'Karl M. Johnson', \"Renie O'Sullivan\", 'Ethleen S. Smith', 'Sally Trippel', '', 'Tony C. Tong']\n",
      "['Eric A J Hoste', '', 'Gilles Clermont', 'Alexander Kersten', 'Ramesh Venkataraman', 'Derek C Angus', 'Dirk De Bacquer', 'John A Kellum']\n",
      "['Sean M Bagshaw', '', 'Carol George', 'Rinaldo Bellomo', '']\n",
      "['Rebecca DerSimonian', '', 'Nan Laird', '']\n",
      "['D F Stroup', 'J A Berlin', 'S C Morton', 'I Olkin', 'G D Williamson', 'D Rennie', '', 'D Moher', 'B J Becker', 'T A Sipe', 'S B Thacker']\n",
      "['Areef Ishani', 'Jay L. Xue', 'Jonathan Himmelfarb', 'Paul W. Eggers', 'Paul L. Kimmel', '', 'Bruce A. Molitoris', 'Allan J. Collins']\n",
      "['Robert N. Foley', 'Patrick S. Parfrey', '', 'John D. Harnett', '', 'Gloria M. Kent', '', 'Christopher J. Martin', '', 'David C. Murray', '', 'Paul E. Barre', '']\n",
      "['Christopher H Heath', '', 'C Terri Orrell', 'Rosie Ce Lee', 'John W Pearman', 'Cheryll McCullough', 'Keryn J Christiansen']\n",
      "['T. G. Wald', '', 'B. A. Miller', 'P. Shult', 'Paul Drinka', 'L. Langer', 'Stefan Gravenstein', '']\n",
      "['Alimuddin Zumla', '', 'Vanya Gant', 'Matthew Bates', 'Peter Mwaba', '', 'Markus Maeurer', 'Ziad A Memish']\n",
      "['Justin Lessler', 'Nicholas G Reich', 'Ron Brookmeyer', 'Trish M Perl', '', 'Kenrad E Nelson', 'Derek A T Cummings']\n",
      "['Christopher J.L. Murray', 'Katrina F. Ortblad', 'Caterina Guinovart', 'Stephen S. Lim', 'Timothy M. Wolock', 'D. Allen Roberts', 'Emily A. Dansereau', 'Nicholas Graetz', 'Ryan M. Barber', 'Jonathan C. Brown', 'Haidong Wang', 'Herbert C. Duber', 'Mohsen Naghavi', 'Daniel Dicker', 'Lalit Dandona', '', 'Joshua A. Salomon', 'Kyle R. Heuton', 'Kyle Foreman', 'David E. Phillips', 'Thomas D. Fleming', 'Abraham D. Flaxman', 'Bryan K. Phillips', 'Elizabeth K. Johnson', 'Megan S. Coggeshall', 'Foad Abd-Allah', 'Semaw Ferede', 'Jerry P. Abraham', 'Ibrahim Abubakar', 'Laith J. Abu-Raddad', 'Niveen Me', 'Tom', 'Austine Olufemi', 'Arsène Kouablan', 'José C. Adsuar', 'Emilie Elisabet', 'Dickens Akena', 'Mazin J. Al', 'Deena', 'Mohammed I.', 'Gabriel', 'Miguel Angel Alegretti', 'Zewdie Aderaw Alemu', 'Rafael Alfonso-Cristancho', 'Samia Alhabib', 'Raghib Ali', 'Francois Alla', 'Peter J.', 'Ubai Alsharif', 'Elena', 'Nelson Alvis-Guzman 17']\n",
      "['Romulus Breban', 'Julien Riou', 'Arnaud Fontanet', '']\n",
      "['Stephen K. Gire', 'Augustine', 'Kristian G. Andersen', '', 'Rachel S G Sealfon', 'Daniel J. Park', 'Lansana', 'Simbirie', 'Mambu', 'Mohamed', 'Gytis Dudas', 'Shirlee Wohl', 'Lina M. Moses', 'Nathan L. Yozwiak', 'Sarah Winnicki', 'Christian B. Matranga', 'Christine M. Malboeuf', 'James Qu', 'Adrianne D. Gladden', 'Stephen F. Schaffner', 'Xiao Yang', 'Pan Pan Jiang', 'Mahan Nekoui', 'Andres Colubri', 'Moinya Ruth', 'Mbalu', 'Alex', 'Michael', 'Fatima K.', 'Veronica', 'Edwin', 'Sidiki', 'Josephine', 'Abdul Azziz', 'Alice', 'James', 'Ibrahim', 'Kandeh', 'Momoh', 'Mohamed', 'Franklyn', 'Willie', 'James L B', 'Sinéad B. Chapman', 'James Bochicchio', 'Cheryl Murphy', 'Chad Nusbaum', 'Sarah Young', 'Bruce W. Birren', 'Donald S.', 'John S. Scheiffelin 5']\n",
      "['Shuo Su', 'Gary Wong', '', 'Yingxia Liu', 'George F Gao', 'Shoujun Li', 'Yuhai Bi']\n",
      "['Gary Wong', '', 'Xiangguo Qiu', 'Jason S. Richardson', 'Todd Cutts', 'Brad Collignon', 'Jason Gren', 'Jenna Aviles', '', 'Carissa Embury-Hyatt', 'Gary P. Kobinger']\n",
      "['Yuhai Bi', 'Weifeng Shi', 'Jianjun Chen', 'Quanjiao Chen', 'Zhenghai Ma', 'Gary Wong', 'Wenxia Tian', 'Renfu Yin', 'Guanghua Fu', 'Yongchun Yang', 'William J. Liu', 'Chuansong Quan', 'Qianli Wang', 'Shenghu He', 'Xiangdong Li', 'Qianfeng Xia', 'Lixin Wang', 'Zhaohui Pan', 'Laixing Li', 'Hong Li', 'Wen Xu', 'Ying Luo', 'Hui Zeng', 'Lianpan Dai', 'Haixia Xiao', 'Kirill Sharshov', 'Alexander Shestopalov', 'Yi Shi', 'Jinghua Yan', 'Xuebing Li', 'Yingxia Liu', 'Fumin Lei', 'Wenjun Liu', 'George F. Gao', '']\n",
      "['Anh Ha Lien Do', 'H. Rogier van Doorn', 'My Ngoc Nghiem', 'Juliet E. Bryant', 'Thanh Hang thi Hoang', 'Quang Ha Do', 'Tan Le Van', 'Tan Thanh Tran', 'Bridget Wills', 'Vinh Chau van Nguyen', 'Minh Hien Vo', 'Cong Khanh Vo', 'Minh Dung Nguyen', 'Jeremy Farrar', 'Tinh Hien Tran', 'Menno D. de Jong', '']\n",
      "['Yu Zhao', 'Chong Cui', 'Kun Zhang', 'Jialin Liu', 'Jinfu Xu', 'Eric Nisenbaum', 'Yixiang Huang', 'Guoyou Qin', 'Bing Chen', 'Michael Hoffer', 'Susan H. Blanton', 'Fred Telischi', 'Joshua M. Hare', 'Sylvia Daunert', 'Bhavarth Shukla', 'Savita G. Pahwa', 'Dushyantha T. Jayaweera', 'Paul E. Farmer', '', '', 'Carlos del Rio', 'Xuezhong Liu', 'Yilai Shu']\n",
      "['Alimuddin Zumla', 'Osman Dar', 'Richard Kock', 'Matthew Muturi', 'Francine Ntoumi', 'Pontiano Kaleebu', 'Macete Eusebio', 'Sayoki Mfinanga', 'Matthew Bates', 'Peter Mwaba', 'Rashid Ansumana', 'Mishal Khan', '', 'Abdulaziz N. Alagaili', 'Matthew Cotten', 'Esam I. Azhar', 'Markus Maeurer', 'Giuseppe Ippolito', 'Eskild Petersen']\n",
      "['Lionel A. Mandell', 'Richard G. Wunderink', 'Antonio Anzueto', 'John G. Bartlett', 'G. Douglas Campbell', 'Nathan C. Dean', '', 'Scott F. Dowell', 'Daniel M. Musher', '', 'Michael S. Niederman', '', 'Antonio Torres', 'Cynthia G. Whitney', 'Michael E. DeBakey Veterans']\n",
      "['Nicole L La Gruta', 'Katherine Kedzierska', 'John Stambas', 'Peter C Doherty', '']\n",
      "['Shigeo Hanada', 'Mina Pirzadeh', '', 'Kyle Y. Carver', '', 'Jane C. Deng', '']\n",
      "['David S. Hui', 'Esam I. Azhar', 'Yae Jean Kim', 'Ziad A. Memish', '', 'Myoung don Oh', 'Alimuddin Zumla']\n",
      "['Jian Feng He', 'Guo Wen Peng', 'Jun Min', 'De Wen Yu', 'Wen Jia Liang', 'Shu Yi', 'Rui Heng Xu', 'Huan Ying Zheng', 'Xin Wei Wu', 'Jun Xu', 'Zhan Hui', 'Ling Fang', 'Xin Zhang', 'Hui Li', 'Xin Ge Yan', 'Jia Hai Lu', 'Zhi Hong', 'Ji Cheng Huang', 'Zhuo Yue Wan', 'Jin Lin', 'Jin Yan Lin', 'Huai Dong Song', 'Sheng Yue Wang', 'Xiang Jun Zhou', 'Guo Wei Zhang', 'Bo Wei Gu', 'Hua Jun Zheng', 'Xiang Lin Zhang', 'Mei He', 'Kui Zheng', 'Bo Fei Wang', 'Gang Fu', 'Xiao Ning', 'Sai Juan Chen', 'Zhu Chen', '', 'Pei', 'Hua Tang', 'Shuang Xi Ren', 'Yang Zhong', 'Zong Ming', 'Qi', 'You Gang', 'Xiang Yin', 'Wei Zhong', 'Yi Xue', 'Chung I. Wu', 'Guo Ping Zhao', 'Rossa W.K. Chiu', 'Stephen S.C. Chim', 'Yu Kwan Tong 8']\n",
      "['Sema Jain', 'W. H. Self', 'R. G. Wunderink', 'S. Fakhran', 'R. Balk', 'A. M. Bramley', 'C. Reed', 'C. G. Grijalva', 'E. J. Anderson', 'D. M. Courtney', 'J. D. Chappell', 'C. Qi', 'E. M. Hart', 'F. Carroll', 'Christopher Trabue', 'H. K. Donnelly', 'D. J. Williams', 'Y. Zhu', 'Sandra Arnold', '', 'K. Ampofo', 'G. W. Waterer', '', 'M. Levine', 'S. Lindstrom', 'J. M. Winchell', 'J. M. Katz', 'D. Erdman', 'E. Schneider', 'L. A. Hicks', 'Jonathan Mccullers', '', 'A. T. Pavia', 'K. M. Edwards', 'L. Finelli']\n",
      "['Ellen Jo Baron', 'J. Michael Miller', 'Melvin P. Weinstein', 'Sandra S. Richter', 'Peter H. Gilligan', 'Richard B. Thomson', 'Paul Bourbeau', 'Karen C. Carroll', 'Sue C. Kehl', 'W. Michael Dunne', '', 'Barbara Robinson-Dunn', 'Joseph D. Schwartzman', 'Kimberle C. Chapin', 'James W. Snyder', 'Betty A. Forbes', 'Robin Patel', 'Jon E. Rosenblatt', 'Bobbi S. Pritt']\n",
      "['Nathan J Brendish', '', 'Ahalya K Malachira', 'Lawrence Armstrong', 'Rebecca Houghton', '', 'Sandra Aitken', 'Esther Nyimbili', 'Sean Ewings', 'Patrick J Lillie', 'Tristan W Clark', '']\n",
      "['K. S. Li', 'Y. Guan', '', 'J. Wang', '', 'G. J. D. Smith', '', 'K. M. Xu', '', 'L. Duan', '', 'A. P. Rahardjo', 'P. Puthavathana', 'C. Buranathai', 'T. D. Nguyen', 'A. T. S. Estoepangestie', 'A. Chaisingh', 'P. Auewarakul', 'H. T. Long', 'N. T. H. Hanh', 'R. J. Webby', 'L. L. M. Poon', 'H. Chen', '', 'K. F. Shortridge', '', 'K. Y. Yuen', 'R. G. Webster', '', 'J. S. M. Peiris', '']\n",
      "['Toshiro Sato', 'Daniel E. Stange', 'Marc Ferrante', '', 'Robert G.J. Vries', 'Johan H. van Es', 'Stieneke van den Brink', 'Winan J. van Houdt', 'Apollo Pronk', 'Joost van Gorp', 'Peter D. Siersema', 'Hans Clevers']\n",
      "['Victor M. Corman', 'Ali M. Albarrak', 'Ali Senosi Omrani', 'Mohammed M. Albarrak', 'Mohamed Elamin Farah', 'Malak Almasri', 'Doreen Muth', 'Andrea Sieberg', 'Benjamin Meyer', 'Abdullah M. Assiri', 'Tabea Binger', 'Katja Steinhagen', 'Erik Lattwein', 'Jaffar Al-Tawfiq', '', 'Marcel A. Müller', 'Christian Drosten', 'Ziad A. Memish']\n",
      "['Si Quang Le', '', 'Cuong Cao Dang', 'Olivier Gascuel']\n"
     ]
    }
   ],
   "source": [
    "for paper in data:\n",
    "    if '' in paper[\"authors\"]:\n",
    "        print(paper[\"authors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
