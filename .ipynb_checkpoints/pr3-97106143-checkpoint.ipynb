{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funky-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-representative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"CrawledPapers.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hollywood-belarus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in data:\n",
    "    i = x[\"id\"]\n",
    "    for j in data:\n",
    "        if j[\"id\"] == i and j != x:\n",
    "            print(x)\n",
    "            print()\n",
    "            print(j)\n",
    "            print(\"\\n\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "helpful-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_to_index = {}\n",
    "for index in range(len(data)):\n",
    "    paper_to_index[data[index][\"id\"]] = index\n",
    "# paper_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "essential-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrice = np.zeros((len(data), len(data)))\n",
    "for i in range(len(data)):\n",
    "    refs = data[i][\"references\"]\n",
    "    for ref in refs:\n",
    "        j = paper_to_index.get(ref, -1)\n",
    "        if j != -1:\n",
    "            adj_matrice[i, j] = 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "settled-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pageRank(adj_matrice, alpha):\n",
    "    P = adj_matrice.copy()\n",
    "    V = np.zeros(len(P[0]))\n",
    "    V.fill(1 / len(P[0]))\n",
    "    \n",
    "    for i in range(len(P)):\n",
    "        if sum(P[i]) == 0:\n",
    "            P[i] = V\n",
    "        else:\n",
    "            P[i] = (1 - alpha) * P[i] + alpha * V\n",
    "    res = V.reshape((1, len(V))).dot(P)\n",
    "    for i in range(10):\n",
    "        res = res.dot(P)\n",
    "    \n",
    "    return dict([(i, res[0, paper_to_index[i]]) for i in paper_to_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tropical-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = cal_pageRank(adj_matrice, 0.5)\n",
    "# P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "encouraging-herald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '2981549002',\n",
       "  'title': 'Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes.',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '27',\n",
       "  'abstract': 'Wide neural networks with random weights and biases are Gaussian processes, as originally observed by Neal (1995) and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at this http URL.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Greg Yang'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Multilayer perceptron',\n",
       "   'Gaussian process',\n",
       "   'Normalization (statistics)',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Tensor',\n",
       "   'Algorithm',\n",
       "   'Feed forward',\n",
       "   'Computation',\n",
       "   'Pooling',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2964308564',\n",
       "   '2963446712',\n",
       "   '1677182931',\n",
       "   '2157331557',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '1533861849']},\n",
       " {'id': '3105081694',\n",
       "  'title': 'COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images.',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '743',\n",
       "  'abstract': \"The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Linda Wang', 'Zhong Qiu Lin', 'Alexander Wong'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Image processing',\n",
       "   'Benchmark (computing)',\n",
       "   'Machine learning',\n",
       "   'Key (cryptography)',\n",
       "   'Computer science',\n",
       "   'Radiography',\n",
       "   'Artificial intelligence',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Tomography x ray computed',\n",
       "   'X ray image'],\n",
       "  'references': ['2194775991',\n",
       "   '3001118548',\n",
       "   '2962835968',\n",
       "   '3008827533',\n",
       "   '2919115771',\n",
       "   '2108598243',\n",
       "   '2963446712',\n",
       "   '3007497549',\n",
       "   '3010604545',\n",
       "   '3008985036']},\n",
       " {'id': '2950893734',\n",
       "  'title': 'Self-Attention Generative Adversarial Networks',\n",
       "  'reference_count': '51',\n",
       "  'citation_count': '1,558',\n",
       "  'abstract': 'In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Han Zhang 1, Ian Goodfellow 1, Dimitris Metaxas 2, Augustus Odena 1'],\n",
       "  'related_topics': ['Boosting (machine learning)',\n",
       "   'Visualization',\n",
       "   'Normalization (statistics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Generative grammar',\n",
       "   'Adversarial system',\n",
       "   'Artificial intelligence',\n",
       "   'Self attention'],\n",
       "  'references': ['2964121744',\n",
       "   '2963403868',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '2964308564',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2963373786',\n",
       "   '2963470893']},\n",
       " {'id': '3119786062',\n",
       "  'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '315',\n",
       "  'abstract': 'While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Alexey Dosovitskiy 1, Lucas Beyer 1, Alexander Kolesnikov 1, Dirk Weissenborn 2, Xiaohua Zhai 1, Thomas Unterthiner 1, Mostafa Dehghani 1, Matthias Minderer 1, Georg Heigold 2, Sylvain Gelly 1, Jakob Uszkoreit 1, Neil Houlsby 3'],\n",
       "  'related_topics': ['Transformer (machine learning model)',\n",
       "   'Contextual image classification',\n",
       "   'Computer vision',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Scale (chemistry)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Conjunction (grammar)',\n",
       "   'Artificial intelligence',\n",
       "   'Self attention'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2964121744',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2108598243',\n",
       "   '3118608800',\n",
       "   '2963091558',\n",
       "   '3034978746']},\n",
       " {'id': '2145339207',\n",
       "  'title': 'Human-level control through deep reinforcement learning',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '14,862',\n",
       "  'abstract': 'The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Volodymyr Mnih',\n",
       "   'Koray Kavukcuoglu',\n",
       "   'David Silver',\n",
       "   'Andrei A. Rusu',\n",
       "   'Joel Veness',\n",
       "   'Marc G. Bellemare',\n",
       "   'Alex Graves',\n",
       "   'Martin Riedmiller',\n",
       "   'Andreas K. Fidjeland',\n",
       "   'Georg Ostrovski',\n",
       "   'Stig Petersen',\n",
       "   'Charles Beattie',\n",
       "   'Amir Sadik',\n",
       "   'Ioannis Antonoglou',\n",
       "   'Helen King',\n",
       "   'Dharshan Kumaran',\n",
       "   'Daan Wierstra',\n",
       "   'Shane Legg',\n",
       "   'Demis Hassabis'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Q-learning',\n",
       "   'Temporal difference learning',\n",
       "   'Artificial neural network',\n",
       "   'General video game playing',\n",
       "   'Set (psychology)',\n",
       "   'Sensory processing',\n",
       "   'Reinforcement',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2121863487',\n",
       "   '2952509347',\n",
       "   '1652505363']},\n",
       " {'id': '2153579005',\n",
       "  'title': 'Distributed Representations of Words and Phrases and their Compositionality',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '25,996',\n",
       "  'abstract': 'The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Tomas Mikolov',\n",
       "   'Ilya Sutskever',\n",
       "   'Kai Chen',\n",
       "   'Greg S Corrado',\n",
       "   'Jeff Dean'],\n",
       "  'related_topics': ['Word2vec',\n",
       "   'Word embedding',\n",
       "   'Word order',\n",
       "   'Word (computer architecture)',\n",
       "   'Principle of compositionality',\n",
       "   'Softmax function',\n",
       "   'Syntax',\n",
       "   'Distributional semantics',\n",
       "   'Simple (philosophy)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1614298861',\n",
       "   '2117130368',\n",
       "   '2141599568',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '1498436455',\n",
       "   '1662133657',\n",
       "   '1889268436',\n",
       "   '2131462252']},\n",
       " {'id': '2194775991',\n",
       "  'title': 'Deep Residual Learning for Image Recognition',\n",
       "  'reference_count': '52',\n",
       "  'citation_count': '79,410',\n",
       "  'abstract': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Residual',\n",
       "   'Convolutional neural network',\n",
       "   'Feature learning',\n",
       "   'Vanishing gradient problem',\n",
       "   'Artificial neural network',\n",
       "   'MNIST database',\n",
       "   'Object detection',\n",
       "   'Test set',\n",
       "   'Transfer of learning',\n",
       "   'Softmax function',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Artificial intelligence',\n",
       "   'Residual neural network'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1536680647']},\n",
       " {'id': '2963403868',\n",
       "  'title': 'Attention is All You Need',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '19,086',\n",
       "  'abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Ashish Vaswani 1, Noam Shazeer 1, Niki Parmar 2, Jakob Uszkoreit 1, Llion Jones 1, Aidan N. Gomez 1, Lukasz Kaiser 1, Illia Polosukhin 1'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Encoder',\n",
       "   'BLEU',\n",
       "   'Speech translation',\n",
       "   'Artificial neural network',\n",
       "   'Transduction (machine learning)',\n",
       "   'Byte pair encoding',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)'],\n",
       "  'references': ['2963341956',\n",
       "   '2963420686',\n",
       "   '2965373594',\n",
       "   '2970597249',\n",
       "   '2963091558',\n",
       "   '2923014074',\n",
       "   '2911489562']},\n",
       " {'id': '1836465849',\n",
       "  'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '26,211',\n",
       "  'abstract': \"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.\",\n",
       "  'date': 2015,\n",
       "  'authors': ['Sergey Ioffe', 'Christian Szegedy'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Initialization',\n",
       "   'Contextual image classification'],\n",
       "  'references': ['2097117768',\n",
       "   '2117539524',\n",
       "   '2095705004',\n",
       "   '1677182931',\n",
       "   '2146502635',\n",
       "   '2310919327',\n",
       "   '1665214252',\n",
       "   '2168231600',\n",
       "   '1533861849',\n",
       "   '104184427']},\n",
       " {'id': '2964308564',\n",
       "  'title': 'Neural Machine Translation by Jointly Learning to Align and Translate',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '17,608',\n",
       "  'abstract': 'Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Dzmitry Bahdanau 1, Kyunghyun Cho 2, Yoshua Bengio 2'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Artificial neural network',\n",
       "   'Phrase',\n",
       "   'Sentence',\n",
       "   'Encoder',\n",
       "   'Artificial intelligence',\n",
       "   'Bottleneck',\n",
       "   'Byte pair encoding',\n",
       "   'Computer science',\n",
       "   'Closed captioning'],\n",
       "  'references': ['2157331557',\n",
       "   '2064675550',\n",
       "   '6908809',\n",
       "   '2132339004',\n",
       "   '1753482797',\n",
       "   '2294059674',\n",
       "   '1810943226',\n",
       "   '2964199361',\n",
       "   '1815076433',\n",
       "   '2153653739']},\n",
       " {'id': '2963446712',\n",
       "  'title': 'Densely Connected Convolutional Networks',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '16,384',\n",
       "  'abstract': 'Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Gao Huang 1, Zhuang Liu 2, Laurens van der Maaten 3, Kilian Q. Weinberger 1'],\n",
       "  'related_topics': ['Convolutional code',\n",
       "   'Network architecture',\n",
       "   'Vanishing gradient problem'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '1677182931',\n",
       "   '2183341477']},\n",
       " {'id': '1677182931',\n",
       "  'title': 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '11,676',\n",
       "  'abstract': 'Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 1, Jian Sun 1'],\n",
       "  'related_topics': ['Rectifier (neural networks)',\n",
       "   'Initialization',\n",
       "   'Overfitting',\n",
       "   'Artificial neural network',\n",
       "   'Contextual image classification',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '1536680647']},\n",
       " {'id': '2157331557',\n",
       "  'title': 'Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '13,086',\n",
       "  'abstract': 'In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Kyunghyun Cho 1, Bart van Merrienboer 2, Caglar Gulcehre 2, Dzmitry Bahdanau 3, Fethi Bougares 3, Holger Schwenk 3, Yoshua Bengio 4, 5, 6'],\n",
       "  'related_topics': ['Encoder',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Machine translation',\n",
       "   'Phrase',\n",
       "   'Feature (machine learning)',\n",
       "   'Conditional probability',\n",
       "   'Sequence',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2153579005',\n",
       "   '2064675550',\n",
       "   '2147768505',\n",
       "   '6908809',\n",
       "   '2132339004',\n",
       "   '1753482797',\n",
       "   '2294059674',\n",
       "   '2156387975',\n",
       "   '2963504252']},\n",
       " {'id': '2310919327',\n",
       "  'title': 'Gradient-based learning applied to document recognition',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '37,798',\n",
       "  'abstract': 'Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Yann Lecun 1, Leon Bottou 2, 3, Yoshua Bengio 3, 4, 5, Patrick Haffner 3, 6'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Convolutional neural network',\n",
       "   'Handwriting recognition',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Language model',\n",
       "   'Graph (abstract data type)',\n",
       "   'Decision boundary',\n",
       "   'Network architecture',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2147880316',\n",
       "   '2156163116',\n",
       "   '2963399829',\n",
       "   '2964311892',\n",
       "   '2157364932',\n",
       "   '1510526001',\n",
       "   '2158778629',\n",
       "   '1944615693']},\n",
       " {'id': '2064675550',\n",
       "  'title': 'Long short-term memory',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '49,372',\n",
       "  'abstract': \"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.\",\n",
       "  'date': 1997,\n",
       "  'authors': ['Sepp Hochreiter 1, Jürgen Schmidhuber 2'],\n",
       "  'related_topics': ['Vanishing gradient problem',\n",
       "   'Backpropagation through time',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Time complexity',\n",
       "   'Chunking (psychology)',\n",
       "   'Computational complexity theory',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2107878631',\n",
       "   '2128499899',\n",
       "   '2007431958',\n",
       "   '194249466',\n",
       "   '2123716044',\n",
       "   '2143503258',\n",
       "   '2154890045',\n",
       "   '2103452139',\n",
       "   '2048060899',\n",
       "   '1674799117']},\n",
       " {'id': '1533861849',\n",
       "  'title': 'Understanding the difficulty of training deep feedforward neural networks',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '13,013',\n",
       "  'abstract': 'Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).',\n",
       "  'date': 2010,\n",
       "  'authors': ['Xavier Glorot', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Vanishing gradient problem',\n",
       "   'Initialization',\n",
       "   'Artificial neural network',\n",
       "   'Feedforward neural network',\n",
       "   'Activation function',\n",
       "   'Gradient descent',\n",
       "   'Feature (machine learning)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning'],\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '1498436455',\n",
       "   '1994197834',\n",
       "   '2131462252',\n",
       "   '2172174689']},\n",
       " {'id': '3001118548',\n",
       "  'title': 'Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '31,613',\n",
       "  'abstract': 'A recent cluster of pneumonia cases in Wuhan, China, was caused by a novel betacoronavirus, the 2019 novel coronavirus (2019-nCoV). We report the epidemiological, clinical, laboratory, and radiological characteristics and treatment and clinical outcomes of these patients. All patients with suspected 2019-nCoV were admitted to a designated hospital in Wuhan. We prospectively collected and analysed data on patients with laboratory-confirmed 2019-nCoV infection by real-time RT-PCR and next-generation sequencing. Data were obtained with standardised data collection forms shared by the International Severe Acute Respiratory and Emerging Infection Consortium from electronic medical records. Researchers also directly communicated with patients or their families to ascertain epidemiological and symptom data. Outcomes were also compared between patients who had been admitted to the intensive care unit (ICU) and those who had not.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Chaolin Huang 1, Yeming Wang 2, Xingwang Li 3, Lili Ren 4, Jianping Zhao 5, Yi Hu 5, Li Zhang 1, Guohui Fan 2, Jiuyang Xu 6, Xiaoying Gu 2, Zhenshun Cheng 7, Ting Yu 1, Jiaan Xia 1, Yuan Wei 1, Wenjuan Wu 1, Xuelei Xie 1, Wen Yin 5, Hui Li 2, Min Liu 2, Yan Xiao 4, Hong Gao 4, Li Guo 4, Jungang Xie 5, Guangfa Wang 8, Rongmeng Jiang 3, Zhancheng Gao 8, Qi Jin 4, Jianwei Wang 4, Bin Cao 2'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Intensive care unit',\n",
       "   'Viral pneumonia',\n",
       "   'Medical record',\n",
       "   'Epidemiology',\n",
       "   'Pneumonia',\n",
       "   'Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Internal medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '3000413850',\n",
       "   '2132260239',\n",
       "   '2026274122',\n",
       "   '2104548316',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '3017468735',\n",
       "   '2725497285']},\n",
       " {'id': '2962835968',\n",
       "  'title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '84,164',\n",
       "  'abstract': 'Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Karen Simonyan', 'Andrew Zisserman'],\n",
       "  'related_topics': ['Convolution',\n",
       "   'Computer vision',\n",
       "   'Scale (map)',\n",
       "   'Computer science',\n",
       "   'Basis (linear algebra)',\n",
       "   'Architecture',\n",
       "   'Artificial intelligence',\n",
       "   'Crowd counting',\n",
       "   'Region proposal'],\n",
       "  'references': ['2194775991',\n",
       "   '639708223',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '3106250896']},\n",
       " {'id': '3008827533',\n",
       "  'title': 'Clinical Characteristics of Coronavirus Disease 2019 in China',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '18,220',\n",
       "  'abstract': 'Abstract Background Since December 2019, when coronavirus disease 2019 (Covid-19) emerged in Wuhan city and rapidly spread throughout China, data have been needed on the clinical characteristics of...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Wei-jie Guan',\n",
       "   'Zheng-yi Ni',\n",
       "   'Yu Hu',\n",
       "   'Wenhua Liang',\n",
       "   'Chun-quan Ou',\n",
       "   'Jianxing He',\n",
       "   'Lei Liu',\n",
       "   'Hong Shan',\n",
       "   'Chunliang Lei',\n",
       "   'David S.C. Hui',\n",
       "   'Bin Du',\n",
       "   'Lan-juan Li',\n",
       "   'Guang Zeng',\n",
       "   'Kwok-Yung Yuen',\n",
       "   'Ruchong Chen',\n",
       "   'Chun-Li Tang',\n",
       "   'Tao Wang',\n",
       "   'Ping-yan Chen',\n",
       "   'Jie Xiang',\n",
       "   'Shiyue Li',\n",
       "   'Jinlin Wang',\n",
       "   'Zi Jing Liang',\n",
       "   'Yi-xiang Peng',\n",
       "   'Li Wei',\n",
       "   'Yong Liu',\n",
       "   'Ya-hua Hu',\n",
       "   'Peng Peng',\n",
       "   'Jian-ming Wang',\n",
       "   'Ji-yang Liu',\n",
       "   'Zhong Chen',\n",
       "   'Gang Li',\n",
       "   'Zhi-jian Zheng',\n",
       "   'Shao-qin Qiu',\n",
       "   'Jie Luo',\n",
       "   'Chang-jiang Ye',\n",
       "   'Shao-yong Zhu',\n",
       "   'Nanshan Zhong'],\n",
       "  'related_topics': ['Pandemic', 'Betacoronavirus', 'Viral Epidemiology'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '3004239190',\n",
       "   '3003573988']},\n",
       " {'id': '2919115771',\n",
       "  'title': 'Deep learning',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '40,041',\n",
       "  'abstract': 'Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Yann LeCun 1, 2, Yoshua Bengio 3, Geoffrey Hinton 4, 5'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Theano',\n",
       "   'Backpropagation',\n",
       "   'Representation (systemics)',\n",
       "   'Computational model',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2145339207',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2064675550',\n",
       "   '2163922914',\n",
       "   '2160815625',\n",
       "   '2022508996',\n",
       "   '2025768430',\n",
       "   '1993882792']},\n",
       " {'id': '2108598243',\n",
       "  'title': 'ImageNet: A large-scale hierarchical image database',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '28,822',\n",
       "  'abstract': 'The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Jia Deng',\n",
       "   'Wei Dong',\n",
       "   'Richard Socher',\n",
       "   'Li-Jia Li',\n",
       "   'Kai Li',\n",
       "   'Li Fei-Fei'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Image retrieval',\n",
       "   'Contextual image classification',\n",
       "   'Ontology (information science)',\n",
       "   'Cluster analysis',\n",
       "   'Information retrieval',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Ontology',\n",
       "   'The Internet',\n",
       "   'Robustness (computer science)',\n",
       "   'Computer science'],\n",
       "  'references': ['2151103935',\n",
       "   '2038721957',\n",
       "   '2128017662',\n",
       "   '2110764733',\n",
       "   '1782590233',\n",
       "   '1576445103',\n",
       "   '2145607950',\n",
       "   '2141282920',\n",
       "   '2115733720',\n",
       "   '1528789833']},\n",
       " {'id': '3007497549',\n",
       "  'title': 'Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases.',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '3,893',\n",
       "  'abstract': 'Chest CT had higher sensitivity for diagnosis of COVID-19 as compared with initial reverse-transcription polymerase chain reaction from swab samples in the epidemic area of China.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Tao Ai 1, Zhenlu Yang 2, Hongyan Hou 3, Chenao Zhan 1, Chong Chen 1, Wenzhi Lv 1, Qian Tao 1, Ziyong Sun 1, Liming Xia 1'],\n",
       "  'related_topics': ['Real-time polymerase chain reaction',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Pneumonia',\n",
       "   'Polymerase chain reaction',\n",
       "   'Radiology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Follow up studies'],\n",
       "  'references': ['3001118548',\n",
       "   '3008818676',\n",
       "   '3004906315',\n",
       "   '3006643024',\n",
       "   '3006110666',\n",
       "   '3006354146',\n",
       "   '3003901880',\n",
       "   '3005656138',\n",
       "   '3004511262']},\n",
       " {'id': '3010604545',\n",
       "  'title': 'Detection of SARS-CoV-2 in Different Types of Clinical Specimens.',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '3,893',\n",
       "  'abstract': 'This study describes results of PCR and viral RNA testing for SARS-CoV-2 in bronchoalveolar fluid, sputum, feces, blood, and urine specimens from patients with COVID-19 infection in China to identify possible means of non-respiratory transmission.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Wenling Wang 1, Yanli Xu 2, Ruqin Gao 3, Roujian Lu 1, Kai Han 2, Guizhen Wu 1, Wenjie Tan 1'],\n",
       "  'related_topics': ['Sputum',\n",
       "   'Viral load',\n",
       "   'Feces',\n",
       "   'Betacoronavirus',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Pneumonia (non-human)',\n",
       "   'Polymerase chain reaction',\n",
       "   'Urine',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'references': ['3005079553',\n",
       "   '3008962515',\n",
       "   '3008452791',\n",
       "   '3033453353',\n",
       "   '3034408674',\n",
       "   '3035275617',\n",
       "   '3034059415',\n",
       "   '3033952286',\n",
       "   '3036958556',\n",
       "   '3035464429']},\n",
       " {'id': '3008985036',\n",
       "  'title': 'Sensitivity of Chest CT for COVID-19: Comparison to RT-PCR',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '2,128',\n",
       "  'abstract': 'In a series of 51 patients with chest CT and real-time polymerase chain reaction assay (RT-PCR) performed within 3 days, the sensitivity of CT for 2019 novel coronavirus infection was 98% and that ...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yicheng Fang',\n",
       "   'Huangqi Zhang',\n",
       "   'Jicheng Xie',\n",
       "   'Minjie Lin',\n",
       "   'Lingjun Ying',\n",
       "   'Peipei Pang',\n",
       "   'Wenbin Ji'],\n",
       "  'related_topics': ['Real-time polymerase chain reaction',\n",
       "   'Polymerase chain reaction',\n",
       "   'Pneumonia',\n",
       "   'Nuclear medicine',\n",
       "   'Tomography',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3002108456',\n",
       "   '3006643024',\n",
       "   '3006110666',\n",
       "   '3028749392',\n",
       "   '3032185657',\n",
       "   '3042098369',\n",
       "   '3037255629']},\n",
       " {'id': '2964121744',\n",
       "  'title': 'Adam: A Method for Stochastic Optimization',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '69,925',\n",
       "  'abstract': 'Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Diederik P. Kingma 1, Jimmy Lei Ba 2'],\n",
       "  'related_topics': ['Stochastic optimization',\n",
       "   'Convex optimization',\n",
       "   'Rate of convergence',\n",
       "   'Invariant (mathematics)',\n",
       "   'Diagonal',\n",
       "   'Uniform norm',\n",
       "   'Mathematical optimization',\n",
       "   'Byte pair encoding',\n",
       "   'Regret',\n",
       "   'Computer science'],\n",
       "  'references': ['2963403868',\n",
       "   '2962739339',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2963470893',\n",
       "   '2331128040',\n",
       "   '2964015378',\n",
       "   '1514535095']},\n",
       " {'id': '2117539524',\n",
       "  'title': 'ImageNet Large Scale Visual Recognition Challenge',\n",
       "  'reference_count': '97',\n",
       "  'citation_count': '23,640',\n",
       "  'abstract': 'The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Olga Russakovsky 1, Jia Deng 2, Hao Su 1, Jonathan Krause 1, Sanjeev Satheesh 1, Sean Ma 1, Zhiheng Huang 1, Andrej Karpathy 1, Aditya Khosla 3, Michael Bernstein 1, Alexander C. Berg 4, Li Fei-Fei 1'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition (psychology)'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '1614298861',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2168356304',\n",
       "   '1849277567']},\n",
       " {'id': '2099471712',\n",
       "  'title': 'Generative Adversarial Nets',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '27,297',\n",
       "  'abstract': 'We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Ian Goodfellow 1, Jean Pouget-Abadie 1, Mehdi Mirza 1, Bing Xu 1, David Warde-Farley 1, Sherjil Ozair 2, Aaron Courville 1, Yoshua Bengio 1'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Discriminative model',\n",
       "   'Approximate inference',\n",
       "   'Markov chain',\n",
       "   'Minimax',\n",
       "   'Adversarial machine learning',\n",
       "   'Perceptron',\n",
       "   'Backpropagation',\n",
       "   'Theoretical computer science',\n",
       "   'Machine learning',\n",
       "   'Image translation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2964153729',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2025768430']},\n",
       " {'id': '2963073614',\n",
       "  'title': 'Image-to-Image Translation with Conditional Adversarial Networks',\n",
       "  'reference_count': '52',\n",
       "  'citation_count': '9,243',\n",
       "  'abstract': 'We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Phillip Isola',\n",
       "   'Jun-Yan Zhu',\n",
       "   'Tinghui Zhou',\n",
       "   'Alexei A. Efros'],\n",
       "  'related_topics': ['Image translation',\n",
       "   'Image (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'User interface',\n",
       "   'Translation (geometry)',\n",
       "   'Function (engineering)',\n",
       "   'Image resolution',\n",
       "   'Iterative reconstruction'],\n",
       "  'references': ['2964121744',\n",
       "   '1836465849',\n",
       "   '1901129140',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '2133665775',\n",
       "   '2963684088',\n",
       "   '2100495367',\n",
       "   '2340897893']},\n",
       " {'id': '2962793481',\n",
       "  'title': 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '8,408',\n",
       "  'abstract': 'Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Jun-Yan Zhu',\n",
       "   'Taesung Park',\n",
       "   'Phillip Isola',\n",
       "   'Alexei A. Efros'],\n",
       "  'related_topics': ['Image translation',\n",
       "   'Image (category theory)',\n",
       "   'Translation (geometry)',\n",
       "   'Graphics',\n",
       "   'Object (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Domain (software engineering)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Training set'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '1959608418',\n",
       "   '2963684088',\n",
       "   '2100495367',\n",
       "   '2340897893',\n",
       "   '2963373786']},\n",
       " {'id': '2963684088',\n",
       "  'title': 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '8,928',\n",
       "  'abstract': 'Abstract: In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Alec Radford 1, Luke Metz 1, Soumith Chintala 2'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'Feature learning',\n",
       "   'Artificial intelligence',\n",
       "   'Class (computer programming)',\n",
       "   'Hierarchy',\n",
       "   'Computer science',\n",
       "   'Object (computer science)',\n",
       "   'Generative grammar',\n",
       "   'Generator (mathematics)'],\n",
       "  'references': ['2962793481',\n",
       "   '2963470893',\n",
       "   '2331128040',\n",
       "   '2963420272',\n",
       "   '2405756170',\n",
       "   '2963800363',\n",
       "   '2963836885',\n",
       "   '2893749619',\n",
       "   '2738588019']},\n",
       " {'id': '2963373786',\n",
       "  'title': 'Improved techniques for training GANs',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '4,731',\n",
       "  'abstract': 'We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Tim Salimans 1, Ian Goodfellow 2, Wojciech Zaremba 3, Vicki Cheung',\n",
       "   'Alec Radford 1, Xi Chen 4'],\n",
       "  'related_topics': ['MNIST database',\n",
       "   'Pattern recognition',\n",
       "   'Turing test',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1836465849',\n",
       "   '2183341477',\n",
       "   '2963684088',\n",
       "   '2964153729',\n",
       "   '2271840356',\n",
       "   '648143168',\n",
       "   '2949416428',\n",
       "   '2963685250',\n",
       "   '830076066',\n",
       "   '1487641199']},\n",
       " {'id': '2963470893',\n",
       "  'title': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network',\n",
       "  'reference_count': '68',\n",
       "  'citation_count': '5,688',\n",
       "  'abstract': 'Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Christian Ledig 1, Lucas Theis 1, Ferenc Huszar 2, Jose Caballero 3, Andrew Cunningham',\n",
       "   'Alejandro Acosta 4, Andrew Aitken 2, Alykhan Tejani 2, Johannes Totz 2, Zehan Wang 2, Wenzhe Shi 2'],\n",
       "  'related_topics': ['Image texture',\n",
       "   'Image resolution',\n",
       "   'Convolutional neural network',\n",
       "   'Image translation',\n",
       "   'Pixel',\n",
       "   'Iterative reconstruction',\n",
       "   'Network architecture',\n",
       "   'Similarity (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '1677182931',\n",
       "   '1849277567']},\n",
       " {'id': '2618530766',\n",
       "  'title': 'ImageNet classification with deep convolutional neural networks',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '166,585',\n",
       "  'abstract': 'We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Alex Krizhevsky 1, Ilya Sutskever 1, Geoffrey E. Hinton 2'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Dropout (neural networks)',\n",
       "   'Overfitting'],\n",
       "  'references': ['2194775991',\n",
       "   '2097117768',\n",
       "   '2108598243',\n",
       "   '2911964244',\n",
       "   '3118608800',\n",
       "   '1904365287',\n",
       "   '1665214252',\n",
       "   '2546302380',\n",
       "   '2110764733',\n",
       "   '2130325614']},\n",
       " {'id': '2963341956',\n",
       "  'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
       "  'reference_count': '52',\n",
       "  'citation_count': '22,961',\n",
       "  'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       "  'date': 2018,\n",
       "  'authors': ['Jacob Devlin',\n",
       "   'Ming-Wei Chang',\n",
       "   'Kenton Lee',\n",
       "   'Kristina N. Toutanova'],\n",
       "  'related_topics': ['Question answering',\n",
       "   'Language model',\n",
       "   'Natural language understanding',\n",
       "   'Named-entity recognition',\n",
       "   'SemEval',\n",
       "   'Inference',\n",
       "   'Winograd Schema Challenge',\n",
       "   'Sequence labeling',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)'],\n",
       "  'references': ['2963403868',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2108598243',\n",
       "   '2962739339',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '2117130368',\n",
       "   '2025768430']},\n",
       " {'id': '3118608800',\n",
       "  'title': 'Learning Multiple Layers of Features from Tiny Images',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '11,906',\n",
       "  'abstract': 'In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Alex Krizhevsky'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Generative model',\n",
       "   'Boltzmann machine',\n",
       "   'Classifier (UML)',\n",
       "   'MNIST database',\n",
       "   'Vanishing gradient problem',\n",
       "   'Pattern recognition',\n",
       "   'Pixel',\n",
       "   'Focus (optics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2096192494', '2081580037', '2165225968']},\n",
       " {'id': '2963091558',\n",
       "  'title': 'Non-local Neural Networks',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '2,925',\n",
       "  'abstract': 'Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Xiaolong Wang 1, Ross Girshick 1, Abhinav Gupta 2, Kaiming He 1'],\n",
       "  'related_topics': ['Pose',\n",
       "   'Object detection',\n",
       "   'Image segmentation',\n",
       "   'Contextual image classification',\n",
       "   'Block (programming)',\n",
       "   'Artificial neural network',\n",
       "   'Feature extraction',\n",
       "   'Process (computing)',\n",
       "   'Segmentation',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1677182931',\n",
       "   '2806070179',\n",
       "   '1861492603',\n",
       "   '2565639579']},\n",
       " {'id': '3034978746',\n",
       "  'title': 'A Simple Framework for Contrastive Learning of Visual Representations',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,377',\n",
       "  'abstract': 'This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ting Chen',\n",
       "   'Simon Kornblith',\n",
       "   'Mohammad Norouzi',\n",
       "   'Geoffrey Hinton'],\n",
       "  'related_topics': ['Supervised learning',\n",
       "   'Linear classifier',\n",
       "   'Machine learning',\n",
       "   'Memory bank',\n",
       "   'Representation (mathematics)',\n",
       "   'Matching (statistics)',\n",
       "   'Computer science',\n",
       "   'Quality (business)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear transformation'],\n",
       "  'references': ['3119786062',\n",
       "   '3116298410',\n",
       "   '3099495704',\n",
       "   '3108316907',\n",
       "   '3098053103',\n",
       "   '3132401450',\n",
       "   '3125947392',\n",
       "   '3145385912',\n",
       "   '3154503084',\n",
       "   '3122924117']},\n",
       " {'id': '2100495367',\n",
       "  'title': 'Reducing the Dimensionality of Data with Neural Networks',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '15,414',\n",
       "  'abstract': 'High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.',\n",
       "  'date': 2006,\n",
       "  'authors': ['G. E. Hinton', 'R. R. Salakhutdinov'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Dimensionality reduction',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Artificial neural network',\n",
       "   'Deep belief network',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Gradient descent',\n",
       "   'Curse of dimensionality',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2293063825',\n",
       "   '2121122425',\n",
       "   '2032647857',\n",
       "   '2021774695']},\n",
       " {'id': '2187089797',\n",
       "  'title': 'Visualizing Data using t-SNE',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '20,331',\n",
       "  'abstract': 'We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Laurens van der Maaten', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Sammon mapping',\n",
       "   't-distributed stochastic neighbor embedding',\n",
       "   'Isomap',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Dimensionality reduction',\n",
       "   'Visualization',\n",
       "   'Embedding',\n",
       "   'Multidimensional scaling',\n",
       "   'Data mining',\n",
       "   'Mathematics'],\n",
       "  'references': ['2100495367',\n",
       "   '2072128103',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2156718197',\n",
       "   '2125637308',\n",
       "   '2139823104',\n",
       "   '2137570937',\n",
       "   '2157444450',\n",
       "   '1742512077']},\n",
       " {'id': '1665214252',\n",
       "  'title': 'Rectified Linear Units Improve Restricted Boltzmann Machines',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '13,596',\n",
       "  'abstract': 'Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Vinod Nair', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Binary number',\n",
       "   'Vanishing gradient problem',\n",
       "   'Sigmoid function',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Rule of inference',\n",
       "   'Unit (ring theory)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Infinite number'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2546302380',\n",
       "   '2116064496',\n",
       "   '1782590233',\n",
       "   '2134557905',\n",
       "   '2099866409',\n",
       "   '1994197834',\n",
       "   '2536626143',\n",
       "   '2157364932']},\n",
       " {'id': '2072128103',\n",
       "  'title': 'Learning Deep Architectures for AI',\n",
       "  'reference_count': '227',\n",
       "  'citation_count': '9,739',\n",
       "  'abstract': 'Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Yoshua Bengio'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Online machine learning',\n",
       "   'Feature learning',\n",
       "   'Robot learning',\n",
       "   'Computational learning theory',\n",
       "   'Learning classifier system',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2156909104',\n",
       "   '2911964244',\n",
       "   '2136922672',\n",
       "   '2296616510',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2187089797',\n",
       "   '2129131372',\n",
       "   '2119821739',\n",
       "   '2053186076']},\n",
       " {'id': '2546302380',\n",
       "  'title': 'What is the best multi-stage architecture for object recognition?',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '2,435',\n",
       "  'abstract': 'In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (≫ 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).',\n",
       "  'date': 2009,\n",
       "  'authors': ['Kevin Jarrett',\n",
       "   'Koray Kavukcuoglu',\n",
       "   \"Marc'Aurelio Ranzato\",\n",
       "   'Yann LeCun'],\n",
       "  'related_topics': ['Feature extraction',\n",
       "   'Feature (machine learning)',\n",
       "   'Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'Filter bank',\n",
       "   'MNIST database',\n",
       "   'Filter (signal processing)',\n",
       "   'Word error rate',\n",
       "   'Normalization (statistics)',\n",
       "   'Histogram',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2110798204',\n",
       "   '2130325614',\n",
       "   '2097018403',\n",
       "   '2166049352',\n",
       "   '2134557905']},\n",
       " {'id': '2121863487',\n",
       "  'title': 'Reinforcement Learning: An Introduction',\n",
       "  'reference_count': '84',\n",
       "  'citation_count': '44,248',\n",
       "  'abstract': \"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.\",\n",
       "  'date': 1987,\n",
       "  'authors': ['R.S. Sutton', 'A.G. Barto'],\n",
       "  'related_topics': ['Learning classifier system',\n",
       "   'Reinforcement learning',\n",
       "   'Apprenticeship learning',\n",
       "   'Unsupervised learning',\n",
       "   'Temporal difference learning',\n",
       "   'Robot learning',\n",
       "   'Computational learning theory',\n",
       "   'Q-learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Machine learning'],\n",
       "  'references': ['1639032689',\n",
       "   '2154642048',\n",
       "   '3017143921',\n",
       "   '2100677568',\n",
       "   '1535810436',\n",
       "   '1603765807',\n",
       "   '3011120880',\n",
       "   '1569320505',\n",
       "   '94523489',\n",
       "   '2178806388']},\n",
       " {'id': '2952509347',\n",
       "  'title': 'The arcade learning environment: an evaluation platform for general agents',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '1,854',\n",
       "  'abstract': 'In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Marc G. Bellemare 1, Yavar Naddaf 2, Joel Veness 1, Michael Bowling 1'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Learning environment',\n",
       "   'Transfer of learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Interface (Java)',\n",
       "   'Human–computer interaction',\n",
       "   'Computer science',\n",
       "   'Software',\n",
       "   'Set (psychology)'],\n",
       "  'references': ['2145339207',\n",
       "   '2952509347',\n",
       "   '2126316555',\n",
       "   '1515851193',\n",
       "   '1625390266',\n",
       "   '1502916507',\n",
       "   '2099587183',\n",
       "   '2013391942',\n",
       "   '2101355568',\n",
       "   '2132622533']},\n",
       " {'id': '1652505363',\n",
       "  'title': 'Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '23,606',\n",
       "  'abstract': 'The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.',\n",
       "  'date': 1986,\n",
       "  'authors': ['David E. Rumelhart 1, James L. McClelland 2'],\n",
       "  'related_topics': ['Competitive learning',\n",
       "   'Connectionism',\n",
       "   'Parallel processing (DSP implementation)',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Linear algebra',\n",
       "   'Resource (project management)',\n",
       "   'Data processing',\n",
       "   'Cognition',\n",
       "   'Quickprop'],\n",
       "  'references': ['1614298861',\n",
       "   '2145339207',\n",
       "   '2076063813',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2145094598',\n",
       "   '2107941094',\n",
       "   '2154642048',\n",
       "   '1498436455']},\n",
       " {'id': '1614298861',\n",
       "  'title': 'Efficient Estimation of Word Representations in Vector Space',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '17,853',\n",
       "  'abstract': 'We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Tomas Mikolov 1, Kai Chen 2, Greg S. Corrado 2, Jeffrey Dean 2'],\n",
       "  'related_topics': ['Word2vec',\n",
       "   'Word embedding',\n",
       "   'Word (computer architecture)',\n",
       "   'Test set',\n",
       "   'Similarity (psychology)',\n",
       "   'Artificial neural network',\n",
       "   'Data set',\n",
       "   'Pattern recognition',\n",
       "   'Vector space',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153579005',\n",
       "   '2250539671',\n",
       "   '2271840356',\n",
       "   '3104097132',\n",
       "   '1895577753',\n",
       "   '1888005072',\n",
       "   '1486649854',\n",
       "   '2964321699',\n",
       "   '2100664567',\n",
       "   '2123024445']},\n",
       " {'id': '2117130368',\n",
       "  'title': 'A unified architecture for natural language processing: deep neural networks with multitask learning',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '5,718',\n",
       "  'abstract': 'We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Ronan Collobert', 'Jason Weston'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Language identification',\n",
       "   'Language model',\n",
       "   'Semantic role labeling',\n",
       "   'Sentence',\n",
       "   'Convolutional neural network',\n",
       "   'Host (network)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '2132339004',\n",
       "   '2130903752',\n",
       "   '2158847908',\n",
       "   '2107008379',\n",
       "   '2914746235',\n",
       "   '2173629880',\n",
       "   '2885050925',\n",
       "   '2158823144',\n",
       "   '2163568299']},\n",
       " {'id': '2141599568',\n",
       "  'title': 'Linguistic Regularities in Continuous Space Word Representations',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '3,585',\n",
       "  'abstract': 'Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Tomas Mikolov 1, Wen-tau Yih 2, Geoffrey Zweig 2'],\n",
       "  'related_topics': ['Syntax',\n",
       "   'Language model',\n",
       "   'Analogy',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1614298861',\n",
       "   '2100495367',\n",
       "   '2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '2147152072',\n",
       "   '1632114991',\n",
       "   '2131462252',\n",
       "   '1970689298']},\n",
       " {'id': '2132339004',\n",
       "  'title': 'A neural probabilistic language model',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '7,596',\n",
       "  'abstract': 'A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Réjean Ducharme',\n",
       "   'Pascal Vincent',\n",
       "   'Christian Janvin'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Cache language model',\n",
       "   'Word embedding',\n",
       "   'Sentence',\n",
       "   'Joint probability distribution',\n",
       "   'Word (computer architecture)',\n",
       "   'Generalization',\n",
       "   'Probabilistic logic',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2038721957',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '1575350781',\n",
       "   '2158195707',\n",
       "   '2914484425',\n",
       "   '2121227244']},\n",
       " {'id': '2158139315',\n",
       "  'title': 'Word Representations: A Simple and General Method for Semi-Supervised Learning',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '2,502',\n",
       "  'abstract': 'If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/',\n",
       "  'date': 2010,\n",
       "  'authors': ['Joseph Turian 1, Lev-Arie Ratinov 2, Yoshua Bengio 1'],\n",
       "  'related_topics': ['Word (computer architecture)',\n",
       "   'Semi-supervised learning',\n",
       "   'Chunking (psychology)',\n",
       "   'Natural language processing',\n",
       "   'Code (cryptography)',\n",
       "   'Computer science',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'General method',\n",
       "   'Word representation'],\n",
       "  'references': ['1880262756',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '1662133657',\n",
       "   '168564468',\n",
       "   '2131462252',\n",
       "   '2296073425',\n",
       "   '2158997610',\n",
       "   '2004763266',\n",
       "   '2156515921']},\n",
       " {'id': '1423339008',\n",
       "  'title': 'Parsing Natural Scenes and Natural Language with Recursive Neural Networks',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '1,490',\n",
       "  'abstract': 'Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Richard Socher',\n",
       "   'Cliff C. Lin',\n",
       "   'Chris Manning',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Parse tree',\n",
       "   'Treebank',\n",
       "   'Natural language',\n",
       "   'Sentence',\n",
       "   'Syntax',\n",
       "   'Artificial neural network',\n",
       "   'Segmentation',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Annotation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2100495367',\n",
       "   '2162915993',\n",
       "   '2067191022',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2130325614',\n",
       "   '1574901103',\n",
       "   '1566135517',\n",
       "   '1528789833',\n",
       "   '2536208356']},\n",
       " {'id': '1498436455',\n",
       "  'title': 'Learning representations by back-propagating errors',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '23,174',\n",
       "  'abstract': 'We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.',\n",
       "  'date': 1987,\n",
       "  'authors': ['David E. Rumelhart 1, Geoffrey E. Hinton 2, Ronald J. Williams 1'],\n",
       "  'related_topics': ['Domain (software engineering)',\n",
       "   'Task (project management)',\n",
       "   'Measure (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Neurophysiology',\n",
       "   'The Internet'],\n",
       "  'references': ['1652505363', '2322002063']},\n",
       " {'id': '1662133657',\n",
       "  'title': 'From frequency to meaning: vector space models of semantics',\n",
       "  'reference_count': '178',\n",
       "  'citation_count': '3,223',\n",
       "  'abstract': 'Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Peter D. Turney 1, Patrick Pantel 2'],\n",
       "  'related_topics': ['Statistical semantics',\n",
       "   'Distributional semantics',\n",
       "   'Semantics',\n",
       "   'Random indexing',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Meaning (linguistics)',\n",
       "   'Natural language processing',\n",
       "   'Field (computer science)',\n",
       "   'Process (engineering)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2173213060',\n",
       "   '1880262756',\n",
       "   '1532325895',\n",
       "   '3013264884',\n",
       "   '2038721957',\n",
       "   '2117130368',\n",
       "   '2024165284',\n",
       "   '1660390307',\n",
       "   '2166706824',\n",
       "   '1992419399']},\n",
       " {'id': '1889268436',\n",
       "  'title': 'Semantic Compositionality through Recursive Matrix-Vector Spaces',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '1,449',\n",
       "  'abstract': 'Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Richard Socher',\n",
       "   'Brody Huval',\n",
       "   'Christopher D. Manning',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Parse tree',\n",
       "   'Principle of compositionality',\n",
       "   'Natural language',\n",
       "   'Syntax',\n",
       "   'Recurrent neural network',\n",
       "   'Propositional calculus',\n",
       "   'Vector space',\n",
       "   'Noun',\n",
       "   'Meaning (non-linguistic)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2251939518',\n",
       "   '2117130368',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '1662133657',\n",
       "   '2097606805',\n",
       "   '2103305545',\n",
       "   '2163455955',\n",
       "   '1984052055',\n",
       "   '2151048449']},\n",
       " {'id': '2131462252',\n",
       "  'title': 'A Scalable Hierarchical Distributed Language Model',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '1,132',\n",
       "  'abstract': 'Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Andriy Mnih', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Binary tree',\n",
       "   'Tree (data structure)',\n",
       "   'Word (computer architecture)',\n",
       "   'Probabilistic logic',\n",
       "   'Scalability',\n",
       "   'Feature (machine learning)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2038721957',\n",
       "   '2132339004',\n",
       "   '36903255',\n",
       "   '2091812280',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2127314673',\n",
       "   '2056590938',\n",
       "   '2111305191',\n",
       "   '1558797106']},\n",
       " {'id': '2097117768',\n",
       "  'title': 'Going deeper with convolutions',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '31,038',\n",
       "  'abstract': 'We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Christian Szegedy 1, Wei Liu 2, Yangqing Jia 1, Pierre Sermanet 1, Scott Reed 3, Dragomir Anguelov 1, Dumitru Erhan 1, Vincent Vanhoucke 1, Andrew Rabinovich 4'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Artificial neural network',\n",
       "   'Contextual image classification',\n",
       "   'Hebbian theory',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Residual neural network'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2963911037',\n",
       "   '2168231600',\n",
       "   '2068730032',\n",
       "   '104184427']},\n",
       " {'id': '639708223',\n",
       "  'title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '27,656',\n",
       "  'abstract': 'State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Shaoqing Ren 1, Kaiming He 2, Ross Girshick 3, Jian Sun 2'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Convolutional neural network',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Feature extraction',\n",
       "   'Frame rate',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1536680647',\n",
       "   '2168356304']},\n",
       " {'id': '2102605133',\n",
       "  'title': 'Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '19,676',\n",
       "  'abstract': 'Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Ross Girshick',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell',\n",
       "   'Jitendra Malik'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Feature (computer vision)',\n",
       "   'Convolutional neural network',\n",
       "   'Feature extraction',\n",
       "   'Support vector machine',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Minimum bounding box',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2031489346',\n",
       "   '2088049833',\n",
       "   '2155541015']},\n",
       " {'id': '1903029394',\n",
       "  'title': 'Fully convolutional networks for semantic segmentation',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '23,343',\n",
       "  'abstract': 'Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Jonathan Long', 'Evan Shelhamer', 'Trevor Darrell'],\n",
       "  'related_topics': ['Scale-space segmentation',\n",
       "   'Segmentation',\n",
       "   'Inference',\n",
       "   'Image translation',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Pascal (programming language)',\n",
       "   'Image (mathematics)',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2155893237',\n",
       "   '1663973292',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2109255472',\n",
       "   '2155541015']},\n",
       " {'id': '2155893237',\n",
       "  'title': 'Caffe: Convolutional Architecture for Fast Feature Embedding',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '14,888',\n",
       "  'abstract': 'Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Yangqing Jia 1, Evan Shelhamer 2, Jeff Donahue 2, Sergey Karayev 2, Jonathan Long 2, Ross Girshick 2, Sergio Guadarrama 2, Trevor Darrell 2'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Theano',\n",
       "   'CUDA',\n",
       "   'Python (programming language)',\n",
       "   'Convolutional neural network',\n",
       "   'Cloud computing',\n",
       "   'Caffè',\n",
       "   'TrueNorth',\n",
       "   'Artificial neural network',\n",
       "   'Computer architecture',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2963542991',\n",
       "   '2088049833',\n",
       "   '2155541015',\n",
       "   '753012316',\n",
       "   '1825604117',\n",
       "   '2147414309',\n",
       "   '1872489089',\n",
       "   '2962883796']},\n",
       " {'id': '1536680647',\n",
       "  'title': 'Fast R-CNN',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '14,493',\n",
       "  'abstract': 'This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Ross Girshick'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Python (programming language)',\n",
       "   'Pascal (programming language)',\n",
       "   'Feature extraction',\n",
       "   'Computational science',\n",
       "   'Real-time computing',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Open source software'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2168356304',\n",
       "   '1861492603',\n",
       "   '2963542991',\n",
       "   '2109255472',\n",
       "   '2164598857']},\n",
       " {'id': '2963420686',\n",
       "  'title': 'Squeeze-and-Excitation Networks',\n",
       "  'reference_count': '81',\n",
       "  'citation_count': '6,969',\n",
       "  'abstract': 'The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of ${\\\\sim }$ ∼ 25 percent. Models and code are available at https://github.com/hujie-frank/SENet .',\n",
       "  'date': 2018,\n",
       "  'authors': ['Jie Hu 1, Li Shen 2, Samuel Albanie 2, Gang Sun 1, Enhua Wu 1'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Block (data storage)',\n",
       "   'Convolutional code',\n",
       "   'Feature (machine learning)',\n",
       "   'Construct (python library)',\n",
       "   'Code (cryptography)',\n",
       "   'Convolution',\n",
       "   'Theoretical computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2963446712']},\n",
       " {'id': '2965373594',\n",
       "  'title': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '5,486',\n",
       "  'abstract': 'Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Yinhan Liu',\n",
       "   'Myle Ott',\n",
       "   'Naman Goyal',\n",
       "   'Jingfei Du',\n",
       "   'Mandar Joshi',\n",
       "   'Danqi Chen',\n",
       "   'Omer Levy',\n",
       "   'Mike Lewis',\n",
       "   'Luke Zettlemoyer',\n",
       "   'Veselin Stoyanov'],\n",
       "  'related_topics': ['Hyperparameter',\n",
       "   'Language model',\n",
       "   'Machine learning',\n",
       "   'Replication (statistics)',\n",
       "   'Computer science',\n",
       "   'Code (cryptography)',\n",
       "   'Key (cryptography)',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2964121744',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '2899771611',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '2962784628',\n",
       "   '1840435438',\n",
       "   '2970597249',\n",
       "   '2963026768']},\n",
       " {'id': '2970597249',\n",
       "  'title': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,628',\n",
       "  'abstract': 'With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Zhilin Yang 1, Zihang Dai 1, Yiming Yang 1, Jaime G. Carbonell 1, Ruslan Salakhutdinov 1, Quoc V. Le 2'],\n",
       "  'related_topics': ['Autoregressive model',\n",
       "   'Language model',\n",
       "   'Question answering',\n",
       "   'Sentiment analysis',\n",
       "   'Margin (machine learning)',\n",
       "   'Ranking',\n",
       "   'Machine learning',\n",
       "   'Dependency (UML)',\n",
       "   'Computer science',\n",
       "   'Noise reduction',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2996035354',\n",
       "   '2990704537',\n",
       "   '3127550471',\n",
       "   '3105966348',\n",
       "   '3100307207',\n",
       "   '3099342932',\n",
       "   '3100345210']},\n",
       " {'id': '2923014074',\n",
       "  'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '1,669',\n",
       "  'abstract': 'Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Alex Wang 1, Amanpreet Singh 1, Julian Michael 2, Felix Hill 3, Omer Levy 4, Samuel R. Bowman 1'],\n",
       "  'related_topics': ['Natural language understanding',\n",
       "   'Benchmark (computing)',\n",
       "   'Test set',\n",
       "   'Transfer of learning',\n",
       "   'Test data',\n",
       "   'Sentence',\n",
       "   'Task (project management)',\n",
       "   'Feature (machine learning)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2962739339',\n",
       "   '2251939518',\n",
       "   '2963748441',\n",
       "   '1486649854',\n",
       "   '2963918774',\n",
       "   '2963846996',\n",
       "   '2525127255',\n",
       "   '2962736243',\n",
       "   '3104033643',\n",
       "   '2130158090']},\n",
       " {'id': '2911489562',\n",
       "  'title': 'BioBERT: a pre-trained biomedical language representation model for biomedical text mining.',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '1,040',\n",
       "  'abstract': 'Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Jinhyuk Lee 1, Wonjin Yoon 1, Sungdong Kim 2, Donghyeon Kim 1, Sunkyu Kim 1, Chan Ho So 1, Jaewoo Kang 1'],\n",
       "  'related_topics': ['Biomedical text mining',\n",
       "   'Named-entity recognition',\n",
       "   'Language model',\n",
       "   'Question answering',\n",
       "   'Relationship extraction',\n",
       "   'Deep learning',\n",
       "   'Natural language processing',\n",
       "   'F1 score',\n",
       "   'Source code',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2963341956',\n",
       "   '2963403868',\n",
       "   '2919115771',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2962739339',\n",
       "   '2963748441',\n",
       "   '2132339004',\n",
       "   '2525778437',\n",
       "   '2963756346']},\n",
       " {'id': '2095705004',\n",
       "  'title': 'Dropout: a simple way to prevent neural networks from overfitting',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '28,707',\n",
       "  'abstract': 'Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Nitish Srivastava',\n",
       "   'Geoffrey Hinton',\n",
       "   'Alex Krizhevsky',\n",
       "   'Ilya Sutskever',\n",
       "   'Ruslan Salakhutdinov'],\n",
       "  'related_topics': ['Overfitting',\n",
       "   'Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Supervised learning',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial neural network',\n",
       "   'Regularization (mathematics)',\n",
       "   'Document classification',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2135046866',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '2145094598',\n",
       "   '2131241448',\n",
       "   '2335728318']},\n",
       " {'id': '2146502635',\n",
       "  'title': 'Adaptive Subgradient Methods for Online Learning and Stochastic Optimization',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '8,863',\n",
       "  'abstract': 'We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.',\n",
       "  'date': 2011,\n",
       "  'authors': ['John Duchi 1, Elad Hazan 2, Yoram Singer 3'],\n",
       "  'related_topics': ['Subgradient method',\n",
       "   'Online machine learning',\n",
       "   'Empirical risk minimization',\n",
       "   'Stochastic optimization',\n",
       "   'Regularization (mathematics)',\n",
       "   'Mathematical optimization',\n",
       "   'Regret',\n",
       "   'Function (mathematics)',\n",
       "   'Domain (software engineering)',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296319761',\n",
       "   '2108598243',\n",
       "   '3120740533',\n",
       "   '2798766386',\n",
       "   '2610857016',\n",
       "   '2150102617',\n",
       "   '2167732364',\n",
       "   '1992208280',\n",
       "   '2160218441',\n",
       "   '1978394996']},\n",
       " {'id': '2168231600',\n",
       "  'title': 'Large Scale Distributed Deep Networks',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '3,338',\n",
       "  'abstract': 'Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Jeffrey Dean',\n",
       "   'Greg Corrado',\n",
       "   'Rajat Monga',\n",
       "   'Kai Chen',\n",
       "   'Matthieu Devin',\n",
       "   'Mark Mao',\n",
       "   \"Marc'aurelio Ranzato\",\n",
       "   'Andrew Senior',\n",
       "   'Paul Tucker',\n",
       "   'Ke Yang',\n",
       "   'Quoc V. Le',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Feature learning',\n",
       "   'Stochastic gradient descent',\n",
       "   'Theano',\n",
       "   'Asynchronous communication',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2108598243',\n",
       "   '2173213060',\n",
       "   '3118608800',\n",
       "   '2146502635',\n",
       "   '2117130368',\n",
       "   '2147768505',\n",
       "   '2132339004',\n",
       "   '2141125852',\n",
       "   '2184045248',\n",
       "   '2118858186']},\n",
       " {'id': '104184427',\n",
       "  'title': 'On the importance of initialization and momentum in deep learning',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '3,619',\n",
       "  'abstract': 'Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Ilya Sutskever 1, James Martens 2, George Dahl 2, Geoffrey Hinton 2'],\n",
       "  'related_topics': ['Initialization',\n",
       "   'Stochastic gradient descent',\n",
       "   'Recurrent neural network',\n",
       "   'Deep learning',\n",
       "   'Momentum (technical analysis)',\n",
       "   'Schedule',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Curvature',\n",
       "   'Training (meteorology)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2064675550',\n",
       "   '1533861849',\n",
       "   '2147768505',\n",
       "   '2110798204',\n",
       "   '1993882792',\n",
       "   '3141595720',\n",
       "   '2184045248']},\n",
       " {'id': '6908809',\n",
       "  'title': 'ADADELTA: An Adaptive Learning Rate Method',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '6,447',\n",
       "  'abstract': 'We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Matthew D. Zeiler'],\n",
       "  'related_topics': ['Stochastic gradient descent',\n",
       "   'Gradient descent',\n",
       "   'Online machine learning',\n",
       "   'MNIST database',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Hyperparameter',\n",
       "   'Selection (genetic algorithm)',\n",
       "   'Computer science',\n",
       "   'Scale (ratio)',\n",
       "   'Task (project management)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2168231600',\n",
       "   '2147768505',\n",
       "   '1498436455',\n",
       "   '2120420045',\n",
       "   '19621276',\n",
       "   '1994616650']},\n",
       " {'id': '1753482797',\n",
       "  'title': 'Recurrent Continuous Translation Models',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '1,282',\n",
       "  'abstract': 'We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Nal Kalchbrenner', 'Phil Blunsom'],\n",
       "  'related_topics': ['Rule-based machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Dynamic and formal equivalence',\n",
       "   'Sentence',\n",
       "   'Language model',\n",
       "   'Perplexity',\n",
       "   'Syntax',\n",
       "   'Translation (geometry)',\n",
       "   'Word order',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2146502635',\n",
       "   '2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '1889268436',\n",
       "   '2171928131',\n",
       "   '2251222643',\n",
       "   '2103305545',\n",
       "   '2006969979',\n",
       "   '196214544']},\n",
       " {'id': '2294059674',\n",
       "  'title': 'Maxout Networks',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '2,224',\n",
       "  'abstract': \"We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.\",\n",
       "  'date': 2013,\n",
       "  'authors': ['Ian Goodfellow',\n",
       "   'David Warde-Farley',\n",
       "   'Mehdi Mirza',\n",
       "   'Aaron Courville',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['MNIST database',\n",
       "   'Leverage (statistics)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '2546302380',\n",
       "   '2912934387',\n",
       "   '2131241448',\n",
       "   '2335728318',\n",
       "   '2156387975',\n",
       "   '189596042']},\n",
       " {'id': '1810943226',\n",
       "  'title': 'Generating Sequences With Recurrent Neural Networks',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '3,331',\n",
       "  'abstract': 'This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Alex Graves'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Handwriting',\n",
       "   'Sequence',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Point (typography)',\n",
       "   'Structure (mathematical logic)'],\n",
       "  'references': ['2064675550',\n",
       "   '1554663460',\n",
       "   '2143612262',\n",
       "   '44815768',\n",
       "   '1632114991',\n",
       "   '2131462252',\n",
       "   '196214544',\n",
       "   '2108677974',\n",
       "   '2120861206',\n",
       "   '3023071679']},\n",
       " {'id': '2964199361',\n",
       "  'title': 'On the Properties of Neural Machine Translation: Encoder--Decoder Approaches',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '3,650',\n",
       "  'abstract': 'Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder‐Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Kyunghyun Cho 1, Bart van Merrienboer 1, Dzmitry Bahdanau 2, Yoshua Bengio 3, 4, 5'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Convolutional neural network',\n",
       "   'Artificial neural network',\n",
       "   'Encoder',\n",
       "   'Sentence',\n",
       "   'Translation (geometry)',\n",
       "   'Speech recognition',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Focus (optics)'],\n",
       "  'references': ['2130942839',\n",
       "   '2157331557',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '1810943226',\n",
       "   '2153653739',\n",
       "   '2395935897',\n",
       "   '1828163288',\n",
       "   '1905522558',\n",
       "   '2341457423']},\n",
       " {'id': '1815076433',\n",
       "  'title': 'On the difficulty of training recurrent neural networks',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '4,011',\n",
       "  'abstract': 'There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Razvan Pascanu 1, Tomas Mikolov 2, Yoshua Bengio 1'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Dynamical systems theory',\n",
       "   'Artificial intelligence',\n",
       "   'Clipping (computer graphics)',\n",
       "   'Mathematics',\n",
       "   'Effective solution'],\n",
       "  'references': ['2146502635',\n",
       "   '2064675550',\n",
       "   '1498436455',\n",
       "   '1606347560',\n",
       "   '196214544',\n",
       "   '2110485445',\n",
       "   '2171865010',\n",
       "   '2122585011',\n",
       "   '2118706537',\n",
       "   '2107878631']},\n",
       " {'id': '2153653739',\n",
       "  'title': 'Statistical phrase-based translation',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '3,371',\n",
       "  'abstract': 'We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Philipp Koehn', 'Franz Josef Och', 'Daniel Marcu'],\n",
       "  'related_topics': ['Noun phrase',\n",
       "   'Phrase',\n",
       "   'Determiner phrase',\n",
       "   'Phrase search',\n",
       "   'Dependency grammar',\n",
       "   'Phrase structure rules',\n",
       "   'Generalized phrase structure grammar',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Hybrid machine translation',\n",
       "   'Example-based machine translation',\n",
       "   'Computer-assisted translation',\n",
       "   'Interactive machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Machine translation software usability',\n",
       "   'Pivot language',\n",
       "   'Evaluation of machine translation',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'BLEU',\n",
       "   'Artificial intelligence',\n",
       "   'Lexicography'],\n",
       "  'references': ['2101105183',\n",
       "   '2006969979',\n",
       "   '1508165687',\n",
       "   '1973923101',\n",
       "   '1986543644',\n",
       "   '2116316001',\n",
       "   '1517947178',\n",
       "   '2161792612',\n",
       "   '1549285799',\n",
       "   '2158388102']},\n",
       " {'id': '2183341477',\n",
       "  'title': 'Rethinking the Inception Architecture for Computer Vision',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '13,669',\n",
       "  'abstract': 'Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Christian Szegedy 1, Vincent Vanhoucke 1, Sergey Ioffe 1, Jon Shlens 1, Zbigniew Wojna 2'],\n",
       "  'related_topics': ['Test set',\n",
       "   'Set (abstract data type)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Inference',\n",
       "   'Artificial intelligence',\n",
       "   'State (computer science)',\n",
       "   'Computation',\n",
       "   'Computer vision',\n",
       "   'Variety (cybernetics)',\n",
       "   'Regularization (mathematics)'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '1677182931',\n",
       "   '2096733369',\n",
       "   '2016053056']},\n",
       " {'id': '2147768505',\n",
       "  'title': 'Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition',\n",
       "  'reference_count': '84',\n",
       "  'citation_count': '3,179',\n",
       "  'abstract': 'We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.',\n",
       "  'date': 2011,\n",
       "  'authors': ['G. E. Dahl 1, Dong Yu 2, Li Deng 2, A. Acero 2'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Hidden Markov model',\n",
       "   'Word error rate',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Triphone',\n",
       "   'Context model',\n",
       "   'Context (language use)',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2116064496',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '1993882792',\n",
       "   '1498436455']},\n",
       " {'id': '2156387975',\n",
       "  'title': 'Deep Sparse Rectifier Neural Networks',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '6,986',\n",
       "  'abstract': 'While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity',\n",
       "  'date': 2011,\n",
       "  'authors': ['Xavier Glorot 1, Antoine Bordes 2, Yoshua Bengio 1'],\n",
       "  'related_topics': ['Rectifier (neural networks)',\n",
       "   'Winner-take-all',\n",
       "   'Artificial neural network',\n",
       "   'Logistic function',\n",
       "   'Hyperbolic function',\n",
       "   'Topology',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1665214252',\n",
       "   '1533861849',\n",
       "   '2072128103',\n",
       "   '2129131372',\n",
       "   '2546302380',\n",
       "   '2097726431',\n",
       "   '2025768430']},\n",
       " {'id': '2963504252',\n",
       "  'title': 'Exact solutions to the nonlinear dynamics of learning in deep linear neural networks',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '985',\n",
       "  'abstract': 'Abstract: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Andrew M. Saxe', 'James L. McClelland', 'Surya Ganguli'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Deep learning',\n",
       "   'Nonlinear system',\n",
       "   'Gradient descent',\n",
       "   'Convergence (routing)',\n",
       "   'Edge of chaos',\n",
       "   'Gaussian',\n",
       "   'Linearity',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '104184427',\n",
       "   '2110798204',\n",
       "   '2141125852',\n",
       "   '1993882792',\n",
       "   '1815076433']},\n",
       " {'id': '2147880316',\n",
       "  'title': 'Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '15,804',\n",
       "  'abstract': 'We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.',\n",
       "  'date': 2001,\n",
       "  'authors': ['John D. Lafferty', 'Andrew McCallum', 'Fernando C. N. Pereira'],\n",
       "  'related_topics': ['Variable-order Markov model',\n",
       "   'Maximum-entropy Markov model',\n",
       "   'Graphical model',\n",
       "   'Conditional random field',\n",
       "   'Markov model',\n",
       "   'Markov property',\n",
       "   'Sequence labeling',\n",
       "   'Conditional entropy',\n",
       "   'Random field',\n",
       "   'Bayesian network',\n",
       "   'Discriminative model',\n",
       "   'Hidden Markov model',\n",
       "   'Structured prediction',\n",
       "   'Hidden semi-Markov model',\n",
       "   'Probabilistic logic',\n",
       "   'Probabilistic relevance model',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '3124955340',\n",
       "   '1574901103',\n",
       "   '2009570821',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '1773803948',\n",
       "   '2160842254',\n",
       "   '3021452258',\n",
       "   '2117400858']},\n",
       " {'id': '2156163116',\n",
       "  'title': 'Best practices for convolutional neural networks applied to visual document analysis',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '2,771',\n",
       "  'abstract': 'Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple \"do-it-yourself\" implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependentlearning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.',\n",
       "  'date': 2003,\n",
       "  'authors': ['P.Y. Simard', 'D. Steinkraus', 'J.C. Platt'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'MNIST database',\n",
       "   'Artificial neural network',\n",
       "   'Handwriting recognition',\n",
       "   'Set (abstract data type)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Support vector machine',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2310919327',\n",
       "   '1554663460',\n",
       "   '2159737176',\n",
       "   '2027197837',\n",
       "   '2068017609',\n",
       "   '2147345686',\n",
       "   '51975515',\n",
       "   '2166469100']},\n",
       " {'id': '2963399829',\n",
       "  'title': 'mixup: Beyond Empirical Risk Minimization',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,648',\n",
       "  'abstract': 'Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Hongyi Zhang 1, Moustapha Cisse 2, Yann N. Dauphin 2, David Lopez-Paz 2'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Empirical risk minimization',\n",
       "   'Robustness (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization',\n",
       "   'Computer science',\n",
       "   'Memorization',\n",
       "   'Sensitivity (control systems)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Generative grammar'],\n",
       "  'references': ['2964274690',\n",
       "   '2966415767',\n",
       "   '2963855133',\n",
       "   '3098350627',\n",
       "   '3129973872',\n",
       "   '2970902013',\n",
       "   '2971149989',\n",
       "   '3035743198']},\n",
       " {'id': '2964311892',\n",
       "  'title': 'Spectral Networks and Locally Connected Networks on Graphs',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '2,211',\n",
       "  'abstract': 'Abstract: Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Joan Bruna 1, Wojciech Zaremba 1, Arthur Szlam 2, Yann LeCun 1'],\n",
       "  'related_topics': ['Laplacian matrix',\n",
       "   'Convolutional neural network',\n",
       "   'Hierarchical clustering',\n",
       "   'Domain (software engineering)',\n",
       "   'Image (mathematics)',\n",
       "   'Translation (geometry)',\n",
       "   'Theoretical computer science',\n",
       "   'Group (mathematics)',\n",
       "   'SIGNAL (programming language)',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2310919327',\n",
       "   '1554944419',\n",
       "   '2160815625',\n",
       "   '2132914434',\n",
       "   '1578099820',\n",
       "   '2156718197',\n",
       "   '2962820688',\n",
       "   '1999192586',\n",
       "   '2147860648']},\n",
       " {'id': '2157364932',\n",
       "  'title': 'Learning a similarity metric discriminatively, with application to face verification',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '2,950',\n",
       "  'abstract': 'We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.',\n",
       "  'date': 2005,\n",
       "  'authors': ['S. Chopra', 'R. Hadsell', 'Y. LeCun'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Norm (mathematics)',\n",
       "   'Discriminative model',\n",
       "   'Robustness (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Face verification'],\n",
       "  'references': []},\n",
       " {'id': '2128499899',\n",
       "  'title': 'Induction of Multiscale Temporal Structure',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '156',\n",
       "  'abstract': 'Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation.',\n",
       "  'date': 1991,\n",
       "  'authors': ['Michael C Mozer'],\n",
       "  'related_topics': ['Computational problem',\n",
       "   'Sequence',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Backpropagation',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Musical composition',\n",
       "   'Fraction (mathematics)',\n",
       "   'Musical form'],\n",
       "  'references': ['2154642048',\n",
       "   '2016589492',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '1959983357',\n",
       "   '2028629011',\n",
       "   '2053127376',\n",
       "   '2167607759']},\n",
       " {'id': '2007431958',\n",
       "  'title': 'Generalization of back-propagation to recurrent neural networks.',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '1,210',\n",
       "  'abstract': 'An adaptive neural network with asymmetric connections is introduced. This network is related to the Hopfield network with graded neurons and uses a recurrent generalization of the \\\\ensuremath{\\\\delta} rule of Rumelhart, Hinton, and Williams to modify adaptively the synaptic weights. The new network bears a resemblance to the master/slave network of Lapedes and Farber but it is architecturally simpler.',\n",
       "  'date': 1987,\n",
       "  'authors': ['Fernando J. Pineda'],\n",
       "  'related_topics': ['Hopfield network',\n",
       "   'Recurrent neural network',\n",
       "   'Time delay neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Feedforward neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Physical neural network',\n",
       "   'Stochastic neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['1652505363', '2177721432', '2075510082']},\n",
       " {'id': '2123716044',\n",
       "  'title': 'Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '759',\n",
       "  'abstract': 'Although the potential of the powerful mapping and representational capabilities of recurrent network architectures is generally recognized by the neural network research community, recurrent neural networks have not been widely used for the control of nonlinear dynamical systems, possibly due to the relative ineffectiveness of simple gradient descent training algorithms. Developments in the use of parameter-based extended Kalman filter algorithms for training recurrent networks may provide a mechanism by which these architectures will prove to be of practical value. This paper presents a decoupled extended Kalman filter (DEKF) algorithm for training of recurrent networks with special emphasis on application to control problems. We demonstrate in simulation the application of the DEKF algorithm to a series of example control problems ranging from the well-known cart-pole and bioreactor benchmark problems to an automotive subsystem, engine idle speed control. These simulations suggest that recurrent controller networks trained by Kalman filter methods can combine the traditional features of state-space controllers and observers in a homogeneous architecture for nonlinear dynamical systems, while simultaneously exhibiting less sensitivity than do purely feedforward controller networks to changes in plant parameters and measurement noise. >',\n",
       "  'date': 1994,\n",
       "  'authors': ['G.V. Puskorius', 'L.A. Feldkamp'],\n",
       "  'related_topics': ['Extended Kalman filter',\n",
       "   'Recurrent neural network',\n",
       "   'Kalman filter',\n",
       "   'Control theory',\n",
       "   'Artificial neural network',\n",
       "   'Dynamical systems theory',\n",
       "   'Gradient descent',\n",
       "   'Control system',\n",
       "   'Feed forward',\n",
       "   'Control theory',\n",
       "   'Computer science',\n",
       "   'Moving horizon estimation'],\n",
       "  'references': ['2138484437',\n",
       "   '2016589492',\n",
       "   '2150355110',\n",
       "   '1583833196',\n",
       "   '2143787696',\n",
       "   '2057653135',\n",
       "   '2132152975',\n",
       "   '2112462566',\n",
       "   '2090248140',\n",
       "   '1529008516']},\n",
       " {'id': '2143503258',\n",
       "  'title': 'Learning state space trajectories in recurrent neural networks',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '997',\n",
       "  'abstract': 'Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.',\n",
       "  'date': 1989,\n",
       "  'authors': ['Barak A. Pearlmutter'],\n",
       "  'related_topics': ['Echo state network',\n",
       "   'Recurrent neural network',\n",
       "   'Time delay neural network'],\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '1597286183',\n",
       "   '2173629880',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '2007431958',\n",
       "   '1971129545',\n",
       "   '1959983357',\n",
       "   '3121926921']},\n",
       " {'id': '2154890045',\n",
       "  'title': 'Gradient calculations for dynamic recurrent neural networks: a survey',\n",
       "  'reference_count': '117',\n",
       "  'citation_count': '713',\n",
       "  'abstract': 'Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman\\'s history cutoff, and Jordan\\'s output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed. >',\n",
       "  'date': 1995,\n",
       "  'authors': ['B.A. Pearlmutter'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Deep learning',\n",
       "   'Recurrent neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Artificial intelligence',\n",
       "   'Fixed point',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Boltzmann constant',\n",
       "   'Recurrent neural nets'],\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2138484437',\n",
       "   '2110485445',\n",
       "   '2147800946',\n",
       "   '2895674046',\n",
       "   '1597286183',\n",
       "   '2173629880',\n",
       "   '1535810436',\n",
       "   '2016589492']},\n",
       " {'id': '2103452139',\n",
       "  'title': 'Learning long-term dependencies in NARX recurrent neural networks',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '702',\n",
       "  'abstract': 'It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Tsungnan Lin 1, B.G. Horne 2, P. Tino 3, C.L. Giles 4'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Nonlinear system identification'],\n",
       "  'references': ['2064675550',\n",
       "   '2154642048',\n",
       "   '2138484437',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '2798813531',\n",
       "   '2128499899',\n",
       "   '2123716044',\n",
       "   '1674799117',\n",
       "   '2098398123']},\n",
       " {'id': '2048060899',\n",
       "  'title': 'A time-delay neural network architecture for isolated word recognition',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '815',\n",
       "  'abstract': \"Abstract A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.\",\n",
       "  'date': 1989,\n",
       "  'authors': ['Kevin J. Lang 1, Alex H. Waibel 1, Geoffrey E. Hinton 2'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Word recognition',\n",
       "   'Hidden Markov model',\n",
       "   'Network architecture',\n",
       "   'Artificial neural network',\n",
       "   'Vocabulary',\n",
       "   'Speech recognition',\n",
       "   'Segmentation',\n",
       "   'Computer science'],\n",
       "  'references': ['1498436455',\n",
       "   '3017143921',\n",
       "   '2173629880',\n",
       "   '2176028050',\n",
       "   '1966812932',\n",
       "   '2101926813',\n",
       "   '1991133427',\n",
       "   '1959983357',\n",
       "   '3121926921',\n",
       "   '2048330959']},\n",
       " {'id': '1674799117',\n",
       "  'title': 'Gradient-based learning algorithms for recurrent networks and their computational complexity',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '548',\n",
       "  'abstract': '',\n",
       "  'date': 1994,\n",
       "  'authors': ['Ronald J. Williams', 'David Zipser'],\n",
       "  'related_topics': ['Computational resource',\n",
       "   'Computational learning theory',\n",
       "   'Deep learning',\n",
       "   'Probabilistic analysis of algorithms',\n",
       "   'Artificial neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Stability (learning theory)',\n",
       "   'Types of artificial neural networks',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Theoretical computer science'],\n",
       "  'references': ['2064675550',\n",
       "   '1810943226',\n",
       "   '2144499799',\n",
       "   '2136848157',\n",
       "   '1828163288',\n",
       "   '1735317348',\n",
       "   '2079735306',\n",
       "   '3099873379',\n",
       "   '2147568880']},\n",
       " {'id': '2136922672',\n",
       "  'title': 'A fast learning algorithm for deep belief nets',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '15,773',\n",
       "  'abstract': 'We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Geoffrey E. Hinton 1, Simon Osindero 1, Yee-Whye Teh 2'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Generative model',\n",
       "   'Content-addressable memory',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Greedy algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Boltzmann machine',\n",
       "   'Artificial intelligence',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2310919327',\n",
       "   '2116064496',\n",
       "   '2057175746',\n",
       "   '2159080219',\n",
       "   '2156163116',\n",
       "   '2131686571',\n",
       "   '2158778629',\n",
       "   '2567948266',\n",
       "   '2159737176',\n",
       "   '2124914669']},\n",
       " {'id': '2025768430',\n",
       "  'title': 'Extracting and composing robust features with denoising autoencoders',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '5,670',\n",
       "  'abstract': 'Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Pascal Vincent',\n",
       "   'Hugo Larochelle',\n",
       "   'Yoshua Bengio',\n",
       "   'Pierre-Antoine Manzagol'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Unsupervised learning',\n",
       "   'Generative model'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2110798204',\n",
       "   '2153663612',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '1994197834',\n",
       "   '2293063825',\n",
       "   '2172174689']},\n",
       " {'id': '2110798204',\n",
       "  'title': 'Greedy Layer-Wise Training of Deep Networks',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '5,558',\n",
       "  'abstract': 'Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Pascal Lamblin',\n",
       "   'Dan Popovici',\n",
       "   'Hugo Larochelle'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Optimization problem',\n",
       "   'Generative model',\n",
       "   'Initialization',\n",
       "   'Context (language use)',\n",
       "   'Artificial intelligence',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2613634265',\n",
       "   '2124914669',\n",
       "   '1993845689',\n",
       "   '2109779438',\n",
       "   '2103626435',\n",
       "   '2125569215',\n",
       "   '2167967601']},\n",
       " {'id': '1994197834',\n",
       "  'title': 'An empirical evaluation of deep architectures on problems with many factors of variation',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '1,061',\n",
       "  'abstract': 'Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Hugo Larochelle',\n",
       "   'Dumitru Erhan',\n",
       "   'Aaron Courville',\n",
       "   'James Bergstra',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Online machine learning',\n",
       "   'Instance-based learning',\n",
       "   'Computational learning theory',\n",
       "   'Stability (learning theory)',\n",
       "   'Deep learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Ensemble learning',\n",
       "   'Unsupervised learning',\n",
       "   'Competitive learning',\n",
       "   'Artificial neural network',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Algorithmic learning theory',\n",
       "   'Support vector machine',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Generalization error'],\n",
       "  'references': ['2153635508',\n",
       "   '2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2134557905',\n",
       "   '2147800946',\n",
       "   '2613634265',\n",
       "   '2159737176',\n",
       "   '2124914669']},\n",
       " {'id': '2172174689',\n",
       "  'title': 'Efficient Learning of Sparse Representations with an Energy-Based Model',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '1,552',\n",
       "  'abstract': 'We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces \"stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.',\n",
       "  'date': 2006,\n",
       "  'authors': [\"Marc'aurelio Ranzato\",\n",
       "   'Christopher Poultney',\n",
       "   'Sumit Chopra',\n",
       "   'Yann L. Cun'],\n",
       "  'related_topics': ['Encoder',\n",
       "   'Sparse approximation',\n",
       "   'MNIST database',\n",
       "   'Code (cryptography)',\n",
       "   'Filter (signal processing)',\n",
       "   'Word error rate',\n",
       "   'Energy (signal processing)',\n",
       "   'Pattern recognition',\n",
       "   'Detector',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2116064496',\n",
       "   '1902027874',\n",
       "   '2156163116',\n",
       "   '2105464873',\n",
       "   '1802356529',\n",
       "   '2075187489',\n",
       "   '2102409316',\n",
       "   '11828546']},\n",
       " {'id': '2903899730',\n",
       "  'title': 'Origin and evolution of pathogenic coronaviruses',\n",
       "  'reference_count': '145',\n",
       "  'citation_count': '3,159',\n",
       "  'abstract': 'Severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) are two highly transmissible and pathogenic viruses that emerged in humans at the beginning of the 21st century. Both viruses likely originated in bats, and genetically diverse coronaviruses that are related to SARS-CoV and MERS-CoV were discovered in bats worldwide. In this Review, we summarize the current knowledge on the origin and evolution of these two pathogenic coronaviruses and discuss their receptor usage; we also highlight the diversity and potential of spillover of bat-borne coronaviruses, as evidenced by the recent spillover of swine acute diarrhoea syndrome coronavirus (SADS-CoV) to pigs. Coronaviruses have a broad host range and distribution, and some highly pathogenic lineages have spilled over to humans and animals. Here, Cui, Li and Shi explore the viral factors that enabled the emergence of diseases such as severe acute respiratory syndrome and Middle East respiratory syndrome.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Jie Cui 1, Fang Li 2, Zheng Li Shi 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Viral evolution',\n",
       "   'Viral pathogenesis',\n",
       "   'Virology',\n",
       "   'Phylogenetics',\n",
       "   'Biology',\n",
       "   'Acute diarrhoea',\n",
       "   'Severe acute respiratory syndrome coronavirus'],\n",
       "  'references': ['2166867592',\n",
       "   '2144081223',\n",
       "   '2111211467',\n",
       "   '2470646526',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2306794997',\n",
       "   '1993577573',\n",
       "   '2775086803',\n",
       "   '2119111857']},\n",
       " {'id': '2166867592',\n",
       "  'title': 'Isolation of a Novel Coronavirus from a Man with Pneumonia in Saudi Arabia',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '4,563',\n",
       "  'abstract': 'A previously unknown coronavirus was isolated from the sputum of a 60-year-old man who presented with acute pneumonia and subsequent renal failure with a fatal outcome in Saudi Arabia. The virus (called HCoV-EMC) replicated readily in cell culture, producing cytopathic effects of rounding, detachment, and syncytium formation. The virus represents a novel betacoronavirus species. The closest known relatives are bat coronaviruses HKU4 and HKU5. Here, the clinical data, virus isolation, and molecular identification are presented. The clinical picture was remarkably similar to that of the severe acute respiratory syndrome (SARS) outbreak in 2003 and reminds us that animal coronaviruses can cause severe disease in humans.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Ali Moh Zaki 1, Sander Van Boheemen 2, Theo M. Bestebroer 2, Albert D.M.E. Osterhaus',\n",
       "   'Ron A.M. Fouchier'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Pneumonia',\n",
       "   'Alphacoronavirus',\n",
       "   'Virus',\n",
       "   'Outbreak',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'references': ['2132260239',\n",
       "   '2025170735',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '2116586125',\n",
       "   '1987783718',\n",
       "   '2111412754',\n",
       "   '1963953102',\n",
       "   '2170933940',\n",
       "   '2126707939']},\n",
       " {'id': '3000413850',\n",
       "  'title': 'Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV.',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '1,351',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus (MERS-CoV) is the causative agent of a severe respiratory disease associated with more than 2468 human infections and over 851 deaths in 27 countries since 2012. There are no approved treatments for MERS-CoV infection although a combination of lopinavir, ritonavir and interferon beta (LPV/RTV-IFNb) is currently being evaluated in humans in the Kingdom of Saudi Arabia. Here, we show that remdesivir (RDV) and IFNb have superior antiviral activity to LPV and RTV in vitro. In mice, both prophylactic and therapeutic RDV improve pulmonary function and reduce lung viral loads and severe lung pathology. In contrast, prophylactic LPV/RTV-IFNb slightly reduces viral loads without impacting other disease parameters. Therapeutic LPV/RTV-IFNb improves pulmonary function but does not reduce virus replication or severe lung pathology. Thus, we provide in vivo evidence of the potential for RDV to treat MERS-CoV infections.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Timothy P. Sheahan 1, Amy C. Sims 1, Sarah R. Leist 1, Alexandra Schäfer 1, John Won 1, Ariane J. Brown 1, Stephanie A. Montgomery 1, Alison Hogg 2, Darius Babusis 2, Michael O. Clarke 2, Jamie E. Spahn 2, Laura Bauer 2, Scott Sellers 2, Danielle Porter 2, Joy Y. Feng 2, Tomas Cihlar 2, Robert Jordan 2, Mark R. Denison 3, Ralph S. Baric 1'],\n",
       "  'related_topics': ['Lopinavir/ritonavir', 'Lopinavir', 'Lung injury'],\n",
       "  'references': ['2107277218',\n",
       "   '2470646526',\n",
       "   '2725497285',\n",
       "   '1993577573',\n",
       "   '2565805236',\n",
       "   '2791599184',\n",
       "   '2255243349',\n",
       "   '2292021561',\n",
       "   '2290466312',\n",
       "   '2793022939']},\n",
       " {'id': '2132260239',\n",
       "  'title': 'Identification of a novel coronavirus in patients with severe acute respiratory syndrome.',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '5,072',\n",
       "  'abstract': 'BACKGROUND: The severe acute respiratory syndrome (SARS) has recently been identified as a new clinical entity. SARS is thought to be caused by an unknown infectious agent. METHODS: Clinical specimens from patients with SARS were searched for unknown viruses with the use of cell cultures and molecular techniques. RESULTS: A novel coronavirus was identified in patients with SARS. The virus was isolated in cell culture, and a sequence 300 nucleotides in length was obtained by a polymerase-chain-reaction (PCR)-based random-amplification procedure. Genetic characterization indicated that the virus is only distantly related to known coronaviruses (identical in 50 to 60 percent of the nucleotide sequence). On the basis of the obtained sequence, conventional and real-time PCR assays for specific and sensitive detection of the novel virus were established. Virus was detected in a variety of clinical specimens from patients with SARS but not in controls. High concentrations of viral RNA of up to 100 million molecules per milliliter were found in sputum. Viral RNA was also detected at extremely low concentrations in plasma during the acute phase and in feces during the late convalescent phase. Infected patients showed seroconversion on the Vero cells in which the virus was isolated. CONCLUSIONS: The novel coronavirus might have a role in causing SARS.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Christian Drosten 1, Stephan Günther 1, Wolfgang Preiser 2, Sylvie van der Werf 3, Hans-Reinhard Brodt 4, Stephan Becker 5, Holger Rabenau 2, Marcus Panning 1, Larissa Kolesnikova 5, Ron A.M. Fouchier 6, Annemarie Berger 2, Ana-Maria Burguière 3, Jindrich Cinatl 2, Markus Eickmann 5, Nicolas Escriou 3, Klaus Grywna 1, Stefanie Kramme 1, Jean-Claude Manuguerra 3, Stefanie Müller 1, Volker Rickerts 4, Martin Stürmer 2, Simon Vieth 1, Hans-Dieter Klenk 5, Albert D.M.E. Osterhaus 6, Herbert Schmitz 1, Hans Wilhelm Doerr 2'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Human coronavirus OC43'],\n",
       "  'references': ['2100820722',\n",
       "   '2125251240',\n",
       "   '2107922358',\n",
       "   '2127062009',\n",
       "   '2084994773',\n",
       "   '2149579937',\n",
       "   '2090060897',\n",
       "   '2004869546',\n",
       "   '2030133843']},\n",
       " {'id': '2026274122',\n",
       "  'title': 'KDIGO clinical practice guidelines for acute kidney injury.',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '5,041',\n",
       "  'abstract': 'tion’, implying that most patients ‘should’ receive a particular action. In contrast, level 2 guidelines are essentially ‘suggestions’ and are deemed to be ‘weak’ or discretionary, recognising that management decisions may vary in different clinical contexts. Each recommendation was further graded from A to D by the quality of evidence underpinning them, with grade A referring to a high quality of evidence whilst grade D recognised a ‘very low’ evidence base. The overall strength and quality of the supporting evidence is summarised in table 1 . The guidelines focused on 4 key domains: (1) AKI definition, (2) prevention and treatment of AKI, (3) contrastinduced AKI (CI-AKI) and (4) dialysis interventions for the treatment of AKI. The full summary of clinical practice statements is available at www.kdigo.org, but a few key recommendation statements will be highlighted here.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Arif Khwaja'],\n",
       "  'related_topics': ['Renal angina',\n",
       "   'Psychological intervention',\n",
       "   'Intensive care medicine',\n",
       "   'Health care',\n",
       "   'MEDLINE',\n",
       "   'Dialysis',\n",
       "   'Quality (business)',\n",
       "   'Acute kidney injury',\n",
       "   'Medicine',\n",
       "   'Clinical Practice'],\n",
       "  'references': ['1967300023',\n",
       "   '2131419242',\n",
       "   '2143432233',\n",
       "   '2117958746',\n",
       "   '1531106656',\n",
       "   '2157775267',\n",
       "   '2028701043',\n",
       "   '2111704803',\n",
       "   '2135163018',\n",
       "   '2042074736']},\n",
       " {'id': '2104548316',\n",
       "  'title': 'A novel coronavirus associated with severe acute respiratory syndrome.',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '5,221',\n",
       "  'abstract': 'background A worldwide outbreak of severe acute respiratory syndrome (SARS) has been associated with exposures originating from a single ill health care worker from Guangdong Province, China. We conducted studies to identify the etiologic agent of this outbreak. methods We received clinical specimens from patients in six countries and tested them, using virus isolation techniques, electron-microscopical and histologic studies, and molecular and serologic assays, in an attempt to identify a wide range of potential pathogens. results No classic respiratory or bacterial respiratory pathogen was consistently identified. However, a novel coronavirus was isolated from patients who met the case definition of SARS. Cytopathological features were noted microscopically in Vero E6 cells inoculated with a throat-swab specimen. Electron-microscopical examination of cultures revealed ultrastructural features characteristic of coronaviruses. Immunohistochemical and immunofluorescence staining revealed reactivity with group I coronavirus polyclonal antibodies. Consensus coronavirus primers designed to amplify a fragment of the polymerase gene by reverse transcription–polymerase chain reaction (RT-PCR) were used to obtain a sequence that clearly identified the isolate as a unique coronavirus only distantly related to previously sequenced coronaviruses. With specific diagnostic RT-PCR primers we identified several identical nucleotide sequences in 12 patients from several locations, a finding consistent with a point source outbreak. Indirect fluorescent antibody tests and enzyme-linked immunosorbent assays made with the new coronavirus isolate have been used to demonstrate a virus-specific serologic response. Preliminary studies suggest that this virus may never before have infected the U.S. population. conclusions A novel coronavirus is associated with this outbreak, and the evidence indicates that this virus has an etiologic role in SARS. The name Urbani SARS-associated coronavirus is proposed for the virus.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Ksiazek Tg 1, Erdman D 1, Goldsmith Cs 1, Zaki 1, Peret T 1, Emery S 1, Tong S 1, Urbani C 2, Comer Ja 1, Lim W 3, Rollin Pe 1, Dowell Sf 4, Ling Ae 5, Humphrey Cd 1, Shieh Wj 1, Guarner J 1, Paddock Cd 1, Rota P 1, Fields B 1, DeRisi J 6, Yang Jy 1, Cox N 1, Hughes Jm 1, LeDuc Jw 1, Bellini Wj 1, Anderson Lj 1'],\n",
       "  'related_topics': ['Human coronavirus OC43',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus 229E'],\n",
       "  'references': ['2106882534',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2463755683',\n",
       "   '2398786667',\n",
       "   '2127949919',\n",
       "   '1576737979',\n",
       "   '2128788856',\n",
       "   '2076620790']},\n",
       " {'id': '2131262274',\n",
       "  'title': 'A Major Outbreak of Severe Acute Respiratory Syndrome in Hong Kong',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '2,974',\n",
       "  'abstract': 'background There has been an outbreak of the severe acute respiratory syndrome (SARS) worldwide. We report the clinical, laboratory, and radiologic features of 138 cases of suspected SARS during a hospital outbreak in Hong Kong. methods From March 11 to 25, 2003, all patients with suspected SARS after exposure to an index patient or ward were admitted to the isolation wards of the Prince of Wales Hospital. Their demographic, clinical, laboratory, and radiologic characteristics were analyzed. Clinical end points included the need for intensive care and death. Univariate and multivariate analyses were performed. results There were 66 male patients and 72 female patients in this cohort, 69 of whom were health care workers. The most common symptoms included fever (in 100 percent of the patients); chills, rigors, or both (73.2 percent); and myalgia (60.9 percent). Cough and headache were also reported in more than 50 percent of the patients. Other common findings were lymphopenia (in 69.6 percent), thrombocytopenia (44.8 percent), and elevated lactate dehydrogenase and creatine kinase levels (71.0 percent and 32.1 percent, respectively). Peripheral air-space consolidation was commonly observed on thoracic computed tomographic scanning. A total of 32 patients (23.2 percent) were admitted to the intensive care unit; 5 patients died, all of whom had coexisting conditions. In a multivariate analysis, the independent predictors of an adverse outcome were advanced age (odds ratio per decade of life, 1.80; 95 percent confidence interval, 1.16 to 2.81; P=0.009), a high peak lactate dehydrogenase level (odds ratio per 100 U per liter, 2.09; 95 percent confidence interval, 1.28 to 3.42; P=0.003), and an absolute neutrophil count that exceeded the upper limit of the normal range on presentation (odds ratio, 1.60; 95 percent confidence interval, 1.03 to 2.50; P=0.04). conclusions SARS is a serious respiratory illness that led to significant morbidity and mortality in our cohort.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Nelson Lee 1, David Hui 1, Alan Wu 1, Paul Chan 1, Peter Cameron 2, Gavin M Joynt 1, Anil Ahuja 1, Man Yee Yung 1, C B Leung 1, K F To 1, S F Lui 1, C C Szeto 1, Sydney Chung 1, Joseph J Y Sung 1'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Odds ratio',\n",
       "   'Intensive care unit',\n",
       "   'Chills',\n",
       "   'Confidence interval',\n",
       "   'Cohort',\n",
       "   'myalgia',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Surgery'],\n",
       "  'references': ['2123324969',\n",
       "   '2130141864',\n",
       "   '2463755683',\n",
       "   '1991467275',\n",
       "   '1982444609']},\n",
       " {'id': '2006434809',\n",
       "  'title': 'Epidemiological, demographic, and clinical characteristics of 47 cases of Middle East respiratory syndrome coronavirus disease from Saudi Arabia: a descriptive study',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '1,320',\n",
       "  'abstract': 'Summary Background Middle East respiratory syndrome (MERS) is a new human disease caused by a novel coronavirus (CoV). Clinical data on MERS-CoV infections are scarce. We report epidemiological, demographic, clinical, and laboratory characteristics of 47 cases of MERS-CoV infections, identify knowledge gaps, and define research priorities. Methods We abstracted and analysed epidemiological, demographic, clinical, and laboratory data from confirmed cases of sporadic, household, community, and health-care-associated MERS-CoV infections reported from Saudi Arabia between Sept 1, 2012, and June 15, 2013. Cases were confirmed as having MERS-CoV by real-time RT-PCR. Findings 47 individuals (46 adults, one child) with laboratory-confirmed MERS-CoV disease were identified; 36 (77%) were male (male:female ratio 3·3:1). 28 patients died, a 60% case-fatality rate. The case-fatality rate rose with increasing age. Only two of the 47 cases were previously healthy; most patients (45 [96%]) had underlying comorbid medical disorders, including diabetes (32 [68%]), hypertension (16 [34%]), chronic cardiac disease (13 [28%]), and chronic renal disease (23 [49%]). Common symptoms at presentation were fever (46 [98%]), fever with chills or rigors (41 [87%]), cough (39 [83%]), shortness of breath (34 [72%]), and myalgia (15 [32%]). Gastrointestinal symptoms were also frequent, including diarrhoea (12 [26%]), vomiting (ten [21%]), and abdominal pain (eight [17%]). All patients had abnormal findings on chest radiography, ranging from subtle to extensive unilateral and bilateral abnormalities. Laboratory analyses showed raised concentrations of lactate dehydrogenase (23 [49%]) and aspartate aminotransferase (seven [15%]) and thrombocytopenia (17 [36%]) and lymphopenia (16 [34%]). Interpretation Disease caused by MERS-CoV presents with a wide range of clinical manifestations and is associated with substantial mortality in admitted patients who have medical comorbidities. Major gaps in our knowledge of the epidemiology, community prevalence, and clinical spectrum of infection and disease need urgent definition. Funding None.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Abdullah Assiri 1, Jaffar A Al-Tawfiq 2, Abdullah A Al-Rabeeah 1, Fahad A Al-Rabiah 3, Sami Al-Hajjar 3, Ali Al-Barrak 4, Hesham Flemban 5, Wafa N Al-Nassir 6, Hanan H Balkhy 7, Rafat F Al-Hakeem 1, Hatem Q Makhdoom 8, Alimuddin I Zumla 9, 10, Ziad A Memish 11'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Epidemiology',\n",
       "   'myalgia'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2131262274',\n",
       "   '1703839189',\n",
       "   '2112147913',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2163627712',\n",
       "   '2140143765',\n",
       "   '2119775949']},\n",
       " {'id': '3017468735',\n",
       "  'title': 'A Novel Coronavirus Genome Identified in a Cluster of Pneumonia Cases — Wuhan, China 2019−2020',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '377',\n",
       "  'abstract': '',\n",
       "  'date': 2019,\n",
       "  'authors': ['Wenjie Tan',\n",
       "   'Xiang Zhao',\n",
       "   'Xuejun Ma',\n",
       "   'Wenling Wang',\n",
       "   'Peihua Niu',\n",
       "   'Wenbo Xu',\n",
       "   'George F. Gao',\n",
       "   'Guizhen Wu'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Disease cluster',\n",
       "   'Genome',\n",
       "   'Medicine',\n",
       "   'Virology'],\n",
       "  'references': ['3001118548',\n",
       "   '3004318991',\n",
       "   '3010819577',\n",
       "   '3007273493',\n",
       "   '3007814559',\n",
       "   '3002764620',\n",
       "   '3006354146',\n",
       "   '3005655936',\n",
       "   '3015190630',\n",
       "   '3009739970']},\n",
       " {'id': '2725497285',\n",
       "  'title': 'Broad-spectrum antiviral GS-5734 inhibits both epidemic and zoonotic coronaviruses.',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '1,095',\n",
       "  'abstract': 'Emerging viral infections are difficult to control because heterogeneous members periodically cycle in and out of humans and zoonotic hosts, complicating the development of specific antiviral therapies and vaccines. Coronaviruses (CoVs) have a proclivity to spread rapidly into new host species causing severe disease. Severe acute respiratory syndrome CoV (SARS-CoV) and Middle East respiratory syndrome CoV (MERS-CoV) successively emerged, causing severe epidemic respiratory disease in immunologically naive human populations throughout the globe. Broad-spectrum therapies capable of inhibiting CoV infections would address an immediate unmet medical need and could be invaluable in the treatment of emerging and endemic CoV infections. We show that a nucleotide prodrug, GS-5734, currently in clinical development for treatment of Ebola virus disease, can inhibit SARS-CoV and MERS-CoV replication in multiple in vitro systems, including primary human airway epithelial cell cultures with submicromolar IC50 values. GS-5734 was also effective against bat CoVs, prepandemic bat CoVs, and circulating contemporary human CoV in primary human lung cells, thus demonstrating broad-spectrum anti-CoV activity. In a mouse model of SARS-CoV pathogenesis, prophylactic and early therapeutic administration of GS-5734 significantly reduced lung viral load and improved clinical signs of disease as well as respiratory function. These data provide substantive evidence that GS-5734 may prove effective against endemic MERS-CoV in the Middle East, circulating human CoV, and, possibly most importantly, emerging CoV of the future.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Timothy P. Sheahan 1, Amy C. Sims 1, Rachel L. Graham 1, Vineet D. Menachery 1, Lisa E. Gralinski 1, James B. Case 2, Sarah R. Leist 1, Krzysztof Pyrc 3, Joy Y. Feng 4, Iva Trantcheva 4, Roy Bannister 4, Yeojin Park 4, Darius Babusis 4, Michael O. Clarke 4, Richard L. Mackman 4, Jamie E. Spahn 4, Christopher A. Palmiotti 4, Dustin Siegel 4, Adrian S. Ray 4, Tomas Cihlar 4, Robert Jordan 4, Mark R. Denison 5, Ralph S. Baric 1'],\n",
       "  'related_topics': ['Respiratory function',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus'],\n",
       "  'references': ['2470646526',\n",
       "   '2129542667',\n",
       "   '2195009776',\n",
       "   '2255243349',\n",
       "   '2292021561',\n",
       "   '2115555188',\n",
       "   '2298153446',\n",
       "   '2525468044',\n",
       "   '1945961678',\n",
       "   '2099941783']},\n",
       " {'id': '1901129140',\n",
       "  'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '26,984',\n",
       "  'abstract': 'There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .',\n",
       "  'date': 2015,\n",
       "  'authors': ['Olaf Ronneberger', 'Philipp Fischer', 'Thomas Brox'],\n",
       "  'related_topics': ['Brain segmentation',\n",
       "   'Deep learning',\n",
       "   'Segmentation',\n",
       "   'Image processing',\n",
       "   'Context (language use)',\n",
       "   'Artificial neural network',\n",
       "   'Image translation',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '1903029394',\n",
       "   '2155893237',\n",
       "   '1677182931',\n",
       "   '1948751323',\n",
       "   '2167510172',\n",
       "   '1893585201',\n",
       "   '2148349024',\n",
       "   '2147800946']},\n",
       " {'id': '3106250896',\n",
       "  'title': 'SSD: Single Shot MultiBox Detector',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '14,661',\n",
       "  'abstract': 'We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\\\(300 \\\\times 300\\\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\\\(512 \\\\times 512\\\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Wei Liu 1, Dragomir Anguelov 2, Dumitru Erhan 3, Christian Szegedy 3, Scott E. Reed 4, Cheng-Yang Fu 1, Alexander C. Berg 1'],\n",
       "  'related_topics': ['Minimum bounding box',\n",
       "   'Convolutional neural network',\n",
       "   'Pixel',\n",
       "   'Algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Image resolution',\n",
       "   'Pascal (programming language)',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237']},\n",
       " {'id': '3001897055',\n",
       "  'title': 'A Novel Coronavirus from Patients with Pneumonia in China, 2019.',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '15,426',\n",
       "  'abstract': 'In December 2019, a cluster of patients with pneumonia of unknown cause was linked to a seafood wholesale market in Wuhan, China. A previously unknown betacoronavirus was discovered through the use of unbiased sequencing in samples from patients with pneumonia. Human airway epithelial cells were used to isolate a novel coronavirus, named 2019-nCoV, which formed a clade within the subgenus sarbecovirus, Orthocoronavirinae subfamily. Different from both MERS-CoV and SARS-CoV, 2019-nCoV is the seventh member of the family of coronaviruses that infect humans. Enhanced surveillance and further investigation are ongoing. (Funded by the National Key Research and Development Program of China and the National Major Project for Control and Prevention of Infectious Disease in China.).',\n",
       "  'date': 2020,\n",
       "  'authors': ['Na Zhu 1, Dingyu Zhang 2, Wenling Wang 1, Xingwang Li 3, Bo Yang 1, Jingdong Song 1, Xiang Zhao 1, Baoying Huang 1, Weifeng Shi 4, Roujian Lu 1, Peihua Niu 1, Faxian Zhan 1, Xuejun Ma 1, 5, Dayan Wang 1, Wenbo Xu 6, Guizhen Wu 1, George F. Gao 7, Wenjie Tan 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Infectious disease (medical specialty)'],\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2306794997',\n",
       "   '1909499787',\n",
       "   '3027518954',\n",
       "   '2792024998',\n",
       "   '2955025503',\n",
       "   '2257005270']},\n",
       " {'id': '3002108456',\n",
       "  'title': 'Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan, China: a descriptive study',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '13,827',\n",
       "  'abstract': 'In December, 2019, a pneumonia associated with the 2019 novel coronavirus (2019-nCoV) emerged in Wuhan, China. We aimed to further clarify the epidemiological and clinical characteristics of 2019-nCoV pneumonia. In this retrospective, single-centre study, we included all confirmed cases of 2019-nCoV in Wuhan Jinyintan Hospital from Jan 1 to Jan 20, 2020. Cases were confirmed by real-time RT-PCR and were analysed for epidemiological, demographic, clinical, and radiological features and laboratory data. Outcomes were followed up until Jan 25, 2020.',\n",
       "  'date': 2020,\n",
       "  'authors': [\"Nanshan Chen 1, Min Zhou 2, Xuan Dong 1, Jieming Qu 2, Fengyun Gong 1, Yang Han 1, Yang Qiu 3, Jingli Wang 1, Ying Liu 1, Yuan Wei 1, Jia'an Xia 1, Ting Yu 1, Xinxin Zhang 2, Li Zhang 1\"],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Epidemiology',\n",
       "   'Coronavirus',\n",
       "   'Retrospective cohort study',\n",
       "   'Comorbidity',\n",
       "   'Public health',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed'],\n",
       "  'references': ['3001118548',\n",
       "   '2903899730',\n",
       "   '2166867592',\n",
       "   '2999409984',\n",
       "   '2999318660',\n",
       "   '2132260239',\n",
       "   '2999364275',\n",
       "   '2909194930',\n",
       "   '2991899552',\n",
       "   '2775086803']},\n",
       " {'id': '3005079553',\n",
       "  'title': 'Clinical Characteristics of 138 Hospitalized Patients With 2019 Novel Coronavirus-Infected Pneumonia in Wuhan, China.',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '15,476',\n",
       "  'abstract': 'Importance In December 2019, novel coronavirus (2019-nCoV)–infected pneumonia (NCIP) occurred in Wuhan, China. The number of cases has increased rapidly but information on the clinical characteristics of affected patients is limited. Objective To describe the epidemiological and clinical characteristics of NCIP. Design, Setting, and Participants Retrospective, single-center case series of the 138 consecutive hospitalized patients with confirmed NCIP at Zhongnan Hospital of Wuhan University in Wuhan, China, from January 1 to January 28, 2020; final date of follow-up was February 3, 2020. Exposures Documented NCIP. Main Outcomes and Measures Epidemiological, demographic, clinical, laboratory, radiological, and treatment data were collected and analyzed. Outcomes of critically ill patients and noncritically ill patients were compared. Presumed hospital-related transmission was suspected if a cluster of health professionals or hospitalized patients in the same wards became infected and a possible source of infection could be tracked. Results Of 138 hospitalized patients with NCIP, the median age was 56 years (interquartile range, 42-68; range, 22-92 years) and 75 (54.3%) were men. Hospital-associated transmission was suspected as the presumed mechanism of infection for affected health professionals (40 [29%]) and hospitalized patients (17 [12.3%]). Common symptoms included fever (136 [98.6%]), fatigue (96 [69.6%]), and dry cough (82 [59.4%]). Lymphopenia (lymphocyte count, 0.8\\u2009×\\u2009109/L [interquartile range {IQR}, 0.6-1.1]) occurred in 97 patients (70.3%), prolonged prothrombin time (13.0 seconds [IQR, 12.3-13.7]) in 80 patients (58%), and elevated lactate dehydrogenase (261 U/L [IQR, 182-403]) in 55 patients (39.9%). Chest computed tomographic scans showed bilateral patchy shadows or ground glass opacity in the lungs of all patients. Most patients received antiviral therapy (oseltamivir, 124 [89.9%]), and many received antibacterial therapy (moxifloxacin, 89 [64.4%]; ceftriaxone, 34 [24.6%]; azithromycin, 25 [18.1%]) and glucocorticoid therapy (62 [44.9%]). Thirty-six patients (26.1%) were transferred to the intensive care unit (ICU) because of complications, including acute respiratory distress syndrome (22 [61.1%]), arrhythmia (16 [44.4%]), and shock (11 [30.6%]). The median time from first symptom to dyspnea was 5.0 days, to hospital admission was 7.0 days, and to ARDS was 8.0 days. Patients treated in the ICU (n\\u2009=\\u200936), compared with patients not treated in the ICU (n\\u2009=\\u2009102), were older (median age, 66 years vs 51 years), were more likely to have underlying comorbidities (26 [72.2%] vs 38 [37.3%]), and were more likely to have dyspnea (23 [63.9%] vs 20 [19.6%]), and anorexia (24 [66.7%] vs 31 [30.4%]). Of the 36 cases in the ICU, 4 (11.1%) received high-flow oxygen therapy, 15 (41.7%) received noninvasive ventilation, and 17 (47.2%) received invasive ventilation (4 were switched to extracorporeal membrane oxygenation). As of February 3, 47 patients (34.1%) were discharged and 6 died (overall mortality, 4.3%), but the remaining patients are still hospitalized. Among those discharged alive (n\\u2009=\\u200947), the median hospital stay was 10 days (IQR, 7.0-14.0). Conclusions and Relevance In this single-center case series of 138 hospitalized patients with confirmed NCIP in Wuhan, China, presumed hospital-related transmission of 2019-nCoV was suspected in 41% of patients, 26% of patients received ICU care, and mortality was 4.3%.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Dawei Wang',\n",
       "   'Bo Hu',\n",
       "   'Chang Hu',\n",
       "   'Fangfang Zhu',\n",
       "   'Xing Liu',\n",
       "   'Jing Zhang',\n",
       "   'Binbin Wang',\n",
       "   'Hui Xiang',\n",
       "   'Zhenshun Cheng',\n",
       "   'Yong Xiong',\n",
       "   'Yan Zhao',\n",
       "   'Yirong Li',\n",
       "   'Xinghuan Wang',\n",
       "   'Zhiyong Peng'],\n",
       "  'related_topics': ['Interquartile range',\n",
       "   'Intensive care unit',\n",
       "   'Pneumonia'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3000834295',\n",
       "   '3003951199',\n",
       "   '2999409984',\n",
       "   '2999318660',\n",
       "   '1803784511']},\n",
       " {'id': '3003668884',\n",
       "  'title': 'Early Transmission Dynamics in Wuhan, China, of Novel Coronavirus-Infected Pneumonia.',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '11,868',\n",
       "  'abstract': 'Abstract Background The initial cases of novel coronavirus (2019-nCoV)–infected pneumonia (NCIP) occurred in Wuhan, Hubei Province, China, in December 2019 and January 2020. We analyzed data on the...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Qun Li 1, Xuhua Guan 1, Peng Wu 2, Xiaoye Wang 1, Lei Zhou 1, Yeqing Tong 1, Ruiqi Ren 1, Kathy S.M. Leung 2, Eric H.Y. Lau 2, Jessica Y. Wong 2, Xuesen Xing 1, Nijuan Xiang 1, Yang Wu 1, Chao Li 1, Qi Chen 1, Dan Li 1, Tian Liu 1, Jing Zhao 1, Man Liu 1, Wenxiao Tu 1, Chuding Chen 1, Lianmei Jin 1, Rui Yang 1, Qi Wang 1, Suhua Zhou 1, Rui Wang 1, Hui Liu 1, Yingbo Luo 1, Yuan Liu 1, Ge Shao 1, Huan Li 1, Zhongfa Tao 1, Yang Yang 3, Zhiqiang Deng 3, Boxi Liu 3, Zhitao Ma 3, Yanping Zhang 1, Guoqing Shi 1, Tommy T.Y. Lam 2, Joseph T. Wu 2, George F. Gao 1, Benjamin J. Cowling 2, Bo Yang 3, Gabriel M. Leung 2, Zijian Feng 1'],\n",
       "  'related_topics': ['Coronavirus', 'Pneumonia', 'Betacoronavirus'],\n",
       "  'references': ['3001897055',\n",
       "   '3002539152',\n",
       "   '3000834295',\n",
       "   '3002533507',\n",
       "   '2470646526',\n",
       "   '3002715510',\n",
       "   '1909499787',\n",
       "   '3001971765',\n",
       "   '2147166346',\n",
       "   '2149508011']},\n",
       " {'id': '3002539152',\n",
       "  'title': 'A familial cluster of pneumonia associated with the 2019 novel coronavirus indicating person-to-person transmission: a study of a family cluster.',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '6,982',\n",
       "  'abstract': \"Summary Background An ongoing outbreak of pneumonia associated with a novel coronavirus was reported in Wuhan city, Hubei province, China. Affected patients were geographically linked with a local wet market as a potential source. No data on person-to-person or nosocomial transmission have been published to date. Methods In this study, we report the epidemiological, clinical, laboratory, radiological, and microbiological findings of five patients in a family cluster who presented with unexplained pneumonia after returning to Shenzhen, Guangdong province, China, after a visit to Wuhan, and an additional family member who did not travel to Wuhan. Phylogenetic analysis of genetic sequences from these patients were done. Findings From Jan 10, 2020, we enrolled a family of six patients who travelled to Wuhan from Shenzhen between Dec 29, 2019 and Jan 4, 2020. Of six family members who travelled to Wuhan, five were identified as infected with the novel coronavirus. Additionally, one family member, who did not travel to Wuhan, became infected with the virus after several days of contact with four of the family members. None of the family members had contacts with Wuhan markets or animals, although two had visited a Wuhan hospital. Five family members (aged 36–66 years) presented with fever, upper or lower respiratory tract symptoms, or diarrhoea, or a combination of these 3–6 days after exposure. They presented to our hospital (The University of Hong Kong-Shenzhen Hospital, Shenzhen) 6–10 days after symptom onset. They and one asymptomatic child (aged 10 years) had radiological ground-glass lung opacities. Older patients (aged >60 years) had more systemic symptoms, extensive radiological ground-glass lung changes, lymphopenia, thrombocytopenia, and increased C-reactive protein and lactate dehydrogenase levels. The nasopharyngeal or throat swabs of these six patients were negative for known respiratory microbes by point-of-care multiplex RT-PCR, but five patients (four adults and the child) were RT-PCR positive for genes encoding the internal RNA-dependent RNA polymerase and surface Spike protein of this novel coronavirus, which were confirmed by Sanger sequencing. Phylogenetic analysis of these five patients' RT-PCR amplicons and two full genomes by next-generation sequencing showed that this is a novel coronavirus, which is closest to the bat severe acute respiatory syndrome (SARS)-related coronaviruses found in Chinese horseshoe bats. Interpretation Our findings are consistent with person-to-person transmission of this novel coronavirus in hospital and family settings, and the reports of infected travellers in other geographical regions. Funding The Shaw Foundation Hong Kong, Michael Seak-Kan Tong, Respiratory Viral Research Foundation Limited, Hui Ming, Hui Hoy and Chow Sin Lan Charity Fund Limited, Marina Man-Wai Lee, the Hong Kong Hainan Commercial Association South China Microbiology Research Fund, Sanming Project of Medicine (Shenzhen), and High Level-Hospital Program (Guangdong Health Commission).\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Jasper Fuk Woo Chan 1, Shuofeng Yuan 1, Kin Hang Kok 1, Kelvin Kai Wang To 1, 2, Hin Chu 1, Jin Yang 2, Fanfan Xing 2, Jieling Liu 2, Cyril Chik Yan Yip 1, Rosana Wing Shan Poon 1, Hoi Wah Tsoi 1, Simon Kam Fai Lo 2, Kwok Hung Chan 1, Vincent Kwok Man Poon 1, Wan Mui Chan 1, Jonathan Daniel Ip 1, Jian Piao Cai 1, Vincent Chi Chung Cheng 1, Honglin Chen 1, 2, Christopher Kim Ming Hui 2, Kwok Yung Yuen 2'],\n",
       "  'related_topics': ['Coronavirus', 'Betacoronavirus', 'Epidemiology'],\n",
       "  'references': ['2025170735',\n",
       "   '2129542667',\n",
       "   '2103503670',\n",
       "   '2115555188',\n",
       "   '2807736175',\n",
       "   '2105637133',\n",
       "   '2889758689',\n",
       "   '2769543984',\n",
       "   '2140338292',\n",
       "   '2170933940']},\n",
       " {'id': '3004318991',\n",
       "  'title': 'Genomic characterisation and epidemiology of 2019 novel coronavirus: implications for virus origins and receptor binding.',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '7,834',\n",
       "  'abstract': 'Summary Background In late December, 2019, patients presenting with viral pneumonia due to an unidentified microbial agent were reported in Wuhan, China. A novel coronavirus was subsequently identified as the causative pathogen, provisionally named 2019 novel coronavirus (2019-nCoV). As of Jan 26, 2020, more than 2000 cases of 2019-nCoV infection have been confirmed, most of which involved people living in or visiting Wuhan, and human-to-human transmission has been confirmed. Methods We did next-generation sequencing of samples from bronchoalveolar lavage fluid and cultured isolates from nine inpatients, eight of whom had visited the Huanan seafood market in Wuhan. Complete and partial 2019-nCoV genome sequences were obtained from these individuals. Viral contigs were connected using Sanger sequencing to obtain the full-length genomes, with the terminal regions determined by rapid amplification of cDNA ends. Phylogenetic analysis of these 2019-nCoV genomes and those of other coronaviruses was used to determine the evolutionary history of the virus and help infer its likely origin. Homology modelling was done to explore the likely receptor-binding properties of the virus. Findings The ten genome sequences of 2019-nCoV obtained from the nine patients were extremely similar, exhibiting more than 99·98% sequence identity. Notably, 2019-nCoV was closely related (with 88% identity) to two bat-derived severe acute respiratory syndrome (SARS)-like coronaviruses, bat-SL-CoVZC45 and bat-SL-CoVZXC21, collected in 2018 in Zhoushan, eastern China, but were more distant from SARS-CoV (about 79%) and MERS-CoV (about 50%). Phylogenetic analysis revealed that 2019-nCoV fell within the subgenus Sarbecovirus of the genus Betacoronavirus, with a relatively long branch length to its closest relatives bat-SL-CoVZC45 and bat-SL-CoVZXC21, and was genetically distinct from SARS-CoV. Notably, homology modelling revealed that 2019-nCoV had a similar receptor-binding domain structure to that of SARS-CoV, despite amino acid variation at some key residues. Interpretation 2019-nCoV is sufficiently divergent from SARS-CoV to be considered a new human-infecting betacoronavirus. Although our phylogenetic analysis suggests that bats might be the original host of this virus, an animal sold at the seafood market in Wuhan might represent an intermediate host facilitating the emergence of the virus in humans. Importantly, structural analysis suggests that 2019-nCoV might be able to bind to the angiotensin-converting enzyme 2 receptor in humans. The future evolution, adaptation, and spread of this virus warrant urgent investigation. Funding National Key Research and Development Program of China, National Major Project for Control and Prevention of Infectious Disease in China, Chinese Academy of Sciences, Shandong First Medical University.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Roujian Lu 1, Xiang Zhao 1, Juan Li 2, Peihua Niu 1, Bo Yang 3, Honglong Wu 4, Wenling Wang 1, Hao Song 5, Baoying Huang 1, Na Zhu 1, Yuhai Bi 5, Xuejun Ma 1, Faxian Zhan 3, Liang Wang 5, Tao Hu 2, Hong Zhou 2, Zhenhong Hu 6, Weimin Zhou 1, Li Zhao 1, Jing Chen 7, Yao Meng 1, Ji Wang 1, Yang Lin 4, Jianying Yuan 4, Zhihao Xie 4, Jinmin Ma 4, William J Liu 1, Dayan Wang 1, Wenbo Xu 1, Edward C Holmes 8, George F Gao 1, 5, Guizhen Wu 1, Weijun Chen 4, Weifeng Shi 2, Wenjie Tan 1, 5'],\n",
       "  'related_topics': ['Betacoronavirus', 'Coronavirus', 'Phylogenetics'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '2103441770',\n",
       "   '2141052558',\n",
       "   '2166867592',\n",
       "   '2306794997',\n",
       "   '3017468735',\n",
       "   '2804822363']},\n",
       " {'id': '3003465021',\n",
       "  'title': 'First Case of 2019 Novel Coronavirus in the United States.',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '4,801',\n",
       "  'abstract': \"An outbreak of novel coronavirus (2019-nCoV) that began in Wuhan, China, has spread rapidly, with cases now confirmed in multiple countries. We report the first case of 2019-nCoV infection confirmed in the United States and describe the identification, diagnosis, clinical course, and management of the case, including the patient's initial mild symptoms at presentation with progression to pneumonia on day 9 of illness. This case highlights the importance of close coordination between clinicians and public health authorities at the local, state, and federal levels, as well as the need for rapid dissemination of clinical information related to the care of patients with this emerging infection.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Michelle L Holshue 1, Chas DeBolt 2, Scott Lindquist',\n",
       "   'Kathy H Lofy',\n",
       "   'John Wiesman',\n",
       "   'Hollianne Bruce',\n",
       "   'Christopher Spitters',\n",
       "   'Keith Ericson',\n",
       "   'Sara Wilkerson',\n",
       "   'Ahmet Tural',\n",
       "   'George Diaz',\n",
       "   'Amanda Cohn',\n",
       "   'LeAnne Fox',\n",
       "   'Anita Patel',\n",
       "   'Susan I Gerber',\n",
       "   'Lindsay Kim',\n",
       "   'Suxiang Tong',\n",
       "   'Xiaoyan Lu',\n",
       "   'Steve Lindstrom',\n",
       "   'Mark A Pallansch',\n",
       "   'William C Weldon',\n",
       "   'Holly M Biggs',\n",
       "   'Timothy M Uyeki',\n",
       "   'Satish K Pillai'],\n",
       "  'related_topics': ['Public health',\n",
       "   'Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Outbreak',\n",
       "   'Intensive care medicine',\n",
       "   'MEDLINE',\n",
       "   'Medicine',\n",
       "   'Clinical course',\n",
       "   'Clinical information',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002539152',\n",
       "   '3003951199',\n",
       "   '3000413850',\n",
       "   '2991491848',\n",
       "   '2605343262']},\n",
       " {'id': '3004239190',\n",
       "  'title': 'Transmission of 2019-nCoV Infection from an Asymptomatic Contact in Germany.',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '3,539',\n",
       "  'abstract': '2019-nCoV Transmission from Asymptomatic Patient In this report, investigators in Germany detected the spread of the novel coronavirus (2019-nCoV) from a person who had recently traveled from China...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Camilla Rothe 1, Mirjam Schunk 1, Peter Sothmann 1, Gisela Bretzel 1, Guenter Froeschl 1, Claudia Wallrauch 1, Thorbjörn Zimmer 1, Verena Thiel 1, Christian Janke 1, Wolfgang Guggemos 2, Michael Seilmaier 2, Christian Drosten 3, Patrick Vollmar 4, Katrin Zwirglmaier 4, Sabine Zange 4, Roman Wölfel 4, Michael Hoelscher 1'],\n",
       "  'related_topics': ['Asymptomatic Diseases',\n",
       "   'Asymptomatic',\n",
       "   'Transmission (mechanics)',\n",
       "   'Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Pneumonia',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001897055', '3001388158']},\n",
       " {'id': '3003573988',\n",
       "  'title': 'Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study.',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '3,286',\n",
       "  'abstract': 'Summary Background Since Dec 31, 2019, the Chinese city of Wuhan has reported an outbreak of atypical pneumonia caused by the 2019 novel coronavirus (2019-nCoV). Cases have been exported to other Chinese cities, as well as internationally, threatening to trigger a global outbreak. Here, we provide an estimate of the size of the epidemic in Wuhan on the basis of the number of cases exported from Wuhan to cities outside mainland China and forecast the extent of the domestic and global public health risks of epidemics, accounting for social and non-pharmaceutical prevention interventions. Methods We used data from Dec 31, 2019, to Jan 28, 2020, on the number of cases exported from Wuhan internationally (known days of symptom onset from Dec 25, 2019, to Jan 19, 2020) to infer the number of infections in Wuhan from Dec 1, 2019, to Jan 25, 2020. Cases exported domestically were then estimated. We forecasted the national and global spread of 2019-nCoV, accounting for the effect of the metropolitan-wide quarantine of Wuhan and surrounding cities, which began Jan 23–24, 2020. We used data on monthly flight bookings from the Official Aviation Guide and data on human mobility across more than 300 prefecture-level cities in mainland China from the Tencent database. Data on confirmed cases were obtained from the reports published by the Chinese Center for Disease Control and Prevention. Serial interval estimates were based on previous studies of severe acute respiratory syndrome coronavirus (SARS-CoV). A susceptible-exposed-infectious-recovered metapopulation model was used to simulate the epidemics across all major cities in China. The basic reproductive number was estimated using Markov Chain Monte Carlo methods and presented using the resulting posterior mean and 95% credibile interval (CrI). Findings In our baseline scenario, we estimated that the basic reproductive number for 2019-nCoV was 2·68 (95% CrI 2·47–2·86) and that 75\\u2008815 individuals (95% CrI 37\\u2008304–130\\u2008330) have been infected in Wuhan as of Jan 25, 2020. The epidemic doubling time was 6·4 days (95% CrI 5·8–7·1). We estimated that in the baseline scenario, Chongqing, Beijing, Shanghai, Guangzhou, and Shenzhen had imported 461 (95% CrI 227–805), 113 (57–193), 98 (49–168), 111 (56–191), and 80 (40–139) infections from Wuhan, respectively. If the transmissibility of 2019-nCoV were similar everywhere domestically and over time, we inferred that epidemics are already growing exponentially in multiple major cities of China with a lag time behind the Wuhan outbreak of about 1–2 weeks. Interpretation Given that 2019-nCoV is no longer contained within Wuhan, other major Chinese cities are probably sustaining localised outbreaks. Large cities overseas with close transport links to China could also become outbreak epicentres, unless substantial public health interventions at both the population and personal levels are implemented immediately. Independent self-sustaining outbreaks in major cities globally could become inevitable because of substantial exportation of presymptomatic cases and in the absence of large-scale public health interventions. Preparedness plans and mitigation interventions should be readied for quick deployment globally. Funding Health and Medical Research Fund (Hong Kong, China).',\n",
       "  'date': 2020,\n",
       "  'authors': ['Joseph T Wu', 'Kathy Leung', 'Gabriel M Leung'],\n",
       "  'related_topics': ['Population',\n",
       "   'Mainland China',\n",
       "   'China',\n",
       "   'Beijing',\n",
       "   'Outbreak',\n",
       "   'Serial interval',\n",
       "   'Public health',\n",
       "   'Preparedness',\n",
       "   'Socioeconomics',\n",
       "   'Geography'],\n",
       "  'references': ['3003668884',\n",
       "   '3004397688',\n",
       "   '3002764620',\n",
       "   '2147166346',\n",
       "   '3002533591',\n",
       "   '1815575713',\n",
       "   '2069251911',\n",
       "   '2096145431',\n",
       "   '2104595316',\n",
       "   '1968393246']},\n",
       " {'id': '2163922914',\n",
       "  'title': 'Representation Learning: A Review and New Perspectives',\n",
       "  'reference_count': '237',\n",
       "  'citation_count': '9,127',\n",
       "  'abstract': 'The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Y. Bengio', 'A. Courville', 'P. Vincent'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Active learning (machine learning)'],\n",
       "  'references': ['2618530766',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2158899491',\n",
       "   '2187089797',\n",
       "   '1665214252',\n",
       "   '2162915993',\n",
       "   '2160815625']},\n",
       " {'id': '2160815625',\n",
       "  'title': 'Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups',\n",
       "  'reference_count': '63',\n",
       "  'citation_count': '10,041',\n",
       "  'abstract': 'Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.',\n",
       "  'date': 2012,\n",
       "  'authors': ['G. Hinton 1, Li Deng 2, Dong Yu 2, G. E. Dahl 1, A. Mohamed 1, N. Jaitly 1, Andrew Senior 3, V. Vanhoucke 3, P. Nguyen 3, T. N. Sainath 4, B. Kingsbury 4'],\n",
       "  'related_topics': ['Acoustic model',\n",
       "   'Time delay neural network',\n",
       "   'FMLLR',\n",
       "   'Hidden Markov model',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Speech recognition',\n",
       "   'Margin (machine learning)',\n",
       "   'Frame (networking)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '1533861849',\n",
       "   '2116064496',\n",
       "   '2145094598',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '44815768',\n",
       "   '1498436455',\n",
       "   '1994197834']},\n",
       " {'id': '2022508996',\n",
       "  'title': 'Learning Hierarchical Features for Scene Labeling',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '2,778',\n",
       "  'abstract': 'Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.',\n",
       "  'date': 2013,\n",
       "  'authors': ['C. Farabet 1, C. Couprie 1, L. Najman 2, Y. LeCun 1'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Image texture',\n",
       "   'Feature vector',\n",
       "   'Feature extraction',\n",
       "   'Contextual image classification',\n",
       "   'Pixel',\n",
       "   'Segmentation',\n",
       "   'Deep learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '2110158442',\n",
       "   '2546302380',\n",
       "   '2130325614',\n",
       "   '1999478155',\n",
       "   '2143516773',\n",
       "   '1423339008',\n",
       "   '2156163116',\n",
       "   '2169551590',\n",
       "   '2031342017']},\n",
       " {'id': '1993882792',\n",
       "  'title': 'Acoustic Modeling Using Deep Belief Networks',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '1,900',\n",
       "  'abstract': 'Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.',\n",
       "  'date': 2011,\n",
       "  'authors': ['A. Mohamed', 'G. E. Dahl', 'G. Hinton'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Hidden Markov model',\n",
       "   'Deep belief network',\n",
       "   'Discriminative model',\n",
       "   'Mixture model',\n",
       "   'Artificial neural network',\n",
       "   'Feature vector',\n",
       "   'TIMIT',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147768505',\n",
       "   '2159080219',\n",
       "   '44815768',\n",
       "   '1994197834',\n",
       "   '2913932916',\n",
       "   '2103359087']},\n",
       " {'id': '2151103935',\n",
       "  'title': 'Distinctive Image Features from Scale-Invariant Keypoints',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '63,557',\n",
       "  'abstract': 'This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.',\n",
       "  'date': 2004,\n",
       "  'authors': ['David G. Lowe'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Haar-like features',\n",
       "   'Feature (computer vision)'],\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '2154422044',\n",
       "   '2012778485',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2165497495',\n",
       "   '1949116567']},\n",
       " {'id': '2038721957',\n",
       "  'title': 'WordNet : an electronic lexical database',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '20,553',\n",
       "  'abstract': 'Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Christiane Fellbaum'],\n",
       "  'related_topics': ['eXtended WordNet',\n",
       "   'EuroWordNet',\n",
       "   'WordNet',\n",
       "   'Normalized Google distance',\n",
       "   'IndoWordNet',\n",
       "   'Lexical database',\n",
       "   'GermaNet',\n",
       "   'Semantic network',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2108598243',\n",
       "   '1861492603',\n",
       "   '2031489346',\n",
       "   '2097726431',\n",
       "   '2110764733',\n",
       "   '2132339004',\n",
       "   '2160660844',\n",
       "   '2952122856',\n",
       "   '2145607950',\n",
       "   '2022166150']},\n",
       " {'id': '2128017662',\n",
       "  'title': 'Scalable Recognition with a Vocabulary Tree',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '4,716',\n",
       "  'abstract': 'A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\\x92s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.',\n",
       "  'date': 2006,\n",
       "  'authors': ['D. Nister', 'H. Stewenius'],\n",
       "  'related_topics': ['Vocabulary',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Search engine indexing'],\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '2131846894',\n",
       "   '1980911747',\n",
       "   '2104978738',\n",
       "   '2172188317',\n",
       "   '2124404372',\n",
       "   '2147717514',\n",
       "   '2162006472',\n",
       "   '2165497495']},\n",
       " {'id': '2110764733',\n",
       "  'title': 'LabelMe: A Database and Web-Based Tool for Image Annotation',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '3,313',\n",
       "  'abstract': 'We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Bryan C. Russell 1, Antonio Torralba 1, Kevin P. Murphy 2, William T. Freeman 1'],\n",
       "  'related_topics': ['Image retrieval',\n",
       "   'LabelMe',\n",
       "   'Automatic image annotation',\n",
       "   'Object detection',\n",
       "   'Object (computer science)',\n",
       "   'Supervised learning',\n",
       "   'WordNet',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Information retrieval',\n",
       "   'Data mining',\n",
       "   'Computer science'],\n",
       "  'references': ['2156909104',\n",
       "   '2164598857',\n",
       "   '2038721957',\n",
       "   '2138451337',\n",
       "   '2154422044',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2134557905',\n",
       "   '2156598602']},\n",
       " {'id': '1782590233',\n",
       "  'title': 'Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '5,215',\n",
       "  'abstract': 'Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Gary B. Huang 1, Marwan Mattar 1, Tamara Berg 2, Eric Learned-Miller 1'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Face detection',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Database',\n",
       "   'Quality (business)',\n",
       "   'Range (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Face synthesis',\n",
       "   'Image acquisition'],\n",
       "  'references': ['3097096317',\n",
       "   '2121647436',\n",
       "   '1999478155',\n",
       "   '2033419168',\n",
       "   '2123921160',\n",
       "   '2137659841',\n",
       "   '2098693229',\n",
       "   '2125310925',\n",
       "   '2994340921',\n",
       "   '2006793117']},\n",
       " {'id': '1576445103',\n",
       "  'title': 'Caltech-256 Object Category Dataset',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,353',\n",
       "  'abstract': 'We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Gregory Griffin', 'Alex Holub', 'Pietro Perona'],\n",
       "  'related_topics': ['Caltech 101',\n",
       "   'Pyramid (image processing)',\n",
       "   'Set (abstract data type)',\n",
       "   'Clutter',\n",
       "   'Object (computer science)',\n",
       "   'Matching (graph theory)',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer vision',\n",
       "   'Measure (mathematics)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2117539524',\n",
       "   '2108598243',\n",
       "   '1861492603',\n",
       "   '2031489346',\n",
       "   '2963173190',\n",
       "   '2295107390']},\n",
       " {'id': '2145607950',\n",
       "  'title': '80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '1,899',\n",
       "  'abstract': 'With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.',\n",
       "  'date': 2008,\n",
       "  'authors': ['A. Torralba 1, R. Fergus 2, W.T. Freeman 1'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Object detection',\n",
       "   'Image processing',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Lexical database',\n",
       "   'Human visual system model',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2162915993',\n",
       "   '2124386111',\n",
       "   '2038721957',\n",
       "   '2128017662',\n",
       "   '2110764733',\n",
       "   '1576445103',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2111993661']},\n",
       " {'id': '2141282920',\n",
       "  'title': 'Labeling images with a computer game',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '2,875',\n",
       "  'abstract': \"We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.\",\n",
       "  'date': 2004,\n",
       "  'authors': ['Luis von Ahn', 'Laura Dabbish'],\n",
       "  'related_topics': ['Computer game',\n",
       "   'Game design',\n",
       "   'Game development tool',\n",
       "   'Game design document',\n",
       "   'Game art design',\n",
       "   'Multimedia',\n",
       "   'Computer science',\n",
       "   'Block (data storage)',\n",
       "   'Image (mathematics)'],\n",
       "  'references': ['1666447063',\n",
       "   '1934863104',\n",
       "   '2166770390',\n",
       "   '1587328194',\n",
       "   '2293605478',\n",
       "   '2970081408',\n",
       "   '2055225264',\n",
       "   '2050457084',\n",
       "   '181417509',\n",
       "   '2612148268']},\n",
       " {'id': '2115733720',\n",
       "  'title': 'One-shot learning of object categories',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '2,041',\n",
       "  'abstract': 'Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Li Fei-Fei 1, R. Fergus 2, P. Perona 2'],\n",
       "  'related_topics': ['One-shot learning',\n",
       "   'Supervised learning',\n",
       "   'Maximum a posteriori estimation'],\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2124386111',\n",
       "   '2217896605',\n",
       "   '2154422044',\n",
       "   '2045656233',\n",
       "   '2166049352',\n",
       "   '2134557905',\n",
       "   '2130416410',\n",
       "   '2030536784']},\n",
       " {'id': '1528789833',\n",
       "  'title': 'TextonBoost : joint appearance, shape and context modeling for multi-class object recognition and segmentation',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '1,518',\n",
       "  'abstract': 'This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. High classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).',\n",
       "  'date': 2006,\n",
       "  'authors': ['Jamie Shotton 1, John Winn 2, Carsten Rother 2, Antonio Criminisi 2'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'One-class classification',\n",
       "   'Conditional random field',\n",
       "   'Discriminative model',\n",
       "   'Context model',\n",
       "   'Segmentation',\n",
       "   'Feature selection',\n",
       "   'Boosting (machine learning)',\n",
       "   'Supervised learning',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Linear discriminant analysis',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2147880316',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2124351162',\n",
       "   '2169551590',\n",
       "   '2024046085',\n",
       "   '1666447063',\n",
       "   '2168002178',\n",
       "   '1484228140']},\n",
       " {'id': '3008818676',\n",
       "  'title': 'The epidemiological characteristics of an outbreak of 2019 novel coronavirus diseases (COVID-19) in China',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '2,805',\n",
       "  'abstract': 'Objective An outbreak of 2019 novel coronavirus diseases (COVID-19) in Wuhan, China has spread quickly nationwide. Here, we report results of a descriptive, exploratory analysis of all cases diagnosed as of February 11, 2020. Methods All COVID-19 cases reported through February 11, 2020 were extracted from China’s Infectious Disease Information System. Analyses included: 1) summary of patient characteristics; 2) examination of age distributions and sex ratios; 3) calculation of case fatality and mortality rates; 4) geo-temporal analysis of viral spread; 5) epidemiological curve construction; and 6) subgroup analysis. Results A total of 72 314 patient records-44 672 (61.8%) confirmed cases, 16 186 (22.4%) suspected cases, 10567 (14.6%) clinical diagnosed cases (Hubei only), and 889 asymptomatic cases (1.2%)-contributed data for the analysis. Among confirmed cases, most were aged 30-79 years (86.6%), diagnosed in Hubei (74.7%), and considered mild/mild pneumonia (80.9%). A total of 1 023 deaths occurred among confirmed cases for an overall case-fatality rate of 2.3%. The COVID-19 spread outward from Hubei sometime after December 2019 and by February 11, 2020, 1 386 counties across all 31 provinces were affected. The epidemic curve of onset of symptoms peaked in January 23-26, then began to decline leading up to February 11. A total of 1 716 health workers have become infected and 5 have died (0.3%). Conclusions The COVID-19 epidemic has spread very quickly. It only took 30 days to expand from Hubei to the rest of Mainland China. With many people returning from a long holiday, China needs to prepare for the possible rebound of the epidemic. Key words: 2019 Novel Coronavirus; Outbreak; Epidemiological characteristics',\n",
       "  'date': 2020,\n",
       "  'authors': ['Novel Coronavirus Pneumonia Emergency Response Epidemiology Team'],\n",
       "  'related_topics': ['Case fatality rate',\n",
       "   'Outbreak',\n",
       "   'Mortality rate',\n",
       "   'Epidemiology',\n",
       "   'Asymptomatic',\n",
       "   'Subgroup analysis',\n",
       "   'Mainland China',\n",
       "   'Pediatrics',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Medicine'],\n",
       "  'references': ['3033453353',\n",
       "   '3035018050',\n",
       "   '3037451072',\n",
       "   '3034593359',\n",
       "   '3033301213',\n",
       "   '3021916232',\n",
       "   '3031029566',\n",
       "   '3037552531',\n",
       "   '3037851904']},\n",
       " {'id': '3004906315',\n",
       "  'title': 'CT Imaging Features of 2019 Novel Coronavirus (2019-nCoV).',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '1,879',\n",
       "  'abstract': 'In this retrospective case series, chest CT scans of 21 symptomatic patients from China infected with the 2019 novel coronavirus (2019-nCoV) were reviewed, with emphasis on identifying and characterizing the most common findings. Typical CT findings included bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities, sometimes with a rounded morphology and a peripheral lung distribution. Notably, lung cavitation, discrete pulmonary nodules, pleural effusions, and lymphadenopathy were absent. Follow-up imaging in a subset of patients during the study time window often demonstrated mild or moderate progression of disease, as manifested by increasing extent and density of lung opacities.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Michael Chung 1, Adam Bernheim 1, Xueyan Mei 1, Ning Zhang 2, Mingqian Huang 1, Xianjun Zeng 2, Jiufa Cui 3, Wenjian Xu 3, Yang Yang 1, Zahi A. Fayad 1, Adam Jacobi 1, Kunwei Li 4, Shaolin Li 4, Hong Shan 4'],\n",
       "  'related_topics': ['Lung',\n",
       "   'Pneumonia',\n",
       "   'Radiology',\n",
       "   'Retrospective cohort study',\n",
       "   'Tomography',\n",
       "   'Parenchyma',\n",
       "   'Medicine',\n",
       "   'Ct imaging',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Time windows'],\n",
       "  'references': ['2102634410',\n",
       "   '2800783955',\n",
       "   '2112136274',\n",
       "   '2056155046',\n",
       "   '2279340859',\n",
       "   '2080286891',\n",
       "   '2162899218']},\n",
       " {'id': '3006643024',\n",
       "  'title': 'Time Course of Lung Changes at Chest CT during Recovery from Coronavirus Disease 2019 (COVID-19).',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '1,266',\n",
       "  'abstract': 'Background Chest CT is used to assess the severity of lung involvement in coronavirus disease 2019 (COVID-19). Purpose To determine the changes in chest CT findings associated with COVID-19 from initial diagnosis until patient recovery. Materials and Methods This retrospective review included patients with real-time polymerase chain reaction-confirmed COVID-19 who presented between January 12, 2020, and February 6, 2020. Patients with severe respiratory distress and/or oxygen requirement at any time during the disease course were excluded. Repeat chest CT was performed at approximately 4-day intervals. Each of the five lung lobes was visually scored on a scale of 0 to 5, with 0 indicating no involvement and 5 indicating more than 75% involvement. The total CT score was determined as the sum of lung involvement, ranging from 0 (no involvement) to 25 (maximum involvement). Results Twenty-one patients (six men and 15 women aged 25-63 years) with confirmed COVID-19 were evaluated. A total of 82 chest CT scans were obtained in these patients, with a mean interval (±standard deviation) of 4 days ± 1 (range, 1-8 days). All patients were discharged after a mean hospitalization period of 17 days ± 4 (range, 11-26 days). Maximum lung involved peaked at approximately 10 days (with a calculated total CT score of 6) from the onset of initial symptoms (R2 = 0.25, P < .001). Based on quartiles of chest CT scans from day 0 to day 26 involvement, four stages of lung CT findings were defined. CT scans obtained in stage 1 (0-4 days) showed ground-glass opacities (18 of 24 scans [75%]), with a mean total CT score of 2 ± 2; scans obtained in stage 2 (5-8 days) showed an increase in both the crazy-paving pattern (nine of 17 scans [53%]) and total CT score (mean, 6 ± 4; P = .002); scans obtained in stage 3 (9-13 days) showed consolidation (19 of 21 scans [91%]) and a peak in the total CT score (mean, 7 ± 4); and scans obtained in stage 4 (≥14 days) showed gradual resolution of consolidation (15 of 20 scans [75%]) and a decrease in the total CT score (mean, 6 ± 4) without crazy-paving pattern. Conclusion In patients recovering from coronavirus disease 2019 (without severe respiratory distress during the disease course), lung abnormalities on chest CT scans showed greatest severity approximately 10 days after initial onset of symptoms. © RSNA, 2020.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Feng Pan',\n",
       "   'Tianhe Ye',\n",
       "   'Peng Sun',\n",
       "   'Shan Gui',\n",
       "   'Bo Liang',\n",
       "   'Lingli Li',\n",
       "   'Dandan Zheng',\n",
       "   'Jiazheng Wang',\n",
       "   'Richard L Hesketh',\n",
       "   'Lian Yang',\n",
       "   'Chuansheng Zheng'],\n",
       "  'related_topics': ['Lung',\n",
       "   'Respiratory distress',\n",
       "   'Pneumonia',\n",
       "   'Nuclear medicine',\n",
       "   'Stage (cooking)',\n",
       "   'Radiography',\n",
       "   'Retrospective cohort study',\n",
       "   'Quartile',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3004906315',\n",
       "   '2102634410',\n",
       "   '2800783955',\n",
       "   '3004802901',\n",
       "   '2092969802',\n",
       "   '2056155046']},\n",
       " {'id': '3006110666',\n",
       "  'title': 'Chest CT for Typical Coronavirus Disease 2019 (COVID-19) Pneumonia: Relationship to Negative RT-PCR Testing.',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '1,522',\n",
       "  'abstract': 'Some patients with positive chest CT findings may present with negative results of real-time reverse-transcription polymerase chain reaction (RT-PCR) tests for coronavirus disease 2019 (COVID-19). In this study, the authors present chest CT findings from five patients with COVID-19 infection who had initial negative RT-PCR results. All five patients had typical imaging findings, including ground-glass opacity (five patients) and/or mixed ground-glass opacity and mixed consolidation (two patients). After isolation for presumed COVID-19 pneumonia, all patients were eventually confirmed to have COVID-19 infection by means of repeated swab tests. A combination of repeated swab tests and CT scanning may be helpful for individuals with a high clinical suspicion of COVID-19 infection but negative findings at RT-PCR screening.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Xingzhi Xie',\n",
       "   'Zheng Zhong',\n",
       "   'Wei Zhao',\n",
       "   'Chao Zheng',\n",
       "   'Fei Wang',\n",
       "   'Jun Liu'],\n",
       "  'related_topics': ['False Negative Reactions',\n",
       "   'Radiology',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Radiographic image interpretation',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001897055',\n",
       "   '3004906315',\n",
       "   '3005272159',\n",
       "   '2112136274',\n",
       "   '2056155046']},\n",
       " {'id': '3006354146',\n",
       "  'title': 'Initial CT findings and temporal changes in patients with the novel coronavirus pneumonia (2019-nCoV): a study of 63 patients in Wuhan, China.',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '710',\n",
       "  'abstract': \"The purpose of this study was to observe the imaging characteristics of the novel coronavirus pneumonia. Sixty-three confirmed patients were enrolled from December 30, 2019 to January 31, 2020. High-resolution CT (HRCT) of the chest was performed. The number of affected lobes, ground glass nodules (GGO), patchy/punctate ground glass opacities, patchy consolidation, fibrous stripes and irregular solid nodules in each patient's chest CT image were recorded. Additionally, we performed imaging follow-up of these patients. CT images of 63 confirmed patients were collected. M/F ratio: 33/30. The mean age was 44.9 ± 15.2 years. The mean number of affected lobes was 3.3 ± 1.8. Nineteen (30.2%) patients had one affected lobe, five (7.9%) patients had two affected lobes, four (6.3%) patients had three affected lobes, seven (11.1%) patients had four affected lobes while 28 (44.4%) patients had 5 affected lobes. Fifty-four (85.7%) patients had patchy/punctate ground glass opacities, 14 (22.2%) patients had GGO, 12 (19.0%) patients had patchy consolidation, 11 (17.5%) patients had fibrous stripes and 8 (12.7%) patients had irregular solid nodules. Fifty-four (85.7%) patients progressed, including single GGO increased, enlarged and consolidated; fibrous stripe enlarged, while solid nodules increased and enlarged. Imaging changes in novel viral pneumonia are rapid. The manifestations of the novel coronavirus pneumonia are diverse. Imaging changes of typical viral pneumonia and some specific imaging features were observed. Therefore, we need to strengthen the recognition of image changes to help clinicians to diagnose quickly and accurately. • High-resolution CT (HRCT) of the chest is critical for early detection, evaluation of disease severity and follow-up of patients with the novel coronavirus pneumonia. • The manifestations of the novel coronavirus pneumonia are diverse and change rapidly. • Radiologists should be aware of the various features of the disease and temporal changes.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Yueying Pan',\n",
       "   'Hanxiong Guan',\n",
       "   'Shuchang Zhou',\n",
       "   'Yujin Wang',\n",
       "   'Qian Li',\n",
       "   'Tingting Zhu',\n",
       "   'Qiongjie Hu',\n",
       "   'Liming Xia'],\n",
       "  'related_topics': ['Viral pneumonia',\n",
       "   'Neuroradiology',\n",
       "   'Radiology',\n",
       "   'Interventional radiology',\n",
       "   'Ultrasound',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Ct findings',\n",
       "   'In patient',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '3001465255',\n",
       "   '3017468735',\n",
       "   '3001456238']},\n",
       " {'id': '3003901880',\n",
       "  'title': 'CT Imaging of the 2019 Novel Coronavirus (2019-nCoV) Pneumonia.',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '479',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Junqiang Lei', 'Junfeng Li', 'Xun Li', 'Xiaolong Qi'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Respiratory sounds',\n",
       "   'Tomography',\n",
       "   'Medicine',\n",
       "   'Radiology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Ct imaging',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed',\n",
       "   'X ray computed'],\n",
       "  'references': ['3002533507']},\n",
       " {'id': '3005656138',\n",
       "  'title': 'Use of Chest CT in Combination with Negative RT-PCR Assay for the 2019 Novel Coronavirus but High Clinical Suspicion.',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '363',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Peikai Huang',\n",
       "   'Tianzhu Liu',\n",
       "   'Lesheng Huang',\n",
       "   'Hailong Liu',\n",
       "   'Ming Lei',\n",
       "   'Wangdong Xu',\n",
       "   'Xiaolu Hu',\n",
       "   'Jun Chen',\n",
       "   'Bo Liu'],\n",
       "  'related_topics': ['Real-time polymerase chain reaction',\n",
       "   'Pneumonia',\n",
       "   'False Negative Reactions',\n",
       "   'Medicine',\n",
       "   'Pathology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Disease progression',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'X ray computed'],\n",
       "  'references': ['3001897055', '3004668429']},\n",
       " {'id': '3004511262',\n",
       "  'title': 'Evolution of CT Manifestations in a Patient Recovered from 2019 Novel Coronavirus (2019-nCoV) Pneumonia in Wuhan, China.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '193',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Heshui Shi 1, Xiaoyu Han 2, Chuansheng Zheng'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Medicine',\n",
       "   'Virology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Lung pathology',\n",
       "   'Patient discharge',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed'],\n",
       "  'references': ['3007497549',\n",
       "   '3012211282',\n",
       "   '3008627141',\n",
       "   '3025334942',\n",
       "   '3009992310',\n",
       "   '3008801544',\n",
       "   '3034593359',\n",
       "   '3008928918',\n",
       "   '3014051579',\n",
       "   '3013468450']},\n",
       " {'id': '3008962515',\n",
       "  'title': 'Molecular and serological investigation of 2019-nCoV infected patients: implication of multiple shedding routes.',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '1,382',\n",
       "  'abstract': 'In December 2019, a novel coronavirus (2019-nCoV) caused an outbreak in Wuhan, China, and soon spread to other parts of the world. It was believed that 2019-nCoV was transmitted through respiratory tract and then induced pneumonia, thus molecular diagnosis based on oral swabs was used for confirmation of this disease. Likewise, patient will be released upon two times of negative detection from oral swabs. However, many coronaviruses can also be transmitted through oral-fecal route by infecting intestines. Whether 2019-nCoV infected patients also carry virus in other organs like intestine need to be tested. We conducted investigation on patients in a local hospital who were infected with this virus. We found the presence of 2019-nCoV in anal swabs and blood as well, and more anal swab positives than oral swab positives in a later stage of infection, suggesting shedding and thereby transmitted through oral-fecal route. We also showed serology test can improve detection positive rate thus should be used in future epidemiology. Our report provides a cautionary warning that 2019-nCoV may be shed through multiple routes.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Wei Zhang 1, Rong-Hui Du 2, Bei Li 1, Xiao-Shuang Zheng 1, Xing-Lou Yang 1, Ben Hu 1, Yan-Yi Wang 1, Geng-Fu Xiao 1, Bing Yan 1, Zheng-Li Shi 1, Peng Zhou 1'],\n",
       "  'related_topics': ['Viral shedding',\n",
       "   'Coronavirus',\n",
       "   'Serology',\n",
       "   'Outbreak',\n",
       "   'Pneumonia (non-human)',\n",
       "   'Virus',\n",
       "   'Disease',\n",
       "   'Epidemiology',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3003668884',\n",
       "   '3004280078',\n",
       "   '2786098272',\n",
       "   '2769543984',\n",
       "   '2021442163',\n",
       "   '3025232310',\n",
       "   '3028321619',\n",
       "   '3027541845']},\n",
       " {'id': '3008452791',\n",
       "  'title': 'Viral load of SARS-CoV-2 in clinical samples.',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '850',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yang Pan 1, 2, Daitao Zhang 2, Peng Yang 1, 2, Leo L M Poon 3, Quanyi Wang 2'],\n",
       "  'related_topics': ['Feces analysis',\n",
       "   'Viral load',\n",
       "   'Pneumonia',\n",
       "   'Pandemic',\n",
       "   'Virology',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3004318991', '3003637715', '2129542667']},\n",
       " {'id': '3033453353',\n",
       "  'title': 'Recent Understandings Toward Coronavirus Disease 2019 (COVID-19): From Bench to Bedside',\n",
       "  'reference_count': '117',\n",
       "  'citation_count': '13',\n",
       "  'abstract': 'In late December 2019, an unprecedented outbreak of coronavirus disease 2019 (COVID-19) caused by SARS coronavirus 2 (SARS-CoV-2) (previously named 2019-nCoV) in Wuhan became the most challenging health emergency. Since its rapid spread in China and many other countries, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern (PHEIC) on 30th January 2020 and a pandemic on 11th March 2020. Thousands of people have died, and there are currently no vaccines or specific antiviral drugs for COVID-19. Therefore, it is critical to have a comprehensive understanding of the virus. In this review, we highlight the etiology, epidemiology, pathogenesis and pathology, clinical characteristics, diagnosis, clinical management, prognosis, infection control and prevention of COVID-19 based on recent studies.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Jie Yu', 'Peiwei Chai', 'Shengfang Ge', 'Xianqun Fan'],\n",
       "  'related_topics': ['Pandemic',\n",
       "   'Public health',\n",
       "   'Epidemiology',\n",
       "   'Outbreak',\n",
       "   'Infection control',\n",
       "   'Etiology',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3004280078',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '3004239190']},\n",
       " {'id': '3034408674',\n",
       "  'title': 'Risk assessment of mixed and displacement ventilation (LAF) during orthopedic and trauma surgery on COVID-19 patients with increased release of infectious aerosols',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4',\n",
       "  'abstract': 'No abstract available Keywords: Displacement ventilation; SARS-CoV-2 spread; laminar air flow; mixed ventilation; negative pressure; no ventilation.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Axel Kramer 1, Rüdiger Külpmann 2, Arnold Brunner 3, Michael Müller 4, Georgi Wassilew 1'],\n",
       "  'related_topics': ['Ventilation (architecture)',\n",
       "   'Displacement ventilation',\n",
       "   'Trauma surgery',\n",
       "   'Orthopedic surgery',\n",
       "   'Anesthesia',\n",
       "   'Risk assessment',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3006961006', '3010604545', '3130405932', '3144171767']},\n",
       " {'id': '3035275617',\n",
       "  'title': 'Recreational waters - A potential transmission route for SARS-CoV-2 to humans?',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '22',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19), the respiratory illness caused by the novel virus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which has lead to high morbidity and mortality rates worldwide, has been causing major public health concerns since first detected in late 2019. Following identification of novel pathogens, questions in relation to dissemination of the pathogen and transmission routes begin to emerge. This rapidly spreading SARS-CoV-2 virus has been detected in both faecal and wastewater samples across the globe, highlighting the potential for faecal-oral transmission of the virus. As a result, concerns regarding the transmission of the virus in the environment and the risk associated with contracting the virus in recreational waters, particularly where inadequately treated wastewater is discharged, have been emerging in recent weeks. This paper highlights the need for further research to be carried out to investigate the presence, infectivity and viability of this newly identified SARS-CoV-2 virus in wastewater effluent and receiving recreational waters.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Niamh Cahill', 'Dearbháile Morris'],\n",
       "  'related_topics': ['Novel virus',\n",
       "   'Coronavirus',\n",
       "   'Transmission (medicine)',\n",
       "   'Virus',\n",
       "   'Pandemic',\n",
       "   'Infectivity',\n",
       "   'Betacoronavirus',\n",
       "   'Pathogen',\n",
       "   'Environmental health',\n",
       "   'Medicine'],\n",
       "  'references': ['3004280078',\n",
       "   '3003465021',\n",
       "   '3013893137',\n",
       "   '3010604545',\n",
       "   '3004824173',\n",
       "   '3003464757',\n",
       "   '3009834387',\n",
       "   '3011863580',\n",
       "   '3010096538',\n",
       "   '3006846061']},\n",
       " {'id': '3034059415',\n",
       "  'title': 'Impact of Lockdown on the Epidemic Dynamics of COVID-19 in France',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '37',\n",
       "  'abstract': 'The COVID-19 epidemic was reported in the Hubei province in China in December 2019 and then spread around the world reaching the pandemic stage at the beginning of March 2020. Since then, several countries went into lockdown. Using a mechanistic-statistical formalism, we estimate the effect of the lockdown in France on the contact rate and the effective reproduction number Re of the COVID-19. We obtain a reduction by a factor 7 (Re=0.47, 95%-CI: 0.45-0.50), compared to the estimates carried out in France at the early stage of the epidemic. We also estimate the fraction of the population that would be infected by the beginning of May, at the official date at which the lockdown should be relaxed. We find a fraction of 3.7% (95%-CI: 3.0-4.8%) of the total French population, without taking into account the number of recovered individuals before April 1st, which is not known. This proportion is seemingly too low to reach herd immunity. Thus, even if the lockdown strongly mitigated the first epidemic wave, keeping a low value of Re is crucial to avoid an uncontrolled second wave (initiated with much more infectious cases than the first wave) and to hence avoid the saturation of hospital facilities.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Lionel Roques 1, Etienne K. Klein 1, Julien Papaïx 1, Antoine Sar 2, Samuel Soubeyrand 1'],\n",
       "  'related_topics': ['Population',\n",
       "   'Epidemic model',\n",
       "   'Herd immunity',\n",
       "   'Pandemic',\n",
       "   'Demography',\n",
       "   'Geography',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Epidemic dynamics',\n",
       "   'Formalism (philosophy of mathematics)'],\n",
       "  'references': ['3003668884',\n",
       "   '3009885589',\n",
       "   '3008443627',\n",
       "   '3010604545',\n",
       "   '3013967887',\n",
       "   '3015571324',\n",
       "   '3006642361',\n",
       "   '3004397688',\n",
       "   '3013594674',\n",
       "   '3012789146']},\n",
       " {'id': '3033952286',\n",
       "  'title': 'Real-time reverse transcription loop-mediated isothermal amplification for rapid detection of SARS-CoV-2',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '10',\n",
       "  'abstract': 'Background Highly sensitive real-time reverse transcription polymerase chain reaction (RT-qPCR) methods have been developed for the detection of SARS-CoV-2. However, they are costly. Loop-mediated isothermal amplification (LAMP) assay has emerged as a novel alternative isothermal amplification method for the detection of nucleic acid. Methods A rapid, sensitive and specific real-time reverse transcription LAMP (RT-LAMP) assay was developed for SARS-CoV-2 detection. Results This assay detected one copy/reaction of SARS-CoV-2 RNA in 30 min. Both the clinical sensitivity and specificity of this assay were 100%. The RT-LAMP showed comparable performance with RT-qPCR. Combining simplicity and cost-effectiveness, this assay is therefore recommended for use in resource resource-limited settings.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yee Ling Lau 1, Ilyiana Ismail 2, Nur Izati Mustapa 2, Meng Yee Lai 1, Tuan Suhaila Tuan Soh 2, Afifah Hassan 2, Kalaiarasu M Peariasamy 3, Yee Leng Lee 3, Yoong Min Chong 1, I-Ching Sam 1, Pik Pin Goh 4'],\n",
       "  'related_topics': ['Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Reverse transcriptase',\n",
       "   'Nucleic acid',\n",
       "   'RNA',\n",
       "   'Molecular biology',\n",
       "   'Chemistry',\n",
       "   'Rapid detection',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001195213',\n",
       "   '3010604545',\n",
       "   '2105275554',\n",
       "   '3011969828',\n",
       "   '2770752141',\n",
       "   '2263084061',\n",
       "   '2175815746',\n",
       "   '1991420168',\n",
       "   '2073600962',\n",
       "   '2084576921']},\n",
       " {'id': '3036958556',\n",
       "  'title': 'Involvement of digestive system in COVID-19: manifestations, pathology, management and challenges',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '20',\n",
       "  'abstract': 'The pandemic of novel coronavirus disease (COVID-19) has developed as a tremendous threat to global health. Although most COVID-19 patients present with respiratory symptoms, some present with gastrointestinal (GI) symptoms like diarrhoea, loss of appetite, nausea/vomiting and abdominal pain as the major complaints. These features may be attributable to the following facts: (a) COVID-19 is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and its receptor angiotensin converting enzyme 2 (ACE2) was found to be highly expressed in GI epithelial cells, providing a prerequisite for SARS-CoV-2 infection; (b) SARS-CoV-2 viral RNA has been found in stool specimens of infected patients, and 20% of patients showed prolonged presence of SARS-CoV-2 RNA in faecal samples after the virus converting to negative in the respiratory system. These findings suggest that SARS-CoV-2 may be able to actively infect and replicate in the GI tract. Moreover, GI infection could be the first manifestation antedating respiratory symptoms; patients suffering only digestive symptoms but no respiratory symptoms as clinical manifestation have also been reported. Thus, the implications of digestive symptoms in patients with COVID-19 is of great importance. In this review, we summarise recent findings on the epidemiology of GI tract involvement, potential mechanisms of faecal-oral transmission, GI and liver manifestation, pathological/histological features in patients with COVID-19 and the diagnosis, management of patients with pre-existing GI and liver diseases as well as precautions for preventing SARS-CoV-2 infection during GI endoscopy procedures.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Song Su 1, Jun Shen 2, Liangru Zhu 3, Yun Qiu 4, Jin-Shen He 4, Jin-Yu Tan 4, Marietta Iacucci 5, Siew C Ng 6, Subrata Ghosh 5, Ren Mao 4, Jie Liang 1'],\n",
       "  'related_topics': ['Coronavirus', 'Vomiting', 'Disease'],\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3002539152',\n",
       "   '3003465021',\n",
       "   '3008090866',\n",
       "   '3007940623',\n",
       "   '3010604545',\n",
       "   '3011242477']},\n",
       " {'id': '3035464429',\n",
       "  'title': 'Sampling and detection of corona viruses in air: A mini review.',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '22',\n",
       "  'abstract': \"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is a strain of coronaviruses that causes coronavirus disease 2019 (COVID-19). In these days, the spread of the SARS-CoV-2 virus through the air has become a controversial topic among scientists. Various organizations provide standard methods for monitoring biological agents in the air. Nevertheless, there has been no standard recommended method for sampling and determination of viruses in air. This manuscript aimed at reviewing published papers for sampling and detection of corona viruses, especially SARS-Cov-2 as a global health concern. It was found that SARS-Cov 2 was present in some air samples that were collected from patient's rooms in hospitals. This result warrants its airborne transmission potential. However, due to the fact that in the most reviewed studies, sampling was performed in the patient's room, it seems difficult to discriminate whether it is airborne or is transmitted through respiratory droplets. Moreover, some other disrupting factors such as patient distance from the sampler, using protective or oxygen masks by patients, patient activities, coughing and sneezing during sampling time, air movement, air conditioning, sampler type, sampling conditions, storage and transferring conditions, can affect the results. About the sampling methods, most of the used samplers such as PTFE filters, gelatin filers and cyclones showed suitable performance for trapping SARS-Co and MERS-Cov viruses followed by PCR analysis.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Ali Reza Rahmani',\n",
       "   'Mostafa Leili',\n",
       "   'Ghasem Azarian',\n",
       "   'Ali Poormohammadi'],\n",
       "  'related_topics': ['Airborne transmission',\n",
       "   'Sampling (statistics)',\n",
       "   'Air conditioning',\n",
       "   'Emergency medicine',\n",
       "   'Environmental science',\n",
       "   'Air movement',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Mini review',\n",
       "   'Pcr analysis',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3010604545',\n",
       "   '2132260239',\n",
       "   '3010449299',\n",
       "   '3018334611',\n",
       "   '2103503670',\n",
       "   '3015704123',\n",
       "   '3030968929',\n",
       "   '2158121945',\n",
       "   '3015636815',\n",
       "   '3018724240']},\n",
       " {'id': '3028749392',\n",
       "  'title': 'A Collaborative Multidisciplinary Approach to the Management of Coronavirus Disease 2019 in the Hospital Setting.',\n",
       "  'reference_count': '136',\n",
       "  'citation_count': '15',\n",
       "  'abstract': 'The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19), which presents an unprecedented challenge to medical providers worldwide. Although most SARS-CoV-2-infected individuals manifest with a self-limited mild disease that resolves with supportive care in the outpatient setting, patients with moderate to severe COVID-19 will require a multidisciplinary collaborative management approach for optimal care in the hospital setting. Laboratory and radiologic studies provide critical information on disease severity, management options, and overall prognosis. Medical management is mostly supportive with antipyretics, hydration, oxygen supplementation, and other measures as dictated by clinical need. Among its medical complications is a characteristic proinflammatory cytokine storm often associated with end-organ dysfunction, including respiratory failure, liver and renal insufficiency, cardiac injury, and coagulopathy. Specific recommendations for the management of these medical complications are discussed. Despite the issuance of emergency use authorization for remdesivir, there are still no proven effective antiviral and immunomodulatory therapies, and their use in COVID-19 management should be guided by clinical trial protocols or treatment registries. The medical care of patients with COVID-19 extends beyond their hospitalization. Postdischarge follow-up and monitoring should be performed, preferably using telemedicine, until the patients have fully recovered from their illness and are released from home quarantine protocols.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Raymund R. Razonable',\n",
       "   'Kelly M. Pennington',\n",
       "   'Anne M. Meehan',\n",
       "   'John W. Wilson',\n",
       "   'Adam T. Froemming',\n",
       "   'Courtney E. Bennett',\n",
       "   'Ariela L. Marshall',\n",
       "   'Abinash Virk',\n",
       "   'Eva M. Carmona'],\n",
       "  'related_topics': ['Clinical trial', 'Intensive care unit', 'Telemedicine'],\n",
       "  'references': []},\n",
       " {'id': '3032185657',\n",
       "  'title': 'Trajectory of the COVID-19 pandemic: chasing a moving target',\n",
       "  'reference_count': '75',\n",
       "  'citation_count': '18',\n",
       "  'abstract': 'The spread of COVID-19 has already taken a pandemic form, affecting over 180 countries in a matter of three months. The full continuum of disease ranges from mild, self-limiting illness to severe progressive COVID-19 pneumonia, multiorgan failure, cytokine storm and death. Younger and healthy population is now getting affected than before. Possibilities of airborne and fecal oral routes of transmission has increased the concern. In the absence of any specific therapeutic agent for coronavirus infections, the most effective manner to contain this pandemic is probably the non-pharmacological interventions (NPIs). The damage due to the pandemic disease is multifaceted and crippling to economy, trade, and health of the citizens of the countries. The extent of damage in such scenarios is something that is beyond calculation by Gross Domestic Product rate or currency value of the country. Unfortunately, unlike many other diseases, we are still away from the target antiviral drug and vaccine for severe acute respiratory syndrome (SARS-CoV-2) infection. The prime importance of NPIs like social distancing, staying in home, work from home, self-monitoring, public awareness, self-quarantine, etc. are constantly being emphasized by CDC, WHO, health ministries of all countries and social media houses. This is time of introspection and learning from our mistakes. Countries like China and South Korea who were initially the most hit countries could contain the disease spread by liberal testing of their population, stringent quarantine of people under investigation and isolation of the positive cases. Rest of the countries need to act urgently as well to bring an immediate halt in the community transmission.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Kamal Kant Sahu 1, Ajay Kumar Mishra 1, Amos Lal 2'],\n",
       "  'related_topics': ['Population',\n",
       "   'Pandemic',\n",
       "   'Isolation (health care)',\n",
       "   'Quarantine',\n",
       "   'Disease',\n",
       "   'Transmission (medicine)',\n",
       "   'Psychological intervention',\n",
       "   'Gross domestic product',\n",
       "   'Development economics',\n",
       "   'Business'],\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3008090866',\n",
       "   '3007497549',\n",
       "   '3010930696',\n",
       "   '3014294089']},\n",
       " {'id': '3042098369',\n",
       "  'title': 'Laboratory Tests for COVID-19: A Review of Peer-Reviewed Publications and Implications for Clinical Use',\n",
       "  'reference_count': '70',\n",
       "  'citation_count': '11',\n",
       "  'abstract': 'Diagnostic tests for the coronavirus infection 2019 (COVID-19) are critical for prompt diagnosis, treatment and isolation to break the cycle of transmission. A positive real-time reverse-transcriptase polymerase chain reaction (RT-PCR), in conjunction with clinical and epidemiologic data, is the current standard for diagnosis, but several challenges still exist. Serological assays help to understand epidemiology better and to evaluate vaccine responses but they are unreliable for diagnosis in the acute phase of illness or assuming protective immunity. Serology is gaining attention, mainly because of convalescent plasma gaining importance as treatment for clinically worsening COVID-19 patients. We provide a narrative review of peer-reviewed research studies on RT-PCR, serology and antigen immune-assays for COVID-19, briefly describe their lab methods and discuss their limitations for clinical practice.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Daniel Shyu',\n",
       "   'James Dorroh',\n",
       "   'Caleb Holtmeyer',\n",
       "   'Detlef Ritter',\n",
       "   'Anandhi Upendran',\n",
       "   'Raghuraman Kannan',\n",
       "   'Dima Dandachi',\n",
       "   'Christian Rojas-Moreno',\n",
       "   'Stevan P Whitt',\n",
       "   'Hariharan Regunath'],\n",
       "  'related_topics': ['Serology',\n",
       "   'Peer review',\n",
       "   'Epidemiology',\n",
       "   'Isolation (health care)',\n",
       "   'Intensive care medicine',\n",
       "   'MEDLINE',\n",
       "   'Transmission (medicine)',\n",
       "   'Medicine',\n",
       "   'Pandemic',\n",
       "   'Coronavirus disease 2019 (COVID-19)'],\n",
       "  'references': ['3001118548',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '3001195213',\n",
       "   '3007497549',\n",
       "   '3013893137',\n",
       "   '3010604545']},\n",
       " {'id': '3037255629',\n",
       "  'title': 'COVID-19 in children: An ample review',\n",
       "  'reference_count': '63',\n",
       "  'citation_count': '12',\n",
       "  'abstract': 'The aim of this review was to describe the current knowledge about coronavirus disease 2019 (COVID-19, which is caused by severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) in children, from epidemiological, clinical, and laboratory perspectives, including knowledge on the disease course, treatment, and prognosis. An extensive literature search was performed to identify papers on COVID-19 (SARS-CoV-2 infection) in children, published between January 1, 2020 and April 1, 2020. There were 44 relevant papers on COVID-19 in children. The results showed that COVID-19 occurs in 0.39–12.3% of children. Clinical signs and symptoms are comparable to those in adults, but milder forms and a large percentage of asymptomatic carriers are found among children. Elevated inflammatory markers are associated with complications and linked to various co-infections. Chest computed tomography (CT) scans in children revealed structural changes similar to those found in adults, with consolidations surrounded by halos being somewhat specific for children with COVID-19. The recommended treatment includes providing symptomatic therapy, with no specific drug recommendations for children. The prognosis is much better for children compared to adults. This review highlights that COVID-19 in children is similar to the disease in the adult population, but with particularities regarding clinical manifestations, laboratory test results, chest imaging, and treatment. The prognosis is much better for children compared to adults, but with the progression of the pandemic; the cases in children might change in the future.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ioana M Ciuca'],\n",
       "  'related_topics': ['Epidemiology',\n",
       "   'Asymptomatic carrier',\n",
       "   'Disease',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'Pandemic',\n",
       "   'Adult population',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Signs and symptoms'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3008028633',\n",
       "   '3007940623',\n",
       "   '3007497549',\n",
       "   '3010604545',\n",
       "   '3010930696']},\n",
       " {'id': '2962739339',\n",
       "  'title': 'Deep contextualized word representations',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '6,539',\n",
       "  'abstract': 'We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Matthew E. Peters 1, Mark Neumann 1, Mohit Iyyer 2, Matt Gardner 1, Christopher Clark 1, Kenton Lee 3, Luke Zettlemoyer 4'],\n",
       "  'related_topics': ['Textual entailment',\n",
       "   'Text corpus',\n",
       "   'Syntax',\n",
       "   'Language model',\n",
       "   'Polysemy',\n",
       "   'Semantics',\n",
       "   'Question answering',\n",
       "   'Sentiment analysis',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964121744',\n",
       "   '2153579005',\n",
       "   '2095705004',\n",
       "   '2250539671',\n",
       "   '2064675550',\n",
       "   '2158899491',\n",
       "   '2493916176',\n",
       "   '2251939518',\n",
       "   '2147880316',\n",
       "   '2963748441']},\n",
       " {'id': '2331128040',\n",
       "  'title': 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution',\n",
       "  'reference_count': '65',\n",
       "  'citation_count': '5,160',\n",
       "  'abstract': 'We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Justin Johnson', 'Alexandre Alahi', 'Li Fei-Fei'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Image translation',\n",
       "   'Convolutional neural network',\n",
       "   'Optimization problem',\n",
       "   'Image (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Orders of magnitude (time)',\n",
       "   'Perception',\n",
       "   'Work (physics)',\n",
       "   'Artificial intelligence',\n",
       "   'Superresolution',\n",
       "   'Texture transfer'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2133665775',\n",
       "   '1861492603',\n",
       "   '2963684088',\n",
       "   '2964153729']},\n",
       " {'id': '2964015378',\n",
       "  'title': 'Semi-Supervised Classification with Graph Convolutional Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '7,582',\n",
       "  'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Thomas N. Kipf', 'Max Welling'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Scalability',\n",
       "   'Theoretical computer science',\n",
       "   'ENCODE',\n",
       "   'Computer science',\n",
       "   'Graph',\n",
       "   'Graph classification',\n",
       "   'Graph neural networks',\n",
       "   'Hidden layer',\n",
       "   'Knowledge graph'],\n",
       "  'references': ['2962711740',\n",
       "   '2907492528',\n",
       "   '3100848837',\n",
       "   '2963224980',\n",
       "   '2962883549',\n",
       "   '2963184176',\n",
       "   '3100278010',\n",
       "   '2905224888',\n",
       "   '2918342466',\n",
       "   '2796426482']},\n",
       " {'id': '1514535095',\n",
       "  'title': 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '7,190',\n",
       "  'abstract': 'Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Kelvin Xu 1, Jimmy Ba 2, Ryan Kiros 2, Kyunghyun Cho 1, Aaron Courville 1, Ruslan Salakhudinov 2, 3, Rich Zemel 2, 3, Yoshua Bengio 1, 3'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Visualization',\n",
       "   'Machine translation',\n",
       "   'Backpropagation',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Sequence',\n",
       "   'Closed captioning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '2117539524',\n",
       "   '2964308564',\n",
       "   '2095705004',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '1861492603']},\n",
       " {'id': '2168356304',\n",
       "  'title': 'Object Detection with Discriminatively Trained Part-Based Models',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '10,940',\n",
       "  'abstract': 'We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.',\n",
       "  'date': 2010,\n",
       "  'authors': ['P F Felzenszwalb 1, R B Girshick 1, D McAllester 2, D Ramanan 3'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Object detection',\n",
       "   'Support vector machine',\n",
       "   'Discriminative model',\n",
       "   'Pascal (programming language)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Linear discriminant analysis',\n",
       "   'Pedestrian detection',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '3097096317',\n",
       "   '2154422044',\n",
       "   '2120419212',\n",
       "   '2152826865',\n",
       "   '2145072179',\n",
       "   '1576520375',\n",
       "   '2030536784',\n",
       "   '2115763357']},\n",
       " {'id': '1849277567',\n",
       "  'title': 'Visualizing and Understanding Convolutional Networks',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '12,388',\n",
       "  'abstract': 'Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Matthew D. Zeiler', 'Rob Fergus'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Softmax function',\n",
       "   'Classifier (UML)',\n",
       "   'Network model',\n",
       "   'Machine learning',\n",
       "   'Stochastic gradient descent',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2136922672',\n",
       "   '1904365287',\n",
       "   '2155541015',\n",
       "   '2546302380',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '2161381512']},\n",
       " {'id': '1959608418',\n",
       "  'title': 'Auto-Encoding Variational Bayes',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '13,487',\n",
       "  'abstract': 'Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Diederik P Kingma', 'Max Welling'],\n",
       "  'related_topics': ['Approximate inference', 'Inference', 'Estimator'],\n",
       "  'references': ['2146502635',\n",
       "   '2163922914',\n",
       "   '2145094598',\n",
       "   '2166851633',\n",
       "   '2097268041',\n",
       "   '2963173382',\n",
       "   '2951493172',\n",
       "   '2171490498',\n",
       "   '2119196781',\n",
       "   '3104819538']},\n",
       " {'id': '1904365287',\n",
       "  'title': 'Improving neural networks by preventing co-adaptation of feature detectors',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '8,061',\n",
       "  'abstract': 'When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Geoffrey E. Hinton',\n",
       "   'Nitish Srivastava',\n",
       "   'Alex Krizhevsky',\n",
       "   'Ilya Sutskever',\n",
       "   'Ruslan R. Salakhutdinov'],\n",
       "  'related_topics': ['Dropout (neural networks)',\n",
       "   'Feature (computer vision)',\n",
       "   'Overfitting',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial neural network',\n",
       "   'Context (language use)',\n",
       "   'Benchmark (computing)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2108598243',\n",
       "   '2911964244',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2912934387',\n",
       "   '2147768505',\n",
       "   '1993882792',\n",
       "   '4919037']},\n",
       " {'id': '2964153729',\n",
       "  'title': 'Intriguing properties of neural networks',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '8,634',\n",
       "  'abstract': \"Abstract: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.\",\n",
       "  'date': 2013,\n",
       "  'authors': ['Christian Szegedy 1, Wojciech Zaremba 2, Ilya Sutskever 1, Joan Bruna 2, Dumitru Erhan 1, Ian Goodfellow 3, Rob Fergus 2, 4'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Adversarial machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Linear combination',\n",
       "   'Computer science',\n",
       "   'Uninterpretable',\n",
       "   'Backdoor',\n",
       "   'Artificial intelligence',\n",
       "   'Deep neural networks',\n",
       "   'Mean squared prediction error',\n",
       "   'Visual recognition'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '1614298861',\n",
       "   '2108598243',\n",
       "   '2160815625',\n",
       "   '2072128103',\n",
       "   '2206858481',\n",
       "   '2120419212',\n",
       "   '2120480077',\n",
       "   '2150165932']},\n",
       " {'id': '2133665775',\n",
       "  'title': 'Image quality assessment: from error visibility to structural similarity',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '33,167',\n",
       "  'abstract': 'Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Zhou Wang 1, A.C. Bovik 2, H.R. Sheikh 2, E.P. Simoncelli 3'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Subjective video quality',\n",
       "   'Human visual system model',\n",
       "   'Image processing',\n",
       "   'Visibility (geometry)',\n",
       "   'Cyclopean image',\n",
       "   'JPEG',\n",
       "   'JPEG 2000',\n",
       "   'Image compression',\n",
       "   'Image translation',\n",
       "   'Compression artifact',\n",
       "   'PEVQ',\n",
       "   'Data compression',\n",
       "   'View synthesis',\n",
       "   'Transform coding',\n",
       "   'Ringing artifacts',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Structural similarity',\n",
       "   'Artificial intelligence',\n",
       "   'Superresolution'],\n",
       "  'references': ['2159269332',\n",
       "   '2142276208',\n",
       "   '2118217749',\n",
       "   '2053691921',\n",
       "   '2153777140',\n",
       "   '2912116903',\n",
       "   '2107790757',\n",
       "   '2158564760',\n",
       "   '2124731682',\n",
       "   '2115838129']},\n",
       " {'id': '2340897893',\n",
       "  'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding',\n",
       "  'reference_count': '81',\n",
       "  'citation_count': '4,755',\n",
       "  'abstract': 'Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Marius Cordts 1, Mohamed Omran 2, Sebastian Ramos 3, Timo Rehfeld 1, Markus Enzweiler 3, Rodrigo Benenson 2, Uwe Franke 3, Stefan Roth 1, Bernt Schiele 2'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Deep learning',\n",
       "   'Machine learning',\n",
       "   'Leverage (statistics)',\n",
       "   'Annotation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2919115771',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '2168356304',\n",
       "   '1861492603']},\n",
       " {'id': '2963420272',\n",
       "  'title': 'Context Encoders: Feature Learning by Inpainting',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '2,713',\n",
       "  'abstract': 'We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Deepak Pathak',\n",
       "   'Philipp Krahenbuhl',\n",
       "   'Jeff Donahue',\n",
       "   'Trevor Darrell',\n",
       "   'Alexei A. Efros'],\n",
       "  'related_topics': ['Inpainting',\n",
       "   'Feature learning',\n",
       "   'Context (language use)',\n",
       "   'Convolutional neural network',\n",
       "   'Initialization',\n",
       "   'Encoder',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Visualization',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2964121744',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2153579005',\n",
       "   '2099471712',\n",
       "   '2155893237',\n",
       "   '1536680647',\n",
       "   '1849277567']},\n",
       " {'id': '2405756170',\n",
       "  'title': 'Generative adversarial text to image synthesis',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '1,889',\n",
       "  'abstract': 'Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Scott Reed 1, Zeynep Akata 2, Xinchen Yan 1, Lajanugen Logeswaran 1, Bernt Schiele 2, Honglak Lee 1'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Discriminative model',\n",
       "   'Feature (computer vision)',\n",
       "   'Generative grammar',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Image (mathematics)',\n",
       "   'Bridge (nautical)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964121744',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2099471712',\n",
       "   '2963684088',\n",
       "   '2064675550',\n",
       "   '1895577753',\n",
       "   '1514535095',\n",
       "   '2481240925',\n",
       "   '1947481528']},\n",
       " {'id': '2963800363',\n",
       "  'title': 'High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '1,693',\n",
       "  'abstract': 'We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 A— 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Ting-Chun Wang 1, Ming-Yu Liu 1, Jun-Yan Zhu 2, Andrew Tao 1, Jan Kautz 1, Bryan Catanzaro 1'],\n",
       "  'related_topics': ['Object (computer science)',\n",
       "   'Image translation',\n",
       "   'Segmentation',\n",
       "   'Semantics',\n",
       "   'Pattern recognition',\n",
       "   'Generator (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Resolution (logic)',\n",
       "   'Image resolution'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '1959608418',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2340897893',\n",
       "   '2963373786',\n",
       "   '2963470893']},\n",
       " {'id': '2963836885',\n",
       "  'title': 'Spectral Normalization for Generative Adversarial Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,015',\n",
       "  'abstract': 'One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Takeru Miyato 1, Toshiki Kataoka',\n",
       "   'Masanori Koyama 2, Yuichi Yoshida 3'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Discriminator',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Generative grammar',\n",
       "   'Adversarial system',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3003301247',\n",
       "   '2893749619',\n",
       "   '2962974533',\n",
       "   '2804078698',\n",
       "   '3035574324',\n",
       "   '2982763192',\n",
       "   '2890139949',\n",
       "   '2963841322']},\n",
       " {'id': '2893749619',\n",
       "  'title': 'Large Scale GAN Training for High Fidelity Natural Image Synthesis',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,654',\n",
       "  'abstract': 'Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator\\'s input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Andrew Brock 1, Jeff Donahue 2, Karen Simonyan 2'],\n",
       "  'related_topics': ['Fidelity',\n",
       "   'Generator (mathematics)',\n",
       "   'Scale (ratio)',\n",
       "   'High fidelity',\n",
       "   'Pattern recognition',\n",
       "   'Image (mathematics)',\n",
       "   'Regularization (mathematics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Truncation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3003301247',\n",
       "   '2962974533',\n",
       "   '2804078698',\n",
       "   '3035574324',\n",
       "   '2962883549',\n",
       "   '2996035354',\n",
       "   '2890139949',\n",
       "   '2985068832']},\n",
       " {'id': '2738588019',\n",
       "  'title': 'Globally and locally consistent image completion',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '1,057',\n",
       "  'abstract': 'We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Satoshi Iizuka', 'Edgar Simo-Serra', 'Hiroshi Ishikawa'],\n",
       "  'related_topics': ['Real image',\n",
       "   'Context (language use)',\n",
       "   'Discriminator',\n",
       "   'Local consistency',\n",
       "   'Convolutional neural network',\n",
       "   'Consistency (database systems)',\n",
       "   'Computer vision',\n",
       "   'Image (mathematics)',\n",
       "   'Artificial neural network',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1836465849',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '2108598243',\n",
       "   '2963073614',\n",
       "   '2963684088',\n",
       "   '1665214252',\n",
       "   '2963373786',\n",
       "   '2963840672',\n",
       "   '6908809']},\n",
       " {'id': '2271840356',\n",
       "  'title': 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '10,044',\n",
       "  'abstract': 'TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Martín Abadi',\n",
       "   'Ashish Agarwal',\n",
       "   'Paul Barham',\n",
       "   'Eugene Brevdo',\n",
       "   'Zhifeng Chen',\n",
       "   'Craig Citro',\n",
       "   'Gregory S. Corrado',\n",
       "   'Andy Davis',\n",
       "   'Jeffrey Dean',\n",
       "   'Matthieu Devin',\n",
       "   'Sanjay Ghemawat',\n",
       "   'Ian J. Goodfellow',\n",
       "   'Andrew Harp',\n",
       "   'Geoffrey Irving',\n",
       "   'Michael Isard',\n",
       "   'Yangqing Jia',\n",
       "   'Rafal Józefowicz',\n",
       "   'Lukasz Kaiser',\n",
       "   'Manjunath Kudlur',\n",
       "   'Josh Levenberg',\n",
       "   'Dan Mané',\n",
       "   'Rajat Monga',\n",
       "   'Sherry Moore',\n",
       "   'Derek Gordon Murray',\n",
       "   'Chris Olah',\n",
       "   'Mike Schuster',\n",
       "   'Jonathon Shlens',\n",
       "   'Benoit Steiner',\n",
       "   'Ilya Sutskever',\n",
       "   'Kunal Talwar',\n",
       "   'Paul A. Tucker',\n",
       "   'Vincent Vanhoucke',\n",
       "   'Vijay Vasudevan',\n",
       "   'Fernanda B. Viégas',\n",
       "   'Oriol Vinyals',\n",
       "   'Pete Warden',\n",
       "   'Martin Wattenberg',\n",
       "   'Martin Wicke',\n",
       "   'Yuan Yu',\n",
       "   'Xiaoqiang Zheng'],\n",
       "  'related_topics': ['Interface (computing)',\n",
       "   'Deep learning',\n",
       "   'Information extraction',\n",
       "   'Artificial neural network',\n",
       "   'Mobile device',\n",
       "   'CUDA',\n",
       "   'Robotics',\n",
       "   'Distributed computing',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Inference',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2097117768',\n",
       "   '1836465849',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2155893237',\n",
       "   '2064675550',\n",
       "   '2160815625',\n",
       "   '2168231600',\n",
       "   '2016053056',\n",
       "   '2131975293']},\n",
       " {'id': '648143168',\n",
       "  'title': 'Deep generative image models using a Laplacian pyramid of adversarial networks',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '1,941',\n",
       "  'abstract': 'In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Emily Denton 1, Soumith Chintala 2, Arthur Szlam 2, Rob Fergus 2'],\n",
       "  'related_topics': ['Pyramid',\n",
       "   'Real image',\n",
       "   'Parametric model',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Generative grammar',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Laplacian pyramid'],\n",
       "  'references': ['1836465849',\n",
       "   '2099471712',\n",
       "   '2108598243',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2100495367',\n",
       "   '2025768430',\n",
       "   '2125389028',\n",
       "   '2118858186',\n",
       "   '189596042']},\n",
       " {'id': '2949416428',\n",
       "  'title': 'Semi-Supervised Learning with Deep Generative Models',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '1,727',\n",
       "  'abstract': 'The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Diederik P. Kingma',\n",
       "   'Danilo J. Rezende',\n",
       "   'Shakir Mohamed',\n",
       "   'Max Welling'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Bayesian inference',\n",
       "   'Generative grammar',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Generalization',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1959608418',\n",
       "   '2146502635',\n",
       "   '2962897886',\n",
       "   '2335728318',\n",
       "   '2136504847',\n",
       "   '2107008379',\n",
       "   '1676820704',\n",
       "   '2407712691',\n",
       "   '2158049734',\n",
       "   '2122457239']},\n",
       " {'id': '2963685250',\n",
       "  'title': 'Weight normalization: a simple reparameterization to accelerate training of deep neural networks',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '1,093',\n",
       "  'abstract': 'We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Tim Salimans', 'Diederik P. Kingma'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Artificial neural network',\n",
       "   'Stochastic gradient descent',\n",
       "   'Reinforcement learning',\n",
       "   'Optimization problem',\n",
       "   'Speedup',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Deep neural networks'],\n",
       "  'references': ['2194775991',\n",
       "   '1836465849',\n",
       "   '2145339207',\n",
       "   '1959608418',\n",
       "   '3118608800',\n",
       "   '2064675550',\n",
       "   '2963911037',\n",
       "   '1533861849',\n",
       "   '104184427',\n",
       "   '2962897886']},\n",
       " {'id': '830076066',\n",
       "  'title': 'Semi-supervised learning with Ladder networks',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '930',\n",
       "  'abstract': 'We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training. Our work builds on top of the Ladder network proposed by Valpola [1] which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in semi-supervised MNIST and CIFAR-10 classification in addition to permutation-invariant MNIST classification with all labels.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Antti Rasmus 1, Harri Valpola 1, Mikko Honkala 2, Mathias Berglund 3, Tapani Raiko 3'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Deep learning',\n",
       "   'Competitive learning',\n",
       "   'Artificial neural network',\n",
       "   'Supervised learning',\n",
       "   'MNIST database',\n",
       "   'Backpropagation',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2964121744',\n",
       "   '1836465849',\n",
       "   '2095705004',\n",
       "   '2963207607',\n",
       "   '2100495367',\n",
       "   '2145094598',\n",
       "   '2294059674',\n",
       "   '2963382180',\n",
       "   '1479807131',\n",
       "   '1606347560']},\n",
       " {'id': '1487641199',\n",
       "  'title': 'Generative Moment Matching Networks',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '543',\n",
       "  'abstract': 'We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Yujia Li 1, Kevin Swersky 1, Rich Zemel 1, 2'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Generative topographic map',\n",
       "   'Multilayer perceptron',\n",
       "   'MNIST database',\n",
       "   'Backpropagation',\n",
       "   'Statistical hypothesis testing',\n",
       "   'Matching (statistics)',\n",
       "   'Minimax',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2097117768',\n",
       "   '2099471712',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2963542991',\n",
       "   '1959608418',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '1665214252']},\n",
       " {'id': '2911964244',\n",
       "  'title': 'Random Forests',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '74,907',\n",
       "  'abstract': 'Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'related_topics': ['Random forest',\n",
       "   'Multivariate random variable',\n",
       "   'Random subspace method',\n",
       "   'AdaBoost',\n",
       "   'Alternating decision tree',\n",
       "   'Tree (graph theory)',\n",
       "   'Ensemble learning',\n",
       "   'Gradient boosting',\n",
       "   'Statistics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2912934387',\n",
       "   '2112076978',\n",
       "   '1975846642',\n",
       "   '2152761983',\n",
       "   '2113242816',\n",
       "   '1605688901',\n",
       "   '2120240539',\n",
       "   '2099968818',\n",
       "   '2067885219',\n",
       "   '1580948147']},\n",
       " {'id': '2130325614',\n",
       "  'title': 'Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '2,986',\n",
       "  'abstract': 'There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Honglak Lee',\n",
       "   'Roger Grosse',\n",
       "   'Rajesh Ranganath',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Convolutional Deep Belief Networks',\n",
       "   'Deep belief network',\n",
       "   'Deep learning',\n",
       "   'Unsupervised learning',\n",
       "   'Generative model',\n",
       "   'Probabilistic logic',\n",
       "   'Inference',\n",
       "   'Pattern recognition',\n",
       "   'Scalability',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2166049352',\n",
       "   '2147800946',\n",
       "   '2145889472',\n",
       "   '2122922389',\n",
       "   '2139427956']},\n",
       " {'id': '2250539671',\n",
       "  'title': 'Glove: Global Vectors for Word Representation',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '21,544',\n",
       "  'abstract': 'Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Jeffrey Pennington 1, Richard Socher 2, Christopher Manning 1'],\n",
       "  'related_topics': ['Word2vec',\n",
       "   'Word embedding',\n",
       "   'Sparse matrix',\n",
       "   'SemEval',\n",
       "   'Sequence labeling',\n",
       "   'Word (computer architecture)',\n",
       "   'Context (language use)',\n",
       "   'Distributional semantics',\n",
       "   'Named-entity recognition',\n",
       "   'Sememe',\n",
       "   'Vocabulary mismatch',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2146502635',\n",
       "   '2158899491',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2141599568',\n",
       "   '2132339004',\n",
       "   '2118020653',\n",
       "   '2158139315']},\n",
       " {'id': '2131744502',\n",
       "  'title': 'Distributed Representations of Sentences and Documents',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '7,097',\n",
       "  'abstract': 'Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Quoc Le', 'Tomas Mikolov'],\n",
       "  'related_topics': ['Topic model',\n",
       "   'Feature vector',\n",
       "   'Feature (machine learning)',\n",
       "   'Sentiment analysis',\n",
       "   'Paragraph',\n",
       "   'Semantics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2158899491',\n",
       "   '2131744502',\n",
       "   '2251939518',\n",
       "   '2117130368',\n",
       "   '2141599568',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008']},\n",
       " {'id': '2251939518',\n",
       "  'title': 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '5,188',\n",
       "  'abstract': 'Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Richard Socher 1, Alex Perelygin',\n",
       "   'Jean Wu 1, Jason Chuang 2, Christopher D. Manning 1, Andrew Ng 1, Christopher Potts 1'],\n",
       "  'related_topics': ['Treebank', 'Principle of compositionality', 'Parsing'],\n",
       "  'references': ['2146502635',\n",
       "   '2097726431',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '1662133657',\n",
       "   '1889268436',\n",
       "   '2164019165',\n",
       "   '2097606805']},\n",
       " {'id': '2963748441',\n",
       "  'title': 'SQuAD: 100,000+ Questions for Machine Comprehension of Text',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '2,884',\n",
       "  'abstract': 'We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at this https URL',\n",
       "  'date': 2016,\n",
       "  'authors': ['Pranav Rajpurkar',\n",
       "   'Jian Zhang',\n",
       "   'Konstantin Lopyrev',\n",
       "   'Percy Liang'],\n",
       "  'related_topics': ['Question answering',\n",
       "   'Reading comprehension',\n",
       "   'Reading (process)',\n",
       "   'Comprehension',\n",
       "   'F1 score',\n",
       "   'Natural language processing',\n",
       "   'Set (abstract data type)',\n",
       "   'Computer science',\n",
       "   'Baseline (configuration management)',\n",
       "   'Dependency (UML)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2108598243',\n",
       "   '1544827683',\n",
       "   '1632114991',\n",
       "   '2125436846',\n",
       "   '2964267515',\n",
       "   '2962809918',\n",
       "   '2171278097',\n",
       "   '2962790689',\n",
       "   '2251818205',\n",
       "   '2251349042']},\n",
       " {'id': '2096192494',\n",
       "  'title': 'On the quantitative analysis of deep belief networks',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '493',\n",
       "  'abstract': \"Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.\",\n",
       "  'date': 2008,\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Iain Murray'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Approximate inference',\n",
       "   'Graphical model',\n",
       "   'Model selection',\n",
       "   'Importance sampling',\n",
       "   'Greedy algorithm',\n",
       "   'Hidden variable theory',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2099866409',\n",
       "   '2169415915',\n",
       "   '2158164339',\n",
       "   '66838807',\n",
       "   '2064630666',\n",
       "   '1513873506',\n",
       "   '2135094946']},\n",
       " {'id': '2081580037',\n",
       "  'title': 'WordNet: a lexical database for English',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '21,330',\n",
       "  'abstract': 'Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].',\n",
       "  'date': 1995,\n",
       "  'authors': ['George A. Miller'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Lexical database',\n",
       "   'eXtended WordNet',\n",
       "   'Machine-readable dictionary',\n",
       "   'Reverse dictionary',\n",
       "   'Natural language',\n",
       "   'VerbNet',\n",
       "   'Noun',\n",
       "   'Natural language processing',\n",
       "   'Synonym',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word-sense induction'],\n",
       "  'references': ['2102381086',\n",
       "   '2103318667',\n",
       "   '2065157922',\n",
       "   '1483126227',\n",
       "   '2017668967',\n",
       "   '1518768680',\n",
       "   '13823885']},\n",
       " {'id': '2165225968',\n",
       "  'title': 'Unsupervised learning of distributions on binary vectors using two layer networks',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '401',\n",
       "  'abstract': 'We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images.',\n",
       "  'date': 1991,\n",
       "  'authors': ['Yoav Freund', 'David Haussler'],\n",
       "  'related_topics': ['Wake-sleep algorithm',\n",
       "   'Feature learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Unsupervised learning',\n",
       "   'Online machine learning',\n",
       "   'Boltzmann machine',\n",
       "   'Semi-supervised learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2159080219',\n",
       "   '1652505363',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2293063825',\n",
       "   '1507849272',\n",
       "   '1964724001',\n",
       "   '2121407732',\n",
       "   '2725061391',\n",
       "   '2315016682']},\n",
       " {'id': '2806070179',\n",
       "  'title': 'Mask R-CNN',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '15,089',\n",
       "  'abstract': 'We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron .',\n",
       "  'date': 2020,\n",
       "  'authors': ['Kaiming He',\n",
       "   'Georgia Gkioxari',\n",
       "   'Piotr Dollar',\n",
       "   'Ross Girshick'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Image segmentation',\n",
       "   'Minimum bounding box',\n",
       "   'Pose',\n",
       "   'Segmentation',\n",
       "   'Feature extraction',\n",
       "   'Object (computer science)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '639708223',\n",
       "   '2102605133',\n",
       "   '1903029394',\n",
       "   '1536680647',\n",
       "   '2806070179',\n",
       "   '1861492603',\n",
       "   '2109255472',\n",
       "   '2565639579']},\n",
       " {'id': '1861492603',\n",
       "  'title': 'Microsoft COCO: Common Objects in Context',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '16,319',\n",
       "  'abstract': 'We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Tsung-Yi Lin 1, Michael Maire 2, Serge J. Belongie 1, James Hays 3, Pietro Perona 2, Deva Ramanan 4, Piotr Dollár 5, C. Lawrence Zitnick 5'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Minimum bounding box',\n",
       "   'Segmentation',\n",
       "   'User interface',\n",
       "   'Pascal (programming language)',\n",
       "   'Computer vision',\n",
       "   'Spotting',\n",
       "   'Closed captioning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2963542991',\n",
       "   '3118608800',\n",
       "   '2031489346',\n",
       "   '2110158442',\n",
       "   '2038721957']},\n",
       " {'id': '2565639579',\n",
       "  'title': 'Feature Pyramid Networks for Object Detection',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '7,283',\n",
       "  'abstract': 'Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Tsung-Yi Lin 1, Piotr Dollar 2, Ross Girshick 2, Kaiming He 2, Bharath Hariharan 2, Serge Belongie 1'],\n",
       "  'related_topics': ['Feature (computer vision)',\n",
       "   'Pyramid',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Semantic feature',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Artificial neural network',\n",
       "   'Robustness (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Detector',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '1901129140',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2161969291']},\n",
       " {'id': '3116298410',\n",
       "  'title': 'Self-Supervised Learning of Audio-Visual Objects from Video',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '22',\n",
       "  'abstract': 'Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets.Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Triantafyllos Afouras 1, Andrew Owens 2, Joon Son Chung 3, Andrew Zisserman 1'],\n",
       "  'related_topics': ['Face detection',\n",
       "   'Optical flow',\n",
       "   'Object (computer science)',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer vision',\n",
       "   'Set (psychology)',\n",
       "   'Computer science',\n",
       "   'Generality',\n",
       "   'Aggregate (data warehouse)',\n",
       "   'Detector',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2963173190',\n",
       "   '3034978746',\n",
       "   '3035524453',\n",
       "   '343636949',\n",
       "   '219040644',\n",
       "   '2808631503',\n",
       "   '602397586',\n",
       "   '2619697695',\n",
       "   '2962865004',\n",
       "   '3101577715']},\n",
       " {'id': '3099495704',\n",
       "  'title': 'CrossTransformers: spatially-aware few-shot transfer',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '18',\n",
       "  'abstract': 'Given new tasks with very little data$-$such as new classes in a classification problem or a domain shift in the input$-$performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, find coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classifier that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Carl Doersch 1, Ankush Gupta 1, Andrew Zisserman 2'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Task (computing)',\n",
       "   'Domain (software engineering)',\n",
       "   'Classifier (linguistics)',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Transfer (computing)',\n",
       "   'Small number',\n",
       "   'Shot (filmmaking)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3161838454',\n",
       "   '3119997354',\n",
       "   '3159056052',\n",
       "   '3139613640',\n",
       "   '3143315506',\n",
       "   '3135385999',\n",
       "   '3136670918',\n",
       "   '3139264293',\n",
       "   '3126009523',\n",
       "   '3122301478']},\n",
       " {'id': '3108316907',\n",
       "  'title': 'Contrastive Learning for Unpaired Image-to-Image Translation',\n",
       "  'reference_count': '87',\n",
       "  'citation_count': '50',\n",
       "  'abstract': 'In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so – maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each “domain” is only a single image.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Taesung Park 1, Alexei A. Efros 1, Richard Zhang 2, Jun-Yan Zhu 2'],\n",
       "  'related_topics': ['Image translation',\n",
       "   'Feature vector',\n",
       "   'Mutual information',\n",
       "   'Translation (geometry)',\n",
       "   'Image (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Domain (software engineering)',\n",
       "   'Point (geometry)',\n",
       "   'Computer science',\n",
       "   'Rest (physics)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2962835968',\n",
       "   '2964121744',\n",
       "   '2099471712',\n",
       "   '2108598243',\n",
       "   '2183341477',\n",
       "   '2133665775',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2100495367']},\n",
       " {'id': '3098053103',\n",
       "  'title': 'Reinforcement Learning with Augmented Data',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '84',\n",
       "  'abstract': 'Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at this https URL.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Michael Laskin 1, Kimin Lee 1, Adam Stooke 1, Lerrel Pinto 2, Pieter Abbeel 1, Aravind Srinivas 1'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Convolutional neural network',\n",
       "   'Benchmark (computing)',\n",
       "   'Generalization',\n",
       "   'Machine learning',\n",
       "   'Jitter',\n",
       "   'Code (cryptography)',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Pixel',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3131944163',\n",
       "   '3102762742',\n",
       "   '3137443434',\n",
       "   '3132674603',\n",
       "   '3134085768',\n",
       "   '3094246835',\n",
       "   '3107857059',\n",
       "   '3134032827',\n",
       "   '3133595589',\n",
       "   '3131871335']},\n",
       " {'id': '3132401450',\n",
       "  'title': 'How to represent part-whole hierarchies in a neural network',\n",
       "  'reference_count': '59',\n",
       "  'citation_count': '5',\n",
       "  'abstract': 'This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language',\n",
       "  'date': 2021,\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Parse tree',\n",
       "   'Feature learning',\n",
       "   'Artificial neural network',\n",
       "   'Parsing',\n",
       "   'Interpretability',\n",
       "   'Representation (mathematics)',\n",
       "   'Hierarchy',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)'],\n",
       "  'references': ['2963341956',\n",
       "   '2963403868',\n",
       "   '3097096317',\n",
       "   '2116064496',\n",
       "   '2963703618',\n",
       "   '1821462560',\n",
       "   '2963069010',\n",
       "   '1997063559',\n",
       "   '3034978746',\n",
       "   '3035524453']},\n",
       " {'id': '3125947392',\n",
       "  'title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '28',\n",
       "  'abstract': 'We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Amy Zhang 1, Rowan Thomas McAllister 2, Roberto Calandra 1, Yarin Gal 3, Sergey Levine 2'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Reinforcement learning',\n",
       "   'Bisimulation',\n",
       "   'State space',\n",
       "   'Domain knowledge',\n",
       "   'Generalization',\n",
       "   'Invariant (physics)',\n",
       "   'Machine learning',\n",
       "   'Causal inference',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3129170303', '3139419546', '3162945626', '3134032827']},\n",
       " {'id': '3145385912',\n",
       "  'title': 'Spatiotemporal Contrastive Video Representation Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '29',\n",
       "  'abstract': 'We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentation for video self-supervised learning and find both spatial and temporal information are crucial. We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on the clips that are distant in a video. On the Kinetics-600 dataset, a linear classifier trained on the representations learned by CVRL achieves 70.4% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7% and SimCLR unsupervised pre-training by 18.8% using the same inflated R3D-50. The performance of CVRL can be further improved to 72.6% with a larger R3D-50 (4$\\\\times$ filters) backbone, significantly closing the gap between unsupervised and supervised video representation learning.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Rui Qian 1, Tianjian Meng 2, Boqing Gong 2, Ming-Hsuan Yang',\n",
       "   'Huisheng Wang 2, Serge J. Belongie 1, Yin Cui 2'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Linear classifier',\n",
       "   'Pattern recognition',\n",
       "   'Closing (morphology)',\n",
       "   'Frame (networking)',\n",
       "   'Embedding',\n",
       "   'Computer science',\n",
       "   'Space (commercial competition)',\n",
       "   'Sampling (signal processing)',\n",
       "   'CLIPS',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3159394092']},\n",
       " {'id': '3154503084',\n",
       "  'title': 'Graph Contrastive Learning with Adaptive Augmentation',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '9',\n",
       "  'abstract': 'Recently, contrastive learning (CL) has emerged as a successful method for unsupervised graph representation learning. Most graph CL methods first perform stochastic augmentation on the input graph to obtain two graph views and maximize the agreement of representations in the two views. Despite the prosperous development of graph CL methods, the design of graph augmentation schemes—a crucial component in CL—remains rarely explored. We argue that the data augmentation schemes should preserve intrinsic structures and attributes of graphs, which will force the model to learn representations that are insensitive to perturbation on unimportant nodes and edges. However, most existing methods adopt uniform data augmentation schemes, like uniformly dropping edges and uniformly shuffling features, leading to suboptimal performance. In this paper, we propose a novel graph contrastive representation learning method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Specifically, on the topology level, we design augmentation schemes based on node centrality measures to highlight important connective structures. On the node attribute level, we corrupt node features by adding more noise to unimportant node features, to enforce the model to recognize underlying semantic information. We perform extensive experiments of node classification on a variety of real-world datasets. Experimental results demonstrate that our proposed method consistently outperforms existing state-of-the-art baselines and even surpasses some supervised counterparts, which validates the effectiveness of the proposed contrastive framework with adaptive augmentation.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Yanqiao Zhu 1, Yichen Xu 2, Feng Yu 3, Qiang Liu 1, Shu Wu 1, Liang Wang 1'],\n",
       "  'related_topics': ['Graph (abstract data type)',\n",
       "   'Feature learning',\n",
       "   'Unsupervised learning',\n",
       "   'Centrality',\n",
       "   'Node (networking)',\n",
       "   'Theoretical computer science',\n",
       "   'Topology (electrical circuits)',\n",
       "   'Two-graph',\n",
       "   'Computer science',\n",
       "   'Noise (video)'],\n",
       "  'references': ['3135138557', '3134210100', '3132869322', '3154313998']},\n",
       " {'id': '3122924117',\n",
       "  'title': 'Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning',\n",
       "  'reference_count': '59',\n",
       "  'citation_count': '11',\n",
       "  'abstract': 'State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in the few-shot learning settings, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. The new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled data.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Beliz Gunel 1, Jingfei Du 2, Alexis Conneau 2, Veselin Stoyanov 3'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Generalization',\n",
       "   'Natural language understanding',\n",
       "   'Robustness (computer science)',\n",
       "   'Machine learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Class (biology)',\n",
       "   'Memory bank',\n",
       "   'Noise (video)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2963341956',\n",
       "   '2117539524',\n",
       "   '2183341477',\n",
       "   '3118608800',\n",
       "   '2096733369',\n",
       "   '1821462560',\n",
       "   '2965373594',\n",
       "   '1840435438',\n",
       "   '1544827683']},\n",
       " {'id': '2053186076',\n",
       "  'title': 'Nonlinear dimensionality reduction by locally linear embedding.',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '16,334',\n",
       "  'abstract': 'Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Sam T. Roweis 1, Lawrence K. Saul 2'],\n",
       "  'related_topics': ['Diffusion map',\n",
       "   'Dimensionality reduction',\n",
       "   't-distributed stochastic neighbor embedding',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Isomap',\n",
       "   'Elastic map',\n",
       "   'Curse of dimensionality',\n",
       "   'Local tangent space alignment',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1902027874',\n",
       "   '2610857016',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '2121122425',\n",
       "   '2122538988',\n",
       "   '2047870719',\n",
       "   '2070320140',\n",
       "   '1513400187',\n",
       "   '2019020850']},\n",
       " {'id': '2001141328',\n",
       "  'title': 'A Global Geometric Framework for Nonlinear Dimensionality Reduction',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '14,853',\n",
       "  'abstract': 'Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.',\n",
       "  'date': 2000,\n",
       "  'authors': ['J. B. Tenenbaum 1, V. de Silva 1, J. C. Langford 2'],\n",
       "  'related_topics': ['Diffusion map',\n",
       "   'Dimensionality reduction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   't-distributed stochastic neighbor embedding',\n",
       "   'Isomap',\n",
       "   'Curse of dimensionality',\n",
       "   'Sammon mapping',\n",
       "   'Principal component analysis',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2099741732',\n",
       "   '2108384452',\n",
       "   '2587818897',\n",
       "   '2123977795',\n",
       "   '2107636931',\n",
       "   '2122538988',\n",
       "   '2047870719',\n",
       "   '2070320140',\n",
       "   '2032647857']},\n",
       " {'id': '2293063825',\n",
       "  'title': 'Neural networks and physical systems with emergent collective computational abilities',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '26,202',\n",
       "  'abstract': 'Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.',\n",
       "  'date': 1999,\n",
       "  'authors': ['John J. Hopfield'],\n",
       "  'related_topics': ['Physical system',\n",
       "   'Artificial neural network',\n",
       "   'Asynchronous communication',\n",
       "   'Generalization',\n",
       "   'State (computer science)',\n",
       "   'Parallel processing (DSP implementation)',\n",
       "   'Error detection and correction',\n",
       "   'Categorization',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['1594524188', '2086789740', '2970228278', '2076870593']},\n",
       " {'id': '2121122425',\n",
       "  'title': 'Dimension reduction by local principal component analysis',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '787',\n",
       "  'abstract': 'Reducing or eliminating statistical redundancy between the components of high-dimensional vector data enables a lower-dimensional representation without significant loss of information. Recognizing the limitations of principal component analysis (PCA), researchers in the statistics and neural network communities have developed nonlinear extensions of PCA. This article develops a local linear approach to dimension reduction that provides accurate representations and is fast to compute. We exercise the algorithms on speech and image data, and compare performance with PCA and with neural network implementations of nonlinear PCA. We find that both nonlinear techniques can provide more accurate representations than PCA and show that the local linear techniques outperform neural network implementations.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Nandakishore Kambhatla', 'Todd K. Leen'],\n",
       "  'related_topics': ['Sparse PCA',\n",
       "   'Principal component analysis',\n",
       "   'Dimensionality reduction',\n",
       "   'Artificial neural network',\n",
       "   'Redundancy (engineering)',\n",
       "   'Representation (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Nonlinear system',\n",
       "   'Algorithm',\n",
       "   'Image (mathematics)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2137983211',\n",
       "   '3004157836',\n",
       "   '2140196014',\n",
       "   '1634005169',\n",
       "   '3146803896',\n",
       "   '1971735090',\n",
       "   '2913399920',\n",
       "   '2122538988',\n",
       "   '2096710051',\n",
       "   '2017977879']},\n",
       " {'id': '2032647857',\n",
       "  'title': 'Replicator neural networks for universal optimal source coding.',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '178',\n",
       "  'abstract': 'Replicator neural networks self-organize by using their inputs as desired outputs; they internally form a compressed representation for the input data. A theorem shows that a class of replicator networks can, through the minimization of mean squared reconstruction error (for instance, by training on raw data examples), carry out optimal data compression for arbitrary data vector sources. Data manifolds, a new general model of data sources, are then introduced and a second theorem shows that, in a practically important limiting case, optimal-compression replicator networks operate by creating an essentially unique natural coordinate system for the manifold.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Robert Hecht-Nielsen'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Data compression',\n",
       "   'Manifold',\n",
       "   'Backpropagation',\n",
       "   'Minification',\n",
       "   'Algorithm',\n",
       "   'Source code',\n",
       "   'Coding (social sciences)',\n",
       "   'Essentially unique',\n",
       "   'Computer science'],\n",
       "  'references': ['2137983211',\n",
       "   '2166116275',\n",
       "   '1993845689',\n",
       "   '2122538988',\n",
       "   '5731987',\n",
       "   '2142228262',\n",
       "   '2063971957',\n",
       "   '2079782346',\n",
       "   '2089419199',\n",
       "   '2162604518']},\n",
       " {'id': '2021774695',\n",
       "  'title': 'Learning sets of filters using back-propagation',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '160',\n",
       "  'abstract': 'Abstract A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in “weight space”. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the learning procedure is the way it scales as the size of the task and the network increases. Some preliminary results on scaling are reported and it is shown how the magnitude of the optimal weight changes depends on the fan-in of the units. Additional results show how the amount of interaction between the weights affects the learning speed. The paper is concluded with a discussion of the difficulties that are likely to be encounted in applying back-propagation to more realistic problems in speech recognition, and some promising approaches to overcoming these difficulties.',\n",
       "  'date': 1987,\n",
       "  'authors': ['David C. Plaut', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Semi-supervised learning',\n",
       "   'Backpropagation',\n",
       "   'Edge detection',\n",
       "   'Noise (video)',\n",
       "   'Task (project management)',\n",
       "   'Set (psychology)',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '1507849272',\n",
       "   '2155487652',\n",
       "   '1995169133',\n",
       "   '2591802459',\n",
       "   '2010581677']},\n",
       " {'id': '2156718197',\n",
       "  'title': 'Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '4,981',\n",
       "  'abstract': 'Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Mikhail Belkin', 'Partha Niyogi'],\n",
       "  'related_topics': ['Spectral clustering',\n",
       "   'Manifold alignment',\n",
       "   'Laplacian matrix',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Laplacian smoothing',\n",
       "   'Isomap',\n",
       "   'Cluster analysis',\n",
       "   'Laplace operator',\n",
       "   'Topology',\n",
       "   'Mathematics'],\n",
       "  'references': ['2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328',\n",
       "   '1578099820',\n",
       "   '108654854']},\n",
       " {'id': '2125637308',\n",
       "  'title': 'Random Walks for Image Segmentation',\n",
       "  'reference_count': '76',\n",
       "  'citation_count': '2,817',\n",
       "  'abstract': 'A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs',\n",
       "  'date': 2006,\n",
       "  'authors': ['L. Grady'],\n",
       "  'related_topics': ['Random walker algorithm',\n",
       "   'Image segmentation',\n",
       "   'Scale-space segmentation',\n",
       "   'Pixel',\n",
       "   'Graph theory',\n",
       "   'Image processing',\n",
       "   'Cut',\n",
       "   'Discrete space',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296319761',\n",
       "   '3029645440',\n",
       "   '2177274842',\n",
       "   '2121947440',\n",
       "   '2124351162',\n",
       "   '2143516773',\n",
       "   '2104095591',\n",
       "   '1578099820',\n",
       "   '2169551590',\n",
       "   '2150134853']},\n",
       " {'id': '2139823104',\n",
       "  'title': 'Semi-supervised learning using Gaussian fields and harmonic functions',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '4,318',\n",
       "  'abstract': \"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.\",\n",
       "  'date': 2003,\n",
       "  'authors': ['Xiaojin Zhu', 'Zoubin Ghahramani', 'John Lafferty'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Empirical risk minimization',\n",
       "   'Stability (learning theory)',\n",
       "   'Supervised learning',\n",
       "   'Gaussian random field',\n",
       "   'Random graph',\n",
       "   'Online machine learning',\n",
       "   'Belief propagation',\n",
       "   'Transduction (machine learning)',\n",
       "   'Spectral graph theory',\n",
       "   'Gaussian',\n",
       "   'Feature selection',\n",
       "   'Random walk',\n",
       "   'Prior probability',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization error',\n",
       "   'Graph'],\n",
       "  'references': ['2121947440',\n",
       "   '2143516773',\n",
       "   '2165874743',\n",
       "   '2154579312',\n",
       "   '2122837498',\n",
       "   '1511160855',\n",
       "   '1585385982',\n",
       "   '1979711143',\n",
       "   '2113592823',\n",
       "   '200434350']},\n",
       " {'id': '2137570937',\n",
       "  'title': 'Dimensionality Reduction: A Comparative Review',\n",
       "  'reference_count': '160',\n",
       "  'citation_count': '1,359',\n",
       "  'abstract': 'In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Laurens van der Maaten', 'Eric Postma 1, Jaap van den Herik 2'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Nonlinear system',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Variety (cybernetics)',\n",
       "   'Scaling',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2296319761',\n",
       "   '2136922672',\n",
       "   '1880262756',\n",
       "   '2100495367',\n",
       "   '2187089797',\n",
       "   '1497256448',\n",
       "   '2072128103',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '2116064496']},\n",
       " {'id': '2157444450',\n",
       "  'title': 'Stochastic Neighbor Embedding',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '1,549',\n",
       "  'abstract': 'We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional \"images\" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \"bank\", to have versions close to the images of both \"river\" and \"finance\" without forcing the images of outdoor concepts to be located close to those of corporate concepts.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Geoffrey E. Hinton', 'Sam T. Roweis'],\n",
       "  'related_topics': ['t-distributed stochastic neighbor embedding',\n",
       "   'Dimensionality reduction',\n",
       "   'Object (grammar)',\n",
       "   'Probabilistic logic',\n",
       "   'Probability distribution',\n",
       "   'Embedding',\n",
       "   'Forcing (recursion theory)',\n",
       "   'Gaussian',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2053186076',\n",
       "   '2001141328',\n",
       "   '2148694408',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '23758216',\n",
       "   '2100659887',\n",
       "   '2159174312',\n",
       "   '2106346128']},\n",
       " {'id': '1742512077',\n",
       "  'title': 'Nonlinear Dimensionality Reduction',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,684',\n",
       "  'abstract': 'Methods of dimensionality reduction provide a way to understand and visualize the structure of complex data sets. Traditional methods like principal component analysis and classical metric multidimensional scaling suffer from being based on linear models. Until recently, very few methods were able to reduce the data dimensionality in a nonlinear way. However, since the late nineties, many new methods have been developed and nonlinear dimensionality reduction, also called manifold learning, has become a hot topic. New advances that account for this rapid growth are, e.g. the use of graphs to represent the manifold topology, and the use of new metrics like the geodesic distance. In addition, new optimization schemes, based on kernel techniques and spectral decomposition, have lead to spectral embedding, which encompasses many of the recently developed methods. This book describes existing and advanced methods to reduce the dimensionality of numerical databases. For each method, the description starts from intuitive ideas, develops the necessary mathematical details, and ends by outlining the algorithmic implementation. Methods are compared with each other with the help of different illustrative examples. The purpose of the book is to summarize clear facts and ideas about well-known methods as well as recent developments in the topic of nonlinear dimensionality reduction. With this goal in mind, methods are all described from a unifying point of view, in order to highlight their respective strengths and shortcomings. The book is primarily intended for statisticians, computer scientists and data analysts. It is also accessible to other practitioners having a basic background in statistics and/or computational learning, like psychologists (in psychometry) and economists.',\n",
       "  'date': 2007,\n",
       "  'authors': ['John A. Lee', 'Michel Verleysen'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Curse of dimensionality',\n",
       "   'Linear model',\n",
       "   'Metric (mathematics)',\n",
       "   'Kernel (statistics)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Principal component analysis',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2076063813',\n",
       "   '2187089797',\n",
       "   '2109574129',\n",
       "   '2137570937',\n",
       "   '2963460103',\n",
       "   '255556494',\n",
       "   '1591018827',\n",
       "   '2162584119']},\n",
       " {'id': '2116064496',\n",
       "  'title': 'Training products of experts by minimizing contrastive divergence',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '5,201',\n",
       "  'abstract': 'It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \"expert\" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \"contrastive divergence\" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Product of experts',\n",
       "   'Deep belief network',\n",
       "   'Conditional independence',\n",
       "   'Latent variable',\n",
       "   'Inference',\n",
       "   'Probability distribution',\n",
       "   'Boltzmann machine',\n",
       "   'Data type',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['1652505363',\n",
       "   '1997063559',\n",
       "   '2096175520',\n",
       "   '1746680969',\n",
       "   '1993845689',\n",
       "   '2165225968',\n",
       "   '2083380015',\n",
       "   '2114153178',\n",
       "   '1547224907',\n",
       "   '2101706260']},\n",
       " {'id': '2134557905',\n",
       "  'title': 'Learning methods for generic object recognition with invariance to pose and lighting',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '1,467',\n",
       "  'abstract': 'We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Y. LeCun 1, Fu Jie Huang 1, L. Bottou 2'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Image segmentation',\n",
       "   'Support vector machine',\n",
       "   'Feature extraction',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Clutter',\n",
       "   'Grayscale',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2217896605',\n",
       "   '2124087378',\n",
       "   '2124351082',\n",
       "   '2123977795',\n",
       "   '2155511848',\n",
       "   '2160225842',\n",
       "   '2295106276',\n",
       "   '2141376824']},\n",
       " {'id': '2099866409',\n",
       "  'title': 'Restricted Boltzmann machines for collaborative filtering',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '2,078',\n",
       "  'abstract': \"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.\",\n",
       "  'date': 2007,\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Andriy Mnih', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'Boltzmann machine',\n",
       "   'Collaborative filtering',\n",
       "   'Graphical model',\n",
       "   'Word error rate',\n",
       "   'Inference',\n",
       "   'Machine learning',\n",
       "   'Data set',\n",
       "   'Singular value decomposition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1612003148',\n",
       "   '2122090912',\n",
       "   '2158164339',\n",
       "   '2124914669',\n",
       "   '205159212',\n",
       "   '2165395308']},\n",
       " {'id': '2536626143',\n",
       "  'title': 'Attribute and simile classifiers for face verification',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '1,679',\n",
       "  'abstract': 'We present two novel methods for face verification. Our first method - “attribute” classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - “simile” classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Neeraj Kumar',\n",
       "   'Alexander C. Berg',\n",
       "   'Peter N. Belhumeur',\n",
       "   'Shree K. Nayar'],\n",
       "  'related_topics': ['Feature extraction',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Data set',\n",
       "   'Visualization',\n",
       "   'Pattern recognition',\n",
       "   'Expression (mathematics)',\n",
       "   'Similarity (geometry)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2119821739',\n",
       "   '1782590233',\n",
       "   '1989702938',\n",
       "   '2112076978',\n",
       "   '2033419168',\n",
       "   '2123921160',\n",
       "   '2098947662',\n",
       "   '2905573712',\n",
       "   '2155759509']},\n",
       " {'id': '2156909104',\n",
       "  'title': 'The Nature of Statistical Learning Theory',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '67,802',\n",
       "  'abstract': 'Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Vladimir N. Vapnik'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Statistical learning theory',\n",
       "   'Computational learning theory',\n",
       "   'Instance-based learning',\n",
       "   'Proactive learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Unsupervised learning',\n",
       "   'Probably approximately correct learning',\n",
       "   'Machine learning',\n",
       "   'Mathematics'],\n",
       "  'references': ['2140190241',\n",
       "   '2148603752',\n",
       "   '2164278908',\n",
       "   '2310919327',\n",
       "   '2076063813',\n",
       "   '2129812935',\n",
       "   '1746819321',\n",
       "   '1570448133',\n",
       "   '2072128103',\n",
       "   '2139212933']},\n",
       " {'id': '2296616510',\n",
       "  'title': 'Compressed sensing',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '17,733',\n",
       "  'abstract': 'Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel\\'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel\\'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces',\n",
       "  'date': 2003,\n",
       "  'authors': ['D.L. Donoho'],\n",
       "  'related_topics': ['Basis pursuit',\n",
       "   'Orthonormal basis',\n",
       "   'Linear combination',\n",
       "   'Matching pursuit',\n",
       "   'Basis pursuit denoising',\n",
       "   'Wavelet',\n",
       "   'Linear subspace',\n",
       "   'Restricted isometry property',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2145096794',\n",
       "   '2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2097323375',\n",
       "   '2136235822',\n",
       "   '2147656689',\n",
       "   '2012365979',\n",
       "   '2096613063',\n",
       "   '2050880896']},\n",
       " {'id': '2129131372',\n",
       "  'title': 'Decoding by linear programming',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '7,881',\n",
       "  'abstract': 'This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.',\n",
       "  'date': 2005,\n",
       "  'authors': ['E.J. Candes 1, T. Tao 2'],\n",
       "  'related_topics': ['Underdetermined system',\n",
       "   'Linear code',\n",
       "   'Sigma',\n",
       "   'Restricted isometry property',\n",
       "   'Linear system',\n",
       "   'Convex optimization',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Matching pursuit',\n",
       "   'System of linear equations',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296319761',\n",
       "   '2296616510',\n",
       "   '2145096794',\n",
       "   '2129638195',\n",
       "   '2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2050834445',\n",
       "   '2154332973',\n",
       "   '2136235822']},\n",
       " {'id': '2119821739',\n",
       "  'title': 'Support-Vector Networks',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '48,716',\n",
       "  'abstract': 'The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Corinna Cortes', 'Vladimir Vapnik'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Feature vector',\n",
       "   'Computational learning theory',\n",
       "   'Online machine learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Linear classifier',\n",
       "   'Relevance vector machine',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '1498436455',\n",
       "   '2087347434',\n",
       "   '2154579312',\n",
       "   '1530699444',\n",
       "   '2168228682',\n",
       "   '2504871398',\n",
       "   '1568787085',\n",
       "   '5594912',\n",
       "   '2322002063']},\n",
       " {'id': '2161969291',\n",
       "  'title': 'Histograms of oriented gradients for human detection',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '36,647',\n",
       "  'abstract': 'We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.',\n",
       "  'date': 2005,\n",
       "  'authors': ['N. Dalal', 'B. Triggs'],\n",
       "  'related_topics': ['Histogram of oriented gradients',\n",
       "   'Local binary patterns',\n",
       "   'GLOH',\n",
       "   'Object detection',\n",
       "   'Feature (computer vision)',\n",
       "   'Pedestrian detection',\n",
       "   'Feature extraction',\n",
       "   'Normalization (image processing)',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Implicit Shape Model',\n",
       "   'Normalization (statistics)',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Caltech 101',\n",
       "   'Haar-like features',\n",
       "   'LabelMe',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Support vector machine',\n",
       "   'Histogram',\n",
       "   'Motion History Images',\n",
       "   'Pattern recognition',\n",
       "   'Traffic sign recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Feature descriptor',\n",
       "   'Fisher vector'],\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '2145072179',\n",
       "   '1576520375',\n",
       "   '2152473410',\n",
       "   '2172188317',\n",
       "   '1992825118',\n",
       "   '1608462934',\n",
       "   '2295106276',\n",
       "   '2156539399']},\n",
       " {'id': '2162915993',\n",
       "  'title': 'Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '9,939',\n",
       "  'abstract': 'This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba\\x92s \"gist\" and Lowe\\x92s SIFT descriptors.',\n",
       "  'date': 2006,\n",
       "  'authors': ['S. Lazebnik 1, C. Schmid 2, J. Ponce 3'],\n",
       "  'related_topics': ['Pyramid (image processing)',\n",
       "   'Pyramid',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Image texture',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image segmentation',\n",
       "   'Caltech 101',\n",
       "   'Visual dictionary',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Histogram',\n",
       "   'LabelMe',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1880262756',\n",
       "   '2154422044',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2104978738',\n",
       "   '2914885528',\n",
       "   '2168002178',\n",
       "   '2134731454',\n",
       "   '2165828254']},\n",
       " {'id': '2097018403',\n",
       "  'title': 'Linear spatial pyramid matching using sparse coding for image classification',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '3,793',\n",
       "  'abstract': 'Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Jianchao Yang 1, Kai Yu 2, Yihong Gong 2, Thomas Huang 1'],\n",
       "  'related_topics': ['Support vector machine',\n",
       "   'Kernel (image processing)',\n",
       "   'Contextual image classification',\n",
       "   'Vector quantization',\n",
       "   'Caltech 101',\n",
       "   'Neural coding',\n",
       "   'Image segmentation',\n",
       "   'Histogram',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153635508',\n",
       "   '2162915993',\n",
       "   '1576445103',\n",
       "   '2153663612',\n",
       "   '2107034620',\n",
       "   '1566135517',\n",
       "   '2166049352',\n",
       "   '2113606819',\n",
       "   '2161516371',\n",
       "   '1624854622']},\n",
       " {'id': '2166049352',\n",
       "  'title': 'Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '8,261',\n",
       "  'abstract': 'Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Li Fei-Fei 1, Rob Fergus 2, Pietro Perona 3'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Learning object',\n",
       "   'Caltech 101',\n",
       "   'Bayesian inference',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Object (computer science)',\n",
       "   'Categorization',\n",
       "   'Bayesian probability',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2124386111',\n",
       "   '2154422044',\n",
       "   '2124087378',\n",
       "   '2155511848',\n",
       "   '1516111018',\n",
       "   '1949116567',\n",
       "   '1746680969',\n",
       "   '2567948266',\n",
       "   '1699734612']},\n",
       " {'id': '1639032689',\n",
       "  'title': 'Genetic algorithms in search, optimization, and machine learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '106,990',\n",
       "  'abstract': 'From the Publisher: This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.',\n",
       "  'date': 1988,\n",
       "  'authors': ['David E. Goldberg'],\n",
       "  'related_topics': ['Genetic representation',\n",
       "   'Genetic programming',\n",
       "   'Pascal (programming language)',\n",
       "   'Evolutionary computation',\n",
       "   'Computer programming',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Computer techniques'],\n",
       "  'references': ['1992419399',\n",
       "   '2165299997',\n",
       "   '2097571405',\n",
       "   '2106334424',\n",
       "   '2168081761',\n",
       "   '2017337590',\n",
       "   '1999284878',\n",
       "   '2167101736',\n",
       "   '2165171393',\n",
       "   '1501500081']},\n",
       " {'id': '2154642048',\n",
       "  'title': 'Learning internal representations by error propagation',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '31,447',\n",
       "  'abstract': 'This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion',\n",
       "  'date': 1987,\n",
       "  'authors': ['D. E. Rumelhart', 'G. E. Hinton', 'R. J. Williams'],\n",
       "  'related_topics': ['Delta rule',\n",
       "   'Semi-supervised learning',\n",
       "   'Backpropagation'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '1535810436',\n",
       "   '1507849272',\n",
       "   '2101926813',\n",
       "   '2073257493',\n",
       "   '2021878536',\n",
       "   '1490454746',\n",
       "   '2115647291',\n",
       "   '1505136099']},\n",
       " {'id': '3017143921',\n",
       "  'title': 'Pattern classification and scene analysis',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '21,827',\n",
       "  'abstract': 'Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.',\n",
       "  'date': 1972,\n",
       "  'authors': ['Richard O. Duda', 'Peter E. Hart'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Cluster analysis',\n",
       "   'Linear discriminant analysis',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Perspective (graphical)',\n",
       "   'Pattern recognition',\n",
       "   'Nonparametric statistics',\n",
       "   'Bayes estimator',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Scene analysis'],\n",
       "  'references': ['2310919327',\n",
       "   '1746819321',\n",
       "   '1570448133',\n",
       "   '2163352848',\n",
       "   '2110158442',\n",
       "   '2067191022',\n",
       "   '2117812871',\n",
       "   '1992419399',\n",
       "   '2121647436']},\n",
       " {'id': '2100677568',\n",
       "  'title': 'Learning to Predict by the Methods of Temporal Differences',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '6,387',\n",
       "  'abstract': \"This article introduces a class of incremental learning procedures specialized for prediction – that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.\",\n",
       "  'date': 1988,\n",
       "  'authors': ['Richard S. Sutton'],\n",
       "  'related_topics': ['Temporal difference learning',\n",
       "   'Supervised learning',\n",
       "   'Heuristic',\n",
       "   'sort',\n",
       "   'Connectionism',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Class (computer programming)',\n",
       "   'Convergence (routing)',\n",
       "   'Computer science',\n",
       "   'Computation'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2895674046',\n",
       "   '1535810436',\n",
       "   '1507849272',\n",
       "   '2178806388',\n",
       "   '1596324102',\n",
       "   '1583833196',\n",
       "   '1569296262',\n",
       "   '2075379212']},\n",
       " {'id': '1535810436',\n",
       "  'title': 'Adaptive switching circuits',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '5,146',\n",
       "  'abstract': '',\n",
       "  'date': 1987,\n",
       "  'authors': ['Bernard Widrow', 'Marcian E. Hoff'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Electronic circuit',\n",
       "   'Electronic engineering',\n",
       "   'Adaptive switching'],\n",
       "  'references': ['2154642048',\n",
       "   '2121863487',\n",
       "   '2042264548',\n",
       "   '114517082',\n",
       "   '2604319603',\n",
       "   '2166851633',\n",
       "   '2025605741',\n",
       "   '1570963478',\n",
       "   '2098257210']},\n",
       " {'id': '1603765807',\n",
       "  'title': 'Parallel and Distributed Computation: Numerical Methods',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '8,514',\n",
       "  'abstract': '',\n",
       "  'date': 1988,\n",
       "  'authors': ['Dimitri P. Bertsekas', 'John N. Tsitsiklis'],\n",
       "  'related_topics': ['Distributed design patterns',\n",
       "   'Distributed algorithm',\n",
       "   'Computation',\n",
       "   'Dynamic programming',\n",
       "   'Computational science',\n",
       "   'Computer science',\n",
       "   'Numerical analysis'],\n",
       "  'references': ['2164278908',\n",
       "   '2121863487',\n",
       "   '2096544401',\n",
       "   '2010630450',\n",
       "   '2963433607',\n",
       "   '78077100',\n",
       "   '2044212084',\n",
       "   '2114791779',\n",
       "   '2951781666']},\n",
       " {'id': '1569320505',\n",
       "  'title': 'Adaptive filtering prediction and control',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '6,774',\n",
       "  'abstract': 'This unified survey focuses on linear discrete-time systems and explores the natural extensions to nonlinear systems. In keeping with the importance of computers to practical applications, the authors emphasize discrete-time systems. Their approach summarizes the theoretical and practical aspects of a large class of adaptive algorithms.1984 edition.',\n",
       "  'date': 1983,\n",
       "  'authors': ['Graham C. Goodwin', 'Kwai Sang Sin'],\n",
       "  'related_topics': ['Control theory',\n",
       "   'Adaptive filter',\n",
       "   'Nonlinear system',\n",
       "   'Control engineering',\n",
       "   'Computer science',\n",
       "   'Control (linguistics)',\n",
       "   'Natural (music)',\n",
       "   'Large class'],\n",
       "  'references': ['2121863487',\n",
       "   '2019207321',\n",
       "   '2161406034',\n",
       "   '2749680651',\n",
       "   '2148885430',\n",
       "   '2095227410',\n",
       "   '2119717200',\n",
       "   '1481420047',\n",
       "   '2044535354',\n",
       "   '2131215403']},\n",
       " {'id': '94523489',\n",
       "  'title': 'Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,973',\n",
       "  'abstract': \"Abstract : The relationship between 'learning' in adaptive layered networks and the fitting of data with high dimensional surfaces is discussed. This leads naturally to a picture of 'generalization in terms of interpolation between known data points and suggests a rational approach to the theory of such networks. A class of adaptive networks is identified which makes the interpolation scheme explicit. This class has the property that learning is equivalent to the solution of a set of linear equations. These networks thus represent nonlinear relationships while having a guaranteed learning rule. Great Britain.\",\n",
       "  'date': 1988,\n",
       "  'authors': ['David S. Broomhead', 'David Lowe'],\n",
       "  'related_topics': ['Interpolation',\n",
       "   'Linear interpolation',\n",
       "   'Bilinear interpolation',\n",
       "   'Trilinear interpolation',\n",
       "   'Learning rule',\n",
       "   'Radial basis function network',\n",
       "   'Generalization',\n",
       "   'Hierarchical RBF',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2121863487',\n",
       "   '1993717606',\n",
       "   '2093229042',\n",
       "   '2143956139',\n",
       "   '2155399784',\n",
       "   '2149723649',\n",
       "   '2809684781',\n",
       "   '2133321814',\n",
       "   '1998442441',\n",
       "   '1553004968']},\n",
       " {'id': '2178806388',\n",
       "  'title': 'Some studies in machine learning using the game of checkers',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '3,434',\n",
       "  'abstract': 'Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done by verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Arthur L. Samuel'],\n",
       "  'related_topics': ['Rote learning',\n",
       "   'Artificial intelligence',\n",
       "   'Period (music)',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Course (navigation)',\n",
       "   'Work (physics)',\n",
       "   'Sense of direction'],\n",
       "  'references': ['1542941925',\n",
       "   '1592847719',\n",
       "   '2103626435',\n",
       "   '2019003829',\n",
       "   '1547925194',\n",
       "   '2104753538',\n",
       "   '2117355432',\n",
       "   '2158091072',\n",
       "   '2995201943']},\n",
       " {'id': '2126316555',\n",
       "  'title': 'A Survey of Monte Carlo Tree Search Methods',\n",
       "  'reference_count': '214',\n",
       "  'citation_count': '2,357',\n",
       "  'abstract': \"Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.\",\n",
       "  'date': 2012,\n",
       "  'authors': ['C. B. Browne 1, E. Powley 2, D. Whitehouse 2, S. M. Lucas 3, P. I. Cowling 2, P. Rohlfshagen 3, S. Tavener 1, D. Perez 3, S. Samothrakis 3, S. Colton 1'],\n",
       "  'related_topics': ['Monte Carlo tree search',\n",
       "   'Computer Go',\n",
       "   'General game playing',\n",
       "   'Monte Carlo method',\n",
       "   'General video game playing',\n",
       "   'Decision theory',\n",
       "   'Machine learning',\n",
       "   'Open research',\n",
       "   'Computer science',\n",
       "   'Game theory',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2122410182',\n",
       "   '2168405694',\n",
       "   '1625390266',\n",
       "   '1714211023',\n",
       "   '131069610',\n",
       "   '2171084228',\n",
       "   '2020135152',\n",
       "   '1888434271',\n",
       "   '1500868819',\n",
       "   '1510812122']},\n",
       " {'id': '1515851193',\n",
       "  'title': 'Introduction to Reinforcement Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '6,275',\n",
       "  'abstract': \"From the Publisher: In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.\",\n",
       "  'date': 1998,\n",
       "  'authors': ['Richard S. Sutton', 'Andrew G. Barto'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Temporal difference learning',\n",
       "   'Q-learning',\n",
       "   'AIXI',\n",
       "   'Cognitive science',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)'],\n",
       "  'references': ['2076063813',\n",
       "   '2072128103',\n",
       "   '2964043796',\n",
       "   '2952509347',\n",
       "   '2168405694',\n",
       "   '2155968351',\n",
       "   '2100235918',\n",
       "   '2165150801']},\n",
       " {'id': '1625390266',\n",
       "  'title': 'Bandit based monte-carlo planning',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '3,113',\n",
       "  'abstract': 'For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Levente Kocsis', 'Csaba Szepesvári'],\n",
       "  'related_topics': ['Monte Carlo tree search',\n",
       "   'Monte Carlo method',\n",
       "   'Decision problem',\n",
       "   'Sampling (statistics)',\n",
       "   'Markov process',\n",
       "   'Proof-number search',\n",
       "   'Mathematical optimization',\n",
       "   'Computer Go',\n",
       "   'Sample (statistics)',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['2168405694',\n",
       "   '2077902449',\n",
       "   '2101861158',\n",
       "   '2009551863',\n",
       "   '1512919909',\n",
       "   '1551466210',\n",
       "   '1863869622',\n",
       "   '2016647253',\n",
       "   '1562282139',\n",
       "   '2135997697']},\n",
       " {'id': '1502916507',\n",
       "  'title': 'Similarity Search in High Dimensions via Hashing',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '4,154',\n",
       "  'abstract': 'The nearestor near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the \\\\curse of dimensionality.\" That is, the data structures scale poorly with data dimensionality; in fact, if the number of dimensions exceeds 10 to 20, searching in k-d trees and related structures involves the inspection of a large fraction of the database, thereby doing no better than brute-force linear search. It has been suggested that since the selection of features and the choice of a distance metric in typical applications is rather heuristic, determining an approximate nearest neighbor should su ce for most practical purposes. In this paper, we examine a novel scheme for approximate similarity search based on hashing. The basic idea is to hash the points Supported by NAVY N00014-96-1-1221 grant and NSF Grant IIS-9811904. Supported by Stanford Graduate Fellowship and NSF NYI Award CCR-9357849. Supported by ARO MURI Grant DAAH04-96-1-0007, NSF Grant IIS-9811904, and NSF Young Investigator Award CCR9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, and Xerox Corporation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 25th VLDB Conference, Edinburgh, Scotland, 1999. from the database so as to ensure that the probability of collision is much higher for objects that are close to each other than for those that are far apart. We provide experimental evidence that our method gives signi cant improvement in running time over other methods for searching in highdimensional spaces based on hierarchical tree decomposition. Experimental results also indicate that our scheme scales well even for a relatively large number of dimensions (more than 50).',\n",
       "  'date': 1999,\n",
       "  'authors': ['Aristides Gionis', 'Piotr Indyk', 'Rajeev Motwani'],\n",
       "  'related_topics': ['Nearest neighbor search',\n",
       "   'Locality-sensitive hashing',\n",
       "   'Very large database',\n",
       "   'Open addressing',\n",
       "   'Dynamic perfect hashing',\n",
       "   'Cover tree',\n",
       "   'Hash function',\n",
       "   'Ball tree',\n",
       "   'Feature hashing',\n",
       "   'Linear search',\n",
       "   'Universal hashing',\n",
       "   'K-independent hashing',\n",
       "   'Hopscotch hashing',\n",
       "   'Metric (mathematics)',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Tree decomposition',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2147152072',\n",
       "   '2147717514',\n",
       "   '1634005169',\n",
       "   '2295428206',\n",
       "   '1956559956',\n",
       "   '2125148312',\n",
       "   '2160066518',\n",
       "   '3017143921',\n",
       "   '2074429597',\n",
       "   '1541459201']},\n",
       " {'id': '2099587183',\n",
       "  'title': 'General Game Playing: Overview of the AAAI Competition',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '618',\n",
       "  'abstract': \"A general game playing system is one that can accept a formal description of a game and play the game effectively without human intervention. Unlike specialized game players, such as Deep Blue, general game players do not rely on algorithms designed in advance for specific games; and, unlike Deep Blue, they are able to play different kinds of games. In order to promote work in this area, the AAAI is sponsoring an open competition at this summer's Twentieth National Conference on Artificial Intelligence. This article is an overview of the technical issues and logistics associated with this summer's competition, as well as the relevance of general game playing to the long range-goals of artificial intelligence.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['Michael R. Genesereth', 'Nathaniel Love', 'Barney Pell'],\n",
       "  'related_topics': ['General game playing',\n",
       "   'General video game playing',\n",
       "   'Game design',\n",
       "   'Video game design',\n",
       "   'Game Developer',\n",
       "   'Non-cooperative game',\n",
       "   'Game mechanics',\n",
       "   'Repeated game',\n",
       "   'Human–computer interaction',\n",
       "   'Operations research',\n",
       "   'Engineering'],\n",
       "  'references': ['1981627423', '1572038152', '2119409989', '3025398977']},\n",
       " {'id': '2013391942',\n",
       "  'title': 'Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '707',\n",
       "  'abstract': 'This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environment, the latter is suited for passive prediction in unknown environment. The book introduces these two well-known but very different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment. Most AI problems can easily be formulated within this theory, which reduces the conceptual problems to pure computational problems. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches to AI. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Marcus Hutter'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Reinforcement learning',\n",
       "   'Algorithmic probability',\n",
       "   'AIXI',\n",
       "   'Algorithmic information theory',\n",
       "   \"Solomonoff's theory of inductive inference\",\n",
       "   'Kolmogorov complexity',\n",
       "   'Computational problem',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2076063813',\n",
       "   '2072128103',\n",
       "   '2952509347',\n",
       "   '2073384958',\n",
       "   '2101493843',\n",
       "   '2792315573',\n",
       "   '2034806191',\n",
       "   '2099397840',\n",
       "   '1564034482']},\n",
       " {'id': '2101355568',\n",
       "  'title': 'An object-oriented representation for efficient reinforcement learning',\n",
       "  'reference_count': '102',\n",
       "  'citation_count': '271',\n",
       "  'abstract': 'Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Carlos Diuk', 'Andre Cohen', 'Michael L. Littman'],\n",
       "  'related_topics': ['Learning classifier system',\n",
       "   'Reinforcement learning',\n",
       "   'Robot learning'],\n",
       "  'references': ['2122410182',\n",
       "   '1506806321',\n",
       "   '2121863487',\n",
       "   '2168405694',\n",
       "   '1515851193',\n",
       "   '1625390266',\n",
       "   '2119567691',\n",
       "   '2107726111',\n",
       "   '1576452626',\n",
       "   '2168359464']},\n",
       " {'id': '2132622533',\n",
       "  'title': 'Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '370',\n",
       "  'abstract': \"Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction.\",\n",
       "  'date': 2011,\n",
       "  'authors': ['Richard S. Sutton 1, Joseph Modayil 1, Michael Delp 1, Thomas Degris 1, Patrick M. Pilarski 1, Adam White 1, Doina Precup 2'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Robot learning',\n",
       "   'Reinforcement learning',\n",
       "   'Knowledge representation and reasoning',\n",
       "   'Temporal difference learning',\n",
       "   'Function approximation',\n",
       "   'General knowledge',\n",
       "   'Mobile robot',\n",
       "   'Robot',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2121863487',\n",
       "   '1515851193',\n",
       "   '2100677568',\n",
       "   '2109910161',\n",
       "   '2075268401',\n",
       "   '2062937587',\n",
       "   '1491843047',\n",
       "   '2149390907',\n",
       "   '2134042548',\n",
       "   '13294968']},\n",
       " {'id': '2076063813',\n",
       "  'title': 'Deep learning in neural networks',\n",
       "  'reference_count': '500',\n",
       "  'citation_count': '14,081',\n",
       "  'abstract': 'In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Jürgen Schmidhuber'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Deep belief network',\n",
       "   'Unsupervised learning',\n",
       "   'Artificial neural network',\n",
       "   'Competitive learning',\n",
       "   'Learning classifier system',\n",
       "   'Semi-supervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Computational learning theory',\n",
       "   'Reinforcement learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Stability (learning theory)',\n",
       "   'Supervised learning',\n",
       "   'Backpropagation',\n",
       "   'Evolutionary computation',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2097117768',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2130942839',\n",
       "   '2156909104',\n",
       "   '1663973292',\n",
       "   '2136922672',\n",
       "   '1849277567',\n",
       "   '2963542991']},\n",
       " {'id': '2145094598',\n",
       "  'title': 'Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '6,141',\n",
       "  'abstract': 'We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Pascal Vincent',\n",
       "   'Hugo Larochelle',\n",
       "   'Isabelle Lajoie',\n",
       "   'Yoshua Bengio',\n",
       "   'Pierre-Antoine Manzagol'],\n",
       "  'related_topics': ['Deep belief network',\n",
       "   'Support vector machine',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Noise reduction',\n",
       "   'Bridging (networking)',\n",
       "   'Image (mathematics)',\n",
       "   'Enhanced Data Rates for GSM Evolution',\n",
       "   'Variation (game tree)',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '1652505363',\n",
       "   '2108384452',\n",
       "   '1479807131',\n",
       "   '1994197834']},\n",
       " {'id': '2107941094',\n",
       "  'title': 'Ant system: optimization by a colony of cooperating agents',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '15,973',\n",
       "  'abstract': 'An analogy with the way ant colonies function has suggested the definition of a new computational paradigm, which we call ant system (AS). We propose it as a viable new approach to stochastic combinatorial optimization. The main characteristics of this model are positive feedback, distributed computation, and the use of a constructive greedy heuristic. Positive feedback accounts for rapid discovery of good solutions, distributed computation avoids premature convergence, and the greedy heuristic helps find acceptable solutions in the early stages of the search process. We apply the proposed methodology to the classical traveling salesman problem (TSP), and report simulation results. We also discuss parameter selection and the early setups of the model, and compare it with tabu search and simulated annealing using TSP. To demonstrate the robustness of the approach, we show how the ant system (AS) can be applied to other optimization problems like the asymmetric traveling salesman, the quadratic assignment and the job-shop scheduling. Finally we discuss the salient characteristics-global data structure revision, distributed communication and probabilistic transitions of the AS.',\n",
       "  'date': 1996,\n",
       "  'authors': ['M. Dorigo 1, V. Maniezzo 2, A. Colorni 3'],\n",
       "  'related_topics': ['Metaheuristic',\n",
       "   'Ant colony optimization algorithms',\n",
       "   'Extremal optimization',\n",
       "   'Greedy algorithm',\n",
       "   'Tabu search',\n",
       "   'Combinatorial optimization',\n",
       "   'Parallel metaheuristic',\n",
       "   'Travelling salesman problem',\n",
       "   'Ant colony',\n",
       "   'Simulated annealing',\n",
       "   'Optimization problem',\n",
       "   'Premature convergence',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science',\n",
       "   'Artificial Ants'],\n",
       "  'references': ['1639032689',\n",
       "   '1497256448',\n",
       "   '2581275558',\n",
       "   '1652505363',\n",
       "   '2152150600',\n",
       "   '2104670598',\n",
       "   '2297395784',\n",
       "   '1492640216',\n",
       "   '3011460294',\n",
       "   '2042986967']},\n",
       " {'id': '3104097132',\n",
       "  'title': 'DeepWalk: online learning of social representations',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '4,992',\n",
       "  'abstract': \"We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.\",\n",
       "  'date': 2014,\n",
       "  'authors': ['Bryan Perozzi', 'Rami Al-Rfou', 'Steven Skiena'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Deep learning',\n",
       "   'Language model',\n",
       "   'Anomaly detection',\n",
       "   'Statistical model',\n",
       "   'Machine learning',\n",
       "   'Class (biology)',\n",
       "   'Graph embedding',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2153579005',\n",
       "   '1614298861',\n",
       "   '2163922914',\n",
       "   '2168231600',\n",
       "   '2122646361',\n",
       "   '2117130368',\n",
       "   '2141599568',\n",
       "   '2118585731',\n",
       "   '2147768505']},\n",
       " {'id': '1895577753',\n",
       "  'title': 'Show and tell: A neural image caption generator',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '4,638',\n",
       "  'abstract': 'Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Oriol Vinyals',\n",
       "   'Alexander Toshev',\n",
       "   'Samy Bengio',\n",
       "   'Dumitru Erhan'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Machine translation',\n",
       "   'Pascal (programming language)',\n",
       "   'Sentence',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Fluency',\n",
       "   'Computer science',\n",
       "   'Closed captioning'],\n",
       "  'references': ['2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2963542991',\n",
       "   '2064675550',\n",
       "   '2155541015']},\n",
       " {'id': '1888005072',\n",
       "  'title': 'LINE: Large-scale Information Network Embedding',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '3,345',\n",
       "  'abstract': \"This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\\\\footnote{\\\\url{https://github.com/tangjianpku/LINE}}.\",\n",
       "  'date': 2015,\n",
       "  'authors': ['Jian Tang 1, Meng Qu 2, Mingzhe Wang 2, Ming Zhang 2, Jun Yan 1, Qiaozhu Mei 3'],\n",
       "  'related_topics': ['Graph embedding',\n",
       "   'Embedding',\n",
       "   'Node (networking)',\n",
       "   'Stochastic gradient descent',\n",
       "   'Feature learning',\n",
       "   'Dimensionality reduction',\n",
       "   'Scalability',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Social network'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2187089797',\n",
       "   '2131744502',\n",
       "   '1532325895',\n",
       "   '2053186076',\n",
       "   '3104097132',\n",
       "   '2001141328',\n",
       "   '1854214752',\n",
       "   '2156718197']},\n",
       " {'id': '1486649854',\n",
       "  'title': 'Skip-thought vectors',\n",
       "  'reference_count': '38',\n",
       "  'citation_count': '1,938',\n",
       "  'abstract': 'We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Ryan Kiros 1, Yukun Zhu 1, Ruslan Salakhutdinov 2, Richard S. Zemel 2, Antonio Torralba 3, Raquel Urtasun 1, Sanja Fidler 1'],\n",
       "  'related_topics': ['Sentence',\n",
       "   'Semantic similarity',\n",
       "   'Vocabulary',\n",
       "   'Unsupervised learning',\n",
       "   'Ranking (information retrieval)',\n",
       "   'Encoder',\n",
       "   'Paraphrase',\n",
       "   'Natural language processing',\n",
       "   'Ranking',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2962835968',\n",
       "   '2964121744',\n",
       "   '2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '1861492603',\n",
       "   '1832693441',\n",
       "   '2064675550',\n",
       "   '2187089797']},\n",
       " {'id': '2964321699',\n",
       "  'title': 'Convolutional neural networks on graphs with fast localized spectral filtering',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '3,141',\n",
       "  'abstract': \"In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.\",\n",
       "  'date': 2016,\n",
       "  'authors': ['Michaël Defferrard', 'Xavier Bresson', 'Pierre Vandergheynst'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Spectral graph theory',\n",
       "   'MNIST database',\n",
       "   'Computational complexity theory',\n",
       "   'Embedding',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Connectome',\n",
       "   'Artificial intelligence',\n",
       "   'Graph'],\n",
       "  'references': ['2964121744',\n",
       "   '1614298861',\n",
       "   '2310919327',\n",
       "   '2121947440',\n",
       "   '603908379',\n",
       "   '2132914434',\n",
       "   '2101491865',\n",
       "   '2158787690',\n",
       "   '2070232376',\n",
       "   '2116341502']},\n",
       " {'id': '2100664567',\n",
       "  'title': 'On Using Very Large Target Vocabulary for Neural Machine Translation',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '870',\n",
       "  'abstract': 'Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Sébastien Jean',\n",
       "   'Kyunghyun Cho',\n",
       "   'Roland Memisevic',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Transfer-based machine translation',\n",
       "   'Rule-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Vocabulary',\n",
       "   'Artificial neural network',\n",
       "   'Translation (geometry)',\n",
       "   'Machine learning',\n",
       "   'Speech recognition',\n",
       "   'Decoding methods',\n",
       "   'Importance sampling',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '2101105183',\n",
       "   '1753482797',\n",
       "   '2964199361',\n",
       "   '2153653739',\n",
       "   '1606347560',\n",
       "   '2118434577']},\n",
       " {'id': '2123024445',\n",
       "  'title': 'DeViSE: A Deep Visual-Semantic Embedding Model',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '2,004',\n",
       "  'abstract': 'Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Andrea Frome',\n",
       "   'Greg S Corrado',\n",
       "   'Jon Shlens',\n",
       "   'Samy Bengio',\n",
       "   'Jeff Dean',\n",
       "   \"Marc'Aurelio Ranzato\",\n",
       "   'Tomas Mikolov'],\n",
       "  'related_topics': ['Cognitive neuroscience of visual object recognition',\n",
       "   'Visual Objects',\n",
       "   'Semantic memory',\n",
       "   'Machine learning',\n",
       "   'Leverage (statistics)',\n",
       "   'Embedding',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2117539524',\n",
       "   '2153579005',\n",
       "   '1614298861',\n",
       "   '2108598243',\n",
       "   '2146502635',\n",
       "   '1904365287',\n",
       "   '2187089797',\n",
       "   '2168231600',\n",
       "   '2132339004']},\n",
       " {'id': '2130903752',\n",
       "  'title': 'A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '1,518',\n",
       "  'abstract': \"One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['Rie Kubota Ando', 'Tong Zhang'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Multi-task learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Online machine learning',\n",
       "   'Learning classifier system',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2148603752',\n",
       "   '1480376833',\n",
       "   '2154455818',\n",
       "   '2139823104',\n",
       "   '2048679005',\n",
       "   '2097089247',\n",
       "   '2107008379',\n",
       "   '2914746235',\n",
       "   '2010353172',\n",
       "   '2101210369']},\n",
       " {'id': '2158847908',\n",
       "  'title': 'The Proposition Bank: An Annotated Corpus of Semantic Roles',\n",
       "  'reference_count': '57',\n",
       "  'citation_count': '2,695',\n",
       "  'abstract': \"The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['Martha Palmer 1, Daniel Gildea 2, Paul Kingsbury 1'],\n",
       "  'related_topics': ['Treebank',\n",
       "   'Semantic role labeling',\n",
       "   'Explicit semantic analysis',\n",
       "   'Semantic computing',\n",
       "   'Semantic similarity',\n",
       "   'PropBank',\n",
       "   'FrameNet',\n",
       "   'VerbNet',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1632114991',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '2115792525',\n",
       "   '2151170651',\n",
       "   '3021452258',\n",
       "   '2039217078',\n",
       "   '1567277581',\n",
       "   '2126851059',\n",
       "   '2154626406']},\n",
       " {'id': '2107008379',\n",
       "  'title': 'Transductive Inference for Text Classification using Support Vector Machines',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '3,787',\n",
       "  'abstract': '',\n",
       "  'date': 1999,\n",
       "  'authors': ['Thorsten Joachims'],\n",
       "  'related_topics': ['Relevance vector machine',\n",
       "   'Transduction (machine learning)',\n",
       "   'Structured support vector machine',\n",
       "   'Support vector machine',\n",
       "   'Co-training',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2158899491',\n",
       "   '2165698076',\n",
       "   '2964015378',\n",
       "   '2117130368',\n",
       "   '2118020653',\n",
       "   '1995903777',\n",
       "   '1479807131',\n",
       "   '2104290444',\n",
       "   '2150102617']},\n",
       " {'id': '2914746235',\n",
       "  'title': 'Multitask learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '8,169',\n",
       "  'abstract': 'Multitask Learning is an approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. In this thesis we demonstrate multitask learning for a dozen problems. We explain how multitask learning works and show that there are many opportunities for multitask learning in real domains. We show that in some cases features that would normally be used as inputs work better if used as multitask outputs instead. We present suggestions for how to get the most out of multitask learning in artificial neural nets, present an algorithm for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Multitask learning improves generalization performance, can be applied in many different kinds of domains, and can be used with different learning algorithms. We conjecture there will be many opportunities for its use on real-world problems.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Rich Caruana'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Inductive transfer',\n",
       "   'Artificial neural network',\n",
       "   'Decision tree',\n",
       "   'Task (project management)',\n",
       "   'Generalization (learning)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Sketch',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1536680647',\n",
       "   '2076063813',\n",
       "   '2165698076',\n",
       "   '2117130368',\n",
       "   '2098411764',\n",
       "   '2184188583',\n",
       "   '2150341604',\n",
       "   '2114315281',\n",
       "   '1512387364',\n",
       "   '2550821151']},\n",
       " {'id': '2173629880',\n",
       "  'title': 'Phoneme recognition using time-delay neural networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,232',\n",
       "  'abstract': 'The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >',\n",
       "  'date': 1994,\n",
       "  'authors': ['Alexander Waibel',\n",
       "   'Toshiyuki Hanazawa',\n",
       "   'Geoffrey Hinton',\n",
       "   'Kiyohiro Shikano',\n",
       "   'Kevin J. Lang'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Hidden Markov model',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Speech recognition',\n",
       "   'Task (computing)',\n",
       "   'Hierarchy (mathematics)',\n",
       "   'Position (vector)',\n",
       "   'Computer science',\n",
       "   'Nonlinear system'],\n",
       "  'references': ['2217896605',\n",
       "   '2973127116',\n",
       "   '1606209288',\n",
       "   '2970350205',\n",
       "   '3007502375',\n",
       "   '3016138882',\n",
       "   '2982413405',\n",
       "   '2098694627',\n",
       "   '3115898234']},\n",
       " {'id': '2885050925',\n",
       "  'title': 'Shallow Semantic Parsing using Support Vector Machines.',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '499',\n",
       "  'abstract': '',\n",
       "  'date': 2003,\n",
       "  'authors': ['Sameer S. Pradhan',\n",
       "   'Wayne H. Ward',\n",
       "   'Kadri Hacioglu',\n",
       "   'James H. Martin',\n",
       "   'Daniel Jurafsky'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Support vector machine',\n",
       "   'Natural language processing',\n",
       "   'PropBank',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2092654472',\n",
       "   '2151170651',\n",
       "   '2166776180',\n",
       "   '1520377376',\n",
       "   '1988995507',\n",
       "   '2145310422',\n",
       "   '2155693943',\n",
       "   '2093647425',\n",
       "   '2040909025',\n",
       "   '2150203234']},\n",
       " {'id': '2158823144',\n",
       "  'title': 'Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data',\n",
       "  'reference_count': '57',\n",
       "  'citation_count': '1,063',\n",
       "  'abstract': 'In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Charles Sutton', 'Khashayar Rohanimanesh', 'Andrew McCallum'],\n",
       "  'related_topics': ['Approximate inference',\n",
       "   'Variable elimination',\n",
       "   'Bayesian network',\n",
       "   'Belief propagation',\n",
       "   'Graphical model',\n",
       "   'Conditional random field',\n",
       "   'Dynamic Bayesian network',\n",
       "   'Inference',\n",
       "   'Probabilistic logic',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2147880316',\n",
       "   '2116064496',\n",
       "   '2125838338',\n",
       "   '1574901103',\n",
       "   '1632114991',\n",
       "   '2008652694',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '2169415915',\n",
       "   '2156515921']},\n",
       " {'id': '2163568299',\n",
       "  'title': 'Effective Self-Training for Parsing',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '608',\n",
       "  'abstract': 'We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.',\n",
       "  'date': 2006,\n",
       "  'authors': ['David McClosky', 'Eugene Charniak', 'Mark Johnson'],\n",
       "  'related_topics': ['Parsing',\n",
       "   'Discriminative model',\n",
       "   'Bootstrapping',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Error reduction',\n",
       "   'Self training'],\n",
       "  'references': ['1632114991',\n",
       "   '2048679005',\n",
       "   '1535015163',\n",
       "   '2158195707',\n",
       "   '3021452258',\n",
       "   '1567570606',\n",
       "   '2125712079',\n",
       "   '2729906263',\n",
       "   '2098379588',\n",
       "   '2037894654']},\n",
       " {'id': '179875071',\n",
       "  'title': 'Recurrent neural network based language model',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '5,581',\n",
       "  'abstract': 'A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition',\n",
       "  'date': 2009,\n",
       "  'authors': ['Tomas Mikolov 1, Martin Karafiát 1, Lukás Burget 1, Jan Cernocký',\n",
       "   'Sanjeev Khudanpur 2'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Language model',\n",
       "   'Recurrent neural network',\n",
       "   'Word error rate',\n",
       "   'Perplexity',\n",
       "   'Connectionism',\n",
       "   'Reduction (complexity)',\n",
       "   'Speech recognition',\n",
       "   'NIST',\n",
       "   'Computer science'],\n",
       "  'references': ['2132339004',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '36903255',\n",
       "   '2096072088',\n",
       "   '2468573742',\n",
       "   '2152808281',\n",
       "   '2292896937',\n",
       "   '2027499299',\n",
       "   '2437096199']},\n",
       " {'id': '2147152072',\n",
       "  'title': 'Indexing by Latent Semantic Analysis',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '17,287',\n",
       "  'abstract': 'A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Scott Deerwester 1, Susan T. Dumais 2, George W. Furnas 2, Thomas K. Landauer 2, Richard Harshman 3'],\n",
       "  'related_topics': ['Document-term matrix',\n",
       "   'Latent semantic analysis',\n",
       "   'Vector space model',\n",
       "   'Automatic indexing',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Explicit semantic analysis',\n",
       "   'Random indexing',\n",
       "   'Document retrieval',\n",
       "   'Information retrieval',\n",
       "   'Mathematics'],\n",
       "  'references': ['1956559956',\n",
       "   '1984565341',\n",
       "   '1964262399',\n",
       "   '2000215628',\n",
       "   '2114804204',\n",
       "   '2151561903',\n",
       "   '3012395598',\n",
       "   '1965061793',\n",
       "   '2024683548',\n",
       "   '2096411881']},\n",
       " {'id': '1632114991',\n",
       "  'title': 'Building a large annotated corpus of English: the penn treebank',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '9,188',\n",
       "  'abstract': 'Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.',\n",
       "  'date': 1993,\n",
       "  'authors': ['Mitchell P. Marcus 1, Mary Ann Marcinkiewicz 1, Beatrice Santorini 2'],\n",
       "  'related_topics': ['Corpus linguistics',\n",
       "   'Brown Corpus',\n",
       "   'Treebank',\n",
       "   'Text corpus',\n",
       "   'PropBank',\n",
       "   'Trigram tagger',\n",
       "   'Computational linguistics',\n",
       "   'Shallow parsing',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2099247782',\n",
       "   '1483126227',\n",
       "   '2439178139',\n",
       "   '2334801970',\n",
       "   '900993354',\n",
       "   '2110190189',\n",
       "   '2012837062',\n",
       "   '2121407024',\n",
       "   '2076526090',\n",
       "   '2166675302']},\n",
       " {'id': '1970689298',\n",
       "  'title': 'Continuous space language models',\n",
       "  'reference_count': '63',\n",
       "  'citation_count': '606',\n",
       "  'abstract': 'This paper describes the use of a neural network language model for large vocabulary continuous speech recognition. The underlying idea of this approach is to attack the data sparseness problem by performing the language model probability estimation in a continuous space. Highly efficient learning algorithms are described that enable the use of training corpora of several hundred million words. It is also shown that this approach can be incorporated into a large vocabulary continuous speech recognizer using a lattice rescoring framework at a very low additional processing time. The neural network language model was thoroughly evaluated in a state-of-the-art large vocabulary continuous speech recognizer for several international benchmark tasks, in particular the Nist evaluations on broadcast news and conversational speech recognition. The new approach is compared to four-gram back-off language models trained with modified Kneser-Ney smoothing which has often been reported to be the best known smoothing method. Usually the neural network language model is interpolated with the back-off language model. In that way, consistent word error rate reductions for all considered tasks and languages were achieved, ranging from 0.4% to almost 1% absolute.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Holger Schwenk'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Time delay neural network',\n",
       "   'Computational linguistics',\n",
       "   'n-gram',\n",
       "   'Word error rate',\n",
       "   'Factored language model',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2148603752',\n",
       "   '1554663460',\n",
       "   '2912934387',\n",
       "   '2132339004',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '36903255',\n",
       "   '2158195707']},\n",
       " {'id': '1631260214',\n",
       "  'title': 'SRILM – An Extensible Language Modeling Toolkit',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '5,423',\n",
       "  'abstract': '',\n",
       "  'date': 2001,\n",
       "  'authors': ['Andreas Stolcke'],\n",
       "  'related_topics': ['Modeling language',\n",
       "   'High-level programming language',\n",
       "   'Computer science',\n",
       "   'Pivot language',\n",
       "   'Language model',\n",
       "   'Programming language',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Inversion transduction grammars',\n",
       "   'Machine translation system',\n",
       "   'Speech transcription',\n",
       "   'System combination'],\n",
       "  'references': ['2158195707',\n",
       "   '2121227244',\n",
       "   '1904457459',\n",
       "   '2594610113',\n",
       "   '1549285799',\n",
       "   '2100506586',\n",
       "   '1797288984',\n",
       "   '2097978681',\n",
       "   '1528470941',\n",
       "   '2127836646']},\n",
       " {'id': '2096175520',\n",
       "  'title': 'A maximum entropy approach to natural language processing',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '4,846',\n",
       "  'abstract': 'The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Adam L. Berger 1, Vincent J. Della Pietra 2, Stephen A. Della Pietra 2'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Statistical model',\n",
       "   'Generalized iterative scaling',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2099111195',\n",
       "   '2049633694',\n",
       "   '2006969979',\n",
       "   '2121227244',\n",
       "   '2160842254',\n",
       "   '2097333193',\n",
       "   '1597533204',\n",
       "   '2099345940',\n",
       "   '2167434254',\n",
       "   '1976241232']},\n",
       " {'id': '2110485445',\n",
       "  'title': 'Finding Structure in Time',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '12,895',\n",
       "  'abstract': 'Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Jeffrey L. Elman'],\n",
       "  'related_topics': ['Task (project management)',\n",
       "   'Context (language use)',\n",
       "   'Semantics',\n",
       "   'Connectionism',\n",
       "   'Set (psychology)',\n",
       "   'Artificial grammar learning',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Prediction in language comprehension',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2173629880',\n",
       "   '2016589492',\n",
       "   '3036751298',\n",
       "   '2118373646',\n",
       "   '2046432185',\n",
       "   '2122988375',\n",
       "   '2170716495',\n",
       "   '2094249282']},\n",
       " {'id': '1575350781',\n",
       "  'title': 'MPI: A Message-Passing Interface Standard',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '4,601',\n",
       "  'abstract': 'The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Message P Forum'],\n",
       "  'related_topics': ['Message passing',\n",
       "   'Message Passing Interface',\n",
       "   'Interface (Java)',\n",
       "   'Standards organization',\n",
       "   'MPICH',\n",
       "   'Set (abstract data type)',\n",
       "   'World Wide Web',\n",
       "   'Computer science',\n",
       "   'iWarp',\n",
       "   'Message passing interface standard'],\n",
       "  'references': ['1480928214',\n",
       "   '1521571223',\n",
       "   '1964564149',\n",
       "   '1978513924',\n",
       "   '2083200599',\n",
       "   '2090683636',\n",
       "   '2010542899',\n",
       "   '2294265735',\n",
       "   '1843937266',\n",
       "   '2010269868']},\n",
       " {'id': '2158195707',\n",
       "  'title': 'An empirical study of smoothing techniques for language modeling',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '3,717',\n",
       "  'abstract': 'We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser?Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Stanley F. Chen 1, Joshua Goodman 2'],\n",
       "  'related_topics': ['Kneser–Ney smoothing',\n",
       "   'Smoothing',\n",
       "   'Bigram',\n",
       "   'Trigram',\n",
       "   'Language model',\n",
       "   'Test data',\n",
       "   'Speech recognition',\n",
       "   'Empirical research',\n",
       "   'Gram',\n",
       "   'Computer science'],\n",
       "  'references': ['2170120409',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '1966812932',\n",
       "   '2611071497',\n",
       "   '2166637769',\n",
       "   '2134237567',\n",
       "   '2075201173']},\n",
       " {'id': '2121227244',\n",
       "  'title': 'Class-based n -gram models of natural language',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '4,038',\n",
       "  'abstract': 'We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Peter F. Brown',\n",
       "   'Peter V. deSouza',\n",
       "   'Robert L. Mercer',\n",
       "   'Vincent J. Della Pietra',\n",
       "   'Jenifer C. Lai'],\n",
       "  'related_topics': ['n-gram',\n",
       "   'Natural language',\n",
       "   'Word (group theory)',\n",
       "   'Natural language processing',\n",
       "   'Class (biology)',\n",
       "   'Sample (statistics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2049633694',\n",
       "   '2097333193',\n",
       "   '1966812932',\n",
       "   '2142901448',\n",
       "   '2751862591',\n",
       "   '1597533204',\n",
       "   '1575431606',\n",
       "   '2007780422',\n",
       "   '2016871293',\n",
       "   '1628850721']},\n",
       " {'id': '1880262756',\n",
       "  'title': 'Latent dirichlet allocation',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '38,976',\n",
       "  'abstract': 'We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.',\n",
       "  'date': 2003,\n",
       "  'authors': ['David M. Blei 1, Andrew Y. Ng 2, Michael I. Jordan 1'],\n",
       "  'related_topics': ['Latent Dirichlet allocation',\n",
       "   'Dynamic topic model',\n",
       "   'Hierarchical Dirichlet process',\n",
       "   'Topic model',\n",
       "   'Pachinko allocation',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Variational message passing',\n",
       "   'Dirichlet-multinomial distribution',\n",
       "   'Data mining',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2045656233',\n",
       "   '2147152072',\n",
       "   '2107743791',\n",
       "   '2097089247',\n",
       "   '1956559956',\n",
       "   '1516111018',\n",
       "   '1508165687',\n",
       "   '1746680969',\n",
       "   '2020842694',\n",
       "   '2063392856']},\n",
       " {'id': '168564468',\n",
       "  'title': 'Software Framework for Topic Modelling with Large Corpora',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '3,281',\n",
       "  'abstract': \"Large corpora are ubiquitous in today's world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model (VSM). We identify gap in existing VSM implementations, which is their scalability and ease of use. We describe a Natural Language Processing software framework which is based on the idea of document streaming, i.e. processing corpora document after document, in a memory independent fashion. In this framework, we implement several popular algorithms for topical inference, including Latent Semantic Analysis and Latent Dirichlet Allocation, in a way that makes them completely independent of the training corpus size. Particular emphasis is placed on straightforward and intuitive framework design, so that modifications and extensions of the methods and/or their application by interested practitioners are effortless. We demonstrate the usefulness of our approach on a real-world scenario of computing document similarities within an existing digital library DML-CZ.\",\n",
       "  'date': 2010,\n",
       "  'authors': ['Radim Řehůřek', 'Petr Sojka'],\n",
       "  'related_topics': ['Latent Dirichlet allocation',\n",
       "   'Topic model',\n",
       "   'Software framework'],\n",
       "  'references': ['1880262756',\n",
       "   '2170120409',\n",
       "   '2001082470',\n",
       "   '2147152072',\n",
       "   '2158266063',\n",
       "   '2047804403',\n",
       "   '2143017621',\n",
       "   '2159426623',\n",
       "   '2334889010',\n",
       "   '2063392856']},\n",
       " {'id': '2296073425',\n",
       "  'title': 'Curriculum learning',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '2,888',\n",
       "  'abstract': 'Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).',\n",
       "  'date': 2009,\n",
       "  'authors': ['Yoshua Bengio 1, Jérôme Louradour 2, Ronan Collobert 3, Jason Weston 3'],\n",
       "  'related_topics': ['Active learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Instance-based learning',\n",
       "   'Curriculum',\n",
       "   'Context (language use)',\n",
       "   'Generalization (learning)',\n",
       "   'Stochastic neural network',\n",
       "   'Process (engineering)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2117130368',\n",
       "   '2025768430',\n",
       "   '2110798204',\n",
       "   '2099866409',\n",
       "   '1994197834',\n",
       "   '2172174689',\n",
       "   '205159212']},\n",
       " {'id': '2158997610',\n",
       "  'title': 'An introduction to latent semantic analysis',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '6,475',\n",
       "  'abstract': \"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual‐usage meaning of words by statistical computations applied to a large corpus of text (Landauer & Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA's reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word‐word and passage‐word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.\",\n",
       "  'date': 1997,\n",
       "  'authors': ['Thomas K Landauer 1, Peter W. Foltz 2, Darrell Laham 1'],\n",
       "  'related_topics': ['Latent semantic analysis',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Vocabulary',\n",
       "   'Similarity (psychology)',\n",
       "   'Semantics',\n",
       "   'Learnability',\n",
       "   'Priming (psychology)',\n",
       "   'Automated essay scoring',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2147152072',\n",
       "   '1983578042',\n",
       "   '1674947250',\n",
       "   '2072773380',\n",
       "   '2056029990',\n",
       "   '1981617416',\n",
       "   '2059086756',\n",
       "   '2092919341',\n",
       "   '2928502135',\n",
       "   '1996650435']},\n",
       " {'id': '2004763266',\n",
       "  'title': 'Design Challenges and Misconceptions in Named Entity Recognition',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '1,573',\n",
       "  'abstract': 'We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Lev Ratinov', 'Dan Roth'],\n",
       "  'related_topics': ['Named-entity recognition',\n",
       "   'Inference',\n",
       "   'F1 score',\n",
       "   'Process (engineering)',\n",
       "   'Machine learning',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Representation (mathematics)'],\n",
       "  'references': ['2147880316',\n",
       "   '2125838338',\n",
       "   '2096765155',\n",
       "   '2008652694',\n",
       "   '1996430422',\n",
       "   '2148540243',\n",
       "   '2144578941',\n",
       "   '2121227244',\n",
       "   '2128634885',\n",
       "   '1979711143']},\n",
       " {'id': '2156515921',\n",
       "  'title': 'Shallow parsing with conditional random fields',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '1,926',\n",
       "  'abstract': 'Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Fei Sha', 'Fernando Pereira'],\n",
       "  'related_topics': ['Conditional random field',\n",
       "   'Shallow parsing',\n",
       "   'Sequence labeling',\n",
       "   'Chunking (psychology)',\n",
       "   'Machine learning',\n",
       "   'Natural language processing',\n",
       "   'Sequence',\n",
       "   'Computer science',\n",
       "   'Task (computing)',\n",
       "   'Generative grammar',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1995945562',\n",
       "   '2147880316',\n",
       "   '2008652694',\n",
       "   '2096175520',\n",
       "   '1934019294',\n",
       "   '1773803948',\n",
       "   '2160842254',\n",
       "   '2962735828',\n",
       "   '2117400858',\n",
       "   '1520377376']},\n",
       " {'id': '2067191022',\n",
       "  'title': 'Mean shift: a robust approach toward feature space analysis',\n",
       "  'reference_count': '69',\n",
       "  'citation_count': '14,727',\n",
       "  'abstract': 'A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.',\n",
       "  'date': 2002,\n",
       "  'authors': ['D. Comaniciu', 'P. Meer'],\n",
       "  'related_topics': ['Mean-shift',\n",
       "   'Smoothing',\n",
       "   'Estimator',\n",
       "   'Kernel (statistics)',\n",
       "   'Feature vector',\n",
       "   'Image segmentation',\n",
       "   'Estimation theory',\n",
       "   'Kernel regression',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2798766386',\n",
       "   '2140235142',\n",
       "   '2099244020',\n",
       "   '2132549764',\n",
       "   '2159128898',\n",
       "   '2150134853',\n",
       "   '1971784203',\n",
       "   '2129905273',\n",
       "   '2999729612',\n",
       "   '2482402870']},\n",
       " {'id': '1574901103',\n",
       "  'title': 'Foundations of Statistical Natural Language Processing',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '15,340',\n",
       "  'abstract': 'Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Christopher D. Manning 1, Hinrich Schütze 2'],\n",
       "  'related_topics': ['Computational linguistics',\n",
       "   'Natural language',\n",
       "   'Parsing',\n",
       "   'Collocation extraction',\n",
       "   'Data-oriented parsing',\n",
       "   'Probabilistic logic',\n",
       "   'Collocation',\n",
       "   'Natural language processing',\n",
       "   'Construct (python library)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1508165687',\n",
       "   '182831726',\n",
       "   '1994851566',\n",
       "   '2108321481',\n",
       "   '1549026077',\n",
       "   '2949237929',\n",
       "   '1795234945',\n",
       "   '1746620543',\n",
       "   '1736036918']},\n",
       " {'id': '1566135517',\n",
       "  'title': 'Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '7,535',\n",
       "  'abstract': 'In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Aude Oliva 1, Antonio Torralba 2'],\n",
       "  'related_topics': ['Scene statistics',\n",
       "   'Representation (systemics)',\n",
       "   'Envelope (motion)',\n",
       "   'Naturalness',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Categorization',\n",
       "   'LabelMe',\n",
       "   'Segmentation',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2117812871',\n",
       "   '2128716185',\n",
       "   '2012352340',\n",
       "   '2130259898',\n",
       "   '2156406284',\n",
       "   '1524408959',\n",
       "   '2180838288',\n",
       "   '2142796031',\n",
       "   '2104825706',\n",
       "   '2167034998']},\n",
       " {'id': '2536208356',\n",
       "  'title': 'Decomposing a scene into geometric and semantically consistent regions',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '815',\n",
       "  'abstract': 'High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Stephen Gould', 'Richard Fulton', 'Daphne Koller'],\n",
       "  'related_topics': ['Orientation (computer vision)',\n",
       "   'Image segmentation',\n",
       "   'Object (computer science)',\n",
       "   'Solid modeling',\n",
       "   '3D reconstruction',\n",
       "   'Representation (mathematics)',\n",
       "   'Pixel',\n",
       "   'Pattern recognition',\n",
       "   'Inference',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2161969291',\n",
       "   '2033819227',\n",
       "   '3097096317',\n",
       "   '2067191022',\n",
       "   '2110764733',\n",
       "   '1528789833',\n",
       "   '2132947399',\n",
       "   '2125310925',\n",
       "   '2116877738',\n",
       "   '2112301665']},\n",
       " {'id': '2173213060',\n",
       "  'title': 'MapReduce: simplified data processing on large clusters',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '30,307',\n",
       "  'abstract': \"MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.\",\n",
       "  'date': 2007,\n",
       "  'authors': ['Jeffrey Dean', 'Sanjay Ghemawat'],\n",
       "  'related_topics': ['Data-intensive computing',\n",
       "   'Runtime system',\n",
       "   'Many-task computing',\n",
       "   'Programming with Big Data in R',\n",
       "   'Jaql',\n",
       "   'Programming paradigm',\n",
       "   'NewSQL',\n",
       "   'Data center network architectures',\n",
       "   'Parallel computing',\n",
       "   'Petabyte',\n",
       "   'Distributed computing',\n",
       "   'Computer science',\n",
       "   'Massively parallel computation'],\n",
       "  'references': ['2173213060',\n",
       "   '2119565742',\n",
       "   '2148317584',\n",
       "   '2073965851',\n",
       "   '2109722477',\n",
       "   '2104644701',\n",
       "   '1510543252',\n",
       "   '2044534358',\n",
       "   '1988243929',\n",
       "   '2045271686']},\n",
       " {'id': '1532325895',\n",
       "  'title': 'Introduction to Information Retrieval',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '19,593',\n",
       "  'abstract': 'Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Christopher D. Manning 1, Prabhakar Raghavan 2, Hinrich Schütze 3'],\n",
       "  'related_topics': ['Human–computer information retrieval',\n",
       "   'Cognitive models of information retrieval',\n",
       "   'Concept search',\n",
       "   'Relevance (information retrieval)',\n",
       "   'Information science',\n",
       "   'Document retrieval',\n",
       "   'Adversarial information retrieval',\n",
       "   'Search engine indexing',\n",
       "   'Multimedia',\n",
       "   'Information retrieval',\n",
       "   'Computer science'],\n",
       "  'references': []},\n",
       " {'id': '2024165284',\n",
       "  'title': 'Tensor Decompositions and Applications',\n",
       "  'reference_count': '223',\n",
       "  'citation_count': '7,947',\n",
       "  'abstract': 'This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\\\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Tamara G. Kolda', 'Brett W. Bader'],\n",
       "  'related_topics': ['Tensor contraction',\n",
       "   'Cartesian tensor',\n",
       "   'Symmetric tensor',\n",
       "   'Tensor',\n",
       "   'Matricization',\n",
       "   'Tensor product of Hilbert spaces',\n",
       "   'Tucker decomposition',\n",
       "   'Tensor product',\n",
       "   'Algebra',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['1902027874',\n",
       "   '2798909945',\n",
       "   '2099741732',\n",
       "   '2147152072',\n",
       "   '2013912476',\n",
       "   '2090208105',\n",
       "   '2752853835',\n",
       "   '2072773380',\n",
       "   '2113722075',\n",
       "   '2132267493']},\n",
       " {'id': '1660390307',\n",
       "  'title': 'Modern Information Retrieval',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '19,760',\n",
       "  'abstract': 'From the Publisher: This is a rigorous and complete textbook for a first course on information retrieval from the computer science (as opposed to a user-centred) perspective. The advent of the Internet and the enormous increase in volume of electronically stored information generally has led to substantial work on IR from the computer science perspective - this book provides an up-to-date student oriented treatment of the subject.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Ricardo A. Baeza-Yates', 'Berthier Ribeiro-Neto'],\n",
       "  'related_topics': ['Human–computer information retrieval',\n",
       "   'Information science',\n",
       "   'Electronically stored information',\n",
       "   'Adversarial information retrieval',\n",
       "   'The Internet',\n",
       "   'Okapi BM25',\n",
       "   'Subject (documents)',\n",
       "   'World Wide Web',\n",
       "   'Multimedia',\n",
       "   'Perspective (graphical)',\n",
       "   'Information retrieval',\n",
       "   'Computer science'],\n",
       "  'references': ['1499900670',\n",
       "   '166263196',\n",
       "   '2107745473',\n",
       "   '2122962290',\n",
       "   '2037959956']},\n",
       " {'id': '2166706824',\n",
       "  'title': 'Thumbs up? Sentiment Classification using Machine Learning Techniques',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '10,538',\n",
       "  'abstract': 'We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Bo Pang 1, Lillian Lee 1, Shivakumar Vaithyanathan 2'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Multiclass classification',\n",
       "   'Relevance vector machine',\n",
       "   'Structured support vector machine',\n",
       "   'Naive Bayes classifier',\n",
       "   'Support vector machine',\n",
       "   'Categorization',\n",
       "   'Machine learning',\n",
       "   'Principle of maximum entropy',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3146306708',\n",
       "   '2149684865',\n",
       "   '1576520375',\n",
       "   '2096175520',\n",
       "   '1550206324',\n",
       "   '2140785063',\n",
       "   '2155328222',\n",
       "   '2199803028',\n",
       "   '2160842254',\n",
       "   '1924689489']},\n",
       " {'id': '1992419399',\n",
       "  'title': 'Data clustering: a review',\n",
       "  'reference_count': '191',\n",
       "  'citation_count': '18,126',\n",
       "  'abstract': 'Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.',\n",
       "  'date': 1999,\n",
       "  'authors': ['A. K. Jain 1, M. N. Murty 2, P. J. Flynn 3'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Correlation clustering',\n",
       "   'Single-linkage clustering'],\n",
       "  'references': ['2912565176',\n",
       "   '1639032689',\n",
       "   '1497256448',\n",
       "   '2581275558',\n",
       "   '2152150600',\n",
       "   '2049633694',\n",
       "   '2133671888',\n",
       "   '1971784203',\n",
       "   '2095897464',\n",
       "   '2138745909']},\n",
       " {'id': '71795751',\n",
       "  'title': 'Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '1,484',\n",
       "  'abstract': \"We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.\",\n",
       "  'date': 2011,\n",
       "  'authors': ['Richard Socher',\n",
       "   'Jeffrey Pennington',\n",
       "   'Eric H. Huang',\n",
       "   'Andrew Y. Ng',\n",
       "   'Christopher D. Manning'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Machine learning',\n",
       "   'Multinomial distribution',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1880262756',\n",
       "   '2097726431',\n",
       "   '2117130368',\n",
       "   '2166706824',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '3146306708',\n",
       "   '1423339008',\n",
       "   '2114524997',\n",
       "   '2022204871']},\n",
       " {'id': '2097606805',\n",
       "  'title': 'Accurate Unlexicalized Parsing',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '3,795',\n",
       "  'abstract': 'We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Dan Klein', 'Christopher D. Manning'],\n",
       "  'related_topics': ['Statistical parsing',\n",
       "   'Parsing',\n",
       "   'Treebank',\n",
       "   'Natural language processing',\n",
       "   'Independence (mathematical logic)',\n",
       "   'Grammar',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': []},\n",
       " {'id': '2151048449',\n",
       "  'title': 'Local and Global Algorithms for Disambiguation to Wikipedia',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '793',\n",
       "  'abstract': 'Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call \"global\" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Lev Ratinov 1, Dan Roth 1, Doug Downey 2, Mike Anderson 3'],\n",
       "  'related_topics': ['Entity linking',\n",
       "   'Online encyclopedia',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word-sense disambiguation'],\n",
       "  'references': ['2120779048',\n",
       "   '2100341149',\n",
       "   '86887328',\n",
       "   '2096152098',\n",
       "   '2131357087',\n",
       "   '1548663377',\n",
       "   '2165897980',\n",
       "   '158057341',\n",
       "   '1960027552',\n",
       "   '2123142779']},\n",
       " {'id': '36903255',\n",
       "  'title': 'Hierarchical Probabilistic Neural Network Language Model.',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '1,120',\n",
       "  'abstract': 'In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Frederic Morin', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Hierarchical network model',\n",
       "   'Language model',\n",
       "   'Time delay neural network',\n",
       "   'Hierarchical clustering',\n",
       "   'Probabilistic neural network',\n",
       "   'WordNet',\n",
       "   'Nervous system network models',\n",
       "   'Component (UML)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2038721957',\n",
       "   '2116064496',\n",
       "   '2132339004',\n",
       "   '2147152072',\n",
       "   '1631260214',\n",
       "   '2096175520',\n",
       "   '2110485445',\n",
       "   '1978394996',\n",
       "   '2121227244',\n",
       "   '2127314673']},\n",
       " {'id': '2091812280',\n",
       "  'title': 'Three new graphical models for statistical language modelling',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '702',\n",
       "  'abstract': 'The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Andriy Mnih', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Graphical model',\n",
       "   'Word (computer architecture)',\n",
       "   'Probabilistic logic',\n",
       "   'Parametric model',\n",
       "   'Theoretical computer science',\n",
       "   'Sequence',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Large set (Ramsey theory)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2116064496',\n",
       "   '2132339004',\n",
       "   '1631260214',\n",
       "   '36903255',\n",
       "   '2158195707',\n",
       "   '2147010501',\n",
       "   '145476170',\n",
       "   '2437096199',\n",
       "   '1558797106']},\n",
       " {'id': '2127314673',\n",
       "  'title': 'DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '1,464',\n",
       "  'abstract': 'We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.',\n",
       "  'date': 1993,\n",
       "  'authors': ['Fernando Pereira 1, Naftali Tishby 2, Lillian Lee 3'],\n",
       "  'related_topics': ['Single-linkage clustering',\n",
       "   'Complete-linkage clustering',\n",
       "   'Correlation clustering'],\n",
       "  'references': ['2099111195',\n",
       "   '2049633694',\n",
       "   '3017143921',\n",
       "   '2121227244',\n",
       "   '2099247782',\n",
       "   '2123084125',\n",
       "   '2025887562',\n",
       "   '2059800182',\n",
       "   '2016001305',\n",
       "   '1982944197']},\n",
       " {'id': '2056590938',\n",
       "  'title': 'Connectionist language modeling for large vocabulary continuous speech recognition',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '175',\n",
       "  'abstract': 'This paper describes ongoing work on a new approach for language modeling for large vocabulary continuous speech recognition. Almost all state.. o. f-the-art systems use statistical n-gram language models estimated on text corpora. One principle problem with such language models is the fact that many of the n-grams are never observed even in very large training corpora, and therefore it is common to back-off to a lower-order model. In this paper we propose to address this problem by carrying out the estimation task in a continuous space, enabling a smooth interpolation of the probabilities. A neural network is used to learn the projection of the words onto a continuous space and to estimate the n-gram probabilities. The connectionist language model is being evaluated on the DARPA HUB5 conversational telephone speech recognition task and preliminary results show consistent improvements in both perplexity and word error rate.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Holger Schwenk', 'Jean-Luc Gauvain'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Perplexity',\n",
       "   'Word error rate',\n",
       "   'Text corpus',\n",
       "   'Connectionism',\n",
       "   'Artificial neural network',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1554663460',\n",
       "   '2132339004',\n",
       "   '2016243284',\n",
       "   '2140679639',\n",
       "   '82490022',\n",
       "   '17500809']},\n",
       " {'id': '2111305191',\n",
       "  'title': 'A bit of progress in language modeling',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '630',\n",
       "  'abstract': 'In the past several years, a number of different language modeling improvements over simple trigram models have been found, including caching, higher-order n -grams, skipping, interpolated Kneser?Ney smoothing, and clustering. We present explorations of variations on, or of the limits of, each of these techniques, including showing that sentence mixture models may have more potential. While all of these techniques have been studied separately, they have rarely been studied in combination. We compare a combination of all techniques together to a Katz smoothed trigram model with no count cutoffs. We achieve perplexity reductions between 38 and 50% (1 bit of entropy), depending on training data size, as well as a word error rate reduction of 8.9%. Our perplexity reductions are perhaps the highest reported compared to a fair baseline.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Joshua T. Goodman'],\n",
       "  'related_topics': ['Perplexity',\n",
       "   'Trigram',\n",
       "   'Word error rate',\n",
       "   'Smoothing',\n",
       "   'Language model',\n",
       "   'Mixture model',\n",
       "   'Cluster analysis',\n",
       "   'Entropy (information theory)',\n",
       "   'Statistics',\n",
       "   'Speech recognition',\n",
       "   'Mathematics'],\n",
       "  'references': ['2170120409',\n",
       "   '2158195707',\n",
       "   '2121227244',\n",
       "   '1996764654',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '2127314673',\n",
       "   '2595741664',\n",
       "   '2134237567',\n",
       "   '47415966']},\n",
       " {'id': '1558797106',\n",
       "  'title': 'Quick Training of Probabilistic Neural Nets by Importance Sampling.',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '206',\n",
       "  'abstract': '',\n",
       "  'date': 2002,\n",
       "  'authors': ['Yoshua Bengio', 'Jean-Sébastien Senecal'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Artificial neural network',\n",
       "   'Probabilistic logic',\n",
       "   'Importance sampling',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Training (meteorology)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2116064496',\n",
       "   '2132339004',\n",
       "   '1574901103',\n",
       "   '2096175520',\n",
       "   '1535015163',\n",
       "   '2092654472',\n",
       "   '1746680969',\n",
       "   '2134237567',\n",
       "   '145476170',\n",
       "   '1989705153']},\n",
       " {'id': '2963542991',\n",
       "  'title': 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,487',\n",
       "  'abstract': 'Abstract: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Pierre Sermanet',\n",
       "   'David Eigen',\n",
       "   'Xiang Zhang',\n",
       "   'Michael Mathieu',\n",
       "   'Rob Fergus',\n",
       "   'Yann LeCun'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Feature (computer vision)',\n",
       "   'Sliding window protocol',\n",
       "   'Pattern recognition',\n",
       "   'Task (project management)',\n",
       "   'Bounding overwatch',\n",
       "   'Object (computer science)',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Scale (ratio)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '639708223',\n",
       "   '1903029394',\n",
       "   '2155893237']},\n",
       " {'id': '2963911037',\n",
       "  'title': 'Network In Network',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,847',\n",
       "  'abstract': 'Abstract: We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Min Lin', 'Qiang Chen', 'Shuicheng Yan'],\n",
       "  'related_topics': ['Multilayer perceptron',\n",
       "   'Activation function',\n",
       "   'Artificial neural network',\n",
       "   'MNIST database',\n",
       "   'Overfitting',\n",
       "   'Feature (machine learning)',\n",
       "   'Linear filter',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2962835968',\n",
       "   '2097117768',\n",
       "   '2117539524',\n",
       "   '1536680647',\n",
       "   '2963446712',\n",
       "   '2963037989',\n",
       "   '2109255472',\n",
       "   '2096733369',\n",
       "   '2302255633']},\n",
       " {'id': '2068730032',\n",
       "  'title': 'Scalable Object Detection Using Deep Neural Networks',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '1,074',\n",
       "  'abstract': 'Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Dumitru Erhan',\n",
       "   'Christian Szegedy',\n",
       "   'Alexander Toshev',\n",
       "   'Dragomir Anguelov'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Object detection',\n",
       "   'Minimum bounding box',\n",
       "   'Convolutional neural network',\n",
       "   '3D single-object recognition',\n",
       "   'Artificial neural network',\n",
       "   'Object (computer science)',\n",
       "   'Context (language use)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2168356304',\n",
       "   '2031489346',\n",
       "   '2088049833',\n",
       "   '2130306094',\n",
       "   '2129305389',\n",
       "   '2017691720',\n",
       "   '2128715914',\n",
       "   '2122146326']},\n",
       " {'id': '2031489346',\n",
       "  'title': 'The Pascal Visual Object Classes (VOC) Challenge',\n",
       "  'reference_count': '57',\n",
       "  'citation_count': '11,341',\n",
       "  'abstract': 'The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Mark Everingham 1, Luc Gool 2, Christopher K. Williams 3, John Winn 4, Andrew Zisserman 5'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pascal (programming language)',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Image processing',\n",
       "   'Annotation',\n",
       "   'Object category recognition',\n",
       "   'Semantic image segmentation',\n",
       "   'Statistical analysis'],\n",
       "  'references': []},\n",
       " {'id': '2088049833',\n",
       "  'title': 'Selective Search for Object Recognition',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '5,522',\n",
       "  'abstract': 'This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).',\n",
       "  'date': 2013,\n",
       "  'authors': ['J. R. Uijlings 1, K. E. Sande 2, T. Gevers 2, A. W. Smeulders 2'],\n",
       "  'related_topics': ['Beam search',\n",
       "   'Brute-force search',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Segmentation',\n",
       "   'Software',\n",
       "   'Pattern recognition',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2164598857',\n",
       "   '2031489346',\n",
       "   '3097096317',\n",
       "   '2162915993',\n",
       "   '2163352848',\n",
       "   '2121947440',\n",
       "   '2110158442']},\n",
       " {'id': '2155541015',\n",
       "  'title': 'DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '4,386',\n",
       "  'abstract': 'We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Jeff Donahue',\n",
       "   'Yangqing Jia',\n",
       "   'Oriol Vinyals',\n",
       "   'Judy Hoffman',\n",
       "   'Ning Zhang',\n",
       "   'Eric Tzeng',\n",
       "   'Trevor Darrell'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Machine learning',\n",
       "   'Set (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Concept learning',\n",
       "   'Computer science',\n",
       "   'Range (mathematics)',\n",
       "   'Variety (cybernetics)',\n",
       "   'Artificial intelligence',\n",
       "   'Visual recognition'],\n",
       "  'references': ['2618530766',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '1904365287',\n",
       "   '2187089797',\n",
       "   '1677409904',\n",
       "   '2546302380']},\n",
       " {'id': '1663973292',\n",
       "  'title': 'Pattern Recognition and Machine Learning',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '52,518',\n",
       "  'abstract': 'Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Christopher M. Bishop'],\n",
       "  'related_topics': ['Kernel method',\n",
       "   'Kernel (statistics)',\n",
       "   'Graphical model',\n",
       "   'Variational Bayesian methods',\n",
       "   'Approximate inference',\n",
       "   'Artificial neural network',\n",
       "   'Mixture model',\n",
       "   'Linear model',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2117812871', '1496357020']},\n",
       " {'id': '2109255472',\n",
       "  'title': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '5,746',\n",
       "  'abstract': 'Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224 $\\\\times$ 224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 $\\\\times$ faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 3, Jian Sun 1'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Contextual image classification',\n",
       "   'Object detection'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2153635508',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304']},\n",
       " {'id': '753012316',\n",
       "  'title': 'Torch7: A Matlab-like Environment for Machine Learning',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '1,762',\n",
       "  'abstract': 'Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua’s light interface.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Ronan Collobert 1, Koray Kavukcuoglu 2, Clément Farabet 2'],\n",
       "  'related_topics': ['Scripting language',\n",
       "   'CUDA',\n",
       "   'Interface (Java)',\n",
       "   'MATLAB',\n",
       "   'Software',\n",
       "   'Theano',\n",
       "   'Computer science',\n",
       "   'Flexibility (engineering)',\n",
       "   'Machine learning',\n",
       "   'Implementation',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2606594511']},\n",
       " {'id': '1825604117',\n",
       "  'title': 'Open-vocabulary Object Retrieval',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '81',\n",
       "  'abstract': '',\n",
       "  'date': 2014,\n",
       "  'authors': ['Sergio Guadarrama 1, Erik Rodner 2, Kate Saenko 3, Ning Zhang 1, Ryan Farrell 1, Jeff Donahue 1, Trevor Darrell 1'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Deep-sky object',\n",
       "   'Artificial intelligence',\n",
       "   'Vocabulary Object'],\n",
       "  'references': ['2088049833',\n",
       "   '2131846894',\n",
       "   '2128017662',\n",
       "   '2141362318',\n",
       "   '2094728533',\n",
       "   '1889268436',\n",
       "   '1618905105',\n",
       "   '21006490',\n",
       "   '1897761818',\n",
       "   '2066134726']},\n",
       " {'id': '2147414309',\n",
       "  'title': 'PANDA: Pose Aligned Networks for Deep Attribute Modeling',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '513',\n",
       "  'abstract': 'We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.',\n",
       "  'date': 2014,\n",
       "  'authors': [\"Ning Zhang 1, Manohar Paluri 2, Marc'Aurelio Ranzato 2, Trevor Darrell 1, Lubomir Bourdev 2\"],\n",
       "  'related_topics': ['3D pose estimation',\n",
       "   'Deep learning',\n",
       "   'Context (language use)',\n",
       "   'Minimum bounding box',\n",
       "   'Artificial neural network',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Expression (mathematics)',\n",
       "   'Variation (game tree)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2168356304',\n",
       "   '2310919327',\n",
       "   '2155541015',\n",
       "   '2162915993',\n",
       "   '2546302380',\n",
       "   '1498436455',\n",
       "   '2536626143',\n",
       "   '2253807446',\n",
       "   '2098411764']},\n",
       " {'id': '1872489089',\n",
       "  'title': 'Pylearn2: a machine learning research library',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '307',\n",
       "  'abstract': \"Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.\",\n",
       "  'date': 2013,\n",
       "  'authors': ['Ian J. Goodfellow',\n",
       "   'David Warde-Farley',\n",
       "   'Pascal Lamblin',\n",
       "   'Vincent Dumoulin',\n",
       "   'Mehdi Mirza',\n",
       "   'Razvan Pascanu',\n",
       "   'James Bergstra',\n",
       "   'Frédéric Bastien',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Flexibility (engineering)',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Order (business)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': []},\n",
       " {'id': '2899771611',\n",
       "  'title': 'Automatic differentiation in PyTorch',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '7,688',\n",
       "  'abstract': '',\n",
       "  'date': 2017,\n",
       "  'authors': ['Adam Paszke',\n",
       "   'Sam Gross',\n",
       "   'Soumith Chintala',\n",
       "   'Gregory Chanan',\n",
       "   'Edward Yang',\n",
       "   'Zachary DeVito',\n",
       "   'Zeming Lin',\n",
       "   'Alban Desmaison',\n",
       "   'Luca Antiga',\n",
       "   'Adam Lerer'],\n",
       "  'related_topics': ['Automatic differentiation',\n",
       "   'Computer science',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2754835109', '1585773866', '1579027943']},\n",
       " {'id': '2962784628',\n",
       "  'title': 'Neural Machine Translation of Rare Words with Subword Units',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '3,854',\n",
       "  'abstract': 'Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Rico Sennrich', 'Barry Haddow', 'Alexandra Birch'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Text segmentation',\n",
       "   'Vocabulary',\n",
       "   'Natural language processing',\n",
       "   'Transliteration',\n",
       "   'Data compression',\n",
       "   'Computer science',\n",
       "   'Transformer (machine learning model)',\n",
       "   'BLEU',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2157331557',\n",
       "   '1902237438',\n",
       "   '6908809',\n",
       "   '1753482797',\n",
       "   '2124807415',\n",
       "   '2251012068',\n",
       "   '1815076433',\n",
       "   '2100664567']},\n",
       " {'id': '1840435438',\n",
       "  'title': 'A large annotated corpus for learning natural language inference',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '1,988',\n",
       "  'abstract': 'Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Samuel R. Bowman',\n",
       "   'Gabor Angeli',\n",
       "   'Christopher Potts',\n",
       "   'Christopher D. Manning'],\n",
       "  'related_topics': ['Textual entailment',\n",
       "   'Inference',\n",
       "   'Natural language',\n",
       "   'Sentence',\n",
       "   'Logical consequence',\n",
       "   'Natural language processing',\n",
       "   'Closed captioning',\n",
       "   'Artificial neural network',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2095705004',\n",
       "   '2250539671',\n",
       "   '2064675550',\n",
       "   '2251939518',\n",
       "   '6908809',\n",
       "   '2081580037',\n",
       "   '2097606805',\n",
       "   '2185175083',\n",
       "   '2584341106',\n",
       "   '2154359981']},\n",
       " {'id': '2963026768',\n",
       "  'title': 'Universal Language Model Fine-tuning for Text Classification',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '1,951',\n",
       "  'abstract': 'Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Jeremy Howard', 'Sebastian Ruder'],\n",
       "  'related_topics': ['Inductive transfer',\n",
       "   'Language model',\n",
       "   'Transfer of learning',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Code (cryptography)',\n",
       "   'Key (cryptography)',\n",
       "   'Universal language',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '1836465849',\n",
       "   '1903029394',\n",
       "   '2153579005',\n",
       "   '2963446712',\n",
       "   '2962739339',\n",
       "   '2155541015',\n",
       "   '2165698076',\n",
       "   '2062118960',\n",
       "   '1948751323']},\n",
       " {'id': '2996035354',\n",
       "  'title': 'ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '389',\n",
       "  'abstract': 'While masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Kevin Clark 1, Minh-Thang Luong 2, Quoc V. Le 2, Christopher D. Manning 1'],\n",
       "  'related_topics': ['Security token',\n",
       "   'Language model',\n",
       "   'Discriminative model',\n",
       "   'Feature learning',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Speech recognition',\n",
       "   'Encoder',\n",
       "   'Computer science',\n",
       "   'Natural language understanding'],\n",
       "  'references': ['2964121744',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '2099471712',\n",
       "   '1614298861',\n",
       "   '2250539671',\n",
       "   '2962739339',\n",
       "   '2963684088',\n",
       "   '2158899491',\n",
       "   '2251939518']},\n",
       " {'id': '2990704537',\n",
       "  'title': 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '377',\n",
       "  'abstract': 'In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Alex Wang 1, Yada Pruksachatkun 1, Nikita Nangia 1, Amanpreet Singh 2, Julian Michael 3, Felix Hill 4, Omer Levy 2, Samuel R. Bowman 1'],\n",
       "  'related_topics': ['General-purpose language',\n",
       "   'Benchmark (computing)',\n",
       "   'Transfer of learning',\n",
       "   'Software engineering',\n",
       "   'Computer science',\n",
       "   'Set (psychology)',\n",
       "   'Software',\n",
       "   'Metric (mathematics)'],\n",
       "  'references': ['2965373594',\n",
       "   '3098903812',\n",
       "   '2980282514',\n",
       "   '3098824823',\n",
       "   '3127550471',\n",
       "   '3100307207',\n",
       "   '2970986510',\n",
       "   '3007332492']},\n",
       " {'id': '3127550471',\n",
       "  'title': 'Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks',\n",
       "  'reference_count': '60',\n",
       "  'citation_count': '283',\n",
       "  'abstract': 'Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Suchin Gururangan 1, Ana Marasović 1, Swabha Swayamdipta 1, Kyle Lo 1, Iz Beltagy 1, Doug Downey 2, Noah A. Smith 1'],\n",
       "  'related_topics': ['Task (project management)',\n",
       "   'Language model',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Variety (linguistics)',\n",
       "   'Domain (software engineering)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964121744',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '2965373594',\n",
       "   '2970597249',\n",
       "   '2525778437',\n",
       "   '2963026768',\n",
       "   '2963012544',\n",
       "   '2113459411',\n",
       "   '2923014074']},\n",
       " {'id': '3105966348',\n",
       "  'title': 'TinyBERT: Distilling BERT for Natural Language Understanding',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '282',\n",
       "  'abstract': 'Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be effectively transferred to a small “student” TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Xiaoqi Jiao 1, Yichun Yin 2, Lifeng Shang 2, Xin Jiang 2, Xiao Chen 2, Linlin Li 2, Fang Wang 2, Qun Liu 2'],\n",
       "  'related_topics': ['Transformer (machine learning model)',\n",
       "   'Language model',\n",
       "   'Natural language understanding',\n",
       "   'Inference',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3138154797',\n",
       "   '3101045333',\n",
       "   '3092111011',\n",
       "   '3064953855',\n",
       "   '3037029945',\n",
       "   '3098576111',\n",
       "   '3146365155']},\n",
       " {'id': '3100307207',\n",
       "  'title': 'Experience Grounds Language',\n",
       "  'reference_count': '181',\n",
       "  'citation_count': '65',\n",
       "  'abstract': 'Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yonatan Bisk 1, Ari Holtzman 2, Jesse Thomason 2, Jacob Andreas 3, Yoshua Bengio 4, Joyce Chai 5, Mirella Lapata 6, Angeliki Lazaridou 7, Jonathan May 8, Aleksandr Nisnevich 9, Nicolas Pinto 3, Joseph P. Turian 4'],\n",
       "  'related_topics': ['Cognitive science',\n",
       "   'Feature learning',\n",
       "   'Social environment',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Language research',\n",
       "   'Language understanding'],\n",
       "  'references': []},\n",
       " {'id': '3100345210',\n",
       "  'title': 'Supervised Contrastive Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '181',\n",
       "  'abstract': 'Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Prannay Khosla',\n",
       "   'Piotr Teterwak 1, Chen Wang 2, Aaron Sarna 1, Yonglong Tian 3, Phillip Isola 3, Aaron Maschinot 1, Ce Liu 1, Dilip Krishnan 1'],\n",
       "  'related_topics': ['Supervised learning',\n",
       "   'Cross entropy',\n",
       "   'Hyperparameter',\n",
       "   'Leverage (statistics)',\n",
       "   'Robustness (computer science)',\n",
       "   'Contextual image classification',\n",
       "   'Machine learning',\n",
       "   'Calibration (statistics)',\n",
       "   'Embedding',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3095121901',\n",
       "   '3106428938',\n",
       "   '3103215803',\n",
       "   '3109684201',\n",
       "   '3128741952',\n",
       "   '3124635886',\n",
       "   '3122924117']},\n",
       " {'id': '2963918774',\n",
       "  'title': 'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '1,313',\n",
       "  'abstract': 'Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Alexis Conneau 1, Douwe Kiela 2, Holger Schwenk 3, Loïc Barrault 3, Antoine Bordes 1'],\n",
       "  'related_topics': ['Supervised learning',\n",
       "   'Sentence',\n",
       "   'Transfer of learning',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Computer science',\n",
       "   'Encoder',\n",
       "   'Range (mathematics)',\n",
       "   'Base (topology)',\n",
       "   'Artificial intelligence',\n",
       "   'Natural language inference'],\n",
       "  'references': ['2194775991',\n",
       "   '2964121744',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2108598243',\n",
       "   '2130942839',\n",
       "   '2064675550',\n",
       "   '2158899491',\n",
       "   '2131744502',\n",
       "   '2145287260']},\n",
       " {'id': '2963846996',\n",
       "  'title': 'A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '1,340',\n",
       "  'abstract': 'This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Adina Williams', 'Nikita Nangia', 'Samuel R. Bowman'],\n",
       "  'related_topics': ['Textual entailment',\n",
       "   'Inference',\n",
       "   'Sentence',\n",
       "   'Natural language processing',\n",
       "   'Task (project management)',\n",
       "   'Resource (project management)',\n",
       "   'Computer science',\n",
       "   'Agreement',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2964121744',\n",
       "   '2095705004',\n",
       "   '2250539671',\n",
       "   '1849277567',\n",
       "   '2064675550',\n",
       "   '2155541015',\n",
       "   '1840435438',\n",
       "   '2963918774',\n",
       "   '1632114991']},\n",
       " {'id': '2525127255',\n",
       "  'title': 'The PASCAL Recognising Textual Entailment Challenge',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,185',\n",
       "  'abstract': 'This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark 1 . The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Ido Dagan', 'Oren Glickman', 'Bernardo Magnini'],\n",
       "  'related_topics': ['Textual entailment',\n",
       "   'Pascal (programming language)',\n",
       "   'Semantics',\n",
       "   'Natural language processing',\n",
       "   'Inference',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2965373594',\n",
       "   '1840435438',\n",
       "   '2923014074',\n",
       "   '1970381522',\n",
       "   '2963846996',\n",
       "   '2996428491',\n",
       "   '2185175083']},\n",
       " {'id': '2962736243',\n",
       "  'title': 'Annotation Artifacts in Natural Language Inference Data',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '512',\n",
       "  'abstract': 'Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Suchin Gururangan 1, Swabha Swayamdipta 2, Omer Levy 1, Roy Schwartz 1, 3, Samuel R. Bowman 4, Noah A. Smith 1'],\n",
       "  'related_topics': ['Inference',\n",
       "   'Premise',\n",
       "   'Sentence',\n",
       "   'Negation',\n",
       "   'Vagueness',\n",
       "   'Natural language processing',\n",
       "   'Annotation',\n",
       "   'Task (project management)',\n",
       "   'Protocol (object-oriented programming)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2963748441',\n",
       "   '1840435438',\n",
       "   '1544827683',\n",
       "   '1933349210',\n",
       "   '2963626623',\n",
       "   '2963918774',\n",
       "   '2963846996',\n",
       "   '2413794162',\n",
       "   '2963969878',\n",
       "   '2962809918']},\n",
       " {'id': '3104033643',\n",
       "  'title': 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation',\n",
       "  'reference_count': '79',\n",
       "  'citation_count': '610',\n",
       "  'abstract': 'Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).',\n",
       "  'date': 2017,\n",
       "  'authors': ['Daniel M. Cer 1, Mona T. Diab 2, Eneko Agirre 3, Iñigo Lopez-Gazpio 3, Lucia Specia 4'],\n",
       "  'related_topics': ['SemEval',\n",
       "   'Semantic search',\n",
       "   'Automatic summarization',\n",
       "   'Machine translation',\n",
       "   'Semantics',\n",
       "   'Question answering',\n",
       "   'Natural language understanding',\n",
       "   'Natural language processing',\n",
       "   'Dialog box',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2250539671',\n",
       "   '2064675550',\n",
       "   '2131744502',\n",
       "   '2123442489',\n",
       "   '2038721957',\n",
       "   '1840435438',\n",
       "   '1486649854',\n",
       "   '2963626623']},\n",
       " {'id': '2130158090',\n",
       "  'title': 'The Third PASCAL Recognizing Textual Entailment Challenge',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '767',\n",
       "  'abstract': \"This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.\",\n",
       "  'date': 2007,\n",
       "  'authors': ['Danilo Giampiccolo 1, Bernardo Magnini 2, Ido Dagan 3, Bill Dolan 4'],\n",
       "  'related_topics': ['Textual entailment',\n",
       "   'Pascal (programming language)',\n",
       "   'Question answering',\n",
       "   'Natural language processing',\n",
       "   'Machine translation',\n",
       "   'Information extraction',\n",
       "   'Logical consequence',\n",
       "   'Computer science',\n",
       "   'Reading comprehension',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2912565176',\n",
       "   '2525127255',\n",
       "   '2115792525',\n",
       "   '2396767181',\n",
       "   '2102065370',\n",
       "   '2002664886',\n",
       "   '1990524510',\n",
       "   '137514618',\n",
       "   '2182572585',\n",
       "   '2134061542']},\n",
       " {'id': '2525778437',\n",
       "  'title': \"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\",\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '4,725',\n",
       "  'abstract': 'Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT\\'s use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google\\'s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT\\'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google\\'s phrase-based production system.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Yonghui Wu',\n",
       "   'Mike Schuster',\n",
       "   'Zhifeng Chen',\n",
       "   'Quoc V. Le',\n",
       "   'Mohammad Norouzi',\n",
       "   'Wolfgang Macherey',\n",
       "   'Maxim Krikun',\n",
       "   'Yuan Cao',\n",
       "   'Qin Gao',\n",
       "   'Klaus Macherey',\n",
       "   'Jeff Klingner',\n",
       "   'Apurva Shah',\n",
       "   'Melvin Johnson',\n",
       "   'Xiaobing Liu',\n",
       "   'Łukasz Kaiser',\n",
       "   'Stephan Gouws',\n",
       "   'Yoshikiyo Kato',\n",
       "   'Taku Kudo',\n",
       "   'Hideto Kazawa',\n",
       "   'Keith Stevens',\n",
       "   'George Kurian',\n",
       "   'Nishant Patil',\n",
       "   'Wei Wang',\n",
       "   'Cliff Young',\n",
       "   'Jason Smith',\n",
       "   'Jason Riesa',\n",
       "   'Alex Rudnick',\n",
       "   'Oriol Vinyals',\n",
       "   'Greg Corrado',\n",
       "   'Macduff Hughes',\n",
       "   'Jeffrey Dean'],\n",
       "  'related_topics': ['Example-based machine translation',\n",
       "   'Rule-based machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Deep learning',\n",
       "   'Sentence',\n",
       "   'Beam search',\n",
       "   'Phrase',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964121744', '2121879602', '2251743902', '1968594024']},\n",
       " {'id': '2963756346',\n",
       "  'title': 'Learned in translation: contextualized word vectors',\n",
       "  'reference_count': '64',\n",
       "  'citation_count': '655',\n",
       "  'abstract': 'Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Bryan McCann',\n",
       "   'James Bradbury',\n",
       "   'Caiming Xiong',\n",
       "   'Richard Socher'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Sentiment analysis',\n",
       "   'Context (language use)',\n",
       "   'Question answering',\n",
       "   'Word (computer architecture)',\n",
       "   'Natural language processing',\n",
       "   'Initialization',\n",
       "   'Character (computing)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2964308564',\n",
       "   '1614298861',\n",
       "   '2250539671',\n",
       "   '2108598243',\n",
       "   '2130942839']},\n",
       " {'id': '2135046866',\n",
       "  'title': 'Regression Shrinkage and Selection via the Lasso',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '40,310',\n",
       "  'abstract': \"SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.\",\n",
       "  'date': 1995,\n",
       "  'authors': ['Robert Tibshirani'],\n",
       "  'related_topics': ['Lasso (statistics)',\n",
       "   'Elastic net regularization',\n",
       "   'Residual sum of squares',\n",
       "   'Least-angle regression',\n",
       "   'Linear model',\n",
       "   'g-prior',\n",
       "   'Design matrix',\n",
       "   'Shrinkage estimator',\n",
       "   'Applied mathematics',\n",
       "   'Statistics',\n",
       "   'Mathematics'],\n",
       "  'references': ['1995945562',\n",
       "   '1594031697',\n",
       "   '2158940042',\n",
       "   '2797583072',\n",
       "   '2106706098',\n",
       "   '2102201073',\n",
       "   '2117897510',\n",
       "   '191129667',\n",
       "   '2954064014',\n",
       "   '2007069447']},\n",
       " {'id': '2131241448',\n",
       "  'title': 'Practical Bayesian Optimization of Machine Learning Algorithms',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '3,926',\n",
       "  'abstract': 'The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm\\'s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Jasper Snoek 1, Hugo Larochelle 2, Ryan P Adams 3'],\n",
       "  'related_topics': ['Weighted Majority Algorithm',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Bayesian optimization',\n",
       "   'Hyperparameter optimization',\n",
       "   'Support vector machine',\n",
       "   'Convolutional neural network',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Hyperparameter',\n",
       "   'Gaussian process',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Generalization error'],\n",
       "  'references': ['3118608800',\n",
       "   '1746819321',\n",
       "   '2141125852',\n",
       "   '2097998348',\n",
       "   '2106411961',\n",
       "   '2951665052',\n",
       "   '60686164',\n",
       "   '2165599843',\n",
       "   '2099201756',\n",
       "   '1973333099']},\n",
       " {'id': '2335728318',\n",
       "  'title': 'Reading Digits in Natural Images with Unsupervised Feature Learning',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '2,814',\n",
       "  'abstract': 'Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Yuval Netzer 1, Tao Wang 1, Adam Coates 1, Alessandro Bissacco 2, Bo Wu 2, Andrew Y. Ng 2'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Reading (process)',\n",
       "   'Pattern recognition'],\n",
       "  'references': ['2161969291',\n",
       "   '2122410182',\n",
       "   '2145094598',\n",
       "   '2147768505',\n",
       "   '2145607950',\n",
       "   '2097018403',\n",
       "   '2118858186',\n",
       "   '2144161366',\n",
       "   '1998042868',\n",
       "   '2132424367']},\n",
       " {'id': '2296319761',\n",
       "  'title': 'Convex Optimization',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '59,323',\n",
       "  'abstract': 'Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Stephen Boyd 1, Lieven Vandenberghe 2'],\n",
       "  'related_topics': ['Conic optimization',\n",
       "   'Convex optimization',\n",
       "   'Nonlinear programming',\n",
       "   'Engineering optimization',\n",
       "   'Multi-objective optimization',\n",
       "   'Drift plus penalty',\n",
       "   'Semidefinite programming',\n",
       "   'Geometric programming',\n",
       "   'Management science',\n",
       "   'Mathematics'],\n",
       "  'references': ['2030723843',\n",
       "   '2000296233',\n",
       "   '2020830442',\n",
       "   '2006980285',\n",
       "   '82689443',\n",
       "   '2611147814']},\n",
       " {'id': '3120740533',\n",
       "  'title': 'UCI Machine Learning Repository',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '32,061',\n",
       "  'abstract': '',\n",
       "  'date': 2006,\n",
       "  'authors': ['A. Asuncion'],\n",
       "  'related_topics': ['Ensembles of classifiers',\n",
       "   'LPBoost',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Associative classifier',\n",
       "   'Ensemble diversity',\n",
       "   'Ensemble selection',\n",
       "   'Instance selection'],\n",
       "  'references': ['2146502635',\n",
       "   '2963399829',\n",
       "   '2786672974',\n",
       "   '3100515187',\n",
       "   '2963977107',\n",
       "   '2099419573',\n",
       "   '2170505850',\n",
       "   '2106525823',\n",
       "   '2964318098']},\n",
       " {'id': '2798766386',\n",
       "  'title': 'Nonlinear Programming',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '17,017',\n",
       "  'abstract': '',\n",
       "  'date': 1994,\n",
       "  'authors': ['Dimitri Bertsekas'],\n",
       "  'related_topics': ['Nonlinear programming',\n",
       "   'Fritz John conditions',\n",
       "   'Computer science',\n",
       "   'Mathematical optimization',\n",
       "   'Random coordinate descent'],\n",
       "  'references': []},\n",
       " {'id': '2610857016',\n",
       "  'title': 'Matrix Analysis',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '45,346',\n",
       "  'abstract': 'Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This new edition of the acclaimed text presents results of both classic and recent matrix analyses using canonical forms as a unifying theme, and demonstrates their importance in a variety of applications. The authors have thoroughly revised, updated, and expanded on the first edition. The book opens with an extended summary of useful concepts and facts and includes numerous new topics and features, such as: - New sections on the singular value and CS decompositions - New applications of the Jordan canonical form - A new section on the Weyr canonical form - Expanded treatments of inverse problems and of block matrices - A central role for the Von Neumann trace theorem - A new appendix with a modern list of canonical forms for a pair of Hermitian matrices and for a symmetric-skew symmetric pair - Expanded index with more than 3,500 entries for easy reference - More than 1,100 problems and exercises, many with hints, to reinforce understanding and develop auxiliary themes such as finite-dimensional quantum systems, the compound and adjugate matrices, and the Loewner ellipsoid - A new appendix provides a collection of problem-solving hints.',\n",
       "  'date': 1984,\n",
       "  'authors': ['Roger A. Horn 1, Charles R. Johnson 2'],\n",
       "  'related_topics': ['Weyr canonical form',\n",
       "   'Canonical form',\n",
       "   'Matrix (mathematics)',\n",
       "   'Adjugate matrix',\n",
       "   'Commuting matrices',\n",
       "   'Involutory matrix',\n",
       "   'Matrix analysis',\n",
       "   'Hermitian matrix',\n",
       "   'Algebra',\n",
       "   'Pure mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2146502635',\n",
       "   '2107396783',\n",
       "   '2118040894',\n",
       "   '2139212933',\n",
       "   '2053186076',\n",
       "   '2160643434',\n",
       "   '2165744313',\n",
       "   '2146890818',\n",
       "   '2151795416']},\n",
       " {'id': '2150102617',\n",
       "  'title': 'RCV1: A New Benchmark Collection for Text Categorization Research',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '3,020',\n",
       "  'abstract': \"Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.\",\n",
       "  'date': 2004,\n",
       "  'authors': ['David D. Lewis', 'Yiming Yang', 'Tony G. Rose', 'Fan Li'],\n",
       "  'related_topics': ['Documentation',\n",
       "   'Supervised learning',\n",
       "   'Information retrieval'],\n",
       "  'references': ['2118020653',\n",
       "   '2149684865',\n",
       "   '2118202495',\n",
       "   '2098162425',\n",
       "   '2435251607',\n",
       "   '2114535528',\n",
       "   '2005422315',\n",
       "   '2107008379',\n",
       "   '2000672666',\n",
       "   '1620204465']},\n",
       " {'id': '2167732364',\n",
       "  'title': 'Smooth minimization of non-smooth functions',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '2,967',\n",
       "  'abstract': 'In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from ** keeping basically the complexity of each iteration unchanged.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Yu Nesterov'],\n",
       "  'related_topics': ['Convex optimization',\n",
       "   'Proximal Gradient Methods',\n",
       "   'Random coordinate descent',\n",
       "   'Smoothing',\n",
       "   'Functional decomposition',\n",
       "   'Minification',\n",
       "   'Mathematical optimization',\n",
       "   'Numerical analysis',\n",
       "   'Mathematics',\n",
       "   'Non smooth'],\n",
       "  'references': ['3141595720',\n",
       "   '1669104078',\n",
       "   '1568307856',\n",
       "   '2124541940',\n",
       "   '1553702074',\n",
       "   '1568288633',\n",
       "   '2015263936',\n",
       "   '2150126561',\n",
       "   '2969945254',\n",
       "   '2008164266']},\n",
       " {'id': '1992208280',\n",
       "  'title': 'Robust Stochastic Approximation Approach to Stochastic Programming',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '1,886',\n",
       "  'abstract': 'In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.',\n",
       "  'date': 2008,\n",
       "  'authors': ['A. Nemirovski 1, A. Juditsky 2, G. Lan 1, A. Shapiro 1'],\n",
       "  'related_topics': ['Stochastic optimization',\n",
       "   'Stochastic approximation',\n",
       "   'Stochastic programming',\n",
       "   'Subgradient method',\n",
       "   'Optimization problem',\n",
       "   'Monte Carlo method',\n",
       "   'Mathematical optimization',\n",
       "   'Saddle point',\n",
       "   'Structure (category theory)',\n",
       "   'Mathematics'],\n",
       "  'references': ['2038669746',\n",
       "   '2169713291',\n",
       "   '203276351',\n",
       "   '2064076655',\n",
       "   '1983916623',\n",
       "   '2090359754',\n",
       "   '2000257769',\n",
       "   '2086161653',\n",
       "   '1490324987',\n",
       "   '2000953623']},\n",
       " {'id': '2160218441',\n",
       "  'title': 'Online Passive-Aggressive Algorithms',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '2,132',\n",
       "  'abstract': 'We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Koby Crammer 1, 2, Ofer Dekel 2, Joseph Keshet 2, Shai Shalev-Shwartz 2, Yoram Singer 2'],\n",
       "  'related_topics': ['Margin (machine learning)',\n",
       "   'Decision problem',\n",
       "   'Lemma (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Series (mathematics)',\n",
       "   'Binary number',\n",
       "   'Categorization',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Regression',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296319761',\n",
       "   '2148603752',\n",
       "   '1563088657',\n",
       "   '1560724230',\n",
       "   '1601740268',\n",
       "   '2053463056',\n",
       "   '2032210760',\n",
       "   '1978394996',\n",
       "   '2101276256',\n",
       "   '2157791002']},\n",
       " {'id': '1978394996',\n",
       "  'title': 'Term Weighting Approaches in Automatic Text Retrieval',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '11,546',\n",
       "  'abstract': 'The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Gerard Salton', 'Christopher Buckley'],\n",
       "  'related_topics': ['Term Discrimination',\n",
       "   'Weighting',\n",
       "   'Term (time)',\n",
       "   'tf–idf',\n",
       "   'Term indexing',\n",
       "   'Automatic indexing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Probability distribution',\n",
       "   'Baseline (configuration management)'],\n",
       "  'references': ['1956559956',\n",
       "   '2043909051',\n",
       "   '2083605078',\n",
       "   '2068632118',\n",
       "   '3090556797',\n",
       "   '2095396650',\n",
       "   '2075006521',\n",
       "   '11171803',\n",
       "   '3091372544',\n",
       "   '1557757161']},\n",
       " {'id': '2141125852',\n",
       "  'title': 'Multi-column deep neural networks for image classification',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '4,696',\n",
       "  'abstract': 'Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Dan Cireşan', 'Ueli Meier', 'Juergen Schmidhuber'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'MNIST database',\n",
       "   'Traffic sign recognition',\n",
       "   'Contextual image classification',\n",
       "   'Pattern recognition',\n",
       "   'Receptive field',\n",
       "   'Machine learning',\n",
       "   'Visual cortex',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3118608800',\n",
       "   '2310919327',\n",
       "   '2110798204',\n",
       "   '2154642048',\n",
       "   '2134557905',\n",
       "   '2156163116',\n",
       "   '2138857742',\n",
       "   '2148461049',\n",
       "   '2144982973',\n",
       "   '2139427956']},\n",
       " {'id': '2118858186',\n",
       "  'title': 'An analysis of single-layer networks in unsupervised feature learning',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '2,602',\n",
       "  'abstract': 'A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several othe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (“stride”) between extracted features, and the eect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance—so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).',\n",
       "  'date': 2011,\n",
       "  'authors': ['Adam Coates 1, Andrew Y. Ng 2, Honglak Lee 1'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Feature learning',\n",
       "   'Cluster analysis',\n",
       "   'Feature extraction',\n",
       "   'Benchmark (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Hyperparameter',\n",
       "   'Gaussian',\n",
       "   'Computer science',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2162915993',\n",
       "   '2546302380',\n",
       "   '2116064496',\n",
       "   '2025768430',\n",
       "   '2130325614',\n",
       "   '2097018403',\n",
       "   '2107034620',\n",
       "   '1625255723']},\n",
       " {'id': '3141595720',\n",
       "  'title': 'Introductory Lectures on Convex Optimization: A Basic Course',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '5,590',\n",
       "  'abstract': 'It was in the middle of the 1980s, when the seminal paper by Kar- markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op- timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre- diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc- tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop- ing field, which got the name \"polynomial-time interior-point methods\", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].',\n",
       "  'date': 2014,\n",
       "  'authors': ['I︠u︡. E. Nesterov'],\n",
       "  'related_topics': ['Nonlinear programming',\n",
       "   'Convex optimization',\n",
       "   'Linear programming',\n",
       "   'Proximal Gradient Methods',\n",
       "   'Random coordinate descent',\n",
       "   'Diction',\n",
       "   'Field (computer science)',\n",
       "   'Calculus',\n",
       "   'Computer science',\n",
       "   'Epoch (reference date)'],\n",
       "  'references': ['104184427',\n",
       "   '2473418344',\n",
       "   '607505555',\n",
       "   '2107438106',\n",
       "   '2913535645',\n",
       "   '2963433607',\n",
       "   '2167732364',\n",
       "   '2622263826',\n",
       "   '1992371516']},\n",
       " {'id': '2120420045',\n",
       "  'title': 'No more pesky learning rates',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '406',\n",
       "  'abstract': 'The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Tom Schaul', 'Sixin Zhang', 'Yann LeCun'],\n",
       "  'related_topics': ['Online machine learning',\n",
       "   'Stochastic gradient descent',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Generalization error'],\n",
       "  'references': ['3118608800',\n",
       "   '2146502635',\n",
       "   '1533861849',\n",
       "   '2113651538',\n",
       "   '2914484425',\n",
       "   '2156779765',\n",
       "   '2137515395',\n",
       "   '1568229137',\n",
       "   '1598497354',\n",
       "   '2130984546']},\n",
       " {'id': '19621276',\n",
       "  'title': 'Improving the convergence of back-propagation learning with second-order methods',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '507',\n",
       "  'abstract': '',\n",
       "  'date': 1988,\n",
       "  'authors': ['S. Becker', 'Yann Lecun'],\n",
       "  'related_topics': ['Convergence (routing)',\n",
       "   'Order (business)',\n",
       "   'Backpropagation',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science'],\n",
       "  'references': ['2160699933', '1526055535']},\n",
       " {'id': '1994616650',\n",
       "  'title': 'A Stochastic Approximation Method',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '10,467',\n",
       "  'abstract': 'Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where a is a given constant. We give a method for making successive experiments at levels x1, x2, ··· in such a way that xn will tend to θ in probability.',\n",
       "  'date': 1951,\n",
       "  'authors': ['Herbert Robbins', 'Sutton Monro'],\n",
       "  'related_topics': ['Minimax approximation algorithm',\n",
       "   'Stochastic approximation',\n",
       "   'Constant (mathematics)',\n",
       "   'Continuous-time stochastic process',\n",
       "   'Approximation error',\n",
       "   'Stochastic optimization',\n",
       "   'Stochastic gradient descent',\n",
       "   'Expected value',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['6908809',\n",
       "   '1491468723',\n",
       "   '189596042',\n",
       "   '607505555',\n",
       "   '2166851633',\n",
       "   '2523246573',\n",
       "   '2225156818']},\n",
       " {'id': '2171928131',\n",
       "  'title': 'Extensions of recurrent neural network language model',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '5,360',\n",
       "  'abstract': 'We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Tomas Mikolov 1, Stefan Kombrink 1, Lukas Burget 1, Jan Cernocky 1, Sanjeev Khudanpur 2'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Recurrent neural network',\n",
       "   'Language model'],\n",
       "  'references': ['179875071',\n",
       "   '2132339004',\n",
       "   '1498436455',\n",
       "   '2110485445',\n",
       "   '2107878631',\n",
       "   '2613634265',\n",
       "   '36903255',\n",
       "   '2096072088',\n",
       "   '2111305191',\n",
       "   '2152808281']},\n",
       " {'id': '2251222643',\n",
       "  'title': 'Continuous Space Translation Models for Phrase-Based Statistical Machine Translation',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '142',\n",
       "  'abstract': 'This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Holger Schwenk'],\n",
       "  'related_topics': ['Phrase',\n",
       "   'Example-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Phrase search',\n",
       "   'Transfer-based machine translation',\n",
       "   'Rule-based machine translation',\n",
       "   'Evaluation of machine translation',\n",
       "   'Machine translation software usability',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'BLEU',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2132339004',\n",
       "   '2156985047',\n",
       "   '2146574666',\n",
       "   '1970689298',\n",
       "   '2109664771',\n",
       "   '2251098065',\n",
       "   '2140679639',\n",
       "   '2143719855',\n",
       "   '2250379827',\n",
       "   '2103078213']},\n",
       " {'id': '2006969979',\n",
       "  'title': 'The mathematics of statistical machine translation: parameter estimation',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '5,827',\n",
       "  'abstract': 'We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.',\n",
       "  'date': 1993,\n",
       "  'authors': ['Peter F. Brown',\n",
       "   'Vincent J. Della Pietra',\n",
       "   'Stephen A. Della Pietra',\n",
       "   'Robert L. Mercer'],\n",
       "  'related_topics': ['Hybrid machine translation',\n",
       "   'Example-based machine translation',\n",
       "   'Synchronous context-free grammar',\n",
       "   'Machine translation',\n",
       "   'Interactive machine translation',\n",
       "   'Transfer-based machine translation',\n",
       "   'Computer-assisted translation',\n",
       "   'Machine translation software usability',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2049633694',\n",
       "   '2121227244',\n",
       "   '2097333193',\n",
       "   '1489181569',\n",
       "   '2117652747',\n",
       "   '2129139611',\n",
       "   '2154384676',\n",
       "   '2138584836',\n",
       "   '1575431606',\n",
       "   '2048390999']},\n",
       " {'id': '196214544',\n",
       "  'title': 'Generating Text with Recurrent Neural Networks',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '1,441',\n",
       "  'abstract': 'Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling – a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Ilya Sutskever', 'James Martens', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Language model',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2064675550',\n",
       "   '179875071',\n",
       "   '1498436455',\n",
       "   '196761320',\n",
       "   '2131462252',\n",
       "   '2118706537',\n",
       "   '2107878631',\n",
       "   '2110575115',\n",
       "   '1408639475',\n",
       "   '2170942820']},\n",
       " {'id': '189596042',\n",
       "  'title': 'Deep Boltzmann machines',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '1,549',\n",
       "  'abstract': 'We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'MNIST database',\n",
       "   'Markov chain',\n",
       "   'Hidden variable theory',\n",
       "   'Boltzmann constant',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Inference',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2134557905',\n",
       "   '2096192494',\n",
       "   '2613634265',\n",
       "   '2116825644',\n",
       "   '2567948266',\n",
       "   '2159737176',\n",
       "   '2124914669']},\n",
       " {'id': '1554663460',\n",
       "  'title': 'Neural networks for pattern recognition',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '33,991',\n",
       "  'abstract': 'From the Publisher: This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Christopher M. Bishop'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Types of artificial neural networks',\n",
       "   'Feature (machine learning)',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Neural gas',\n",
       "   'Feedforward neural network',\n",
       "   'Rectifier (neural networks)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2140190241',\n",
       "   '1746819321',\n",
       "   '1570448133',\n",
       "   '2139212933',\n",
       "   '2117812871',\n",
       "   '1810943226',\n",
       "   '1964357740',\n",
       "   '2097998348']},\n",
       " {'id': '2143612262',\n",
       "  'title': 'Speech recognition with deep recurrent neural networks',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '7,601',\n",
       "  'abstract': 'Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Alex Graves', 'Abdel-rahman Mohamed', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Deep learning',\n",
       "   'Time delay neural network',\n",
       "   'TIMIT',\n",
       "   'Context (language use)',\n",
       "   'Connectionism',\n",
       "   'Speech recognition',\n",
       "   'Test set',\n",
       "   'Machine learning',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2064675550',\n",
       "   '2160815625',\n",
       "   '1993882792',\n",
       "   '2184045248',\n",
       "   '2127141656',\n",
       "   '2144499799',\n",
       "   '2108677974',\n",
       "   '3023071679',\n",
       "   '2155273149',\n",
       "   '1828163288']},\n",
       " {'id': '44815768',\n",
       "  'title': 'A Practical Guide to Training Restricted Boltzmann Machines',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '3,330',\n",
       "  'abstract': 'Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Deep belief network',\n",
       "   'Artificial intelligence',\n",
       "   'Data type',\n",
       "   'Computer science',\n",
       "   'Set (abstract data type)',\n",
       "   'Generative grammar',\n",
       "   'Training (civil)',\n",
       "   'Contrastive divergence'],\n",
       "  'references': ['2136922672',\n",
       "   '1665214252',\n",
       "   '2116064496',\n",
       "   '2099866409',\n",
       "   '2096192494',\n",
       "   '2293063825',\n",
       "   '2029949252',\n",
       "   '2116825644',\n",
       "   '2158164339',\n",
       "   '2124914669']},\n",
       " {'id': '2108677974',\n",
       "  'title': 'Practical Variational Inference for Neural Networks',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '909',\n",
       "  'abstract': 'Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Alex Graves'],\n",
       "  'related_topics': ['Stochastic neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Minimum description length',\n",
       "   'Bayesian inference',\n",
       "   'TIMIT',\n",
       "   'Inference',\n",
       "   'Variational method',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['2064675550',\n",
       "   '1498436455',\n",
       "   '2127141656',\n",
       "   '2129652681',\n",
       "   '3161062409',\n",
       "   '2170942820',\n",
       "   '2103359087',\n",
       "   '2114766824',\n",
       "   '2150218618',\n",
       "   '2054658115']},\n",
       " {'id': '2120861206',\n",
       "  'title': 'A fast and simple algorithm for training neural probabilistic language models',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '497',\n",
       "  'abstract': 'In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Andriy Mnih', 'Yee W. Teh'],\n",
       "  'related_topics': ['Probabilistic logic',\n",
       "   'Language model',\n",
       "   'Vocabulary',\n",
       "   'Importance sampling',\n",
       "   'Treebank',\n",
       "   'SIMPLE algorithm',\n",
       "   'Machine learning',\n",
       "   'Noise (video)',\n",
       "   'Normalization (statistics)',\n",
       "   'Computer science',\n",
       "   'Sentence completion tests',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2117130368',\n",
       "   '179875071',\n",
       "   '2132339004',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '1521626219',\n",
       "   '1631260214',\n",
       "   '2131462252',\n",
       "   '2138204974',\n",
       "   '2096175520']},\n",
       " {'id': '1828163288',\n",
       "  'title': 'Sequence Transduction with Recurrent Neural Networks',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '1,057',\n",
       "  'abstract': 'Many machine learning tasks can be expressed as the transformation---or \\\\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\\\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Alex Graves'],\n",
       "  'related_topics': ['Sequence learning',\n",
       "   'Recurrent neural network',\n",
       "   'Transduction (machine learning)',\n",
       "   'TIMIT',\n",
       "   'Speech corpus',\n",
       "   'Probabilistic logic',\n",
       "   'Machine translation',\n",
       "   'Speech recognition',\n",
       "   'Invariant (mathematics)',\n",
       "   'Computer science'],\n",
       "  'references': ['2310919327',\n",
       "   '2064675550',\n",
       "   '2147880316',\n",
       "   '179875071',\n",
       "   '2127141656',\n",
       "   '196214544',\n",
       "   '3023071679',\n",
       "   '2131774270',\n",
       "   '2079735306',\n",
       "   '2170942820']},\n",
       " {'id': '1905522558',\n",
       "  'title': 'Domain Adaptation via Pseudo In-Domain Data Selection',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '535',\n",
       "  'abstract': 'We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large general-domain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora -- 1% the size of the original -- can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Amittai Axelrod 1, Xiaodong He 2, Jianfeng Gao 2'],\n",
       "  'related_topics': ['Rule-based machine translation',\n",
       "   'Machine translation',\n",
       "   'Domain (software engineering)',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Task (computing)',\n",
       "   'Computer science',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Artificial intelligence',\n",
       "   'Data selection',\n",
       "   'Domain adaptation'],\n",
       "  'references': ['2124807415',\n",
       "   '2156985047',\n",
       "   '1631260214',\n",
       "   '2146574666',\n",
       "   '2158195707',\n",
       "   '2117278770',\n",
       "   '2137387514',\n",
       "   '2132001515',\n",
       "   '2130450156',\n",
       "   '2148861208']},\n",
       " {'id': '2341457423',\n",
       "  'title': 'BLEU Deconstructed: Designing a Better MT Evaluation Metric',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '20',\n",
       "  'abstract': 'BLEU is the de facto standard automatic evaluation metric in machine translation. While BLEU is undeniably useful, it has a number of limitations. Although it works well for large documents and multiple references, it is unreliable at the sentence or sub-sentence levels, and with a single reference. In this paper, we propose new variants of BLEU which address these limitations, resulting in a more flexible metric which is not only more reliable, but also allows for more accurate discriminative training. Our best metric has better correlation with human judgements than standard BLEU, despite using a simpler formulation. Moreover, these improvements carry over to a system tuned for our new metric.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Xingyi Song', 'Trevor Cohn', 'Lucia Specia'],\n",
       "  'related_topics': ['Evaluation of machine translation',\n",
       "   'Metric (mathematics)',\n",
       "   'BLEU',\n",
       "   'Machine translation',\n",
       "   'De facto standard',\n",
       "   'Discriminative model',\n",
       "   'Natural language processing',\n",
       "   'Sentence',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2101105183',\n",
       "   '2124807415',\n",
       "   '2146574666',\n",
       "   '2123301721',\n",
       "   '2078861931',\n",
       "   '2087735403',\n",
       "   '222053410',\n",
       "   '1489525520',\n",
       "   '2115081467',\n",
       "   '2895810819']},\n",
       " {'id': '1606347560',\n",
       "  'title': 'Theano: new features and speed improvements',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '1,557',\n",
       "  'abstract': \"Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.\",\n",
       "  'date': 2012,\n",
       "  'authors': ['Frédéric Bastien',\n",
       "   'Pascal Lamblin',\n",
       "   'Razvan Pascanu',\n",
       "   'James Bergstra',\n",
       "   'Ian J. Goodfellow',\n",
       "   'Arnaud Bergeron',\n",
       "   'Nicolas Bouchard',\n",
       "   'David Warde-Farley',\n",
       "   'Yoshua Bengio'],\n",
       "  'related_topics': ['Theano',\n",
       "   'Compiler',\n",
       "   'Recurrent neural network',\n",
       "   'Linear algebra',\n",
       "   'Programming language',\n",
       "   'Theoretical computer science',\n",
       "   'Computation',\n",
       "   'Implementation',\n",
       "   'Mathematics'],\n",
       "  'references': []},\n",
       " {'id': '2171865010',\n",
       "  'title': 'Survey: Reservoir computing approaches to recurrent neural network training',\n",
       "  'reference_count': '144',\n",
       "  'citation_count': '1,862',\n",
       "  'abstract': \"Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ''brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ''map'' of it.\",\n",
       "  'date': 2009,\n",
       "  'authors': ['Mantas Lukoševičius', 'Herbert Jaeger'],\n",
       "  'related_topics': ['Reservoir computing',\n",
       "   'Echo state network',\n",
       "   'Recurrent neural network',\n",
       "   'Liquid state machine',\n",
       "   'Field (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Adaptation (computer science)',\n",
       "   'State (computer science)',\n",
       "   'Natural (music)'],\n",
       "  'references': ['2100495367',\n",
       "   '2112090702',\n",
       "   '2008620264',\n",
       "   '2064675550',\n",
       "   '1497256448',\n",
       "   '2154642048',\n",
       "   '2108384452',\n",
       "   '2293063825',\n",
       "   '1659842140',\n",
       "   '2118706537']},\n",
       " {'id': '2122585011',\n",
       "  'title': 'A Novel Connectionist System for Unconstrained Handwriting Recognition',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '1,884',\n",
       "  'abstract': \"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.\",\n",
       "  'date': 2009,\n",
       "  'authors': ['A. Graves 1, M. Liwicki 2, S. Fernandez 3, R. Bertolami 4, H. Bunke 4, J. Schmidhuber 1'],\n",
       "  'related_topics': ['Intelligent character recognition',\n",
       "   'Handwriting recognition',\n",
       "   'Language model',\n",
       "   'Recurrent neural network',\n",
       "   'Sequence labeling',\n",
       "   'Handwriting',\n",
       "   'Hidden Markov model',\n",
       "   'Word recognition',\n",
       "   'Artificial neural network',\n",
       "   'Cursive',\n",
       "   'Speech recognition',\n",
       "   'Machine learning',\n",
       "   'Image segmentation',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': []},\n",
       " {'id': '1508165687',\n",
       "  'title': 'Statistical methods for speech recognition',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '2,864',\n",
       "  'abstract': 'The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Frederick Jelinek'],\n",
       "  'related_topics': ['Cache language model',\n",
       "   'Language model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Hidden Markov model',\n",
       "   'Acoustic model',\n",
       "   'Information theory',\n",
       "   'Decision tree',\n",
       "   'Tree (data structure)',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1980862600']},\n",
       " {'id': '1973923101',\n",
       "  'title': 'Improved statistical alignment models',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '1,323',\n",
       "  'abstract': 'In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Franz Josef Och', 'Hermann Ney'],\n",
       "  'related_topics': ['Viterbi algorithm',\n",
       "   'Machine translation',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Quality (business)',\n",
       "   'Artificial intelligence',\n",
       "   'Statistical alignment'],\n",
       "  'references': ['2006969979',\n",
       "   '2038698865',\n",
       "   '1979102019',\n",
       "   '1575431606',\n",
       "   '3104029765',\n",
       "   '1811404221',\n",
       "   '2030750105',\n",
       "   '1525706028',\n",
       "   '136130055']},\n",
       " {'id': '1986543644',\n",
       "  'title': 'Three Generative, Lexicalised Models for Statistical Parsing',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '1,113',\n",
       "  'abstract': 'In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar. We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement. Results on Wall Street Journal text show that the parser performs at 88.1/87.5% constituent precision/recall, an average improvement of 2.3% over (Collins 96).',\n",
       "  'date': 1997,\n",
       "  'authors': ['Michael Collins'],\n",
       "  'related_topics': ['Parser combinator',\n",
       "   'Top-down parsing',\n",
       "   'Statistical parsing',\n",
       "   'Parsing',\n",
       "   'Data-oriented parsing',\n",
       "   'Generative model',\n",
       "   'Generative grammar',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1632114991',\n",
       "   '1773803948',\n",
       "   '2110882317',\n",
       "   '2153439141',\n",
       "   '2052449326',\n",
       "   '2093647425',\n",
       "   '1972573551',\n",
       "   '2087165009',\n",
       "   '2069912724',\n",
       "   '2162455891']},\n",
       " {'id': '2116316001',\n",
       "  'title': 'A Syntax-based Statistical Translation Model',\n",
       "  'reference_count': '315',\n",
       "  'citation_count': '1,044',\n",
       "  'abstract': 'We present a syntax-based statistical translation model. Our model transforms a source-language parse tree into a target-language string by applying stochastic operations at each node. These operations capture linguistic differences such as word order and case marking. Model parameters are estimated in polynomial time using an EM algorithm. The model produces word alignments that are better than those produced by IBM Model 5.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Kenji Yamada', 'Kevin Knight'],\n",
       "  'related_topics': ['Parse tree',\n",
       "   'Word error rate',\n",
       "   'Word (computer architecture)',\n",
       "   'String (computer science)',\n",
       "   'Syntax (programming languages)',\n",
       "   'Syntax',\n",
       "   'Time complexity',\n",
       "   'Word order',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2101105183',\n",
       "   '2122410182',\n",
       "   '1574901103',\n",
       "   '2049633694',\n",
       "   '2006969979',\n",
       "   '1916559533',\n",
       "   '2092654472',\n",
       "   '2135843243',\n",
       "   '2117400858',\n",
       "   '2101210369']},\n",
       " {'id': '1517947178',\n",
       "  'title': 'Improved Alignment Models for Statistical Machine Translation',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '878',\n",
       "  'abstract': '',\n",
       "  'date': 1998,\n",
       "  'authors': ['Franz Josef Och', 'Christoph Tillmann', 'Hermann Ney'],\n",
       "  'related_topics': ['Interactive machine translation',\n",
       "   'Machine translation',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2006969979',\n",
       "   '2038698865',\n",
       "   '2107551411',\n",
       "   '2113106066',\n",
       "   '2196555355',\n",
       "   '2294072136',\n",
       "   '2158164089']},\n",
       " {'id': '2161792612',\n",
       "  'title': 'A Phrase-Based,Joint Probability Model for Statistical Machine Translation',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '635',\n",
       "  'abstract': 'We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Daniel Marcu 1, Daniel Wong 2'],\n",
       "  'related_topics': ['Machine translation',\n",
       "   'Phrase',\n",
       "   'Word (computer architecture)',\n",
       "   'Joint probability distribution',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Joint (audio engineering)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2049633694',\n",
       "   '2006969979',\n",
       "   '1916559533',\n",
       "   '2001810881',\n",
       "   '2116316001',\n",
       "   '1517947178',\n",
       "   '2129765547',\n",
       "   '1549285799',\n",
       "   '2139403546',\n",
       "   '133045130']},\n",
       " {'id': '1549285799',\n",
       "  'title': 'Statistical Language Modeling using the CMU-Cambridge Toolkit',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '971',\n",
       "  'abstract': '',\n",
       "  'date': 1996,\n",
       "  'authors': ['Philip Clarkson', 'Ronald Rosenfeld'],\n",
       "  'related_topics': ['Modeling language',\n",
       "   'Language model',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153653739',\n",
       "   '1631260214',\n",
       "   '1916559533',\n",
       "   '2143017621',\n",
       "   '2142069714',\n",
       "   '2158195707',\n",
       "   '2134800885',\n",
       "   '2159981908',\n",
       "   '2116316001',\n",
       "   '2016243284']},\n",
       " {'id': '2158388102',\n",
       "  'title': 'Stochastic inversion transduction grammars and bilingual parsing of parallel corpora',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '1,120',\n",
       "  'abstract': \"We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications. Aside from the bilingual orientation, three major features distinguish the formalism from the finite-state transducers more traditionally found in computational linguistics: it skips directly to a context-free rather than finite-state base, it permits a minimal extra degree of ordering flexibility, and its probabilistic formulation admits an efficient maximum-likelihood bilingual parsing algorithm. A convenient normal form is shown to exist. Analysis of the formalism's expressiveness suggests that it is particularly well suited to modeling ordering shifts between languages, balancing needed flexibility against complexity constraints. We discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation, bracketing, phrasal alignment, and parsing.\",\n",
       "  'date': 1997,\n",
       "  'authors': ['Dekai Wu'],\n",
       "  'related_topics': ['S-attributed grammar',\n",
       "   'Parsing',\n",
       "   'Language model',\n",
       "   'Computational linguistics',\n",
       "   'Rule-based machine translation',\n",
       "   'Probabilistic logic',\n",
       "   'Transduction (machine learning)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Segmentation',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2006969979',\n",
       "   '2097333193',\n",
       "   '1489181569',\n",
       "   '2117652747',\n",
       "   '1513168562',\n",
       "   '1991133427',\n",
       "   '2439178139',\n",
       "   '2138584836',\n",
       "   '201288405',\n",
       "   '2110190189']},\n",
       " {'id': '2096733369',\n",
       "  'title': 'FaceNet: A unified embedding for face recognition and clustering',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '7,775',\n",
       "  'abstract': 'Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Florian Schroff', 'Dmitry Kalenichenko', 'James Philbin'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Face detection',\n",
       "   'Object-class detection',\n",
       "   'Facial recognition system',\n",
       "   'Cluster analysis',\n",
       "   'Feature vector',\n",
       "   'Face (geometry)',\n",
       "   'Similarity (geometry)',\n",
       "   'Embedding',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2097117768',\n",
       "   '1849277567',\n",
       "   '2146502635',\n",
       "   '2963911037',\n",
       "   '2168231600',\n",
       "   '2145287260',\n",
       "   '1782590233',\n",
       "   '2294059674',\n",
       "   '1498436455',\n",
       "   '2296073425']},\n",
       " {'id': '2016053056',\n",
       "  'title': 'Large-Scale Video Classification with Convolutional Neural Networks',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '5,802',\n",
       "  'abstract': 'Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).',\n",
       "  'date': 2014,\n",
       "  'authors': ['Andrej Karpathy',\n",
       "   'George Toderici',\n",
       "   'Sanketh Shetty',\n",
       "   'Thomas Leung',\n",
       "   'Rahul Sukthankar',\n",
       "   'Li Fei-Fei'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Feature (machine learning)',\n",
       "   'Feature extraction'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '2168231600',\n",
       "   '2022508996',\n",
       "   '2131846894',\n",
       "   '2062118960']},\n",
       " {'id': '2097726431',\n",
       "  'title': 'Opinion Mining and Sentiment Analysis',\n",
       "  'reference_count': '314',\n",
       "  'citation_count': '8,720',\n",
       "  'abstract': 'An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object. This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Bo Pang 1, Lillian Lee 2'],\n",
       "  'related_topics': ['Sentiment analysis',\n",
       "   'Automatic summarization',\n",
       "   'Information technology',\n",
       "   'Popularity',\n",
       "   'Data science',\n",
       "   'Economic impact analysis',\n",
       "   'Information extraction',\n",
       "   'Object (philosophy)',\n",
       "   'Subjectivity',\n",
       "   'Computer science'],\n",
       "  'references': []},\n",
       " {'id': '3124955340',\n",
       "  'title': 'A Decision Theoretic Generalization of On-Line Learning and an Application to Boosting',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '27,437',\n",
       "  'abstract': 'In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Y. Freund', 'R. Schapire'],\n",
       "  'related_topics': ['Boosting (machine learning)',\n",
       "   'Generalization',\n",
       "   'Bounded function',\n",
       "   'Range (mathematics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Finite set',\n",
       "   'Multiplicative function',\n",
       "   'Repeated game',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science'],\n",
       "  'references': ['2122005484',\n",
       "   '2132511032',\n",
       "   '2109826612',\n",
       "   '2114839088',\n",
       "   '2115781554',\n",
       "   '2120520366',\n",
       "   '2132827378',\n",
       "   '1984194948',\n",
       "   '2070277296',\n",
       "   '2031910487']},\n",
       " {'id': '2009570821',\n",
       "  'title': 'Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '7,702',\n",
       "  'abstract': 'Probablistic models are becoming increasingly important in analyzing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analyzing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it is accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time presents the state of the art in this new and important field.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Richard Durbin 1, Sean Eddy 2, Anders Stærmose Krogh 3, Graeme Mitchison 4'],\n",
       "  'related_topics': ['Probabilistic logic',\n",
       "   'Probabilistic method',\n",
       "   'Hidden Markov model',\n",
       "   'Genomics',\n",
       "   'Field (computer science)',\n",
       "   'Bayesian probability',\n",
       "   'Sequence analysis',\n",
       "   'Machine learning',\n",
       "   'Data mining',\n",
       "   'Human genome',\n",
       "   'Biology',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2140190241',\n",
       "   '2147880316',\n",
       "   '2168133698',\n",
       "   '2158266063',\n",
       "   '2153233077',\n",
       "   '2045843097',\n",
       "   '2138122982',\n",
       "   '2145191876',\n",
       "   '2110575115',\n",
       "   '2162315106']},\n",
       " {'id': '1934019294',\n",
       "  'title': 'Maximum Entropy Markov Models for Information Extraction and Segmentation',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '1,914',\n",
       "  'abstract': 'Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Andrew McCallum', 'Dayne Freitag 1, Fernando C. N. Pereira 2'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Variable-order Markov model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Markov chain',\n",
       "   'Hidden Markov model',\n",
       "   'Markov model',\n",
       "   'Markov property',\n",
       "   'Markov process',\n",
       "   'Markov kernel',\n",
       "   'Conditional probability',\n",
       "   'Probabilistic logic',\n",
       "   'Viterbi algorithm',\n",
       "   'Multinomial distribution',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2125838338',\n",
       "   '2049633694',\n",
       "   '2160842254',\n",
       "   '2117400858',\n",
       "   '1520377376',\n",
       "   '1528056001',\n",
       "   '1557074680',\n",
       "   '2158873310',\n",
       "   '1597379537',\n",
       "   '1592796124']},\n",
       " {'id': '1773803948',\n",
       "  'title': 'A Maximum Entropy Model for Part-Of-Speech Tagging',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '2,392',\n",
       "  'abstract': 'This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems',\n",
       "  'date': 1995,\n",
       "  'authors': ['Adwait Ratnaparkhi'],\n",
       "  'related_topics': ['Maximum-entropy Markov model',\n",
       "   'Principle of maximum entropy',\n",
       "   'Statistical model',\n",
       "   'Part of speech',\n",
       "   'Trigram tagger',\n",
       "   'Natural language processing',\n",
       "   'Consistency (database systems)',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Part-of-speech tagging'],\n",
       "  'references': ['1632114991',\n",
       "   '2096175520',\n",
       "   '2121227244',\n",
       "   '2160842254',\n",
       "   '2153439141',\n",
       "   '2170381724',\n",
       "   '1718065290',\n",
       "   '2112861996',\n",
       "   '2015042937',\n",
       "   '2069912724']},\n",
       " {'id': '2160842254',\n",
       "  'title': 'Inducing features of random fields',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '1,626',\n",
       "  'abstract': 'We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.',\n",
       "  'date': 1997,\n",
       "  'authors': ['S. Della Pietra 1, V. Della Pietra', 'J. Lafferty 2'],\n",
       "  'related_topics': ['Random field',\n",
       "   'Generalized iterative scaling',\n",
       "   'Greedy algorithm',\n",
       "   'Feature extraction',\n",
       "   'Stochastic process',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Cluster analysis',\n",
       "   'Decision tree',\n",
       "   'Iterative method',\n",
       "   'Kullback–Leibler divergence',\n",
       "   'Empirical distribution function',\n",
       "   'Principle of maximum entropy',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Training set'],\n",
       "  'references': ['1594031697',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2096175520',\n",
       "   '2121227244',\n",
       "   '2097333193',\n",
       "   '107938046',\n",
       "   '2167837909',\n",
       "   '2089969354',\n",
       "   '2035301451']},\n",
       " {'id': '3021452258',\n",
       "  'title': 'Discriminative Reranking for Natural Language Parsing',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '716',\n",
       "  'abstract': '',\n",
       "  'date': 2000,\n",
       "  'authors': ['Michael Collins'],\n",
       "  'related_topics': ['Statistical parsing',\n",
       "   'Data-oriented parsing',\n",
       "   'Discriminative model',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Natural language parsing'],\n",
       "  'references': ['2147880316',\n",
       "   '607505555',\n",
       "   '2105842272',\n",
       "   '2158847908',\n",
       "   '2160218441',\n",
       "   '2092654472',\n",
       "   '2107890099',\n",
       "   '2168020168',\n",
       "   '2168029744']},\n",
       " {'id': '2117400858',\n",
       "  'title': 'Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '2,315',\n",
       "  'abstract': 'Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Eric Brill'],\n",
       "  'related_topics': ['Deep linguistic processing',\n",
       "   'Language identification',\n",
       "   'Brill tagger',\n",
       "   'Rule-based machine translation',\n",
       "   'Coh-Metrix',\n",
       "   'Natural language processing',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Encoding (memory)',\n",
       "   'Trigram tagger',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1594031697',\n",
       "   '2149706766',\n",
       "   '1632114991',\n",
       "   '2102381086',\n",
       "   '2099247782',\n",
       "   '2097333193',\n",
       "   '2081687495',\n",
       "   '1489181569',\n",
       "   '2046224275',\n",
       "   '2170381724']},\n",
       " {'id': '2159737176',\n",
       "  'title': 'Training Invariant Support Vector Machines',\n",
       "  'reference_count': '38',\n",
       "  'citation_count': '775',\n",
       "  'abstract': 'Practical experience has shown that in order to obtain the best possible performance, prior knowledge about invariances of a classification problem at hand ought to be incorporated into the training procedure. We describe and review all known methods for doing so in support vector machines, provide experimental results, and discuss their respective merits. One of the significant new results reported in this work is our recent achievement of the lowest reported test error on the well-known MNIST digit recognition benchmark task, with SVM training times that are also significantly faster than previous SVM methods.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Dennis Decoste 1, Bernhard Schölkopf 2'],\n",
       "  'related_topics': ['Sequential minimal optimization',\n",
       "   'Structured support vector machine',\n",
       "   'Least squares support vector machine',\n",
       "   'MNIST database',\n",
       "   'Relevance vector machine',\n",
       "   'Support vector machine',\n",
       "   'Contextual image classification',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Invariant (physics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2310919327',\n",
       "   '2119821739',\n",
       "   '2140095548',\n",
       "   '1512098439',\n",
       "   '2087347434',\n",
       "   '1604938182',\n",
       "   '2147800946',\n",
       "   '2151040995']},\n",
       " {'id': '2027197837',\n",
       "  'title': 'Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '2,324',\n",
       "  'abstract': 'Abstract We give conditions ensuring that multilayer feedforward networks with as few as a single hidden layer and an appropriately smooth hidden layer activation function are capable of arbitrarily accurate approximation to an arbitrary function and its derivatives. In fact, these networks can approximate functions that are not differentiable in the classical sense, but possess only a generalized derivative, as is the case for certain piecewise differentiable functions. The conditions imposed on the hidden layer activation function are relatively mild; the conditions imposed on the domain of the function to be approximated have practical implications. Our approximation results provide a previously missing theoretical justification for the use of multilayer feedforward networks in applications requiring simultaneous approximation of a function and its derivatives.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Kurt Hornik', 'Maxwell Stinchcombe', 'Halbert White'],\n",
       "  'related_topics': ['Piecewise',\n",
       "   'Differentiable function',\n",
       "   'Activation function',\n",
       "   'Function (mathematics)',\n",
       "   'Artificial neural network',\n",
       "   'Function space',\n",
       "   'Sobolev space',\n",
       "   'Domain (mathematical analysis)',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical analysis',\n",
       "   'Mathematics'],\n",
       "  'references': ['2137983211',\n",
       "   '3146803896',\n",
       "   '1971735090',\n",
       "   '2056099894',\n",
       "   '2416739038',\n",
       "   '2090270852',\n",
       "   '1613359937',\n",
       "   '1490039160',\n",
       "   '2090248140',\n",
       "   '1537887709']},\n",
       " {'id': '2147345686',\n",
       "  'title': 'An offline cursive handwritten word recognition system',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '32',\n",
       "  'abstract': 'This paper describes an offline cursive handwritten word recognition system that combines hidden Markov models (HMM) and neural networks (NN). Using a fast left-right slicing method, we generate a segmentation graph that describes all possible ways to segment a word into letters. The NN computes the observation probabilities for each letter hypothesis in the segmentation graph. Then, the HMM compute the likelihood for each word in the lexicon by summing the probabilities over all possible paths through the graph. We present the preprocessing and the recognition process as well as the training procedure for the NN-HMM hybrid system. Another recognition system based on discrete HMM is also presented for performance comparison. The latter is also used for bootstrapping the NN-HMM hybrid system. Recognition performances of the two recognition systems using two image databases of French isolated words are presented. This paper is one of the first publications using the IRONOFF database, and thus can be used as a reference for future work on this database.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Yong Haur Tay',\n",
       "   'P.M. Lallican',\n",
       "   'M. Khalid',\n",
       "   'C. Viard-Gaudin',\n",
       "   'S. Kneer'],\n",
       "  'related_topics': ['Intelligent word recognition',\n",
       "   'Handwriting recognition',\n",
       "   'Word recognition',\n",
       "   'Hidden Markov model',\n",
       "   'Graph (abstract data type)',\n",
       "   'Cursive',\n",
       "   'Image segmentation',\n",
       "   'Artificial neural network',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Lexicon',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '2125838338',\n",
       "   '2142069714',\n",
       "   '183625566',\n",
       "   '2149597185',\n",
       "   '2077863651',\n",
       "   '2148295954',\n",
       "   '101240229',\n",
       "   '2113292028',\n",
       "   '2064838583']},\n",
       " {'id': '51975515',\n",
       "  'title': 'An improved recognition module for the identification of handwritten digits',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '12',\n",
       "  'abstract': '',\n",
       "  'date': 1998,\n",
       "  'authors': ['Anshu Sinha'],\n",
       "  'related_topics': ['Intelligent character recognition',\n",
       "   'Intelligent word recognition',\n",
       "   'Document processing',\n",
       "   'Optical character recognition',\n",
       "   'Identification (information)',\n",
       "   'Speech recognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2124776405',\n",
       "   '2227933188',\n",
       "   '2126727781',\n",
       "   '2256679588',\n",
       "   '2122827492',\n",
       "   '49742075',\n",
       "   '2072181047',\n",
       "   '2107600630',\n",
       "   '1519534430',\n",
       "   '2151513848']},\n",
       " {'id': '2166469100',\n",
       "  'title': 'Effective Training of a Neural Network Character Classifier for Word Recognition',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '106',\n",
       "  'abstract': \"We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton Message Pad. We present some innovations in the training and use of ANNs as character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and stroke warping. A recurring theme of reducing a priori biases emerges and is discussed.\",\n",
       "  'date': 1996,\n",
       "  'authors': ['Larry S. Yaeger 1, Richard F. Lyon 1, Brandyn J. Webb 2'],\n",
       "  'related_topics': ['Intelligent character recognition',\n",
       "   'Intelligent word recognition',\n",
       "   'Word error rate',\n",
       "   'Word recognition',\n",
       "   'Time delay neural network',\n",
       "   'Text segmentation',\n",
       "   'Classifier (linguistics)',\n",
       "   'Character (mathematics)',\n",
       "   'Artificial neural network',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Normalization (statistics)',\n",
       "   'Image warping',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2150884987',\n",
       "   '2137291015',\n",
       "   '2055075080',\n",
       "   '1980501707',\n",
       "   '2099070536',\n",
       "   '2111494971',\n",
       "   '2140539590',\n",
       "   '2124229187',\n",
       "   '2170541567',\n",
       "   '2607313294']},\n",
       " {'id': '2964274690',\n",
       "  'title': 'Joint Optimization Framework for Learning with Noisy Labels',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '250',\n",
       "  'abstract': 'Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Daiki Tanaka',\n",
       "   'Daiki Ikami',\n",
       "   'Toshihiko Yamasaki',\n",
       "   'Kiyoharu Aizawa'],\n",
       "  'related_topics': ['Overfitting',\n",
       "   'Artificial neural network',\n",
       "   'Contextual image classification'],\n",
       "  'references': ['2194775991',\n",
       "   '3118608800',\n",
       "   '2963207607',\n",
       "   '2302255633',\n",
       "   '2073459066',\n",
       "   '3137695714',\n",
       "   '2963399829',\n",
       "   '2136504847',\n",
       "   '2964292098',\n",
       "   '2963096987']},\n",
       " {'id': '2966415767',\n",
       "  'title': 'Interpolation Consistency Training for Semi-supervised Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '179',\n",
       "  'abstract': 'We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-adaptive regularization with unlabeled points which reduces overfitting to labeled points under high confidence values.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Vikas Verma 1, Alex Lamb 2, Juho Kannala 1, Yoshua Bengio 2, David Lopez-Paz 3'],\n",
       "  'related_topics': ['Interpolation',\n",
       "   'Semi-supervised learning',\n",
       "   'Overfitting',\n",
       "   'Artificial neural network',\n",
       "   'Decision boundary',\n",
       "   'Benchmark (computing)',\n",
       "   'Regularization (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Consistency (statistics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3035160371',\n",
       "   '3100859887',\n",
       "   '2962369866',\n",
       "   '2987875759',\n",
       "   '3001197829',\n",
       "   '2996501936',\n",
       "   '3035687950',\n",
       "   '2976016473']},\n",
       " {'id': '2963855133',\n",
       "  'title': 'Bag of Tricks for Image Classification with Convolutional Neural Networks',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '384',\n",
       "  'abstract': \"Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.\",\n",
       "  'date': 2019,\n",
       "  'authors': ['Tong He',\n",
       "   'Zhi Zhang',\n",
       "   'Hang Zhang',\n",
       "   'Zhongyue Zhang',\n",
       "   'Junyuan Xie',\n",
       "   'Mu Li'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Contextual image classification',\n",
       "   'Object detection',\n",
       "   'Transfer of learning',\n",
       "   'Segmentation',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Categorization',\n",
       "   'Computer science'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '1903029394',\n",
       "   '2183341477',\n",
       "   '2963911037',\n",
       "   '1533861849',\n",
       "   '2963420686',\n",
       "   '2560023338']},\n",
       " {'id': '3098350627',\n",
       "  'title': 'Fastai: A Layered API for Deep Learning',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '245',\n",
       "  'abstract': 'fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Jeremy Howard', 'Sylvain Gugger'],\n",
       "  'related_topics': ['Python (programming language)',\n",
       "   'Callback',\n",
       "   'Source lines of code',\n",
       "   'Deep learning',\n",
       "   'Multitier architecture',\n",
       "   'Programming language',\n",
       "   'Computer science',\n",
       "   'Usability',\n",
       "   'Data processing',\n",
       "   'Dynamism',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1901129140',\n",
       "   '2099471712',\n",
       "   '2101234009',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2963446712',\n",
       "   '2899771611',\n",
       "   '2963420686',\n",
       "   '2531409750',\n",
       "   '2011301426']},\n",
       " {'id': '3129973872',\n",
       "  'title': 'Teaching with Commentaries',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '2',\n",
       "  'abstract': 'Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, meta-learned information helpful for training on a particular task or dataset. We present an efficient and scalable gradient-based method to learn commentaries, leveraging recent work on implicit differentiation. We explore diverse applications of commentaries, from learning weights for individual training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. In these settings, we find that commentaries can improve training speed and/or performance and also provide fundamental insights about the dataset and training process.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Aniruddh Raghu 1, Maithra Raghu 2, Simon Kornblith 3, David Duvenaud 4, Geoffrey Hinton 4'],\n",
       "  'related_topics': ['Metalearning',\n",
       "   'Data science',\n",
       "   'Process (engineering)',\n",
       "   'Artificial neural network',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Scalability',\n",
       "   'Salient',\n",
       "   'Scope (project management)',\n",
       "   'Training (civil)'],\n",
       "  'references': ['1901129140',\n",
       "   '2964253222',\n",
       "   '2604763608',\n",
       "   '3137695714',\n",
       "   '2963399829',\n",
       "   '2732026016',\n",
       "   '2296073425',\n",
       "   '2963466845',\n",
       "   '2963749936',\n",
       "   '3149085914']},\n",
       " {'id': '2970902013',\n",
       "  'title': 'On Adversarial Mixup Resynthesis',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '23',\n",
       "  'abstract': 'In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Christopher Beckham 1, Sina Honari 2, Vikas Verma 3, Alex M. Lamb 2, Farnoosh Ghadiri 4, R Devon Hjelm 5, Yoshua Bengio 2, Chris Pal 2'],\n",
       "  'related_topics': ['Context (language use)',\n",
       "   'Machine learning',\n",
       "   'Class (computer programming)',\n",
       "   'Function (engineering)',\n",
       "   'Computer science',\n",
       "   'Adversarial system',\n",
       "   'Mixing (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Qualitative evidence'],\n",
       "  'references': ['3107669106',\n",
       "   '3125645205',\n",
       "   '3118146262',\n",
       "   '3130223764',\n",
       "   '3137863028',\n",
       "   '3093382707',\n",
       "   '3019676419',\n",
       "   '3162734203',\n",
       "   '3108796939']},\n",
       " {'id': '2971149989',\n",
       "  'title': 'How to Initialize your Network? Robust Initialization for WeightNorm & ResNets',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4',\n",
       "  'abstract': 'Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Devansh Arpit 1, Víctor Campos 2, Yoshua Bengio 3'],\n",
       "  'related_topics': ['Initialization',\n",
       "   'Robustness (computer science)',\n",
       "   'Deep learning'],\n",
       "  'references': []},\n",
       " {'id': '3035743198',\n",
       "  'title': 'Adversarial Examples Improve Image Recognition',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '121',\n",
       "  'abstract': 'Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A (+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more parameters. Code and models will be made publicly available.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Cihang Xie 1, Mingxing Tan 2, Boqing Gong 2, Jiang Wang 2, Alan L. Yuille 1, Quoc V. Le 2'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Overfitting',\n",
       "   'Supervised learning',\n",
       "   'Robustness (computer science)',\n",
       "   'Range (mathematics)',\n",
       "   'Code (cryptography)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Adversarial system',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2963446712',\n",
       "   '2183341477',\n",
       "   '2963207607',\n",
       "   '2964153729',\n",
       "   '2964253222']},\n",
       " {'id': '1554944419',\n",
       "  'title': 'The elements of statistical learning : data mining, inference,and prediction',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '28,721',\n",
       "  'abstract': \"During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['Trevor Hastie', 'Robert J. Tibshirani', 'Jerome Friedman'],\n",
       "  'related_topics': ['Least-angle regression',\n",
       "   'Lasso (statistics)',\n",
       "   'Ensemble learning',\n",
       "   'Unsupervised learning',\n",
       "   'Gradient boosting',\n",
       "   'Supervised learning',\n",
       "   'Generalized additive model',\n",
       "   'Random forest',\n",
       "   'Data mining',\n",
       "   'Mathematics'],\n",
       "  'references': ['2140190241',\n",
       "   '2164278908',\n",
       "   '2179438025',\n",
       "   '2122825543',\n",
       "   '2063978378',\n",
       "   '2131975293',\n",
       "   '2087681821',\n",
       "   '2117756735',\n",
       "   '2109574129']},\n",
       " {'id': '2132914434',\n",
       "  'title': 'A tutorial on spectral clustering',\n",
       "  'reference_count': '72',\n",
       "  'citation_count': '9,299',\n",
       "  'abstract': 'In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Ulrike Luxburg'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Correlation clustering',\n",
       "   'Canopy clustering algorithm',\n",
       "   'Fuzzy clustering',\n",
       "   'CURE data clustering algorithm',\n",
       "   'Biclustering',\n",
       "   'Brown clustering',\n",
       "   'DBSCAN',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science'],\n",
       "  'references': ['1480376833',\n",
       "   '2121947440',\n",
       "   '2798909945',\n",
       "   '2165874743',\n",
       "   '2097308346',\n",
       "   '1479807131',\n",
       "   '1578099820',\n",
       "   '2798707604',\n",
       "   '2011832962',\n",
       "   '2071949631']},\n",
       " {'id': '1578099820',\n",
       "  'title': 'Spectral Graph Theory',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '5,980',\n",
       "  'abstract': 'Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Fan R K Chung'],\n",
       "  'related_topics': ['Integral graph',\n",
       "   'Spectral graph theory',\n",
       "   'Laplacian matrix',\n",
       "   'Resistance distance',\n",
       "   'Algebraic connectivity',\n",
       "   'Isoperimetric inequality',\n",
       "   'Sobolev inequality',\n",
       "   'Random walk',\n",
       "   'Combinatorics',\n",
       "   'Computer science'],\n",
       "  'references': ['2901284226', '2053631808', '2027808858']},\n",
       " {'id': '2962820688',\n",
       "  'title': 'Convolutional neural networks applied to house numbers digit classification',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '498',\n",
       "  'abstract': 'We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Pierre Sermanet', 'Soumith Chintala', 'Yann LeCun'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Feature learning',\n",
       "   'Artificial neural network',\n",
       "   'Pooling',\n",
       "   'Feature extraction',\n",
       "   'Convolutional code',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2095705004',\n",
       "   '2963446712',\n",
       "   '2016053056',\n",
       "   '2294059674',\n",
       "   '2607333215',\n",
       "   '2152839228',\n",
       "   '2964311892',\n",
       "   '2308045930',\n",
       "   '2162741153',\n",
       "   '2963574257']},\n",
       " {'id': '1999192586',\n",
       "  'title': 'Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '1,239',\n",
       "  'abstract': 'Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/∼wzou/',\n",
       "  'date': 2011,\n",
       "  'authors': ['Quoc V. Le', 'Will Y. Zou', 'Serena Y. Yeung', 'Andrew Y. Ng'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Unsupervised learning',\n",
       "   'Feature extraction',\n",
       "   'Deep learning',\n",
       "   'Contextual image classification',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Subspace topology',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Invariant (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2161969291',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '1677409904',\n",
       "   '2119605622',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2130325614',\n",
       "   '2142194269']},\n",
       " {'id': '2147860648',\n",
       "  'title': 'Tiled convolutional neural networks',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '368',\n",
       "  'abstract': 'Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular \"tiled\" pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs\\' advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Jiquan Ngiam',\n",
       "   'Zhenghao Chen',\n",
       "   'Daniel Chia',\n",
       "   'Pang W. Koh',\n",
       "   'Quoc V. Le',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Artificial neural network',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Convolution',\n",
       "   'Invariant (mathematics)',\n",
       "   'Theoretical computer science',\n",
       "   'Invariant (physics)',\n",
       "   'Pooling',\n",
       "   'Computer science'],\n",
       "  'references': ['2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2072128103',\n",
       "   '2546302380',\n",
       "   '2117130368',\n",
       "   '1548802052',\n",
       "   '2118585731',\n",
       "   '2110798204']},\n",
       " {'id': '2016589492',\n",
       "  'title': 'A learning algorithm for continually running fully recurrent neural networks',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '5,334',\n",
       "  'abstract': 'The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.',\n",
       "  'date': 1989,\n",
       "  'authors': ['Ronald J. Williams 1, David Zipser 2'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Recurrent neural network',\n",
       "   'Backpropagation through time',\n",
       "   'Artificial neural network',\n",
       "   'Supervised learning',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Interval (mathematics)',\n",
       "   'Computer science',\n",
       "   'Algorithm',\n",
       "   'Basis (linear algebra)',\n",
       "   'Recurrent neural nets'],\n",
       "  'references': ['2154642048',\n",
       "   '2293063825',\n",
       "   '2110485445',\n",
       "   '2143503258',\n",
       "   '1881179843',\n",
       "   '1959983357',\n",
       "   '1984205520',\n",
       "   '1984375561',\n",
       "   '2119796132',\n",
       "   '1527772862']},\n",
       " {'id': '1959983357',\n",
       "  'title': 'Attractor dynamics and parallelism in a connectionist sequential machine',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,539',\n",
       "  'abstract': '',\n",
       "  'date': 1990,\n",
       "  'authors': ['Michael I. Jordan'],\n",
       "  'related_topics': ['Parallelism (grammar)',\n",
       "   'Attractor',\n",
       "   'Connectionism',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Dynamics (music)',\n",
       "   'Sequential machine'],\n",
       "  'references': ['1934184906',\n",
       "   '2128499899',\n",
       "   '2546314413',\n",
       "   '1996605950',\n",
       "   '2999879503',\n",
       "   '2058791286',\n",
       "   '2007800656',\n",
       "   '2028629011',\n",
       "   '2155041441',\n",
       "   '2019385508']},\n",
       " {'id': '2028629011',\n",
       "  'title': 'Learning state space trajectories in recurrent neural networks',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '251',\n",
       "  'abstract': 'A number of procedures are described for finding delta E/ delta W/sub ij/ where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and w/sub ij/ are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E, so these procedures form the kernels of connectionist learning algorithms. Simulations in which networks are taught to move through limit cycles are shown, along with some empirical perturbation sensitivity tests. The author describes a number of elaborations of the basic idea, including mutable time delays and teacher forcing. He includes a complexity analysis of the various learning procedures discussed and analyzed. Temporally continuous recurrent networks seems particularly suited for temporally continuous domains, such as signal processing, control, and speech. >',\n",
       "  'date': 1988,\n",
       "  'authors': ['Pearlmutter'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'State space',\n",
       "   'Gradient descent',\n",
       "   'Connectionism',\n",
       "   'Algorithm',\n",
       "   'Computer science'],\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '1597286183',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '1971129545',\n",
       "   '1959983357',\n",
       "   '3121926921']},\n",
       " {'id': '2053127376',\n",
       "  'title': 'On the time relations of mental processes: An examination of systems of processes in cascade.',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '1,838',\n",
       "  'abstract': '',\n",
       "  'date': 1979,\n",
       "  'authors': ['James L. McClelland'],\n",
       "  'related_topics': ['Cognition',\n",
       "   'Perception',\n",
       "   'Cognitive science',\n",
       "   'Cognitive psychology',\n",
       "   'Computer science',\n",
       "   'Cascade',\n",
       "   'Abstract reasoning',\n",
       "   'Research methodology'],\n",
       "  'references': ['1784695092',\n",
       "   '2135255848',\n",
       "   '2098205603',\n",
       "   '2045597501',\n",
       "   '2094493170',\n",
       "   '1502139053',\n",
       "   '1967670055',\n",
       "   '2106654511',\n",
       "   '2147311265',\n",
       "   '1529340823']},\n",
       " {'id': '2167607759',\n",
       "  'title': 'The \"Moving Targets\" Training Algorithm',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '63',\n",
       "  'abstract': 'A simple method for training the dynamical behavior of a neural network is derived. It is applicable to any training problem in discrete-time networks with arbitrary feedback. The algorithm resembles back-propagation in that an error function is minimized using a gradient-based method, but the optimization is carried out in the hidden part of state space either instead of, or in addition to weight space. Computational results are presented for some simple dynamical training problems, one of which requires response to a signal 100 time steps in the past.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Richard Rohwer'],\n",
       "  'related_topics': ['State space',\n",
       "   'Artificial neural network',\n",
       "   'Error function',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Algorithm',\n",
       "   'Signal',\n",
       "   'Training (meteorology)',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '3004157836',\n",
       "   '2077658674',\n",
       "   '2016589492',\n",
       "   '2796837256',\n",
       "   '2143503258',\n",
       "   '1984205520',\n",
       "   '1984375561',\n",
       "   '2325850497',\n",
       "   '2028629011']},\n",
       " {'id': '2177721432',\n",
       "  'title': 'Neurons with graded response have collective computational properties like those of two-state neurons',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '9,598',\n",
       "  'abstract': 'A model for a large network of \"neurons\" with a graded response (or sigmoid input--output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch--Pitts neurons. The content-addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.',\n",
       "  'date': 1988,\n",
       "  'authors': ['J. J. Hopfield'],\n",
       "  'related_topics': ['Deterministic system',\n",
       "   'Electrical network',\n",
       "   'Sigmoid function',\n",
       "   'Stochastic modelling',\n",
       "   'Connection (algebraic framework)',\n",
       "   'Function (mathematics)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Action (physics)',\n",
       "   'Statistical physics',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['1973108021', '582196039']},\n",
       " {'id': '2075510082',\n",
       "  'title': 'Characteristics of Random Nets of Analog Neuron-Like Elements',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '388',\n",
       "  'abstract': 'The dynamic behavior of randomly connected analog neuron-like elements that process pulse-frequency modulated signals is investigated from the macroscopic point of view. By extracting two statistical parameters, the macroscopic state equations are derived in terms of these parameters under some hypotheses on the stochastics of microscopic states. It is shown that a random net of statistically symmetric structure is monostable or bistable, and the stability criteria are explicitly given. Random nets consisting of many different classes of elements are also analyzed. Special attention is paid to nets of randomly connected excitatory and inhibitory elements. It is shown that a stable oscillation exists in such a net?in contrast with the fact that no stable oscillations exist in a net of statistically symmetric structure even if negative as well as positive synaptic weights are permitted at a time. The results are checked by computer-simulated experiments.',\n",
       "  'date': 1972,\n",
       "  'authors': ['Shun-Ichi Amarimber'],\n",
       "  'related_topics': ['Stochastic process',\n",
       "   'Stability (probability)',\n",
       "   'Bistability',\n",
       "   'Statistical parameter',\n",
       "   'Oscillation',\n",
       "   'Net (mathematics)',\n",
       "   'State (functional analysis)',\n",
       "   'Statistical physics',\n",
       "   'Contrast (statistics)',\n",
       "   'Control theory',\n",
       "   'Mathematics'],\n",
       "  'references': ['2159116087',\n",
       "   '2060666586',\n",
       "   '1968014724',\n",
       "   '1995030605',\n",
       "   '2159187100',\n",
       "   '2072972096',\n",
       "   '2082628174']},\n",
       " {'id': '2138484437',\n",
       "  'title': 'Identification and control of dynamical systems using neural networks',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '11,680',\n",
       "  'abstract': 'It is demonstrated that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis is on models for both identification and control. Static and dynamic backpropagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations, and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout, and theoretical questions that have to be addressed are also described. >',\n",
       "  'date': 1990,\n",
       "  'authors': ['K.S. Narendra', 'K. Parthasarathy'],\n",
       "  'related_topics': ['Nonlinear system identification',\n",
       "   'Adaptive control',\n",
       "   'Dynamical systems theory',\n",
       "   'Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Identification (information)',\n",
       "   'Control system',\n",
       "   'Linear system',\n",
       "   'Nonlinear system',\n",
       "   'Control engineering',\n",
       "   'Control theory',\n",
       "   'Computer science'],\n",
       "  'references': ['2137983211',\n",
       "   '2293063825',\n",
       "   '3146803896',\n",
       "   '1597286183',\n",
       "   '1572161815',\n",
       "   '2174984063',\n",
       "   '3036751298',\n",
       "   '2122136962',\n",
       "   '2095425517',\n",
       "   '2007431958']},\n",
       " {'id': '2150355110',\n",
       "  'title': 'Backpropagation through time: what it does and how to do it',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '5,227',\n",
       "  'abstract': 'Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users. >',\n",
       "  'date': 1990,\n",
       "  'authors': ['P.J. Werbos'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Backpropagation',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Deep learning',\n",
       "   'Time delay neural network',\n",
       "   'Nonlinear system identification',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '2068484625',\n",
       "   '2143503258',\n",
       "   '3148194443',\n",
       "   '1971129545',\n",
       "   '1881179843',\n",
       "   '3121926921',\n",
       "   '2090248140',\n",
       "   '1487148666',\n",
       "   '2028629011']},\n",
       " {'id': '1583833196',\n",
       "  'title': 'Neuronlike adaptive elements that can solve difficult learning control problems',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,306',\n",
       "  'abstract': \"It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.\",\n",
       "  'date': 1990,\n",
       "  'authors': ['Andrew G. Barto', 'Richard S. Sutton', 'Charles W. Anderson'],\n",
       "  'related_topics': ['Evaluation function',\n",
       "   'Relation (database)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Control (management)',\n",
       "   'Element (category theory)',\n",
       "   'Reinforcement',\n",
       "   'Base (topology)',\n",
       "   'Operant conditioning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2170227253',\n",
       "   '1602079996',\n",
       "   '212314082',\n",
       "   '1481405077',\n",
       "   '1504806788',\n",
       "   '2038584310',\n",
       "   '1971728746',\n",
       "   '2834852173',\n",
       "   '2293846015',\n",
       "   '2001185203']},\n",
       " {'id': '2143787696',\n",
       "  'title': 'Gradient methods for the optimization of dynamical systems containing neural networks',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '873',\n",
       "  'abstract': 'An extension of the backpropagation method, termed dynamic backpropagation, which can be applied in a straightforward manner for the optimization of the weights (parameters) of multilayer neural networks is discussed. The method is based on the fact that gradient methods used in linear dynamical systems can be combined with backpropagation methods for neural networks to obtain the gradient of a performance index of nonlinear dynamical systems. The method can be applied to any complex system which can be expressed as the interconnection of linear dynamical systems and multilayer neural networks. To facilitate the practical implementation of the proposed method, emphasis is placed on the diagrammatic representation of the system which generates the gradient of the performance function. >',\n",
       "  'date': 1991,\n",
       "  'authors': ['K.S. Narendra', 'K. Parthasarathy'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Linear dynamical system',\n",
       "   'Gradient method',\n",
       "   'Artificial neural network',\n",
       "   'Dynamical systems theory',\n",
       "   'Linear system',\n",
       "   'Nonlinear system',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2138484437',\n",
       "   '2016589492',\n",
       "   '2150355110',\n",
       "   '2076086013',\n",
       "   '2016261381',\n",
       "   '2062870975']},\n",
       " {'id': '2057653135',\n",
       "  'title': 'An efficient gradient-based algorithm for on-line training of recurrent network trajectories',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '737',\n",
       "  'abstract': 'A novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run continually without ever being reset to an initial state, and it is specifically designed for computationally efficient computer implementation. This algorithm can be viewed as a cross between epochwise backpropagation through time, which is not appropriate for continually running networks, and the widely used on-line gradient approximation technique of truncated backpropagation through time.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Ronald J. Williams', 'Jing Peng'],\n",
       "  'related_topics': ['Backpropagation through time',\n",
       "   'Reset (computing)',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Line (geometry)',\n",
       "   'Computer science',\n",
       "   'Training (meteorology)',\n",
       "   'Gradient based algorithm'],\n",
       "  'references': ['2154642048',\n",
       "   '2016589492',\n",
       "   '2007431958',\n",
       "   '2143503258',\n",
       "   '1881179843',\n",
       "   '2121553911',\n",
       "   '1984205520',\n",
       "   '2139273175',\n",
       "   '2047515372',\n",
       "   '2044422789']},\n",
       " {'id': '2132152975',\n",
       "  'title': 'Decoupled extended Kalman filter training of feedforward layered networks',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '316',\n",
       "  'abstract': 'Presents a training algorithm for feedforward layered networks based on a decoupled extended Kalman filter (DEKF). The authors present an artificial process noise extension to DEKF that increases its convergence rate and assists in the avoidance of local minima. Computationally efficient formulations for two particularly natural and useful cases of DEKF are given. Through a series of pattern classification and function approximation experiments, three members of DEKF are compared with one another and with standard backpropagation (SBP). These studies demonstrate that the judicious grouping of weights along with the use of artificial process noise in DEKF result in input-output mapping performance that is comparable to the global extended Kalman algorithm, and is often superior to SBP, while requiring significantly fewer presentations of training data than SBP and less overall training time than either of these procedures. >',\n",
       "  'date': 1991,\n",
       "  'authors': ['G.V. Puskorius', 'L.A. Feldkamp'],\n",
       "  'related_topics': ['Extended Kalman filter',\n",
       "   'Kalman filter',\n",
       "   'Backpropagation',\n",
       "   'Artificial neural network',\n",
       "   'Function approximation',\n",
       "   'Rate of convergence',\n",
       "   'Maxima and minima',\n",
       "   'Feed forward',\n",
       "   'Control theory',\n",
       "   'Computer science'],\n",
       "  'references': ['2138484437',\n",
       "   '3147404844',\n",
       "   '2112462566',\n",
       "   '1761621746',\n",
       "   '2122568838',\n",
       "   '2077493113',\n",
       "   '107400462',\n",
       "   '2102496649']},\n",
       " {'id': '2112462566',\n",
       "  'title': 'Training Multilayer Perceptrons with the Extended Kalman Algorithm',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '498',\n",
       "  'abstract': 'A large fraction of recent work in artificial neural nets uses multilayer perceptrons trained with the back-propagation algorithm described by Rumelhart et. al. This algorithm converges slowly for large or complex problems such as speech recognition, where thousands of iterations may be needed for convergence even with small data sets. In this paper, we show that training multilayer perceptrons is an identification problem for a nonlinear dynamic system which can be solved using the Extended Kalman Algorithm. Although computationally complex, the Kalman algorithm usually converges in a few iterations. We describe the algorithm and compare it with back-propagation using two-dimensional examples.',\n",
       "  'date': 1987,\n",
       "  'authors': ['Sharad Singhal', 'Lance Wu'],\n",
       "  'related_topics': ['Perceptron',\n",
       "   'Artificial neural network',\n",
       "   'Parameter identification problem',\n",
       "   'Nonlinear system',\n",
       "   'Artificial intelligence',\n",
       "   'Fraction (mathematics)',\n",
       "   'Computer science',\n",
       "   'Training (meteorology)',\n",
       "   'Kalman algorithm'],\n",
       "  'references': ['2154642048',\n",
       "   '2173629880',\n",
       "   '304861154',\n",
       "   '2293807537',\n",
       "   '2105934661',\n",
       "   '1613359937',\n",
       "   '2051992922']},\n",
       " {'id': '2090248140',\n",
       "  'title': 'Generic constraints on underspecified target trajectories',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '222',\n",
       "  'abstract': 'Although general network learning rules are of undeniable interest, it is generally agreed that successful accounts of learning must incorporate domain-specific, a priori knowledge. Such knowledge might be used, for example, to determine the structure of a network or its initial weights. The author discusses a third possibility in which domain-specific knowledge is incorporated directly in a network learning rule via a set of constraints on activations. The approach uses the notion of a forward model to give constraints a domain-specific interpretation. This approach is demonstrated with several examples from the domain of motor learning. >',\n",
       "  'date': 1988,\n",
       "  'authors': ['Jordan'],\n",
       "  'related_topics': ['Learning rule',\n",
       "   'Instance-based learning',\n",
       "   'Learning classifier system',\n",
       "   'Stability (learning theory)',\n",
       "   'Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Leabra',\n",
       "   'Online machine learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Competitive learning',\n",
       "   'Inductive transfer',\n",
       "   'Multi-task learning',\n",
       "   'Computational learning theory',\n",
       "   'Robot learning',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Motor learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Generalization error'],\n",
       "  'references': ['2895674046',\n",
       "   '2016589492',\n",
       "   '1507849272',\n",
       "   '1967377907',\n",
       "   '1885639605',\n",
       "   '1981297107',\n",
       "   '2150367199',\n",
       "   '1892385946',\n",
       "   '1969166509',\n",
       "   '2046329526']},\n",
       " {'id': '1529008516',\n",
       "  'title': 'Supervised learning and systems with excess degrees of freedom',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '238',\n",
       "  'abstract': 'WHEN DISTINCT OUTPUTS OF AN ADAPTIVE SYSTEM HAVE EQUIVALENT EFFECTS ON THE ENVIRONMENT, THE PROBLEM OF FINDING APPROPRIATE ACTIONS GIVEN DESIRED RESULTS IS ILL-POSED. FOR SUPERVISED LEARNING ALGORITHMS, THE ILL-POSEDNESS OF SUCH \"INVERSE LEARNING PROBLEMS\" IMPLIES A CERTAIN FLEXIBILITY---DURING TRAINING, THERE ARE IN GENERAL MANY POSSIBLE TARGET VECTORS CORRESPONDING TO EACH INPUT VECTOR. TO ALLOW SUPERVISED LEARNING ALGORITHMS TO MAKE USE OF THIS FLEXIBILITY, THE CURRENT PAPER CONSIDERS HOW TO SPECIFY TARGETS BY SETS OF CONSTRAINTS, RATHER THAN AS PARTICULAR VECTORS. TWO CLASSES OF CONSTRAINTS ARE DISTINGUISHED---`CONFIGURATIONAL\\'\\' CONSTRAINTS, WHICH DEFINE REGIONS OF OUTPUT SPACE IN WHICH AN OUTPUT VECTOR MUST LIE, AND `TEMPORAL\\'\\' CONSTRAINTS, WHICH DEFINE RELATIONSHIPS BETWEEN OUTPUTS PRODUCED AT DIFFER- ENT POINTS IN TIME. LEARNING ALGORITHMS MINIMIZE A COST FUNCTION THAT CON- TAINS TERMS FOR BOTH KINDS OF CONSTRAINTS. THIS APPROACH TO INVERSE LEARN- ING IS ILLUSTRATED BY A ROBOTICS APPLICATION IN WHICH A NETWORK FINDS TRA- JECTORIES OF INVERSE KINEMATIC SOLUTIONS FOR MANIPULATORS WITH EXCESS DEGREES OF FREEDOM.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Michael I. Jordan'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Supervised learning',\n",
       "   'Degrees of freedom',\n",
       "   'Function (mathematics)',\n",
       "   'Adaptive system',\n",
       "   'Inverse',\n",
       "   'Kinematics',\n",
       "   'Robotics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2123716044',\n",
       "   '2167224731',\n",
       "   '2156562940',\n",
       "   '2118415523',\n",
       "   '2188233853',\n",
       "   '2056655352',\n",
       "   '2152503618',\n",
       "   '2993446282',\n",
       "   '2115072676']},\n",
       " {'id': '2581275558',\n",
       "  'title': 'Optimization by simulated annealing',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '52,083',\n",
       "  'abstract': 'There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.',\n",
       "  'date': 1986,\n",
       "  'authors': ['S. Kirkpatrick 1, C. D. Gelatt 1, M. P. Vecchi 2'],\n",
       "  'related_topics': ['Optimization problem',\n",
       "   'Combinatorial optimization',\n",
       "   'Simulated annealing'],\n",
       "  'references': ['2042986967',\n",
       "   '2022820481',\n",
       "   '2114552889',\n",
       "   '2022494241',\n",
       "   '2143037347',\n",
       "   '2056760934',\n",
       "   '2148673189',\n",
       "   '86906884',\n",
       "   '2014952973',\n",
       "   '2014068360']},\n",
       " {'id': '1597286183',\n",
       "  'title': 'Neural computation of decisions in optimization problems',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '8,064',\n",
       "  'abstract': 'Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.',\n",
       "  'date': 1985,\n",
       "  'authors': ['J. J. Hopfield 1, D. W. Tank 2'],\n",
       "  'related_topics': ['Optimization problem',\n",
       "   'Models of neural computation',\n",
       "   'Hopfield network',\n",
       "   'Computation',\n",
       "   'Complex system',\n",
       "   'Cybernetics',\n",
       "   'Nonlinear system',\n",
       "   'Mathematical optimization',\n",
       "   'Decision theory',\n",
       "   'Algorithm',\n",
       "   'Computer science'],\n",
       "  'references': ['2581275558',\n",
       "   '2011039300',\n",
       "   '2293063825',\n",
       "   '2177721432',\n",
       "   '1666015432',\n",
       "   '307896644',\n",
       "   '2042986967',\n",
       "   '2112325651',\n",
       "   '2032533296',\n",
       "   '1543738661']},\n",
       " {'id': '1507849272',\n",
       "  'title': 'A learning algorithm for Boltzmann machines',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '4,821',\n",
       "  'abstract': 'The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.',\n",
       "  'date': 1987,\n",
       "  'authors': ['David H. Ackley 1, Geoffrey E. Hinton 1, Terrence J. Sejnowski 2'],\n",
       "  'related_topics': ['Massively parallel',\n",
       "   'Learning rule',\n",
       "   'Constraint satisfaction',\n",
       "   'Domain (software engineering)',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Encoding (memory)',\n",
       "   'Connection (vector bundle)',\n",
       "   'Theoretical computer science',\n",
       "   'Computation',\n",
       "   'Computer science',\n",
       "   'Algorithm'],\n",
       "  'references': ['2581275558',\n",
       "   '1997063559',\n",
       "   '2293063825',\n",
       "   '2112325651',\n",
       "   '2056760934',\n",
       "   '2157629899',\n",
       "   '2098205603',\n",
       "   '1597474747',\n",
       "   '1980658026',\n",
       "   '807785616']},\n",
       " {'id': '1971129545',\n",
       "  'title': 'Generalization of backpropagation with application to a recurrent gas market model',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '1,031',\n",
       "  'abstract': 'Abstract Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.',\n",
       "  'date': 1987,\n",
       "  'authors': ['Paul J. Werbos'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Catastrophic interference',\n",
       "   'Artificial neural network',\n",
       "   'Delta rule',\n",
       "   'Hopfield network',\n",
       "   'Generalization',\n",
       "   'Reinforcement learning',\n",
       "   'Least squares',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['22297218',\n",
       "   '2068484625',\n",
       "   '2176028050',\n",
       "   '1583833196',\n",
       "   '2010526455',\n",
       "   '3121926921',\n",
       "   '1969166509',\n",
       "   '3150413596',\n",
       "   '134309601',\n",
       "   '3022423118']},\n",
       " {'id': '3121926921',\n",
       "  'title': 'Beyond regression : new fools for prediction and analysis in the behavioral sciences',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,576',\n",
       "  'abstract': '',\n",
       "  'date': 1973,\n",
       "  'authors': ['P. Werbos'],\n",
       "  'related_topics': ['Regression',\n",
       "   'Behavioural sciences',\n",
       "   'Computer science',\n",
       "   'Cognitive psychology'],\n",
       "  'references': ['2618530766',\n",
       "   '2076063813',\n",
       "   '2141125852',\n",
       "   '2019207321',\n",
       "   '2170973209',\n",
       "   '2132424367',\n",
       "   '2002016471',\n",
       "   '3123753580']},\n",
       " {'id': '2147800946',\n",
       "  'title': 'Backpropagation applied to handwritten zip code recognition',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '8,794',\n",
       "  'abstract': 'The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.',\n",
       "  'date': 1989,\n",
       "  'authors': ['Y. LeCun',\n",
       "   'B. Boser',\n",
       "   'J. S. Denker',\n",
       "   'D. Henderson',\n",
       "   'R. E. Howard',\n",
       "   'W. Hubbard',\n",
       "   'L. D. Jackel'],\n",
       "  'related_topics': ['Backpropagation',\n",
       "   'Character (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Domain (software engineering)',\n",
       "   'Computer science',\n",
       "   'Image (mathematics)',\n",
       "   'Task (project management)',\n",
       "   'Artificial intelligence',\n",
       "   'Postal service',\n",
       "   'Zip code'],\n",
       "  'references': ['2154642048',\n",
       "   '2165758113',\n",
       "   '169539560',\n",
       "   '19621276',\n",
       "   '2101926813',\n",
       "   '56903235',\n",
       "   '2157475639',\n",
       "   '2606594511',\n",
       "   '1965770722',\n",
       "   '2116360511']},\n",
       " {'id': '2895674046',\n",
       "  'title': 'Adaptive Signal Processing',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '10,639',\n",
       "  'abstract': 'GENERAL INTRODUCTION. Adaptive Systems. The Adaptive Linear Combiner. THEORY OF ADAPTATION WITH STATIONARY SIGNALS. Properties of the Quadratic Performance Surface. Searching the Performance Surface. Gradient Estimation and Its Effects on Adaptation. ADAPTIVE ALGORITHMS AND STRUCTURES. The LMS Algorithm. The Z-Transform in Adaptive Signal Processing. Other Adaptive Algorithms and Structures. Adaptive Lattice Filters. APPLICATIONS. Adaptive Modeling and System Identification. Inverse Adaptive Modeling, Deconvolution, and Equalization. Adaptive Control Systems. Adaptive Interference Cancelling. Introduction to Adaptive Arrays and Adaptive Beamforming. Analysis of Adaptive Beamformers.',\n",
       "  'date': 1984,\n",
       "  'authors': ['Bernard Widrow 1, Samuel D. Stearns 2'],\n",
       "  'related_topics': ['Adaptive filter',\n",
       "   'Adaptive beamformer',\n",
       "   'Adaptive control',\n",
       "   'Adaptive system',\n",
       "   'Multidelay block frequency domain adaptive filter',\n",
       "   'Signal processing',\n",
       "   'Least mean squares filter',\n",
       "   'Linear system',\n",
       "   'Algorithm',\n",
       "   'Computer science'],\n",
       "  'references': ['2071707134',\n",
       "   '2019207321',\n",
       "   '1544329015',\n",
       "   '2158518777',\n",
       "   '2148138104',\n",
       "   '2118776392',\n",
       "   '2091881639',\n",
       "   '2148982591',\n",
       "   '2100677568',\n",
       "   '2098674248']},\n",
       " {'id': '2107878631',\n",
       "  'title': 'Learning long-term dependencies with gradient descent is difficult',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '6,898',\n",
       "  'abstract': 'Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. >',\n",
       "  'date': 1994,\n",
       "  'authors': ['Y. Bengio 1, P. Simard 2, P. Frasconi 3'],\n",
       "  'related_topics': ['Vanishing gradient problem',\n",
       "   'Gradient descent',\n",
       "   'Recurrent neural network',\n",
       "   'Gradient method',\n",
       "   'Artificial neural network',\n",
       "   'Term (time)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Machine learning',\n",
       "   'Face (geometry)',\n",
       "   'Dynamical systems theory',\n",
       "   'Computer science',\n",
       "   'Numerical analysis',\n",
       "   'Artificial intelligence',\n",
       "   'Recurrent neural nets'],\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2016589492',\n",
       "   '19621276',\n",
       "   '2128499899',\n",
       "   '2088978850',\n",
       "   '2148099973',\n",
       "   '1996741810',\n",
       "   '2125329357',\n",
       "   '1527772862']},\n",
       " {'id': '2798813531',\n",
       "  'title': 'Linear systems',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '11,071',\n",
       "  'abstract': '',\n",
       "  'date': 1979,\n",
       "  'authors': ['Thomas Kailath'],\n",
       "  'related_topics': ['Linear system',\n",
       "   'Computer science',\n",
       "   'Applied mathematics',\n",
       "   'Rational matrices'],\n",
       "  'references': ['2134673975',\n",
       "   '1506119886',\n",
       "   '2099839128',\n",
       "   '1521785144',\n",
       "   '1588998206',\n",
       "   '2065297540',\n",
       "   '1986922155',\n",
       "   '2012445782',\n",
       "   '2912155302',\n",
       "   '2149072817']},\n",
       " {'id': '2098398123',\n",
       "  'title': 'Non-linear system identification using neural networks',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '1,327',\n",
       "  'abstract': 'Multi-layered neural networks offer an exciting alternative for modelling complex non-linear systems. This paper investigates the identification of discrete-time nonlinear systems using neural networks with a single hidden layer. New parameter estimation algorithms are derived for the neural network model based on a prediction error formulation and the application to both simulated and real data is included to demonstrate the effectiveness of the neural network approach.',\n",
       "  'date': 1989,\n",
       "  'authors': ['S. Chen 1, S. A. Billings 2, Peter Grant 1'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Types of artificial neural networks'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2103496339',\n",
       "   '1971735090',\n",
       "   '1540723801',\n",
       "   '1603277681',\n",
       "   '2102380305',\n",
       "   '1650765400',\n",
       "   '2125812768',\n",
       "   '1535689967']},\n",
       " {'id': '2176028050',\n",
       "  'title': 'Connectionist learning procedures',\n",
       "  'reference_count': '85',\n",
       "  'citation_count': '3,990',\n",
       "  'abstract': 'A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Connectionism',\n",
       "   'Task (computing)',\n",
       "   'Generalization',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Domain (software engineering)',\n",
       "   'Rate of convergence',\n",
       "   'Construct (python library)',\n",
       "   'Connection (mathematics)'],\n",
       "  'references': ['1497256448',\n",
       "   '2581275558',\n",
       "   '2154642048',\n",
       "   '1498436455',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '2293063825',\n",
       "   '2895674046',\n",
       "   '1597286183',\n",
       "   '22297218']},\n",
       " {'id': '1966812932',\n",
       "  'title': 'A Maximum Likelihood Approach to Continuous Speech Recognition',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '1,965',\n",
       "  'abstract': 'Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.',\n",
       "  'date': 1983,\n",
       "  'authors': ['Lalit R. Bahl', 'Frederick Jelinek', 'Robert L. Mercer'],\n",
       "  'related_topics': ['Speech processing',\n",
       "   'Sequential decoding',\n",
       "   'Decoding methods'],\n",
       "  'references': ['2142384583',\n",
       "   '1597533204',\n",
       "   '1575431606',\n",
       "   '2341171179',\n",
       "   '2163929346',\n",
       "   '2157477135',\n",
       "   '2029491572',\n",
       "   '2035227369',\n",
       "   '2137095888',\n",
       "   '1989226853']},\n",
       " {'id': '2101926813',\n",
       "  'title': 'Neocognitron: A Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '4,995',\n",
       "  'abstract': \"A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.\",\n",
       "  'date': 1980,\n",
       "  'authors': ['Kunihiko Fukushima'],\n",
       "  'related_topics': ['Neocognitron',\n",
       "   'Form perception',\n",
       "   'Stimulus (physiology)',\n",
       "   'Artificial neural network',\n",
       "   'Unsupervised learning',\n",
       "   'Hypercomplex number',\n",
       "   'Gestalt psychology',\n",
       "   'Pattern recognition',\n",
       "   'Cascade',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2116360511',\n",
       "   '2322002063',\n",
       "   '2053120614',\n",
       "   '1588340522',\n",
       "   '1594551768',\n",
       "   '2010315761',\n",
       "   '2272360941',\n",
       "   '22889343',\n",
       "   '2324189819',\n",
       "   '2091546412']},\n",
       " {'id': '1991133427',\n",
       "  'title': 'Error bounds for convolutional codes and an asymptotically optimum decoding algorithm',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '9,842',\n",
       "  'abstract': 'The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms.',\n",
       "  'date': 1967,\n",
       "  'authors': ['A. Viterbi'],\n",
       "  'related_topics': ['Sequential decoding',\n",
       "   'Serial concatenated convolutional codes',\n",
       "   'List decoding',\n",
       "   'Convolutional code',\n",
       "   'Concatenated error correction code',\n",
       "   'Linear code',\n",
       "   'Turbo code',\n",
       "   'Low-density parity-check code',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2034274945',\n",
       "   '1993944611',\n",
       "   '2087362480',\n",
       "   '2005530146',\n",
       "   '1976797517',\n",
       "   '1527268325',\n",
       "   '1527096151']},\n",
       " {'id': '2048330959',\n",
       "  'title': 'Cooperative computation of stereo disparity',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '2,307',\n",
       "  'abstract': 'Perhaps one of the most striking differences between a brain and today’s computers is the amount of “wiring.” In a digital computer the ratio of connections to components is about 3, whereas for the mammalian cortex it lies between 10 and 10,000 (1).',\n",
       "  'date': 1987,\n",
       "  'authors': ['D. Marr 1, T. Poggio 2'],\n",
       "  'related_topics': ['Computation',\n",
       "   'Information processing',\n",
       "   'Cortex (anatomy)',\n",
       "   'Correspondence problem',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Perception',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Combinatorial analysis',\n",
       "   'Digital computer'],\n",
       "  'references': ['2170716495',\n",
       "   '1997494543',\n",
       "   '2052810501',\n",
       "   '2087895317',\n",
       "   '2089840306',\n",
       "   '1975068880',\n",
       "   '1992476998',\n",
       "   '1981520343',\n",
       "   '2001963156',\n",
       "   '1989544735']},\n",
       " {'id': '2144499799',\n",
       "  'title': 'Supervised Sequence Labelling with Recurrent Neural Networks',\n",
       "  'reference_count': '137',\n",
       "  'citation_count': '1,803',\n",
       "  'abstract': 'Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of the long short-term memory network architecture to multidimensional data, such as images and video sequences.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Alexander Graves'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Context (language use)',\n",
       "   'Sequence',\n",
       "   'Network architecture',\n",
       "   'Machine learning',\n",
       "   'Labelling',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'Extension (predicate logic)',\n",
       "   'Artificial intelligence',\n",
       "   'Multidimensional data'],\n",
       "  'references': ['2156909104',\n",
       "   '1663973292',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '1554663460',\n",
       "   '2147880316',\n",
       "   '2125838338',\n",
       "   '2110798204',\n",
       "   '1993882792']},\n",
       " {'id': '2136848157',\n",
       "  'title': 'Learning to Forget: Continual Prediction with LSTM',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '7,623',\n",
       "  'abstract': 'Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network\\'s internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive \"forget gate\" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Felix A. Gers', 'Jürgen A. Schmidhuber', 'Fred A. Cummins'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Reset (computing)'],\n",
       "  'references': ['2064675550',\n",
       "   '2107878631',\n",
       "   '194249466',\n",
       "   '2154890045',\n",
       "   '2103452139',\n",
       "   '1674799117',\n",
       "   '1971129545',\n",
       "   '2121029939',\n",
       "   '2057653135',\n",
       "   '1959983357']},\n",
       " {'id': '1735317348',\n",
       "  'title': 'Recurrent Network Models for Human Dynamics',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '495',\n",
       "  'abstract': 'We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoiding drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units [31].',\n",
       "  'date': 2015,\n",
       "  'authors': ['Katerina Fragkiadaki',\n",
       "   'Sergey Levine',\n",
       "   'Panna Felsen',\n",
       "   'Jitendra Malik'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Feature learning',\n",
       "   'Optical flow',\n",
       "   'Motion capture',\n",
       "   'Feature (machine learning)',\n",
       "   'Network model',\n",
       "   'Encoder',\n",
       "   'Motion (physics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2130942839',\n",
       "   '2100495367',\n",
       "   '2064675550',\n",
       "   '1895577753',\n",
       "   '2145094598',\n",
       "   '1810943226',\n",
       "   '2113325037',\n",
       "   '2136391815',\n",
       "   '196214544']},\n",
       " {'id': '2079735306',\n",
       "  'title': '2005 Special Issue: Framewise phoneme classification with bidirectional LSTM and other neural network architectures',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '1,810',\n",
       "  'abstract': 'In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Alex Graves 1, Jürgen Schmidhuber 2'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Perceptron',\n",
       "   'Speech processing',\n",
       "   'Network architecture',\n",
       "   'Benchmark (computing)',\n",
       "   'Speech recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Recurrent neural nets'],\n",
       "  'references': ['2064675550',\n",
       "   '1554663460',\n",
       "   '2131774270',\n",
       "   '2147568880',\n",
       "   '1553004968',\n",
       "   '1485231155',\n",
       "   '1525783482',\n",
       "   '2110871230',\n",
       "   '1566256432',\n",
       "   '1674799117']},\n",
       " {'id': '3099873379',\n",
       "  'title': 'Neural NILM: Deep Neural Networks Applied to Energy Disaggregation',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '445',\n",
       "  'abstract': \"Energy disaggregation estimates appliance-by-appliance electricity consumption from a single meter that measures the whole home's electricity demand. Recently, deep neural networks have driven remarkable improvements in classification performance in neighbouring machine learning fields such as image classification and automatic speech recognition. In this paper, we adapt three deep neural network architectures to energy disaggregation: 1) a form of recurrent neural network called `long short-term memory' (LSTM); 2) denoising autoencoders; and 3) a network which regresses the start time, end time and average power demand of each appliance activation. We use seven metrics to test the performance of these algorithms on real aggregate power data from five appliances. Tests are performed against a house not seen during training and against houses seen during training. We find that all three neural nets achieve better F1 scores (averaged over all five appliances) than either combinatorial optimisation or factorial hidden Markov models and that our neural net algorithms generalise well to an unseen house.\",\n",
       "  'date': 2015,\n",
       "  'authors': ['Jack Kelly', 'William Knottenbelt'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Recurrent neural network',\n",
       "   'Artificial neural network',\n",
       "   'Feature learning',\n",
       "   'Contextual image classification',\n",
       "   'Machine learning',\n",
       "   'Energy (signal processing)',\n",
       "   'Energy conservation',\n",
       "   'Computer science',\n",
       "   'Aggregate (data warehouse)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2145339207',\n",
       "   '2130942839',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '2124386111',\n",
       "   '2025768430',\n",
       "   '1810943226',\n",
       "   '2154642048']},\n",
       " {'id': '2147568880',\n",
       "  'title': 'Learning precise timing with lstm recurrent networks',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '1,395',\n",
       "  'abstract': 'The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Felix A. Gers 1, Nicol N. Schraudolph 2, Jürgen Schmidhuber 1'],\n",
       "  'related_topics': ['Recurrent neural network',\n",
       "   'Hidden Markov model',\n",
       "   'Machine learning',\n",
       "   'Forcing (recursion theory)',\n",
       "   'Motor control',\n",
       "   'Computer science',\n",
       "   'Focus (optics)',\n",
       "   'Rhythm',\n",
       "   'Artificial intelligence',\n",
       "   'Computational Science and Engineering',\n",
       "   'Temporal distance'],\n",
       "  'references': ['2064675550',\n",
       "   '2107878631',\n",
       "   '2136848157',\n",
       "   '2914484425',\n",
       "   '2016589492',\n",
       "   '1525783482',\n",
       "   '194249466',\n",
       "   '2154890045',\n",
       "   '1674799117',\n",
       "   '2121029939']},\n",
       " {'id': '2057175746',\n",
       "  'title': 'Shape matching and object recognition using shape contexts',\n",
       "  'reference_count': '58',\n",
       "  'citation_count': '8,348',\n",
       "  'abstract': 'We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set.',\n",
       "  'date': 2002,\n",
       "  'authors': ['S. Belongie 1, J. Malik 2, J. Puzicha 3'],\n",
       "  'related_topics': ['Shape analysis (digital geometry)',\n",
       "   'Heat kernel signature',\n",
       "   'Shape context',\n",
       "   'Similarity (geometry)',\n",
       "   'Correspondence problem',\n",
       "   'Feature extraction',\n",
       "   'GLOH',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '2124386111',\n",
       "   '2119821739',\n",
       "   '2117812871',\n",
       "   '2138451337',\n",
       "   '2038952578',\n",
       "   '2124087378',\n",
       "   '2123977795',\n",
       "   '2101522199',\n",
       "   '2146766088']},\n",
       " {'id': '2159080219',\n",
       "  'title': 'Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference',\n",
       "  'reference_count': '235',\n",
       "  'citation_count': '24,370',\n",
       "  'abstract': 'From the Publisher: Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertainty\\x97and offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognition\\x97in short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.',\n",
       "  'date': 1987,\n",
       "  'authors': ['Judea Pearl'],\n",
       "  'related_topics': ['Reasoning system',\n",
       "   'Intelligent decision support system',\n",
       "   'Probabilistic logic network',\n",
       "   'Probabilistic argumentation',\n",
       "   'Non-monotonic logic',\n",
       "   'Probabilistic logic',\n",
       "   'Uncertain inference',\n",
       "   'Decision support system',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Machine learning'],\n",
       "  'references': ['2581275558',\n",
       "   '1997063559',\n",
       "   '1593793857',\n",
       "   '2797148637',\n",
       "   '2155322595',\n",
       "   '158727920',\n",
       "   '2138162238',\n",
       "   '2108309071',\n",
       "   '1986808060',\n",
       "   '2142901448']},\n",
       " {'id': '2131686571',\n",
       "  'title': 'Fields of Experts: a framework for learning image priors',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '1,180',\n",
       "  'abstract': 'We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.',\n",
       "  'date': 2005,\n",
       "  'authors': ['S. Roth', 'M.J. Black'],\n",
       "  'related_topics': ['Markov random field',\n",
       "   'Approximate inference',\n",
       "   'Inpainting',\n",
       "   'Machine vision',\n",
       "   'Field (computer science)',\n",
       "   'Pixel',\n",
       "   'Markov process',\n",
       "   'Prior probability',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Iterative reconstruction',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2133665775',\n",
       "   '2116064496',\n",
       "   '2108384452',\n",
       "   '1997063559',\n",
       "   '2121927366',\n",
       "   '2113945798',\n",
       "   '2295936755',\n",
       "   '2116013899',\n",
       "   '2149760002',\n",
       "   '2105464873']},\n",
       " {'id': '2158778629',\n",
       "  'title': 'Toward automatic phenotyping of developing embryos from videos',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '308',\n",
       "  'abstract': 'We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Feng Ning 1, D. Delhomme 2, Y. LeCun 3, F. Piano 1, L. Bottou 4, P.E. Barbano 3'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Image processing',\n",
       "   'Contextual image classification',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Component (UML)',\n",
       "   'Set (abstract data type)',\n",
       "   'Computer science',\n",
       "   'Cytoplasm',\n",
       "   'Embryo',\n",
       "   'Cell wall',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2310919327',\n",
       "   '2147880316',\n",
       "   '2104095591',\n",
       "   '1647075334',\n",
       "   '2134557905',\n",
       "   '2121927366',\n",
       "   '2119823327',\n",
       "   '2157364932',\n",
       "   '1991848143']},\n",
       " {'id': '2567948266',\n",
       "  'title': 'A view of the EM algorithm that justifies incremental, sparse, and other variants',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '3,202',\n",
       "  'abstract': 'The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Radford M. Neal', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Expectation–maximization algorithm',\n",
       "   'Conditional probability distribution',\n",
       "   'Function (mathematics)',\n",
       "   'Range (statistics)',\n",
       "   'Convergence (routing)',\n",
       "   'Standard algorithms',\n",
       "   'Distribution (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Energy (signal processing)',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science'],\n",
       "  'references': ['2049633694',\n",
       "   '2117853077',\n",
       "   '2024476015',\n",
       "   '1580495158',\n",
       "   '1991278573',\n",
       "   '581152777']},\n",
       " {'id': '2124914669',\n",
       "  'title': 'Exponential Family Harmoniums with an Application to Information Retrieval',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '573',\n",
       "  'abstract': 'Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Max Welling 1, Michal Rosen-zvi 1, Geoffrey E. Hinton 2'],\n",
       "  'related_topics': ['Exponential random graph models',\n",
       "   'Divergence-from-randomness model',\n",
       "   'Graphical model',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Exponential family',\n",
       "   'Posterior probability',\n",
       "   'Statistical model',\n",
       "   'Document retrieval',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1880262756',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '1612003148',\n",
       "   '2140124448',\n",
       "   '1934021597',\n",
       "   '2138448681',\n",
       "   '145818128',\n",
       "   '2109720450',\n",
       "   '1813659000']},\n",
       " {'id': '2153663612',\n",
       "  'title': 'Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '5,601',\n",
       "  'abstract': 'We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods',\n",
       "  'date': 2006,\n",
       "  'authors': ['M. Elad', 'M. Aharon'],\n",
       "  'related_topics': ['Non-local means',\n",
       "   'Video denoising',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image processing',\n",
       "   'K-SVD',\n",
       "   'Image quality',\n",
       "   'Sparse approximation',\n",
       "   'Matching pursuit',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2160547390',\n",
       "   '2078204800',\n",
       "   '2146842127',\n",
       "   '2158940042',\n",
       "   '2151693816',\n",
       "   '2097323375',\n",
       "   '2113945798',\n",
       "   '2132680427',\n",
       "   '2079724595',\n",
       "   '2131686571']},\n",
       " {'id': '2613634265',\n",
       "  'title': 'Scaling learning algorithms towards AI',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '1,419',\n",
       "  'abstract': 'One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Yoshua Bengio 1, 2, 3, Yann Lecun'],\n",
       "  'related_topics': ['Kernel method',\n",
       "   'Kernel (statistics)',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Intelligent control',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Curse of dimensionality',\n",
       "   'Machine learning',\n",
       "   'Invariant (computer science)',\n",
       "   'Computer science',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2148603752',\n",
       "   '2310919327',\n",
       "   '2119821739',\n",
       "   '2053186076',\n",
       "   '2116064496',\n",
       "   '2001141328',\n",
       "   '2110798204',\n",
       "   '2057175746',\n",
       "   '2140095548']},\n",
       " {'id': '1993845689',\n",
       "  'title': 'The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '1,284',\n",
       "  'abstract': 'An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Geoffrey E. Hinton',\n",
       "   'Peter Dayan',\n",
       "   'Brendan J. Frey',\n",
       "   'Radford M. Neal'],\n",
       "  'related_topics': ['Helmholtz machine',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Artificial neural network',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Connectionism',\n",
       "   'Stochastic process',\n",
       "   'Pattern recognition',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2740373864',\n",
       "   '2177040213',\n",
       "   '1533169541',\n",
       "   '2044875682',\n",
       "   '94647076']},\n",
       " {'id': '2109779438',\n",
       "  'title': 'The Cascade-Correlation Learning Architecture',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '4,321',\n",
       "  'abstract': 'Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Scott E. Fahlman', 'Christian Lebiere'],\n",
       "  'related_topics': ['Network topology',\n",
       "   'Network simulation',\n",
       "   'Artificial neural network',\n",
       "   'Topology (electrical circuits)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Detector',\n",
       "   'Training set'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2160208155',\n",
       "   '19621276',\n",
       "   '3121126077',\n",
       "   '2160699933',\n",
       "   '50076749',\n",
       "   '2127385318',\n",
       "   '2169163929',\n",
       "   '2167277568']},\n",
       " {'id': '2103626435',\n",
       "  'title': 'Practical Issues in Temporal Difference Learning',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '1,569',\n",
       "  'abstract': \"This paper examines whether temporal difference methods for training connectionist networks, such as Sutton's TD(λ) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and discussed from a general theoretical perspective. These practical issues are then examined in the context of a case study in which TD(λ) is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex non-trivial task. It is found that, with zero knowledge built in, the network is able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. This indicates that TD learning may work better in practice than one would expect based on current theory, and it suggests that further analysis of TD methods, as well as applications in other complex domains, may be worth investigating.\",\n",
       "  'date': 1992,\n",
       "  'authors': ['Gerald Tesauro'],\n",
       "  'related_topics': ['Temporal difference learning',\n",
       "   'Outcome (game theory)',\n",
       "   'Context (language use)',\n",
       "   'Artificial neural network',\n",
       "   'Connectionism',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Zero-knowledge proof',\n",
       "   'Perspective (graphical)'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2137983211',\n",
       "   '3146803896',\n",
       "   '2100677568',\n",
       "   '2178806388',\n",
       "   '1583833196',\n",
       "   '2154952480',\n",
       "   '2159047538',\n",
       "   '1569296262']},\n",
       " {'id': '2125569215',\n",
       "  'title': 'The Curse of Highly Variable Functions for Local Kernel Machines',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '224',\n",
       "  'abstract': 'We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Yoshua Bengio', 'Olivier Delalleau', 'Nicolas L. Roux'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Kernel method',\n",
       "   'Kernel embedding of distributions',\n",
       "   'Radial basis function kernel',\n",
       "   'Unsupervised learning',\n",
       "   'Polynomial kernel',\n",
       "   'Curse of dimensionality',\n",
       "   'Tree kernel',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2119821739',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2140095548',\n",
       "   '2087347434',\n",
       "   '2154455818',\n",
       "   '2139823104',\n",
       "   '1604938182',\n",
       "   '3017143921',\n",
       "   '2160167256']},\n",
       " {'id': '2167967601',\n",
       "  'title': 'Convex Neural Networks',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '190',\n",
       "  'abstract': 'Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Yoshua Bengio',\n",
       "   'Nicolas L. Roux',\n",
       "   'Pascal Vincent',\n",
       "   'Olivier Delalleau',\n",
       "   'Patrice Marcotte'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convex optimization',\n",
       "   'Types of artificial neural networks',\n",
       "   'Artificial neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Competitive learning',\n",
       "   'Feedforward neural network',\n",
       "   'Time delay neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2135046866',\n",
       "   '3124955340',\n",
       "   '1678356000',\n",
       "   '1498436455',\n",
       "   '2151693816',\n",
       "   '2091886411',\n",
       "   '2504871398',\n",
       "   '2108263314',\n",
       "   '2109405055',\n",
       "   '2075887074']},\n",
       " {'id': '2153635508',\n",
       "  'title': 'LIBSVM: A library for support vector machines',\n",
       "  'reference_count': '52',\n",
       "  'citation_count': '43,942',\n",
       "  'abstract': 'LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Chih-Chung Chang', 'Chih-Jen Lin'],\n",
       "  'related_topics': ['Sequential minimal optimization',\n",
       "   'Structured support vector machine',\n",
       "   'Support vector machine'],\n",
       "  'references': ['2148603752',\n",
       "   '2119821739',\n",
       "   '2109943925',\n",
       "   '2172000360',\n",
       "   '1512098439',\n",
       "   '2104978738',\n",
       "   '1576520375',\n",
       "   '2087347434',\n",
       "   '2132870739',\n",
       "   '2124351082']},\n",
       " {'id': '1902027874',\n",
       "  'title': 'Learning the parts of objects by non-negative matrix factorization',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '12,805',\n",
       "  'abstract': 'Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Daniel D. Lee 1, H. Sebastian Seung 1, 2'],\n",
       "  'related_topics': ['Non-negative matrix factorization',\n",
       "   'Matrix decomposition',\n",
       "   'Document-term matrix',\n",
       "   'Factorization',\n",
       "   'Representation (mathematics)',\n",
       "   'Sign (mathematics)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Vector quantization',\n",
       "   'Pattern recognition',\n",
       "   'Physics',\n",
       "   'Bioinformatics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2108384452',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '2145889472',\n",
       "   '1983578042',\n",
       "   '1996355918',\n",
       "   '1993845689',\n",
       "   '2156406284',\n",
       "   '2180838288']},\n",
       " {'id': '2105464873',\n",
       "  'title': 'Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1 ?',\n",
       "  'reference_count': '38',\n",
       "  'citation_count': '4,209',\n",
       "  'abstract': 'The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Bruno A. Olshausen', 'David J. Field'],\n",
       "  'related_topics': ['Basis function',\n",
       "   'Efficient coding hypothesis',\n",
       "   'Neural coding',\n",
       "   'K-SVD',\n",
       "   'Linear independence',\n",
       "   'Gabor wavelet',\n",
       "   'Function (mathematics)',\n",
       "   'Wavelet transform',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Communication'],\n",
       "  'references': ['2132984323',\n",
       "   '2099741732',\n",
       "   '2108384452',\n",
       "   '2145889472',\n",
       "   '1536929369',\n",
       "   '2133069808',\n",
       "   '2107790757',\n",
       "   '2167034998',\n",
       "   '3022628558',\n",
       "   '2145012779']},\n",
       " {'id': '1802356529',\n",
       "  'title': 'Energy-based models for sparse overcomplete representations',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '206',\n",
       "  'abstract': 'We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Yee Whye Teh 1, Max Welling 1, Simon Osindero 2, Geoffrey E. Hinton 1'],\n",
       "  'related_topics': ['Independent component analysis',\n",
       "   'Independence (probability theory)',\n",
       "   'Conditional independence',\n",
       "   'Blind signal separation',\n",
       "   'Probability distribution',\n",
       "   'Density estimation',\n",
       "   'Free parameter',\n",
       "   'Contrast (statistics)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2116064496',\n",
       "   '2078204800',\n",
       "   '2154642048',\n",
       "   '1652505363',\n",
       "   '2099741732',\n",
       "   '2108384452',\n",
       "   '2151693816',\n",
       "   '2145889472',\n",
       "   '2146474141',\n",
       "   '2105464873']},\n",
       " {'id': '2075187489',\n",
       "  'title': 'The Cost of Cortical Computation',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '1,164',\n",
       "  'abstract': 'Electrophysiological recordings show that individual neurons in cortex are strongly activated when engaged in appropriate tasks, but they tell us little about how many neurons might be engaged by a task, which is important to know if we are to understand how cortex encodes information. For human cortex, I estimate the cost of individual spikes, then, from the known energy consumption of cortex, I establish how many neurons can be active concurrently. The cost of a single spike is high, and this severely limits, possibly to fewer than 1%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. The latter constraint explains the investment in local control of hemodynamics, exploited by functional magnetic resonance imaging, and the need for mechanisms of selective attention.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Peter Lennie'],\n",
       "  'related_topics': ['Functional magnetic resonance imaging',\n",
       "   'Cortex (anatomy)',\n",
       "   'Brain mapping',\n",
       "   'Electrophysiology',\n",
       "   'Task (project management)',\n",
       "   'Neuroscience',\n",
       "   'Energy consumption',\n",
       "   'Constraint (information theory)',\n",
       "   'Computation',\n",
       "   'Biology'],\n",
       "  'references': ['2110208125',\n",
       "   '2042422091',\n",
       "   '1907121963',\n",
       "   '1976738367',\n",
       "   '2003739479',\n",
       "   '1603661052',\n",
       "   '1993303421',\n",
       "   '1991233288',\n",
       "   '2096519870',\n",
       "   '43284170']},\n",
       " {'id': '2102409316',\n",
       "  'title': 'Autoencoders, Minimum Description Length and Helmholtz Free Energy',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '1,061',\n",
       "  'abstract': 'An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.',\n",
       "  'date': 1993,\n",
       "  'authors': ['Geoffrey E. Hinton 1, Richard S. Zemel 2'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Minimum description length',\n",
       "   'Upper and lower bounds',\n",
       "   'Code (cryptography)',\n",
       "   'Boltzmann distribution',\n",
       "   'Helmholtz free energy',\n",
       "   'Lyapunov function',\n",
       "   'Set (abstract data type)',\n",
       "   'Algorithm',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics'],\n",
       "  'references': ['2176028050',\n",
       "   '2078626246',\n",
       "   '2063089147',\n",
       "   '1578739277',\n",
       "   '2110553242',\n",
       "   '2151907713']},\n",
       " {'id': '11828546',\n",
       "  'title': '4.7 – Statistical Modeling of Photographic Images',\n",
       "  'reference_count': '61',\n",
       "  'citation_count': '118',\n",
       "  'abstract': '',\n",
       "  'date': 2004,\n",
       "  'authors': ['Eero P. Simoncelli'],\n",
       "  'related_topics': ['Statistical model',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2132984323',\n",
       "   '2053691921',\n",
       "   '2113945798',\n",
       "   '2145889472',\n",
       "   '2105464873',\n",
       "   '2103504761',\n",
       "   '2134929491',\n",
       "   '2127006916',\n",
       "   '2137234026',\n",
       "   '2109863423']},\n",
       " {'id': '2144081223',\n",
       "  'title': 'Coot: model-building tools for molecular graphics.',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '26,438',\n",
       "  'abstract': \"CCP4mg is a project that aims to provide a general-purpose tool for structural biologists, providing tools for X-ray structure solution, structure comparison and analysis, and publication-quality graphics. The map-fitting tools are available as a stand-alone package, distributed as `Coot'.\",\n",
       "  'date': 2004,\n",
       "  'authors': ['Paul Emsley', 'Kevin Cowtan'],\n",
       "  'related_topics': ['Computer graphics',\n",
       "   'Molecular graphics',\n",
       "   'Enzyme structure',\n",
       "   'Graphics',\n",
       "   'Oxidoreductase inhibitor',\n",
       "   'Model building',\n",
       "   'Software engineering',\n",
       "   'Computer science',\n",
       "   'Software',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Bioinformatics'],\n",
       "  'references': ['2130479394',\n",
       "   '2013083986',\n",
       "   '2135839939',\n",
       "   '1986830449',\n",
       "   '1969222787']},\n",
       " {'id': '2111211467',\n",
       "  'title': 'New Algorithms and Methods to Estimate Maximum-Likelihood Phylogenies: Assessing the Performance of PhyML 3.0',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '12,605',\n",
       "  'abstract': 'PhyML is a phylogeny software based on the maximum-likelihood principle. Early PhyML versions used a fast algorithm performing nearest neighbor interchanges to improve a reasonable starting tree topology. Since the original publication (Guindon S., Gascuel O. 2003. A simple, fast and accurate algorithm to estimate large phylogenies by maximum likelihood. Syst. Biol. 52:696-704), PhyML has been widely used (>2500 citations in ISI Web of Science) because of its simplicity and a fair compromise between accuracy and speed. In the meantime, research around PhyML has continued, and this article describes the new algorithms and methods implemented in the program. First, we introduce a new algorithm to search the tree space with user-defined intensity using subtree pruning and regrafting topological moves. The parsimony criterion is used here to filter out the least promising topology modifications with respect to the likelihood function. The analysis of a large collection of real nucleotide and amino acid data sets of various sizes demonstrates the good performance of this method. Second, we describe a new test to assess the support of the data for internal branches of a phylogeny. This approach extends the recently proposed approximate likelihood-ratio test and relies on a nonparametric, Shimodaira-Hasegawa-like procedure. A detailed analysis of real alignments sheds light on the links between this new approach and the more classical nonparametric bootstrap method. Overall, our tests show that the last version (3.0) of PhyML is fast, accurate, stable, and ready to use. A Web server and binary files are available from http://www.atgc-montpellier.fr/phyml/.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Stéphane Guindon 1, Jean-François Dufayard 2, Vincent Lefort 1, Maria Anisimova 2, Wim Hordijk 2, Olivier Gascuel 1'],\n",
       "  'related_topics': ['Likelihood function',\n",
       "   'Tree (data structure)',\n",
       "   'Pruning (decision trees)',\n",
       "   'Algorithm',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Nonparametric statistics',\n",
       "   'Network topology',\n",
       "   'Data mining',\n",
       "   'Filter (signal processing)',\n",
       "   'Web server',\n",
       "   'Biology'],\n",
       "  'references': ['2146058063',\n",
       "   '2168696662',\n",
       "   '2103546861',\n",
       "   '2031611770',\n",
       "   '2127847431',\n",
       "   '2030966943',\n",
       "   '2103088017',\n",
       "   '2098448352',\n",
       "   '2105926960',\n",
       "   '2163627198']},\n",
       " {'id': '2470646526',\n",
       "  'title': 'SARS and MERS: recent insights into emerging coronaviruses',\n",
       "  'reference_count': '156',\n",
       "  'citation_count': '2,421',\n",
       "  'abstract': 'The emergence of Middle East respiratory syndrome coronavirus (MERS-CoV) in 2012 marked the second introduction of a highly pathogenic coronavirus into the human population in the twenty-first century. The continuing introductions of MERS-CoV from dromedary camels, the subsequent travel-related viral spread, the unprecedented nosocomial outbreaks and the high case-fatality rates highlight the need for prophylactic and therapeutic measures. Scientific advancements since the 2002-2003 severe acute respiratory syndrome coronavirus (SARS-CoV) pandemic allowed for rapid progress in our understanding of the epidemiology and pathogenesis of MERS-CoV and the development of therapeutics. In this Review, we detail our present understanding of the transmission and pathogenesis of SARS-CoV and MERS-CoV, and discuss the current state of development of measures to combat emerging coronaviruses.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Emmie de Wit 1, Neeltje van Doremalen 1, Darryl Falzarano 2, Vincent J. Munster 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Population',\n",
       "   'Viral pathogenesis',\n",
       "   'Pandemic',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Highly pathogenic',\n",
       "   'Severe acute respiratory syndrome coronavirus'],\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '1909499787',\n",
       "   '2138324310']},\n",
       " {'id': '2306794997',\n",
       "  'title': 'Epidemiology, Genetic Recombination, and Pathogenesis of Coronaviruses',\n",
       "  'reference_count': '94',\n",
       "  'citation_count': '1,936',\n",
       "  'abstract': 'Human coronaviruses (HCoVs) were first described in the 1960s for patients with the common cold. Since then, more HCoVs have been discovered, including those that cause severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), two pathogens that, upon infection, can cause fatal respiratory disease in humans. It was recently discovered that dromedary camels in Saudi Arabia harbor three different HCoV species, including a dominant MERS HCoV lineage that was responsible for the outbreaks in the Middle East and South Korea during 2015. In this review we aim to compare and contrast the different HCoVs with regard to epidemiology and pathogenesis, in addition to the virus evolution and recombination events which have, on occasion, resulted in outbreaks amongst humans.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Shuo Su 1, Gary Wong 2, Weifeng Shi 3, Jun Liu 2, 4, Alexander C.K. Lai 5, Jiyong Zhou 1, Wenjun Liu 2, Yuhai Bi 2, George F. Gao 6'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Common cold',\n",
       "   'Viral evolution',\n",
       "   'Pathogenesis',\n",
       "   'Epidemiology',\n",
       "   'Respiratory disease',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Biology'],\n",
       "  'references': ['2166867592',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '2138324310',\n",
       "   '2125251240',\n",
       "   '2160011624',\n",
       "   '2115555188',\n",
       "   '2134061616']},\n",
       " {'id': '1993577573',\n",
       "  'title': 'Isolation and characterization of a bat SARS-like coronavirus that uses the ACE2 receptor',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '1,361',\n",
       "  'abstract': 'The 2002-3 pandemic caused by severe acute respiratory syndrome coronavirus (SARS-CoV) was one of the most significant public health events in recent history. An ongoing outbreak of Middle East respiratory syndrome coronavirus suggests that this group of viruses remains a key threat and that their distribution is wider than previously recognized. Although bats have been suggested to be the natural reservoirs of both viruses, attempts to isolate the progenitor virus of SARS-CoV from bats have been unsuccessful. Diverse SARS-like coronaviruses (SL-CoVs) have now been reported from bats in China, Europe and Africa, but none is considered a direct progenitor of SARS-CoV because of their phylogenetic disparity from this virus and the inability of their spike proteins to use the SARS-CoV cellular receptor molecule, the human angiotensin converting enzyme II (ACE2). Here we report whole-genome sequences of two novel bat coronaviruses from Chinese horseshoe bats (family: Rhinolophidae) in Yunnan, China: RsSHC014 and Rs3367. These viruses are far more closely related to SARS-CoV than any previously identified bat coronaviruses, particularly in the receptor binding domain of the spike protein. Most importantly, we report the first recorded isolation of a live SL-CoV (bat SL-CoV-WIV1) from bat faecal samples in Vero E6 cells, which has typical coronavirus morphology, 99.9% sequence identity to Rs3367 and uses ACE2 from humans, civets and Chinese horseshoe bats for cell entry. Preliminary in vitro testing indicates that WIV1 also has a broad species tropism. Our results provide the strongest evidence to date that Chinese horseshoe bats are natural reservoirs of SARS-CoV, and that intermediate hosts may not be necessary for direct human infection by some bat SL-CoVs. They also highlight the importance of pathogen-discovery programs targeting high-risk wildlife groups in emerging disease hotspots as a strategy for pandemic preparedness.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Xing Yi Ge 1, Jia Lu Li 1, Xing Lou Yang 1, Aleksei A. Chmura 2, Guangjian Zhu 2, Jonathan H. Epstein 2, Jonna A Mazet 3, Ben Hu 1, Wei Zhang 1, Cheng Peng 1, Yu Ji Zhang 1, Chu Ming Luo 1, Bing Tan 1, Ning Wang 1, Yan Zhu 1, Gary Crameri 4, Shu Yi Zhang 5, Lin Fa Wang 4, 6, Peter Daszak 2, Zheng Li Shi 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Virus genetics'],\n",
       "  'references': ['2166867592',\n",
       "   '2104548316',\n",
       "   '2119111857',\n",
       "   '1966238900',\n",
       "   '2103503670',\n",
       "   '2140338292',\n",
       "   '2141008678',\n",
       "   '2049975503',\n",
       "   '2101063972',\n",
       "   '1990059132']},\n",
       " {'id': '2775086803',\n",
       "  'title': 'Discovery of a rich gene pool of bat SARS-related coronaviruses provides new insights into the origin of SARS coronavirus.',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '602',\n",
       "  'abstract': 'A large number of SARS-related coronaviruses (SARSr-CoV) have been detected in horseshoe bats since 2005 in different areas of China. However, these bat SARSr-CoVs show sequence differences from SARS coronavirus (SARS-CoV) in different genes (S, ORF8, ORF3, etc) and are considered unlikely to represent the direct progenitor of SARS-CoV. Herein, we report the findings of our 5-year surveillance of SARSr-CoVs in a cave inhabited by multiple species of horseshoe bats in Yunnan Province, China. The full-length genomes of 11 newly discovered SARSr-CoV strains, together with our previous findings, reveals that the SARSr-CoVs circulating in this single location are highly diverse in the S gene, ORF3 and ORF8. Importantly, strains with high genetic similarity to SARS-CoV in the hypervariable N-terminal domain (NTD) and receptor-binding domain (RBD) of the S1 gene, the ORF3 and ORF8 region, respectively, were all discovered in this cave. In addition, we report the first discovery of bat SARSr-CoVs highly similar to human SARS-CoV in ORF3b and in the split ORF8a and 8b. Moreover, SARSr-CoV strains from this cave were more closely related to SARS-CoV in the non-structural protein genes ORF1a and 1b compared with those detected elsewhere. Recombination analysis shows evidence of frequent recombination events within the S gene and around the ORF8 between these SARSr-CoVs. We hypothesize that the direct progenitor of SARS-CoV may have originated after sequential recombination events between the precursors of these SARSr-CoVs. Cell entry studies demonstrated that three newly identified SARSr-CoVs with different S protein sequences are all able to use human ACE2 as the receptor, further exhibiting the close relationship between strains in this cave and SARS-CoV. This work provides new insights into the origin and evolution of SARS-CoV and highlights the necessity of preparedness for future emergence of SARS-like diseases.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Ben Hu 1, Lei Ping Zeng 1, Xing Lou Yang 1, Xing Yi Ge 1, Wei Zhang 1, Bei Li 1, Jia Zheng Xie 1, Xu Rui Shen 1, Yun Zhi Zhang 2, Ning Wang 1, Dong Sheng Luo 1, Xiao Shuang Zheng 1, Mei Niang Wang 1, Peter Daszak 3, Lin Fa Wang 4, Jie Cui 1, Zheng Li Shi 1'],\n",
       "  'related_topics': ['Gene pool',\n",
       "   'Sequence analysis',\n",
       "   'Genome',\n",
       "   'Cave',\n",
       "   'Gene',\n",
       "   'Genomics',\n",
       "   'Sequence alignment',\n",
       "   'Evolutionary biology',\n",
       "   'Polymerase chain reaction',\n",
       "   'Biology'],\n",
       "  'references': ['2111211467',\n",
       "   '1993577573',\n",
       "   '2169198329',\n",
       "   '2195009776',\n",
       "   '2103503670',\n",
       "   '2298153446',\n",
       "   '2134061616',\n",
       "   '2141877163',\n",
       "   '2140338292',\n",
       "   '2141008678']},\n",
       " {'id': '2119111857',\n",
       "  'title': 'Dipeptidyl peptidase 4 is a functional receptor for the emerging human coronavirus-EMC',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '1,602',\n",
       "  'abstract': 'Most human coronaviruses cause mild upper respiratory tract disease but may be associated with more severe pulmonary disease in immunocompromised individuals. However, SARS coronavirus caused severe lower respiratory disease with nearly 10% mortality and evidence of systemic spread. Recently, another coronavirus (human coronavirus-Erasmus Medical Center (hCoV-EMC)) was identified in patients with severe and sometimes lethal lower respiratory tract infection. Viral genome analysis revealed close relatedness to coronaviruses found in bats. Here we identify dipeptidyl peptidase 4 (DPP4; also known as CD26) as a functional receptor for hCoV-EMC. DPP4 specifically co-purified with the receptor-binding S1 domain of the hCoV-EMC spike protein from lysates of susceptible Huh-7 cells. Antibodies directed against DPP4 inhibited hCoV-EMC infection of primary human bronchial epithelial cells and Huh-7 cells. Expression of human and bat (Pipistrellus pipistrellus) DPP4 in non-susceptible COS-7 cells enabled infection by hCoV-EMC. The use of the evolutionarily conserved DPP4 protein from different species as a functional receptor provides clues about the host range potential of hCoV-EMC. In addition, it will contribute critically to our understanding of the pathogenesis and epidemiology of this emerging human coronavirus, and may facilitate the development of intervention strategies.',\n",
       "  'date': 2013,\n",
       "  'authors': ['V. Stalin Raj 1, Huihui Mou 2, Saskia L. Smits 1, Dick H. W. Dekkers 1, Marcel A. Müller 3, Ronald Dijkman 4, Doreen Muth 3, Jeroen A. A. Demmers 1, Ali Zaki 5, Ron A. M. Fouchier 1, Volker Thiel 4, 6, Christian Drosten 3, Peter J. M. Rottier 2, Albert D. M. E. Osterhaus 1, Berend Jan Bosch 2, Bart L. Haagmans 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus'],\n",
       "  'references': ['2166867592',\n",
       "   '1966238900',\n",
       "   '2103503670',\n",
       "   '2113457186',\n",
       "   '2091671824',\n",
       "   '1690366459',\n",
       "   '1982533785',\n",
       "   '2126707939',\n",
       "   '1964982019',\n",
       "   '2101063972']},\n",
       " {'id': '2025170735',\n",
       "  'title': 'Coronavirus as a possible cause of severe acute respiratory syndrome',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '3,689',\n",
       "  'abstract': \"Summary Background An outbreak of severe acute respiratory syndrome (SARS) has been reported in Hong Kong. We investigated the viral cause and clinical presentation among 50 patients. Methods We analysed case notes and microbiological findings for 50 patients with SARS, representing more than five separate epidemiologically linked transmission clusters. We defined the clinical presentation and risk factors associated with severe disease and investigated the causal agents by chest radiography and laboratory testing of nasopharyngeal aspirates and sera samples. We compared the laboratory findings with those submitted for microbiological investigation of other diseases from patients whose identity was masked. Findings Patients' age ranged from 23 to 74 years. Fever, chills, myalgia, and cough were the most frequent complaints. When compared with chest radiographic changes, respiratory symptoms and auscultatory findings were disproportionally mild. Patients who were household contacts of other infected people and had older age, lymphopenia, and liver dysfunction were associated with severe disease. A virus belonging to the family Coronaviridae was isolated from two patients. By use of serological and reverse-transcriptase PCR specific for this virus, 45 of 50 patients with SARS, but no controls, had evidence of infection with this virus. Interpretation A coronavirus was isolated from patients with SARS that might be the primary agent associated with this disease. Serological and molecular tests specific for the virus permitted a definitive laboratory diagnosis to be made and allowed further investigation to define whether other cofactors play a part in disease progression.\",\n",
       "  'date': 2003,\n",
       "  'authors': ['Jsm Peiris 1, ST Lai 2, Llm Poon 1, Y Guan 1, Lyc Yam 3, W Lim 4, J Nicholls 1, Wks Yee 5, WW Yan 2, MT Cheung 3, Vcc Cheng 1, KH Chan 1, Dnc Tsang 6, Rwh Yung 3, TK Ng 2, KY Yuen 1'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Human coronavirus NL63',\n",
       "   'Coronavirus'],\n",
       "  'references': ['2116682907',\n",
       "   '2122399224',\n",
       "   '2104730345',\n",
       "   '2141291230',\n",
       "   '2154664055',\n",
       "   '1970720481',\n",
       "   '576359727',\n",
       "   '2239493136',\n",
       "   '2081510963',\n",
       "   '2336133541']},\n",
       " {'id': '2129542667',\n",
       "  'title': 'Clinical progression and viral load in a community outbreak of coronavirus-associated SARS pneumonia: a prospective study.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '2,486',\n",
       "  'abstract': 'Summary Background We investigated the temporal progression of the clinical, radiological, and virological changes in a community outbreak of severe acute respiratory syndrome (SARS). Methods We followed up 75 patients for 3 weeks managed with a standard treatment protocol of ribavirin and corticosteroids, and assessed the pattern of clinical disease, viral load, risk factors for poor clinical outcome, and the usefulness of virological diagnostic methods. Findings Fever and pneumonia initially improved but 64 (85%) patients developed recurrent fever after a mean of 8·9 (SD 3·1) days, 55 (73%) had watery diarrhoea after 7·5 (2·3) days, 60 (80%) had radiological worsening after 7·4 (2·2) days, and respiratory symptoms worsened in 34 (45%) after 8·6 (3·0) days. In 34 (45%) patients, improvement of initial pulmonary lesions was associated with appearance of new radiological lesions at other sites. Nine (12%) patients developed spontaneous pneumomediastinum and 15 (20%) developed acute respiratory distress syndrome (ARDS) in week 3. Quantitative reverse-transcriptase (RT) PCR of nasopharyngeal aspirates in 14 patients (four with ARDS) showed peak viral load at day 10, and at day 15 a load lower than at admission. Age and chronic hepatitis B virus infection treated with lamivudine were independent significant risk factors for progression to ARDS (p=0·001). SARS-associated coronavirus in faeces was seen on RT-PCR in 65 (97%) of 67 patients at day 14. The mean time to seroconversion was 20 days. Interpretation The consistent clinical progression, shifting radiological infiltrates, and an inverted V viral-load profile suggest that worsening in week 2 is unrelated to uncontrolled viral replication but may be related to immunopathological damage.',\n",
       "  'date': 2003,\n",
       "  'authors': ['J S M Peiris 1, C M Chu 2, V C C Cheng 1, K S Chan 2, I F N Hung 1, L L M Poon 1, K I Law 2, B S F Tang 1, T Y W Hon 2, C S Chan 2, K H Chan 1, J S C Ng 2, B J Zheng 1, W L Ng 2, R W M Lai 2, Y Guan 1, Kwok-Yung Yuen 1'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Viral load',\n",
       "   'ARDS',\n",
       "   'Pneumonia',\n",
       "   'Respiratory disease',\n",
       "   'Ribavirin',\n",
       "   'Standard treatment',\n",
       "   'Coronavirus',\n",
       "   'Internal medicine',\n",
       "   'Immunology',\n",
       "   'Medicine'],\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2107978811',\n",
       "   '2161328469',\n",
       "   '2155583106',\n",
       "   '1675164605',\n",
       "   '2061759246']},\n",
       " {'id': '1703839189',\n",
       "  'title': 'Detection of a novel human coronavirus by real-time reverse-transcription polymerase chain reaction',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '636',\n",
       "  'abstract': 'We present two real-time reverse-transcription polymerase chain reaction assays for a novel human coronavirus (CoV), targeting regions upstream of the E gene (upE) or within open reading frame (ORF)1b, respectively. Sensitivity for upE is 3.4 copies per reaction (95% confidence interval (CI): 2.5-6.9 copies) or 291 copies/mL of sample. No cross-reactivity was observed with coronaviruses OC43, NL63, 229E, SARS-CoV, nor with 92 clinical specimens containing common human respiratory viruses. We recommend using upE for screening and ORF1b for confirmation.',\n",
       "  'date': 2012,\n",
       "  'authors': ['V M Corman 1, I Eckerle 1, T Bleicker 1, A Zaki 2, O Landt 3, M Eschbach-Bludau 1, S van Boheemen 4, R Gopal 5, M Ballhause 3, T M Bestebroer 4, D Muth 1, M A Müller 1, J F Drexler 1, M Zambon 5, A D Osterhaus 4, R M Fouchier 4, C Drosten 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Polymerase chain reaction',\n",
       "   'Respiratory virus',\n",
       "   'Open reading frame',\n",
       "   'Viral replication',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Virology',\n",
       "   'Molecular biology',\n",
       "   'Biology'],\n",
       "  'references': ['2132260239',\n",
       "   '2129542667',\n",
       "   '2167080692',\n",
       "   '1593955729',\n",
       "   '2145810580',\n",
       "   '1975169783',\n",
       "   '2069961370',\n",
       "   '2105870155',\n",
       "   '2100516702',\n",
       "   '2161315652']},\n",
       " {'id': '2116586125',\n",
       "  'title': 'Characterization of a novel coronavirus associated with severe acute respiratory syndrome',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '3,067',\n",
       "  'abstract': 'In March 2003, a novel coronavirus (SARS-CoV) was discovered in association with cases of severe acute respiratory syndrome (SARS). The sequence of the complete genome of SARS-CoV was determined, and the initial characterization of the viral genome is presented in this report. The genome of SARS-CoV is 29,727 nucleotides in length and has 11 open reading frames, and its genome organization is similar to that of other coronaviruses. Phylogenetic analyses and sequence comparisons showed that SARS-CoV is not closely related to any of the previously characterized coronaviruses.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Paul A. Rota 1, M. Steven Oberste 1, Stephan S. Monroe 1, W. Allan Nix 1, Ray Campagnoli 1, Joseph P. Icenogle 1, Silvia Peñaranda 1, Bettina Bankamp 1, Kaija Maher 1, Min hsin Chen 1, Suxiong Tong 1, Azaibi Tamin 1, Luis Lowe 1, Michael Frace 1, Joseph L. DeRisi 2, Qi Chen 1, David Wang 2, Dean D. Erdman 1, Teresa C.T. Peret 1, Cara Burns 1, Thomas G. Ksiazek 1, Pierre E. Rollin 1, Anthony Sanchez 1, Stephanie Liffick 1, Brian Holloway 1, Josef Limor 1, Karen McCaustland 1, Mellissa Olsen-Rasmussen 1, Ron Fouchier 3, Stephan Günther 4, Albert D.H.E. Osterhaus 3, Christian Drosten 4, Mark A. Pallansch 1, Larry J. Anderson 1, William J. Bellini 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Human coronavirus NL63'],\n",
       "  'references': ['2025170735',\n",
       "   '2169198329',\n",
       "   '2163400707',\n",
       "   '2103854602',\n",
       "   '2048093932',\n",
       "   '2152215476',\n",
       "   '72835126',\n",
       "   '2027659163',\n",
       "   '2126980050',\n",
       "   '2014171176']},\n",
       " {'id': '1987783718',\n",
       "  'title': 'Extracorporeal membrane oxygenation for 2009 Influenza A (H1N1) Acute Respiratory Distress Syndrome',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,842',\n",
       "  'abstract': 'CONTEXT The novel influenza A(H1N1) pandemic affected Australia and New Zealand during the 2009 southern hemisphere winter. It caused an epidemic of critical illness and some patients developed severe acute respiratory distress syndrome (ARDS) and were treated with extracorporeal membrane oxygenation (ECMO). OBJECTIVES To describe the characteristics of all patients with 2009 influenza A(H1N1)-associated ARDS treated with ECMO and to report incidence, resource utilization, and patient outcomes. DESIGN, SETTING, AND PATIENTS An observational study of all patients (n = 68) with 2009 influenza A(H1N1)-associated ARDS treated with ECMO in 15 intensive care units (ICUs) in Australia and New Zealand between June 1 and August 31, 2009. MAIN OUTCOME MEASURES Incidence, clinical features, degree of pulmonary dysfunction, technical characteristics, duration of ECMO, complications, and survival. RESULTS Sixty-eight patients with severe influenza-associated ARDS were treated with ECMO, of whom 61 had either confirmed 2009 influenza A(H1N1) (n = 53) or influenza A not subtyped (n = 8), representing an incidence rate of 2.6 ECMO cases per million population. An additional 133 patients with influenza A received mechanical ventilation but no ECMO in the same ICUs. The 68 patients who received ECMO had a median (interquartile range [IQR]) age of 34.4 (26.6-43.1) years and 34 patients (50%) were men. Before ECMO, patients had severe respiratory failure despite advanced mechanical ventilatory support with a median (IQR) Pao(2)/fraction of inspired oxygen (Fio(2)) ratio of 56 (48-63), positive end-expiratory pressure of 18 (15-20) cm H(2)O, and an acute lung injury score of 3.8 (3.5-4.0). The median (IQR) duration of ECMO support was 10 (7-15) days. At the time of reporting, 48 of the 68 patients (71%; 95% confidence interval [CI], 60%-82%) had survived to ICU discharge, of whom 32 had survived to hospital discharge and 16 remained as hospital inpatients. Fourteen patients (21%; 95% CI, 11%-30%) had died and 6 remained in the ICU, 2 of whom were still receiving ECMO. CONCLUSIONS During June to August 2009 in Australia and New Zealand, the ICUs at regional referral centers provided mechanical ventilation for many patients with 2009 influenza A(H1N1)-associated respiratory failure, one-third of whom received ECMO. These ECMO-treated patients were often young adults with severe hypoxemia and had a 21% mortality rate at the end of the study period.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Andrew Davies 1, Daryl Jones 1, Michael Bailey 1, John Beca 2, Rinaldo Bellomo 1, Nikki Blackwell 3, Paul Forrest 4, David Gattas 4, Emily Granger 5, Robert Herkes 4, Andrew Jackson 5, Shay McGuinness 2, Priya Nair 5, Vincent Pellegrino 1, Ville Yrjo Olavi Pettila 1, Brian Plunkett 4, Roger Pye 5, Paul Torzillo 4, Steven Webb 6, Michael Wilson 4, Marc Ziegenfuss 3'],\n",
       "  'related_topics': ['Extracorporeal membrane oxygenation',\n",
       "   'Intensive care',\n",
       "   'Fraction of inspired oxygen'],\n",
       "  'references': ['2166867592',\n",
       "   '3011611380',\n",
       "   '2610582798',\n",
       "   '3012186724',\n",
       "   '2037145800',\n",
       "   '2105123265',\n",
       "   '2094826575',\n",
       "   '2563391002',\n",
       "   '2048608755',\n",
       "   '2096238447']},\n",
       " {'id': '2111412754',\n",
       "  'title': 'Identification of a new human coronavirus',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '1,831',\n",
       "  'abstract': 'Three human coronaviruses are known to exist: human coronavirus 229E (HCoV-229E), HCoV-OC43 and severe acute respiratory syndrome (SARS)-associated coronavirus (SARS-CoV). Here we report the identification of a fourth human coronavirus, HCoV-NL63, using a new method of virus discovery. The virus was isolated from a 7-month-old child suffering from bronchiolitis and conjunctivitis. The complete genome sequence indicates that this virus is not a recombinant, but rather a new group 1 coronavirus. The in vitro host cell range of HCoV-NL63 is notable because it replicates on tertiary monkey kidney cells and the monkey kidney LLC-MK2 cell line. The viral genome contains distinctive features, including a unique N-terminal fragment within the spike protein. Screening of clinical specimens from individuals suffering from respiratory illness identified seven additional HCoV-NL63-infected individuals, indicating that the virus was widely spread within the human population.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Lia van der Hoek 1, Krzysztof Pyrc 1, Maarten F Jebbink 1, Wilma Vermeulen-Oost 2, Ron J M Berkhout 2, Katja C Wolthers 1, Pauline M E Wertheim-van Dillen 1, Jos Kaandorp 3, Joke Spaargaren 2, Ben Berkhout 1'],\n",
       "  'related_topics': ['Human coronavirus OC43',\n",
       "   'Human coronavirus 229E',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus NL63',\n",
       "   'Human coronavirus HKU1',\n",
       "   'Virus',\n",
       "   'Population',\n",
       "   'Alphacoronavirus',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '2129542667',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2134061616',\n",
       "   '2166229810',\n",
       "   '2163400707',\n",
       "   '2159857626',\n",
       "   '2029293367']},\n",
       " {'id': '1963953102',\n",
       "  'title': 'Fingerprinting genomes using PCR with arbitrary primers',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '8,329',\n",
       "  'abstract': 'Simple and reproducible fingerprints of complex genomes can be generated using single arbitrarily chosen primers and the polymerase chain reaction (PCR). No prior sequence information is required. The method, arbitrarily primed PCR (AP-PCR), involves two cycles of low stringency amplification followed by PCR at higher stringency. We show that strains can be distinguished by comparing polymorphisms in genomic fingerprints. The generality of the method is demonstrated by application to twenty four strains from five species of Staphylococcus, eleven strains of Streptococcus pyogenes and three varieties of Oryza sativa (rice).',\n",
       "  'date': 1989,\n",
       "  'authors': ['John Welsh', 'Michael McClelland'],\n",
       "  'related_topics': ['RAPD',\n",
       "   'DNA profiling',\n",
       "   'Polymerase chain reaction',\n",
       "   'Genomic organization',\n",
       "   'Primer (molecular biology)',\n",
       "   'Genome',\n",
       "   'Molecular probe',\n",
       "   'Oryza sativa',\n",
       "   'Genetics',\n",
       "   'Biology'],\n",
       "  'references': ['2166867592',\n",
       "   '2113457186',\n",
       "   '2170881661',\n",
       "   '589702940',\n",
       "   '2029197798',\n",
       "   '1986937402',\n",
       "   '2078917493',\n",
       "   '2141047744',\n",
       "   '2037210253',\n",
       "   '2145525392']},\n",
       " {'id': '2170933940',\n",
       "  'title': 'Characterization and Complete Genome Sequence of a Novel Coronavirus, Coronavirus HKU1, from Patients with Pneumonia',\n",
       "  'reference_count': '52',\n",
       "  'citation_count': '1,484',\n",
       "  'abstract': 'Despite extensive laboratory investigations in patients with respiratory tract infections, no microbiological cause can be identified in a significant proportion of patients. In the past 3 years, several novel respiratory viruses, including human metapneumovirus, severe acute respiratory syndrome (SARS) coronavirus (SARSCoV), and human coronavirus NL63, were discovered. Here we report the discovery of another novel coronavirus, coronavirus HKU1 (CoV-HKU1), from a 71-year-old man with pneumonia who had just returned from Shenzhen, China. Quantitative reverse transcription-PCR showed that the amount of CoV-HKU1 RNA was 8.5 to 9.6 10 6 copies per ml in his nasopharyngeal aspirates (NPAs) during the first week of the illness and dropped progressively to undetectable levels in subsequent weeks. He developed increasing serum levels of specific antibodies against the recombinant nucleocapsid protein of CoV-HKU1, with immunoglobulin M (IgM) titers of 1:20, 1:40, and 1:80 and IgG titers of <1:1,000, 1:2,000, and 1:8,000 in the first, second and fourth weeks of the illness, respectively. Isolation of the virus by using various cell lines, mixed neuron-glia culture, and intracerebral inoculation of suckling mice was unsuccessful. The complete genome sequence of CoV-HKU1 is a 29,926-nucleotide, polyadenylated RNA, with GC content of 32%, the lowest among all known coronaviruses with available genome sequence. Phylogenetic analysis reveals that CoV-HKU1 is a new group 2 coronavirus. Screening of 400 NPAs, negative for SARS-CoV, from patients with respiratory illness during the SARS period identified the presence of CoV-HKU1 RNA in an additional specimen, with a viral load of 1.13 10 6 copies per ml, from a 35-year-old woman with pneumonia. Our data support the existence of a novel group 2 coronavirus associated with pneumonia in humans.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Patrick C. Y. Woo 1, Susanna K. P. Lau 1, Chung-ming Chu 2, Kwok-hung Chan 3, Hoi-wah Tsoi 3, Yi Huang 3, Beatrice H. L. Wong 3, Rosana W. S. Poon 3, James J. Cai 3, Wei-kwang Luk 4, Leo L. M. Poon 1, Samson S. Y. Wong 1, Yi Guan 1, J. S. Malik Peiris 1, Kwok-yung Yuen 1'],\n",
       "  'related_topics': ['Human coronavirus OC43',\n",
       "   'Coronavirus',\n",
       "   'Human coronavirus HKU1',\n",
       "   'Human coronavirus NL63',\n",
       "   'Human metapneumovirus',\n",
       "   'Respiratory tract infections',\n",
       "   'Pneumonia',\n",
       "   'Coronaviridae',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2141885858',\n",
       "   '2025170735',\n",
       "   '2129542667',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2171091522',\n",
       "   '2134061616',\n",
       "   '2170881661',\n",
       "   '2141877163',\n",
       "   '2111412754']},\n",
       " {'id': '2126707939',\n",
       "  'title': 'Severe acute respiratory syndrome',\n",
       "  'reference_count': '146',\n",
       "  'citation_count': '1,006',\n",
       "  'abstract': \"Severe acute respiratory syndrome (SARS) was caused by a previously unrecognized animal coronavirus that exploited opportunities provided by 'wet markets' in southern China to adapt to become a virus readily transmissible between humans. Hospitals and international travel proved to be 'amplifiers' that permitted a local outbreak to achieve global dimensions. In this review we will discuss the substantial scientific progress that has been made towards understanding the virus-SARS coronavirus (SARS-CoV)-and the disease. We will also highlight the progress that has been made towards developing vaccines and therapies The concerted and coordinated response that contained SARS is a triumph for global public health and provides a new paradigm for the detection and control of future emerging infectious disease threats.\",\n",
       "  'date': 2004,\n",
       "  'authors': ['J S M Peiris', 'Y Guan', 'K Y Yuen'],\n",
       "  'related_topics': ['Emerging infectious disease',\n",
       "   'Coronavirus',\n",
       "   'Global health',\n",
       "   'Disease',\n",
       "   'Outbreak',\n",
       "   'Public health',\n",
       "   'Vaccination',\n",
       "   'Scientific progress',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Immunology'],\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2129542667',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2147166346',\n",
       "   '2116586125',\n",
       "   '1966238900']},\n",
       " {'id': '2107277218',\n",
       "  'title': 'ANALYSIS OF RELATIVE GENE EXPRESSION DATA USING REAL-TIME QUANTITATIVE PCR AND THE 2(-DELTA DELTA C(T)) METHOD',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '135,572',\n",
       "  'abstract': 'The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2(-Delta Delta C(T)) method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2(-Delta Delta C(T)) method. In addition, we present the derivation and applications of two variations of the 2(-Delta Delta C(T)) method that may be useful in the analysis of real-time, quantitative PCR data.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Kenneth J. Livak 1, Thomas D. Schmittgen 2'],\n",
       "  'related_topics': ['Cell wall organization',\n",
       "   'Protein kinase B signaling',\n",
       "   'Lupeol synthase',\n",
       "   'Endosperm cellularization',\n",
       "   'Cell wall modification',\n",
       "   'Female sex determination',\n",
       "   'Floral organ abscission',\n",
       "   'Isoflavonoid biosynthesis',\n",
       "   'Molecular biology',\n",
       "   'Biology',\n",
       "   'Bioinformatics'],\n",
       "  'references': ['2164578725',\n",
       "   '2109970232',\n",
       "   '2128088040',\n",
       "   '2145879504',\n",
       "   '1983241347',\n",
       "   '2123325948',\n",
       "   '2134343377',\n",
       "   '1756433044',\n",
       "   '1566892773',\n",
       "   '2069943574']},\n",
       " {'id': '2791599184',\n",
       "  'title': 'Treatment of Middle East Respiratory Syndrome with a combination of lopinavir-ritonavir and interferon-β1b (MIRACLE trial): study protocol for a randomized controlled trial.',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '275',\n",
       "  'abstract': 'It had been more than 5 years since the first case of Middle East Respiratory Syndrome coronavirus infection (MERS-CoV) was recorded, but no specific treatment has been investigated in randomized clinical trials. Results from in vitro and animal studies suggest that a combination of lopinavir/ritonavir and interferon-β1b (IFN-β1b) may be effective against MERS-CoV. The aim of this study is to investigate the efficacy of treatment with a combination of lopinavir/ritonavir and recombinant IFN-β1b provided with standard supportive care, compared to treatment with placebo provided with standard supportive care in patients with laboratory-confirmed MERS requiring hospital admission. The protocol is prepared in accordance with the SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) guidelines. Hospitalized adult patients with laboratory-confirmed MERS will be enrolled in this recursive, two-stage, group sequential, multicenter, placebo-controlled, double-blind randomized controlled trial. The trial is initially designed to include 2 two-stage components. The first two-stage component is designed to adjust sample size and determine futility stopping, but not efficacy stopping. The second two-stage component is designed to determine efficacy stopping and possibly readjustment of sample size. The primary outcome is 90-day mortality. This will be the first randomized controlled trial of a potential treatment for MERS. The study is sponsored by King Abdullah International Medical Research Center, Riyadh, Saudi Arabia. Enrollment for this study began in November 2016, and has enrolled thirteen patients as of Jan 24-2018. ClinicalTrials.gov, ID: NCT02845843. Registered on 27 July 2016.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Yaseen M. Arabi 1, 2, 3, Adel Alothman 2, 3, Hanan H. Balkhy 2, 3, Abdulaziz Al-Dawood 2, 3, Sameera AlJohani 2, 3, Shmeylan Al Harbi 2, 3, Suleiman Kojan 2, 3, Majed Al Jeraisy 2, 3, Ahmad M. Deeb 2, 3, Abdullah M. Assiri 4, Fahad Al-Hameed 2, 3, Asim AlSaedi 2, 3, Yasser Mandourah 5, Ghaleb A. Almekhlafi 5, Nisreen Murad Sherbeeni 6, Fatehi Elnour Elzein 6, Javed Memon 7, Yusri Taha 8, Abdullah Almotairi 9, Khalid A. Maghrabi 10, Ismael Qushmaq 11, Ali Al Bshabshe 12, Ayman Kharaba 13, Sarah Shalhoub 14, Jesna Jose 2, Robert A. Fowler 15, 16, Frederick G. Hayden 17, Mohamed A. Hussein 2'],\n",
       "  'related_topics': ['Lopinavir/ritonavir',\n",
       "   'Randomized controlled trial',\n",
       "   'Lopinavir'],\n",
       "  'references': ['2166867592',\n",
       "   '2145758369',\n",
       "   '2139937737',\n",
       "   '2591646177',\n",
       "   '1703839189',\n",
       "   '2034462612',\n",
       "   '1977050884',\n",
       "   '2586093485',\n",
       "   '2150120685',\n",
       "   '2345375456']},\n",
       " {'id': '2255243349',\n",
       "  'title': 'Coronaviruses - drug discovery and therapeutic options.',\n",
       "  'reference_count': '299',\n",
       "  'citation_count': '1,223',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which are caused by coronaviruses, have attracted substantial attention owing to their high mortality rates and potential to cause epidemics. Yuen and colleagues discuss progress with treatment options for these syndromes, including virus- and host-targeted drugs, and the challenges that need to be overcome in their further development. In humans, infections with the human coronavirus (HCoV) strains HCoV-229E, HCoV-OC43, HCoV-NL63 and HCoV-HKU1 usually result in mild, self-limiting upper respiratory tract infections, such as the common cold. By contrast, the CoVs responsible for severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS), which were discovered in Hong Kong, China, in 2003, and in Saudi Arabia in 2012, respectively, have received global attention over the past 12 years owing to their ability to cause community and health-care-associated outbreaks of severe infections in human populations. These two viruses pose major challenges to clinical management because there are no specific antiviral drugs available. In this Review, we summarize the epidemiology, virology, clinical features and current treatment strategies of SARS and MERS, and discuss the discovery and development of new virus-based and host-based therapeutic options for CoV infections.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Alimuddin Zumla 1, Jasper F. W. Chan 2, Esam I. Azhar 3, David S. C. Hui 4, Kwok-Yung Yuen 2'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Respiratory tract infections',\n",
       "   'Common cold',\n",
       "   'Outbreak',\n",
       "   'Virus',\n",
       "   'Epidemiology',\n",
       "   'Virology',\n",
       "   'Drug discovery',\n",
       "   'Biology',\n",
       "   'High mortality'],\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '2138324310']},\n",
       " {'id': '2292021561',\n",
       "  'title': 'Therapeutic efficacy of the small molecule GS-5734 against Ebola virus in rhesus monkeys',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '940',\n",
       "  'abstract': 'The most recent Ebola virus outbreak in West Africa, which was unprecedented in the number of cases and fatalities, geographic distribution, and number of nations affected, highlights the need for safe, effective, and readily available antiviral agents for treatment and prevention of acute Ebola virus (EBOV) disease (EVD) or sequelae. No antiviral therapeutics have yet received regulatory approval or demonstrated clinical efficacy. Here we report the discovery of a novel small molecule GS-5734, a monophosphoramidate prodrug of an adenosine analogue, with antiviral activity against EBOV. GS-5734 exhibits antiviral activity against multiple variants of EBOV and other filoviruses in cell-based assays. The pharmacologically active nucleoside triphosphate (NTP) is efficiently formed in multiple human cell types incubated with GS-5734 in vitro, and the NTP acts as an alternative substrate and RNA-chain terminator in primer-extension assays using a surrogate respiratory syncytial virus RNA polymerase. Intravenous administration of GS-5734 to nonhuman primates resulted in persistent NTP levels in peripheral blood mononuclear cells (half-life, 14\\u2009h) and distribution to sanctuary sites for viral replication including testes, eyes, and brain. In a rhesus monkey model of EVD, once-daily intravenous administration of 10\\u2009mg\\u2009kg(-1) GS-5734 for 12 days resulted in profound suppression of EBOV replication and protected 100% of EBOV-infected animals against lethal disease, ameliorating clinical disease signs and pathophysiological markers, even when treatments were initiated three days after virus exposure when systemic viral RNA was detected in two out of six treated animals. These results show the first substantive post-exposure protection by a small-molecule antiviral compound against EBOV in nonhuman primates. The broad-spectrum antiviral activity of GS-5734 in vitro against other pathogenic RNA viruses, including filoviruses, arenaviruses, and coronaviruses, suggests the potential for wider medical use. GS-5734 is amenable to large-scale manufacturing, and clinical studies investigating the drug safety and pharmacokinetics are ongoing.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Travis K. Warren 1, Robert Jordan',\n",
       "   'Michael K. Lo 2, Adrian S. Ray',\n",
       "   'Richard L. Mackman',\n",
       "   'Veronica Soloveva 1, Dustin Siegel',\n",
       "   'Michel Perron',\n",
       "   'Roy Bannister',\n",
       "   'Hon C. Hui',\n",
       "   'Nate Larson',\n",
       "   'Robert Strickley',\n",
       "   'Jay Wells 1, Kelly S. Stuthman 1, Sean A. Van Tongeren 1, Nicole L. Garza 1, Ginger Donnelly 1, Amy C. Shurtleff 1, Cary J. Retterer 1, Dima Gharaibeh 1, Rouzbeh Zamani 1, Tara Kenny 1, Brett P. Eaton 1, Elizabeth Grimes 1, Lisa S. Welch 1, Laura Gomba 1, Catherine L. Wilhelmsen 1, Donald K. Nichols 1, Jonathan E. Nuss 1, Elyse R. Nagle 1, Jeffrey R. Kugelman 1, Gustavo Palacios 1, Edward Doerffler',\n",
       "   'Sean Neville',\n",
       "   'Ernest Carra',\n",
       "   'Michael O. Clarke',\n",
       "   'Lijun Zhang',\n",
       "   'Willard Lew',\n",
       "   'Bruce Ross',\n",
       "   'Queenie Wang',\n",
       "   'Kwon Chun',\n",
       "   'Lydia Wolfe',\n",
       "   'Darius Babusis',\n",
       "   'Yeojin Park',\n",
       "   'Kirsten M. Stray',\n",
       "   'Iva Trancheva',\n",
       "   'Joy Y. Feng',\n",
       "   'Ona Barauskas',\n",
       "   'Yili Xu',\n",
       "   'Pamela Wong +12'],\n",
       "  'related_topics': ['Ebola virus', 'Virus', 'Viral pathogenesis'],\n",
       "  'references': ['2094993149',\n",
       "   '1971292277',\n",
       "   '2314681741',\n",
       "   '2326558924',\n",
       "   '277041599',\n",
       "   '2072202424',\n",
       "   '2117671399',\n",
       "   '2030160453',\n",
       "   '2138192885',\n",
       "   '1975876487']},\n",
       " {'id': '2290466312',\n",
       "  'title': 'A simple practice guide for dose conversion between animals and human.',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '1,887',\n",
       "  'abstract': 'Understanding the concept of extrapolation of dose between species is important for pharmaceutical researchers when initiating new animal or human experiments. Interspecies allometric scaling for dose conversion from animal to human studies is one of the most controversial areas in clinical pharmacology. Allometric approach considers the differences in body surface area, which is associated with animal weight while extrapolating the doses of therapeutic agents among the species. This review provides basic information about translation of doses between species and estimation of starting dose for clinical trials using allometric scaling. The method of calculation of injection volume for parenteral formulation based on human equivalent dose is also briefed.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Anroop B Nair 1, Shery Jacob 2'],\n",
       "  'related_topics': ['Clinical pharmacology',\n",
       "   'Body surface area',\n",
       "   'Clinical trial',\n",
       "   'Pharmacology',\n",
       "   'Biology',\n",
       "   'Dose conversion',\n",
       "   'Human studies',\n",
       "   'Injection volume'],\n",
       "  'references': ['2024938615',\n",
       "   '2100521244',\n",
       "   '2142366069',\n",
       "   '2065093669',\n",
       "   '2080335269',\n",
       "   '2034194552',\n",
       "   '2160483062',\n",
       "   '2057666951']},\n",
       " {'id': '2793022939',\n",
       "  'title': 'Coronavirus Susceptibility to the Antiviral Remdesivir (GS-5734) Is Mediated by the Viral Polymerase and the Proofreading Exoribonuclease',\n",
       "  'reference_count': '58',\n",
       "  'citation_count': '781',\n",
       "  'abstract': 'Emerging coronaviruses (CoVs) cause severe disease in humans, but no approved therapeutics are available. The CoV nsp14 exoribonuclease (ExoN) has complicated development of antiviral nucleosides due to its proofreading activity. We recently reported that the nucleoside analogue GS-5734 (remdesivir) potently inhibits human and zoonotic CoVs in vitro and in a severe acute respiratory syndrome coronavirus (SARS-CoV) mouse model. However, studies with GS-5734 have not reported resistance associated with GS-5734, nor do we understand the action of GS-5734 in wild-type (WT) proofreading CoVs. Here, we show that GS-5734 inhibits murine hepatitis virus (MHV) with similar 50% effective concentration values (EC50) as SARS-CoV and Middle East respiratory syndrome coronavirus (MERS-CoV). Passage of WT MHV in the presence of the GS-5734 parent nucleoside selected two mutations in the nsp12 polymerase at residues conserved across all CoVs that conferred up to 5.6-fold resistance to GS-5734, as determined by EC50 The resistant viruses were unable to compete with WT in direct coinfection passage in the absence of GS-5734. Introduction of the MHV resistance mutations into SARS-CoV resulted in the same in vitro resistance phenotype and attenuated SARS-CoV pathogenesis in a mouse model. Finally, we demonstrate that an MHV mutant lacking ExoN proofreading was significantly more sensitive to GS-5734. Combined, the results indicate that GS-5734 interferes with the nsp12 polymerase even in the setting of intact ExoN proofreading activity and that resistance can be overcome with increased, nontoxic concentrations of GS-5734, further supporting the development of GS-5734 as a broad-spectrum therapeutic to protect against contemporary and emerging CoVs.IMPORTANCE Coronaviruses (CoVs) cause severe human infections, but there are no approved antivirals to treat these infections. Development of nucleoside-based therapeutics for CoV infections has been hampered by the presence of a proofreading exoribonuclease. Here, we expand the known efficacy of the nucleotide prodrug remdesivir (GS-5734) to include a group β-2a CoV. Further, GS-5734 potently inhibits CoVs with intact proofreading. Following selection with the GS-5734 parent nucleoside, 2 amino acid substitutions in the nsp12 polymerase at residues that are identical across CoVs provide low-level resistance to GS-5734. The resistance mutations decrease viral fitness of MHV in vitro and attenuate pathogenesis in a SARS-CoV animal model of infection. Together, these studies define the target of GS-5734 activity and demonstrate that resistance is difficult to select, only partial, and impairs fitness and virulence of MHV and SARS-CoV, supporting further development of GS-5734 as a potential effective pan-CoV antiviral.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Maria L. Agostini 1, Erica L. Andres 1, Amy C Sims 2, Rachel Lauren Graham 2, Timothy Patrick Sheahan 2, Xiaotao Lu 1, Everett Clinton Smith 1, 3, James Brett Case 1, Joy Y. Feng 4, Robert Jordan 5, Adrian S. Ray 5, Tomas Cihlar 5, Dustin Siegel 5, Richard L. Mackman 5, Michael O. Clarke 5, Ralph S Baric 2, Mark R. Denison 1'],\n",
       "  'related_topics': ['Coronavirus', 'Proofreading', 'Nucleoside analogue'],\n",
       "  'references': ['2166867592',\n",
       "   '2104548316',\n",
       "   '2725497285',\n",
       "   '2195009776',\n",
       "   '2255243349',\n",
       "   '311927316',\n",
       "   '2292021561',\n",
       "   '2115555188',\n",
       "   '2298153446',\n",
       "   '2586093485']},\n",
       " {'id': '2100820722',\n",
       "  'title': 'Identification of Severe Acute Respiratory Syndrome in Canada',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '1,775',\n",
       "  'abstract': 'background Severe acute respiratory syndrome (SARS) is a condition of unknown cause that has recently been recognized in patients in Asia, North America, and Europe. This report summarizes the initial epidemiologic findings, clinical description, and diagnostic findings that followed the identification of SARS in Canada. methods SARS was first identified in Canada in early March 2003. We collected epidemiologic, clinical, and diagnostic data from each of the first 10 cases prospectively as they were identified. Specimens from all cases were sent to local, provincial, national, and international laboratories for studies to identify an etiologic agent. results The patients ranged from 24 to 78 years old; 60 percent were men. Transmission occurred only after close contact. The most common presenting symptoms were fever (in 100 percent of cases) and malaise (in 70 percent), followed by nonproductive cough (in 100 percent) and dyspnea (in 80 percent) associated with infiltrates on chest radiography (in 100 percent). Lymphopenia (in 89 percent of those for whom data were available), elevated lactate dehydrogenase levels (in 80 percent), elevated aspartate aminotransferase levels (in 78 percent), and elevated creatinine kinase levels (in 56 percent) were common. Empirical therapy most commonly included antibiotics, oseltamivir, and intravenous ribavirin. Mechanical ventilation was required in five patients. Three patients died, and five have had clinical improvement. The results of laboratory investigations were negative or not clinically significant except for the amplification of human metapneumovirus from respiratory specimens from five of nine patients and the isolation and amplification of a novel coronavirus from five of nine patients. In four cases both pathogens were isolated. conclusions SARS is a condition associated with substantial morbidity and mortality. It appears to be of viral origin, with patterns suggesting droplet or contact transmission. The role of human metapneumovirus, a novel coronavirus, or both requires further investigation.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Susan M. Poutanen 1, 2, Donald E. Low 1, Bonnie Henry 3, Sandy Finkelstein 4, David Rose 4, Karen Green 1, Raymond Tellier 5, 6, Ryan Draker 1, Dena Adachi 1, Melissa Ayers 1, Adrienne K. Chan 1, Danuta M. Skowronski 7, Irving Salit 1, Andrew E. Simor 1, Arthur S. Slutsky 1, Patrick W. Doyle 8, Mel Krajden 7, Martin Petric 7, Robert C. Brunham 8, Allison J. McGeer 1'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Human metapneumovirus',\n",
       "   'Epidemiology'],\n",
       "  'references': ['2597070792',\n",
       "   '2161328469',\n",
       "   '2170881661',\n",
       "   '2093852073',\n",
       "   '2463755683',\n",
       "   '2152552492',\n",
       "   '2135259291',\n",
       "   '2136166622',\n",
       "   '2097665403',\n",
       "   '1898899939']},\n",
       " {'id': '2125251240',\n",
       "  'title': 'A cluster of cases of severe acute respiratory syndrome in Hong Kong.',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '1,600',\n",
       "  'abstract': 'Background Information on the clinical features of the severe acute respiratory syndrome (SARS) will be of value to physicians caring for patients suspected of having this disorder. Methods We abstracted data on the clinical presentation and course of disease in 10 epidemiologically linked Chinese patients (5 men and 5 women 38 to 72 years old) in whom SARS was diagnosed between February 22, 2003, and March 22, 2003, at our hospitals in Hong Kong, China. Results Exposure between the source patient and subsequent patients ranged from minimal to that between patient and health care provider. The incubation period ranged from 2 to 11 days. All patients presented with fever (temperature, >38°C for over 24 hours), and most presented with rigor, dry cough, dyspnea, malaise, headache, and hypoxemia. Physical examination of the chest revealed crackles and percussion dullness. Lymphopenia was observed in nine patients, and most patients had mildly elevated aminotransferase levels but normal serum creatinine levels...',\n",
       "  'date': 2003,\n",
       "  'authors': ['Kenneth W. Tsang',\n",
       "   'Pak L. Ho',\n",
       "   'Gaik C. Ooi',\n",
       "   'Wilson K. Yee',\n",
       "   'Teresa Wang',\n",
       "   'Moira Chan-Yeung',\n",
       "   'Wah K. Lam',\n",
       "   'Wing H. Seto',\n",
       "   'Loretta Y. Yam',\n",
       "   'Thomas M. Cheung',\n",
       "   'Poon C. Wong',\n",
       "   'Bing Lam',\n",
       "   'Mary S. Ip',\n",
       "   'Jane Chan',\n",
       "   'Kwok Y. Yuen',\n",
       "   'Kar N. Lai'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Crackles',\n",
       "   'Physical examination',\n",
       "   'Hypoxemia',\n",
       "   'Respiratory disease',\n",
       "   'Malaise',\n",
       "   'Pediatrics',\n",
       "   'Pharmacotherapy',\n",
       "   'Medicine',\n",
       "   'Creatinine',\n",
       "   'Surgery'],\n",
       "  'references': ['1607298558', '1982444609', '2041775285', '2132293969']},\n",
       " {'id': '2107922358',\n",
       "  'title': 'Rapid detection and quantification of RNA of Ebola and Marburg viruses, Lassa virus, Crimean-Congo hemorrhagic fever virus, Rift Valley fever virus, dengue virus, and yellow fever virus by real-time reverse transcription-PCR.',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '784',\n",
       "  'abstract': 'Viral hemorrhagic fevers (VHFs) are acute infections with high case fatality rates. Important VHF agents are Ebola and Marburg viruses (MBGV/EBOV), Lassa virus (LASV), Crimean-Congo hemorrhagic fever virus (CCHFV), Rift Valley fever virus (RVFV), dengue virus (DENV), and yellow fever virus (YFV). VHFs are clinically difficult to diagnose and to distinguish; a rapid and reliable laboratory diagnosis is required in suspected cases. We have established six one-step, real-time reverse transcription-PCR assays for these pathogens based on the Superscript reverse transcriptase-Platinum Taq polymerase enzyme mixture. Novel primers and/or 5′-nuclease detection probes were designed for RVFV, DENV, YFV, and CCHFV by using the latest DNA database entries. PCR products were detected in real time on a LightCycler instrument by using 5′-nuclease technology (RVFV, DENV, and YFV) or SybrGreen dye intercalation (MBGV/EBOV, LASV, and CCHFV). The inhibitory effect of SybrGreen on reverse transcription was overcome by initial immobilization of the dye in the reaction capillaries. Universal cycling conditions for SybrGreen and 5′-nuclease probe detection were established. Thus, up to three assays could be performed in parallel, facilitating rapid testing for several pathogens. All assays were thoroughly optimized and validated in terms of analytical sensitivity by using in vitro-transcribed RNA. The ≥95% detection limits as determined by probit regression analysis ranged from 1,545 to 2,835 viral genome equivalents/ml of serum (8.6 to 16 RNA copies per assay). The suitability of the assays was exemplified by detection and quantification of viral RNA in serum samples of VHF patients.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Christian Drosten',\n",
       "   'Stephan Göttig',\n",
       "   'Stefan Schilling',\n",
       "   'Marcel Asper',\n",
       "   'Marcus Panning',\n",
       "   'Herbert Schmitz',\n",
       "   'Stephan Günther'],\n",
       "  'related_topics': ['Lassa virus',\n",
       "   'Phlebovirus',\n",
       "   'Dengue virus',\n",
       "   'Ebola virus',\n",
       "   'Flavivirus',\n",
       "   'Arenavirus',\n",
       "   'Marburgvirus',\n",
       "   'Crimean Congo hemorrhagic fever virus',\n",
       "   'Virology',\n",
       "   'Microbiology',\n",
       "   'Biology'],\n",
       "  'references': ['2047480444',\n",
       "   '1994091239',\n",
       "   '2070721758',\n",
       "   '2131589770',\n",
       "   '2149579937',\n",
       "   '2137089963',\n",
       "   '2163760194',\n",
       "   '2134971582',\n",
       "   '2171308211',\n",
       "   '2798078005']},\n",
       " {'id': '2127062009',\n",
       "  'title': 'Viruses and Bacteria in the Etiology of the Common Cold',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '890',\n",
       "  'abstract': 'Two hundred young adults with common colds were studied during a 10-month period. Virus culture, antigen detection, PCR, and serology with paired samples were used to identify the infection. Viral etiology was established for 138 of the 200 patients (69%). Rhinoviruses were detected in 105 patients, coronavirus OC43 or 229E infection was detected in 17, influenza A or B virus was detected in 12, and single infections with parainfluenza virus, respiratory syncytial virus, adenovirus, and enterovirus were found in 14 patients. Evidence for bacterial infection was found in seven patients. Four patients had a rise in antibodies against Chlamydia pneumoniae, one had a rise in antibodies against Haemophilus influenzae, one had a rise in antibodies against Streptococcus pneumoniae, and one had immunoglobulin M antibodies against Mycoplasma pneumoniae. The results show that although approximately 50% of episodes of the common cold were caused by rhinoviruses, the etiology can vary depending on the epidemiological situation with regard to circulating viruses. Bacterial infections were rare, supporting the concept that the common cold is almost exclusively a viral disease.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Mika J. Mäkelä 1, Tuomo Puhakka 1, Olli Ruuskanen 1, Maija Leinonen 2, Pekka Saikku 2, Marko Kimpimäki 3, Soile Blomqvist 3, Timo Hyypiä 4, Pertti Arstila 4'],\n",
       "  'related_topics': ['Viral culture',\n",
       "   'Serology',\n",
       "   'Rhinovirus',\n",
       "   'Coronavirus',\n",
       "   'Mycoplasma pneumoniae',\n",
       "   'Common cold',\n",
       "   'Virus',\n",
       "   'Enterovirus',\n",
       "   'Virology',\n",
       "   'Microbiology',\n",
       "   'Biology'],\n",
       "  'references': ['2055750915',\n",
       "   '2337555053',\n",
       "   '2098388207',\n",
       "   '163073849',\n",
       "   '2146133178',\n",
       "   '2032842024',\n",
       "   '1856165804',\n",
       "   '2018812376',\n",
       "   '1830634530',\n",
       "   '1699035432']},\n",
       " {'id': '2084994773',\n",
       "  'title': 'Phylogenetic analysis of a highly conserved region of the polymerase gene from 11 coronaviruses and development of a consensus polymerase chain reaction assay.',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '230',\n",
       "  'abstract': 'Viruses in the genus Coronavirus are currently placed in three groups based on antigenic cross-reactivity and sequence analysis of structural protein genes. Consensus polymerase chain reaction (PCR) primers were used to obtain cDNA, then cloned and sequenced a highly conserved 922 nucleotide region in open reading frame (ORF) 1b of the polymerase (pol) gene from eight coronaviruses. These sequences were compared with published sequences for three additional coronaviruses. In this comparison, it was found that nucleotide substitution frequencies (per 100 nucleotides) varied from 46.40 to 50.13 when viruses were compared among the traditional coronavirus groups and, with one exception (the human coronavirus (HCV) 229E), varied from 2.54 to 15.89 when compared within these groups. (The substitution frequency for 229E, as compared to other members of the same group, varied from 35.37 to 35.72.) Phylogenetic analysis of these pol gene sequences resulted in groupings which correspond closely with the previously described groupings, including recent data which places the two avian coronaviruses--infectious bronchitis virus (IBV) of chickens and turkey coronavirus (TCV)--in the same group [Guy, J.S., Barnes, H.J., Smith L.G., Breslin, J., 1997. Avian Dis. 41:583-590]. A single pair of degenerate primers was identified which amplify a 251 bp region from coronaviruses of all three groups using the same reaction conditions. This consensus PCR assay for the genus Coronavirus may be useful in identifying as yet unknown coronaviruses.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Charles B. Stephensen',\n",
       "   'Donald B. Casebolt',\n",
       "   'Nupur N. Gangopadhyay'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Consensus PCR',\n",
       "   'Turkey coronavirus',\n",
       "   'Sequence analysis',\n",
       "   'Polymerase Gene',\n",
       "   'Conserved sequence',\n",
       "   'Polymerase',\n",
       "   'Sequence alignment',\n",
       "   'Genetics',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2134812217',\n",
       "   '2009310436',\n",
       "   '132455992',\n",
       "   '2156596665',\n",
       "   '1582561043',\n",
       "   '2149495938',\n",
       "   '2087363345',\n",
       "   '2329318335',\n",
       "   '1994193749',\n",
       "   '3011200155']},\n",
       " {'id': '2149579937',\n",
       "  'title': 'Imported Lassa fever in Germany: molecular characterization of a new Lassa virus strain.',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '211',\n",
       "  'abstract': 'We describe the isolation and characterization of a new Lassa virus strain imported into Germany by a traveler who had visited Ghana, Cote D\\'Ivoire, and Burkina Faso. This strain, designated \"AV,\" originated from a region in West Africa where Lassa fever has not been reported. Viral S RNA isolated from the patient\\'s serum was amplified and sequenced. A long-range reverse transcription polymerase chain reaction allowed amplification of the full-length (3.4 kb) S RNA. The coding sequences of strain AV differed from those of all known Lassa prototype strains (Josiah, Nigeria, and LP) by approximately 20%, mainly at third codon positions. Phylogenetically, strain AV appears to be most closely related to strain Josiah from Sierra Leone. Lassa viruses comprise a group of genetically highly diverse strains, which has implications for vaccine development. The new method for full-length S RNA amplification may facilitate identification and molecular analysis of new arenaviruses or arenavirus strains.',\n",
       "  'date': 2000,\n",
       "  'authors': ['S Günther',\n",
       "   'P Emmerich',\n",
       "   'T Laue',\n",
       "   'O Kühle',\n",
       "   'M Asper',\n",
       "   'A Jung',\n",
       "   'T Grewing',\n",
       "   'J ter Meulen',\n",
       "   'H Schmitz'],\n",
       "  'related_topics': ['Lassa fever',\n",
       "   'Lassa virus',\n",
       "   'Arenavirus',\n",
       "   'Sierra leone',\n",
       "   'Strain (biology)',\n",
       "   'RNA',\n",
       "   'Virology',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Vero cell',\n",
       "   'Genetics',\n",
       "   'Biology'],\n",
       "  'references': ['2154128645',\n",
       "   '1967150940',\n",
       "   '2044967254',\n",
       "   '2171229191',\n",
       "   '2163443142',\n",
       "   '1845818816',\n",
       "   '2326037208',\n",
       "   '1941129807',\n",
       "   '2109638242',\n",
       "   '1509033271']},\n",
       " {'id': '2090060897',\n",
       "  'title': 'Evaluation of concurrent shedding of bovine coronavirus via the respiratory tract and enteric route in feedlot cattle.',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '104',\n",
       "  'abstract': 'Objective—To assess the relationship between shedding of bovine coronavirus (BCV) via the respiratory tract and enteric routes and the association with weight gain in feedlot cattle. Animals—56 crossbred steers. Procedures—Paired fecal samples and nasal swab specimens were obtained and were tested for BCV, using antigen-capture ELISA. Paired serum samples obtained were tested for antibodies to BCV, using antibody-detection ELISA. Information was collected on weight gain, clinical signs, and treatments for enteric and respiratory tract disease during the study period. Results—Number of samples positive for bovine respiratory coronavirus (BRCV) or bovine enteric coro navirus (BECV) was 37/224 (17%) and 48/223 (22%), respectively. Some cattle (25/46, 45%) shed BECV and BRCV. There were 25/29 (86%) cattle positive for BECV that shed BRCV, but only 1/27 (4%) cattle negative to BECV shed BRCV. Twenty-seven of 48 (56%) paired nasal swab specimens and fecal samples positive for BECV were positive for BRCV. In con...',\n",
       "  'date': 2001,\n",
       "  'authors': ['Kyoung Oh Cho 1, Armando E. Hoet 2, Steven C. Loerch 3, Thomas E. Wittum 2, Linda J. Saif 2'],\n",
       "  'related_topics': ['Bovine coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Feces',\n",
       "   'Nasal Swab',\n",
       "   'Respiratory tract',\n",
       "   'Respiratory system',\n",
       "   'Antibody',\n",
       "   'Weight gain',\n",
       "   'Veterinary medicine',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['50597173',\n",
       "   '2072381230',\n",
       "   '2409643934',\n",
       "   '1987051173',\n",
       "   '2045765248',\n",
       "   '1963591796',\n",
       "   '2406151794',\n",
       "   '2032346601',\n",
       "   '1979854169',\n",
       "   '2302897180']},\n",
       " {'id': '2030133843',\n",
       "  'title': 'HGV: hepatitis G virus or harmless G virus?',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '20',\n",
       "  'abstract': 'The discovery of the hepatitis G virus (HGV) has given hepatologists a new lease on life. Just when they were becoming frustrated with the slow rate of progress in unravell ing the pathobiological consequences of hepatitis B and C virus infections, along comes another candidate virus. HGV, a single-stranded ribonucleic acid (RNA) virus that belongs to the Flaviviridae family, has a global distribution. The virus is present in 1-2% of blood donors in the USA, a frequency higher than that of either HCV or hepatitis B virus (HBV) (Alter). Even more striking is the seroprevalence of 15\"2% reported in West African residents (JMed Virol 1996; 50: 97). HGVexis t s in a chronic carrier state. The virus is transmitted parenterally and is often present in patients who have received multiple transfusions or who are on haemodialysis (N Engl J Med 1996; 334: 1485), and in intravenous drug users. There is preliminary evidence for perinatal transmission (Lancet 1996; 347: 615). HGV RNA sequences have been identified in serum from patients with non-A-E acute and chronic hepatitis and cirrhosis. Impressive data comes from Brescia, Italy, where 35% of patients with acute hepatitis and 39% of those with chronic hepatitis were positive for HGV RNA (Fiordalisi). Among blood donors the virus is more common in those with raised serum aminotransferase concentrations (3\"9%) than in those with normal concentrations (0\"8%) (ff Med Virol 1996; 50: 97). These findings imply that HGV is a human pathogen, but is it? Other information is more consistent with HGV being an innocent passenger. The great majority of individuals who become HGV-RNA positive after blood transfusion have normal serum aminotronsperase concentrations and neither they, nor those found positive for HGV RNA in other circumstances, develop liver disease during prolonged follow-up (Alter). Moreover, when serum enzyme concentrations are raised they seldom accord with levels of viraemia. HGV and HCV are often, and HGV and HBV less often, found together in serum. In those coinfected with HGV and HCV, aminotransterases run parallel to HCV rather than HGV, and the presence of the latter seems to have no effect on outcome. HGV usually accounts for only a minority of cases of acute non-A-E hepatitis, and there is no evidence yet of progression over time to chronic hepatitis, cirrhosis, or hepatocellular',\n",
       "  'date': 1996,\n",
       "  'authors': ['Michael C Kew', 'Chris Kassianides'],\n",
       "  'related_topics': ['Hepatitis B',\n",
       "   'Hepatitis B virus',\n",
       "   'Hepatitis',\n",
       "   'Flaviviridae',\n",
       "   'Liver disease',\n",
       "   'Virus',\n",
       "   'Cirrhosis',\n",
       "   'Blood transfusion',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'references': ['2083266836',\n",
       "   '2313004219',\n",
       "   '2023962288',\n",
       "   '2155517838',\n",
       "   '2075432722',\n",
       "   '1984200234']},\n",
       " {'id': '1967300023',\n",
       "  'title': 'Acute Kidney Injury Network: Report of an Initiative to Improve Outcomes in Acute Kidney Injury',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '7,437',\n",
       "  'abstract': \"Acute kidney injury (AKI) is a complex disorder for which currently there is no accepted definition. Having a uniform standard for diagnosing and classifying AKI would enhance our ability to manage these patients. Future clinical and translational research in AKI will require collaborative networks of investigators drawn from various disciplines, dissemination of information via multidisciplinary joint conferences and publications, and improved translation of knowledge from pre-clinical research. We describe an initiative to develop uniform standards for defining and classifying AKI and to establish a forum for multidisciplinary interaction to improve care for patients with or at risk for AKI. Members representing key societies in critical care and nephrology along with additional experts in adult and pediatric AKI participated in a two day conference in Amsterdam, The Netherlands, in September 2005 and were assigned to one of three workgroups. Each group's discussions formed the basis for draft recommendations that were later refined and improved during discussion with the larger group. Dissenting opinions were also noted. The final draft recommendations were circulated to all participants and subsequently agreed upon as the consensus recommendations for this report. Participating societies endorsed the recommendations and agreed to help disseminate the results. The term AKI is proposed to represent the entire spectrum of acute renal failure. Diagnostic criteria for AKI are proposed based on acute alterations in serum creatinine or urine output. A staging system for AKI which reflects quantitative changes in serum creatinine and urine output has been developed. We describe the formation of a multidisciplinary collaborative network focused on AKI. We have proposed uniform standards for diagnosing and classifying AKI which will need to be validated in future studies. The Acute Kidney Injury Network offers a mechanism for proceeding with efforts to improve patient outcomes.\",\n",
       "  'date': 2007,\n",
       "  'authors': ['Ravindra L Mehta 1, John A Kellum 2, Sudhir V Shah 3, Bruce A Molitoris 4, Claudio Ronco 5, David G Warnock 6, Adeera Levin 7'],\n",
       "  'related_topics': ['Renal angina',\n",
       "   'Acute kidney injury',\n",
       "   'Translational research',\n",
       "   'Intensive care medicine',\n",
       "   'Nephrology',\n",
       "   'MEDLINE',\n",
       "   'Multidisciplinary approach',\n",
       "   'Medicine',\n",
       "   'Collaborative network',\n",
       "   'Future studies',\n",
       "   'Internal medicine'],\n",
       "  'references': ['2139937737',\n",
       "   '2149687213',\n",
       "   '2131419242',\n",
       "   '1489794536',\n",
       "   '2133482423',\n",
       "   '2116909658',\n",
       "   '2043132544',\n",
       "   '2069722312',\n",
       "   '2009995285',\n",
       "   '2073859061']},\n",
       " {'id': '2131419242',\n",
       "  'title': 'Acute Kidney Injury, Mortality, Length of Stay, and Costs in Hospitalized Patients',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '3,417',\n",
       "  'abstract': 'The marginal effects of acute kidney injury on in-hospital mortality, length of stay (LOS), and costs have not been well described. A consecutive sample of 19,982 adults who were admitted to an urban academic medical center, including 9210 who had two or more serum creatinine (SCr) determinations, was evaluated. The presence and degree of acute kidney injury were assessed using absolute and relative increases from baseline to peak SCr concentration during hospitalization. Large increases in SCr concentration were relatively rare (e.g., >or=2.0 mg/dl in 105 [1%] patients), whereas more modest increases in SCr were common (e.g., >or=0.5 mg/dl in 1237 [13%] patients). Modest changes in SCr were significantly associated with mortality, LOS, and costs, even after adjustment for age, gender, admission International Classification of Diseases, Ninth Revision, Clinical Modification diagnosis, severity of illness (diagnosis-related group weight), and chronic kidney disease. For example, an increase in SCr >or=0.5 mg/dl was associated with a 6.5-fold (95% confidence interval 5.0 to 8.5) increase in the odds of death, a 3.5-d increase in LOS, and nearly 7500 dollars in excess hospital costs. Acute kidney injury is associated with significantly increased mortality, LOS, and costs across a broad spectrum of conditions. Moreover, outcomes are related directly to the severity of acute kidney injury, whether characterized by nominal or percentage changes in serum creatinine.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Glenn M. Chertow',\n",
       "   'Elisabeth Burdick',\n",
       "   'Melissa Honour',\n",
       "   'Joseph V. Bonventre',\n",
       "   'David W. Bates'],\n",
       "  'related_topics': ['Kidney disease', 'Acute kidney injury', 'Nephrology'],\n",
       "  'references': ['2157825442',\n",
       "   '1992332433',\n",
       "   '2133482423',\n",
       "   '1996020381',\n",
       "   '1550111394',\n",
       "   '2072075701',\n",
       "   '2029723446',\n",
       "   '2153767046',\n",
       "   '2154145185',\n",
       "   '2023644538']},\n",
       " {'id': '2143432233',\n",
       "  'title': 'A comparison of albumin and saline for fluid resuscitation in the intensive care unit',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '3,122',\n",
       "  'abstract': 'background It remains uncertain whether the choice of resuscitation fluid for patients in intensive care units (ICUs) affects survival. We conducted a multicenter, randomized, double-blind trial to compare the effect of fluid resuscitation with albumin or saline on mortality in a heterogeneous population of patients in the ICU. methods We randomly assigned patients who had been admitted to the ICU to receive either 4 percent albumin or normal saline for intravascular-fluid resuscitation during the next 28 days. The primary outcome measure was death from any cause during the 28-day period after randomization. results Of the 6997 patients who underwent randomization, 3497 were assigned to receive albumin and 3500 to receive saline; the two groups had similar baseline characteristics. There were 726 deaths in the albumin group, as compared with 729 deaths in the saline group (relative risk of death, 0.99; 95 percent confidence interval, 0.91 to 1.09; P=0.87). The proportion of patients with new single-organ and multiple-organ failure was similar in the two groups (P=0.85). There were no significant differences between the groups in the mean (±SD) numbers of days spent in the ICU (6.5±6.6 in the albumin group and 6.2±6.2 in the saline group, P=0.44), days spent in the hospital (15.3±9.6 and 15.6±9.6, respectively; P = 0.30), days of mechanical ventilation (4.5±6.1 and 4.3±5.7, respectively; P = 0.74), or days of renal-replacement therapy (0.5±2.3 and 0.4±2.0, respectively; P = 0.41). conclusions In patients in the ICU, use of either 4 percent albumin or normal saline for fluid resuscitation results in similar outcomes at 28 days.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Rinaldo Bellomo', 'Julie French', 'John Myburgh'],\n",
       "  'related_topics': ['Resuscitation',\n",
       "   'Intensive care',\n",
       "   'Intensive care unit',\n",
       "   'Saline',\n",
       "   'Randomization',\n",
       "   'Mechanical ventilation',\n",
       "   'Randomized controlled trial',\n",
       "   'Relative risk',\n",
       "   'Anesthesia',\n",
       "   'Surgery',\n",
       "   'Medicine'],\n",
       "  'references': ['2768146862',\n",
       "   '2107978811',\n",
       "   '127034668',\n",
       "   '2161328469',\n",
       "   '1991864206',\n",
       "   '2599527603',\n",
       "   '1554783366',\n",
       "   '2112577081',\n",
       "   '1603121691',\n",
       "   '2046852559']},\n",
       " {'id': '2117958746',\n",
       "  'title': 'Intensity of renal support in critically ill patients with acute kidney injury',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '2,035',\n",
       "  'abstract': 'We randomly assigned critically ill patients with acute kidney injury and failure of at least one nonrenal organ or sepsis to receive intensive or less intensive renal-replacement therapy. The primary end point was death from any cause by day 60. In both study groups, hemodynamically stable patients underwent intermittent hemodialysis, and hemodynamically unstable patients underwent continuous venovenous hemodiafiltration or sustained low-efficiency dialysis. Patients receiving the intensive treatment strategy underwent intermittent hemodialysis and sustained low-efficiency dialysis six times per week and continuous venovenous hemodiafiltration at 35 ml per kilogram of body weight per hour; for patients receiving the less-intensive treatment strategy, the corresponding treatments were provided thrice weekly and at 20 ml per kilogram per hour. Results Baseline characteristics of the 1124 patients in the two groups were similar. The rate of death from any cause by day 60 was 53.6% with intensive therapy and 51.5% with less-intensive therapy (odds ratio, 1.09; 95% confidence interval, 0.86 to 1.40; P = 0.47). There was no significant difference between the two groups in the duration of renalreplacement therapy or the rate of recovery of kidney function or nonrenal organ failure. Hypotension during intermittent dialysis occurred in more patients randomly assigned to receive intensive therapy, although the frequency of hemodialysis sessions complicated by hypotension was similar in the two groups. Conclusions Intensive renal support in critically ill patients with acute kidney injury did not decrease mortality, improve recovery of kidney function, or reduce the rate of nonrenal organ failure as compared with less-intensive therapy involving a defined dose of intermittent hemodialysis three times per week and continuous renal-replacement therapy at 20 ml per kilogram per hour. (ClinicalTrials.gov number, NCT00076219.)',\n",
       "  'date': 2008,\n",
       "  'authors': ['Paul M. Palevsky',\n",
       "   'Jane Hongyuan Zhang',\n",
       "   \"Theresa Z. O'Connor\",\n",
       "   'Glenn M. Chertow',\n",
       "   'Susan T. Crowley',\n",
       "   'Devasmita Choudhury',\n",
       "   'Kevin Finkel',\n",
       "   'John A. Kellum',\n",
       "   'Emil Paganini',\n",
       "   'Roland M.H. Schein',\n",
       "   'Mark W. Smith',\n",
       "   'Kathleen M. Swanson',\n",
       "   'B. Taylor Thompson',\n",
       "   'Anitha Vijayan',\n",
       "   'Suzanne Watnick',\n",
       "   'Robert A. Star',\n",
       "   'Peter Peduzzi',\n",
       "   'E. Young',\n",
       "   'R. Fissel',\n",
       "   'W. Fissel',\n",
       "   'U. Patel',\n",
       "   'K. Belanger',\n",
       "   'A. Raine',\n",
       "   'N. Ricci',\n",
       "   'J. Lohr',\n",
       "   'P. Arora',\n",
       "   'D. Cloen',\n",
       "   'D. Wassel',\n",
       "   'L. Yohe',\n",
       "   'J. Amanzadeh',\n",
       "   'J. Penfield',\n",
       "   'M. Hussain',\n",
       "   'R. Katneni',\n",
       "   'A. Sajgure',\n",
       "   'A. Swann',\n",
       "   'G. Dolson',\n",
       "   'V. Ramanathan',\n",
       "   'G. Tasby',\n",
       "   'R. Bacallao',\n",
       "   'M. Jaradat',\n",
       "   'K. Graves',\n",
       "   'Q. Li',\n",
       "   'M. Krause',\n",
       "   'M. Shaver',\n",
       "   'M. Alam',\n",
       "   'K. Morris',\n",
       "   'T. Bland',\n",
       "   'E. Satter',\n",
       "   'J. Kraut',\n",
       "   'A. Felsenfeld +158'],\n",
       "  'related_topics': ['Dialysis', 'Acute kidney injury', 'Hemodialysis'],\n",
       "  'references': ['1898928487',\n",
       "   '2149687213',\n",
       "   '1996020381',\n",
       "   '2148973700',\n",
       "   '2043132544',\n",
       "   '2069722312',\n",
       "   '1980228387',\n",
       "   '1997278766',\n",
       "   '2107538404',\n",
       "   '1988629947']},\n",
       " {'id': '1531106656',\n",
       "  'title': 'Intensity of continuous renal-replacement therapy in critically ill patients.',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '1,401',\n",
       "  'abstract': 'Background The optimal intensity of continuous renal-replacement therapy remains unclear. We conducted a multicenter, randomized trial to compare the effect of this therapy, delivered at two different levels of intensity, on 90-day mortality among critically ill patients with acute kidney injury. Methods We randomly assigned critically ill adults with acute kidney injury to continuous renal-replacement therapy in the form of postdilution continuous venovenous hemodiafiltration with an effluent flow of either 40 ml per kilogram of body weight per hour (higher intensity) or 25 ml per kilogram per hour (lower intensity). The primary outcome measure was death within 90 days after randomization. Results Of the 1508 enrolled patients, 747 were randomly assigned to higher-intensity therapy, and 761 to lower-intensity therapy with continuous venovenous hemodiafiltration. Data on primary outcomes were available for 1464 patients (97.1%): 721 in the higher-intensity group and 743 in the lower-intensity group. The two study groups had similar baseline characteristics and received the study treatment for an average of 6.3 and 5.9 days, respectively (P = 0.35). At 90 days after randomization, 322 deaths had occurred in the higher-intensity group and 332 deaths in the lower-intensity group, for a mortality of 44.7% in each group (odds ratio, 1.00; 95% confidence interval [CI], 0.81 to 1.23; P = 0.99). At 90 days, 6.8% of survivors in the higher-intensity group (27 of 399), as compared with 4.4% of survivors in the lower-intensity group (18 of 411), were still receiving renal-replacement therapy (odds ratio, 1.59; 95% CI, 0.86 to 2.92; P = 0.14). Hypophosphatemia was more common in the higher-intensity group than in the lower-intensity group (65% vs. 54%, P Conclusions In critically ill patients with acute kidney injury, treatment with higher-intensity continuous renal-replacement therapy did not reduce mortality at 90 days. (ClinicalTrials.gov number, NCT00221013.)',\n",
       "  'date': 2009,\n",
       "  'authors': ['Rinaldo Bellomo 1, Alan Cass 2, Louise Cole 3, Simon Finfer 2, Martin Gallagher 2, Serigne Lo 2, Colin McArthur 4, Shay McGuinness 4, John Myburgh 5, Robyn Norton 2, Carlos Scheinkestel 6, Steve Su 7'],\n",
       "  'related_topics': ['Renal replacement therapy',\n",
       "   'Acute kidney injury',\n",
       "   'Randomized controlled trial',\n",
       "   'Randomization',\n",
       "   'Odds ratio',\n",
       "   'Prospective cohort study',\n",
       "   'Confidence interval',\n",
       "   'Internal medicine',\n",
       "   'Surgery',\n",
       "   'Hypophosphatemia',\n",
       "   'Medicine'],\n",
       "  'references': ['2149687213',\n",
       "   '2098931866',\n",
       "   '1489794536',\n",
       "   '2117958746',\n",
       "   '2116909658',\n",
       "   '2148973700',\n",
       "   '1970593590',\n",
       "   '2156916779',\n",
       "   '2159674113',\n",
       "   '2085529382']},\n",
       " {'id': '2157775267',\n",
       "  'title': 'Intensive insulin therapy and mortality among critically ill patients: a meta-analysis including NICE-SUGAR study data',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '1,341',\n",
       "  'abstract': 'Background: Hyperglycemia is associated with increased mortality in critically ill patients. Randomized trials of intensive insulin therapy have reported inconsistent effects on mortality and increased rates of severe hypoglycemia. We conducted a meta-analysis to update the totality of evidence regarding the influence of intensive insulin therapy compared with conventional insulin therapy on mortality and severe hypoglycemia in the intensive care unit (ICU). Methods: We conducted searches of electronic databases, abstracts from scientific conferences and bibliographies of relevant articles. We included published randomized controlled trials conducted in the ICU that directly compared intensive insulin therapy with conventional glucose management and that documented mortality. We included in our meta-analysis the data from the recent NICE-SUGAR (Normoglycemia in Intensive Care Evaluation — Survival Using Glucose Algorithm Regulation) study. Results: We included 26 trials involving a total of 13 567 patients in our meta-analysis. Among the 26 trials that reported mortality, the pooled relative risk (RR) of death with intensive insulin therapy compared with conventional therapy was 0.93 (95% confidence interval [CI] 0.83–1.04). Among the 14 trials that reported hypoglycemia, the pooled RR with intensive insulin therapy was 6.0 (95% CI 4.5–8.0). The ICU setting was a contributing factor, with patients in surgical ICUs appearing to benefit from intensive insulin therapy (RR 0.63, 95% CI 0.44–0.91); patients in the other ICU settings did not (medical ICU: RR 1.0, 95% CI 0.78–1.28; mixed ICU: RR 0.99, 95% CI 0.86–1.12). The different targets of intensive insulin therapy (glucose level ≤ 6.1 mmol/L v. ≤ 8.3 mmol/L) did not influence either mortality or risk of hypoglycemia. Interpretation: Intensive insulin therapy significantly increased the risk of hypoglycemia and conferred no overall mortality benefit among critically ill patients. However, this therapy may be beneficial to patients admitted to a surgical ICU.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Donald E.G. Griesdale',\n",
       "   'Russell J. de Souza',\n",
       "   'Rob M. van Dam',\n",
       "   'Daren K. Heyland',\n",
       "   'Deborah J. Cook',\n",
       "   'Atul Malhotra',\n",
       "   'Rupinder Dhaliwal',\n",
       "   'William R. Henderson',\n",
       "   'Dean R. Chittock',\n",
       "   'Simon Finfer',\n",
       "   'Daniel Talmor'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Intensive care unit',\n",
       "   'Hypoglycemia',\n",
       "   'Insulin',\n",
       "   'Relative risk',\n",
       "   'Randomized controlled trial',\n",
       "   'Cause of death',\n",
       "   'Meta-analysis',\n",
       "   'Internal medicine',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['2125435699',\n",
       "   '2118858814',\n",
       "   '1980717583',\n",
       "   '1986215651',\n",
       "   '2145053281',\n",
       "   '2107328434',\n",
       "   '2115285670',\n",
       "   '2148706741',\n",
       "   '2081040380',\n",
       "   '2007884458']},\n",
       " {'id': '2028701043',\n",
       "  'title': 'Long-term Risk of Mortality and Other Adverse Outcomes After Acute Kidney Injury: A Systematic Review and Meta-analysis',\n",
       "  'reference_count': '77',\n",
       "  'citation_count': '1,005',\n",
       "  'abstract': 'Background Acute kidney injury (AKI) is common in hospitalized patients. The impact of AKI on long-term outcomes is controversial. Study Design Systematic review and meta-analysis. Setting & Participants Persons with AKI. Selection Criteria for Studies MEDLINE and EMBASE databases were searched from 1985 through October 2007. Original studies describing outcomes of AKI for patients who survived hospital discharge were included. Studies were excluded from review when participants were followed up for less than 6 months. Predictor AKI, defined as acute changes in serum creatinine level or acute need for renal replacement therapy. Outcomes Chronic kidney disease (CKD), cardiovascular disease, and mortality. Results 48 studies that contained a total of 47,017 participants were reviewed; 15 studies reported long-term data for patients without AKI. The incidence rate of mortality was 8.9 deaths/100 person-years in survivors of AKI and 4.3 deaths/100 patient-years in survivors without AKI (rate ratio [RR], 2.59; 95% confidence interval, 1.97 to 3.42). AKI was associated independently with mortality risk in 6 of 6 studies that performed multivariate adjustment (adjusted RR, 1.6 to 3.9) and with myocardial infarction in 2 of 2 studies (RR, 2.05; 95% confidence interval, 1.61 to 2.61). The incidence rate of CKD after an episode of AKI was 7.8 events/100 patient-years, and the rate of end-stage renal disease was 4.9 events/100 patient-years. Limitations The relative risk for CKD and end-stage renal disease after AKI was unattainable because of lack of follow-up of appropriate controls without AKI. Conclusions The development of AKI, defined as acute changes in serum creatinine level, characterizes hospitalized patients at increased risk of long-term adverse outcomes.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Steven G. Coca 1, Bushra Yusuf 1, 2, Michael G. Shlipak 3, 4, Amit X. Garg 5, Chirag R. Parikh 1, 2'],\n",
       "  'related_topics': ['Kidney disease',\n",
       "   'Renal replacement therapy',\n",
       "   'Acute kidney injury',\n",
       "   'Risk factor',\n",
       "   'Rate ratio',\n",
       "   'Nephrology',\n",
       "   'Relative risk',\n",
       "   'Epidemiology',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine'],\n",
       "  'references': ['2125435699',\n",
       "   '2536590171',\n",
       "   '1979423827',\n",
       "   '75245760',\n",
       "   '2107328434',\n",
       "   '2131419242',\n",
       "   '2133482423',\n",
       "   '2153104532',\n",
       "   '2122814783',\n",
       "   '2111704803']},\n",
       " {'id': '2111704803',\n",
       "  'title': 'Incidence and Outcomes in Acute Kidney Injury: A Comprehensive Population-Based Study',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '1,044',\n",
       "  'abstract': 'Epidemiological studies of acute kidney injury (AKI) and acute-on-chronic renal failure (ACRF) are surprisingly sparse and confounded by differences in definition. Reported incidences vary, with few studies being population-based. Given this and our aging population, the incidence of AKI may be much higher than currently thought. We tested the hypothesis that the incidence is higher by including all patients with AKI (in a geographical population base of 523,390) regardless of whether they required renal replacement therapy irrespective of the hospital setting in which they were treated. We also tested the hypothesis that the Risk, Injury, Failure, Loss, and End-Stage Kidney (RIFLE) classification predicts outcomes. We identified all patients with serum creatinine concentrations ≥150 μmol/L (male) or ≥130μmol/L (female) over a 6-mo period in 2003. Clinical outcomes were obtained from each patient9s case records. The incidences of AKI and ACRF were 1811 and 336 per million population, respectively. Median age was 76 yr for AKI and 80.5 yr for ACRF. Sepsis was a precipitating factor in 47% of patients. The RIFLE classification was useful for predicting full recovery of renal function ( P P P P = 0.035). RIFLE did not predict mortality at 90 d or 6 mo. Thus the incidence of AKI is much higher than previously thought, with implications for service planning and providing information to colleagues about methods to prevent deterioration of renal function. The RIFLE classification is useful for identifying patients at greatest risk of adverse short-term outcomes.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Tariq Ali',\n",
       "   'Izhar Khan',\n",
       "   'William Simpson',\n",
       "   'Gordon Prescott',\n",
       "   'John Andrew Townend',\n",
       "   'William Cairns Stewart Smith',\n",
       "   'Alison Murray MacLeod'],\n",
       "  'related_topics': ['Renal replacement therapy',\n",
       "   'Rifle',\n",
       "   'Acute kidney injury',\n",
       "   'Kidney disease',\n",
       "   'Population',\n",
       "   'Nephrology',\n",
       "   'Renal function',\n",
       "   'Incidence (epidemiology)',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine'],\n",
       "  'references': ['2487377689',\n",
       "   '2139937737',\n",
       "   '2149687213',\n",
       "   '2069722312',\n",
       "   '2155939210',\n",
       "   '2036544704',\n",
       "   '2139529386',\n",
       "   '2171344771',\n",
       "   '2095573004',\n",
       "   '2150098992']},\n",
       " {'id': '2135163018',\n",
       "  'title': 'Chronic Dialysis and Death Among Survivors of Acute Kidney Injury Requiring Dialysis',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '699',\n",
       "  'abstract': 'Context Severe acute kidney injury among hospitalized patients often necessitates initiation of short-term dialysis. Little is known about the long-term outcome of those who survive to hospital discharge. Objective To assess the risk of chronic dialysis and all-cause mortality in individuals who experience an episode of acute kidney injury requiring dialysis. Design, Setting, and Participants We conducted a population-based cohort study of all adult patients in Ontario, Canada, with acute kidney injury who required in-hospital dialysis and survived free of dialysis for at least 30 days after discharge between July 1, 1996, and December 31, 2006. These individuals were matched with patients without acute kidney injury or dialysis during their index hospitalization. Matching was by age plus or minus 5 years, sex, history of chronic kidney disease, receipt of mechanical ventilation during the index hospitalization, and a propensity score for developing acute kidney injury requiring dialysis. Patients were followed up until March 31, 2007. Main Outcome Measures The primary end point was the need for chronic dialysis and the secondary outcome was all-cause mortality. Results We identified 3769 adults with acute kidney injury requiring in-hospital dialysis and 13 598 matched controls. The mean age was 62 years and median follow-up was 3 years. The incidence rate of chronic dialysis was 2.63 per 100 person-years among individuals with acute kidney injury requiring dialysis, and 0.91 per 100 person-years among control participants (adjusted hazard ratio, 3.23; 95% confidence interval, 2.70-3.86). All-cause mortality rates were 10.10 and 10.85 per 100 person-years, respectively (adjusted hazard ratio, 0.95; 95% confidence interval, 0.89-1.02). Conclusions Acute kidney injury necessitating in-hospital dialysis was associated with an increased risk of chronic dialysis but not all-cause mortality.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Ron Wald',\n",
       "   'Robert R. Quinn',\n",
       "   'Jin Luo',\n",
       "   'Ping Li',\n",
       "   'Damon C. Scales',\n",
       "   'Muhammad M. Mamdani',\n",
       "   'Joel G. Ray'],\n",
       "  'related_topics': ['Dialysis',\n",
       "   'Kidney disease',\n",
       "   'Acute kidney injury',\n",
       "   'Hemodialysis',\n",
       "   'Mortality rate',\n",
       "   'Hazard ratio',\n",
       "   'Population',\n",
       "   'Retrospective cohort study',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Surgery'],\n",
       "  'references': ['2000445173',\n",
       "   '2149687213',\n",
       "   '2131419242',\n",
       "   '2052806549',\n",
       "   '2117958746',\n",
       "   '2111945961',\n",
       "   '2163900716',\n",
       "   '2043132544',\n",
       "   '2069722312',\n",
       "   '1988629947']},\n",
       " {'id': '2042074736',\n",
       "  'title': 'Acute Renal Failure After Coronary Intervention: Incidence, Risk Factors, and Relationship to Mortality',\n",
       "  'reference_count': '52',\n",
       "  'citation_count': '2,199',\n",
       "  'abstract': 'Abstract PURPOSE: This study set out to define the incidence, predictors, and mortality related to acute renal failure (ARF) and acute renal failure requiring dialysis (ARFD) after coronary intervention. PATIENTS AND METHODS: Derivation-validation set methods were used in 1,826 consecutive patients undergoing coronary intervention with evaluation of baseline creatinine clearance (CrCl), diabetic status, contrast exposure, postprocedure creatinine, ARF, ARFD, in-hospital mortality, and long-term survival (derivation set). Multiple logistic regression was used to derive the prior probability of ARFD in a second set of 1,869 consecutive patients (validation set). RESULTS: The incidence of ARF and ARFD was 144.6/1,000 and 7.7/1,000 cases respectively. The cutoff dose of contrast below which there was no ARFD was 100 mL. No patient with a CrCl > 47 mL/min developed ARFD. These thresholds were confirmed in the validation set. Multivariate analysis found CrCl [odds ratio (OR) = 0.83, 95% confidence interval (CI) 0.77 to 0.89, P P = 0.01), and contrast dose (OR = 1.008, 95% CI 1.002 to 1.013, P = 0.01) to be independent predictors of ARFD. Patients in the validation set who underwent dialysis had a predicted prior probability of ARFD of between 0.07 and 0.73. The in-hospital mortality for those who developed ARFD was 35.7% and the 2-year survival was 18.8%. CONCLUSION: The occurrence of ARFD after coronary intervention is rare (',\n",
       "  'date': 1997,\n",
       "  'authors': ['Peter A McCullough',\n",
       "   'Robert Wolyn',\n",
       "   'Leslie L Rocher',\n",
       "   'Robert N Levin',\n",
       "   'William W O’Neill'],\n",
       "  'related_topics': ['Renal function',\n",
       "   'Odds ratio',\n",
       "   'Dialysis',\n",
       "   'Confidence interval',\n",
       "   'Contrast-induced nephropathy',\n",
       "   'Kidney disease',\n",
       "   'Predictive value of tests',\n",
       "   'Creatinine',\n",
       "   'Internal medicine',\n",
       "   'Surgery',\n",
       "   'Medicine'],\n",
       "  'references': ['1973948212',\n",
       "   '1550111394',\n",
       "   '2029723446',\n",
       "   '2071273488',\n",
       "   '2330776976',\n",
       "   '1996698106',\n",
       "   '2018224788',\n",
       "   '2056699935',\n",
       "   '2316461329',\n",
       "   '2073085411']},\n",
       " {'id': '2106882534',\n",
       "  'title': 'CLUSTAL W: IMPROVING THE SENSITIVITY OF PROGRESSIVE MULTIPLE SEQUENCE ALIGNMENT THROUGH SEQUENCE WEIGHTING, POSITION-SPECIFIC GAP PENALTIES AND WEIGHT MATRIX CHOICE',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '67,974',\n",
       "  'abstract': 'The sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. Firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. Secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. Thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. Fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. These modifications are incorporated into a new program, CLUSTAL W which is freely available.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Julie D. Thompson 1, Desmond G. Higgins 2, Toby J. Gibson 2'],\n",
       "  'related_topics': ['Gap penalty',\n",
       "   'Multiple sequence alignment',\n",
       "   'Structural alignment',\n",
       "   'Substitution matrix',\n",
       "   'MUSCLE',\n",
       "   'Alignment-free sequence analysis',\n",
       "   'BLOSUM',\n",
       "   'Sequence logo',\n",
       "   'Genetics',\n",
       "   'Biology'],\n",
       "  'references': ['2097706568',\n",
       "   '2015292449',\n",
       "   '2009310436',\n",
       "   '2143210482',\n",
       "   '2065461553',\n",
       "   '1998300401',\n",
       "   '2008708467',\n",
       "   '2045391589',\n",
       "   '2149208773',\n",
       "   '2102122585']},\n",
       " {'id': '2463755683',\n",
       "  'title': 'Update: Outbreak of severe acute respiratory syndrome - Worldwide, 2003',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '345',\n",
       "  'abstract': '',\n",
       "  'date': 2002,\n",
       "  'authors': ['T. Tsang',\n",
       "   'L. Pak-Yin',\n",
       "   'M. Lee',\n",
       "   'J.-S. Wu',\n",
       "   'Y.-C. Wu',\n",
       "   'I.-H. Chiang',\n",
       "   'K.-T. Chen',\n",
       "   'K.-H. Hsu',\n",
       "   'T.-J. Chen',\n",
       "   'H.-T. Lee',\n",
       "   'S.-J. Twu',\n",
       "   'S. Chunsuttiwat',\n",
       "   'P. Sawanpanyalert',\n",
       "   'K. Ungchusak',\n",
       "   'A. Chaovavanich'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Respiratory system',\n",
       "   'Medicine',\n",
       "   'Emergency medicine'],\n",
       "  'references': ['2089784797']},\n",
       " {'id': '2398786667',\n",
       "  'title': 'Microbial Threats to Health: Emergence, Detection, and Response',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '659',\n",
       "  'abstract': 'Infectious diseases are a global hazard that puts every nation and every person at risk. The recent SARS outbreak is a prime example. Knowing neither geographic nor political borders, often arriving silently and lethally, microbial pathogens constitute a grave threat to the health of humans. Indeed, a majority of countries recently identified the spread of infectious disease as the greatest global problem they confront. Throughout history, humans have struggled to control both the causes and consequences of infectious diseases and we will continue to do so into the foreseeable future.Following up on a high-profile 1992 report from the Institute of Medicine, Microbial Threats to Health examines the current state of knowledge and policy pertaining to emerging and re-emerging infectious diseases from around the globe. It examines the spectrum of microbial threats, factors in disease emergence, and the ultimate capacity of the United States to meet the challenges posed by microbial threats to human health. From the impact of war or technology on disease emergence to the development of enhanced disease surveillance and vaccine strategies, Microbial Threats to Health contains valuable information for researchers, students, health care providers, policymakers, public health officials. and the interested public.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Mark S. Smolinski', 'Margaret A. Hamburg', 'Joshua Lederberg'],\n",
       "  'related_topics': ['Public health',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Disease surveillance',\n",
       "   'Health care',\n",
       "   'Hazard',\n",
       "   'Outbreak',\n",
       "   'Disease',\n",
       "   'Economic growth',\n",
       "   'Politics',\n",
       "   'Political science'],\n",
       "  'references': ['1981665776',\n",
       "   '2104548316',\n",
       "   '2127435093',\n",
       "   '2119629693',\n",
       "   '2126707939',\n",
       "   '2556073860',\n",
       "   '2101273146',\n",
       "   '2114878600',\n",
       "   '2023092952',\n",
       "   '2098459942']},\n",
       " {'id': '2127949919',\n",
       "  'title': 'Nipah Virus: A Recently Emergent Deadly Paramyxovirus',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '1,316',\n",
       "  'abstract': 'A paramyxovirus virus termed Nipah virus has been identified as the etiologic agent of an outbreak of severe encephalitis in people with close contact exposure to pigs in Malaysia and Singapore. The outbreak was first noted in late September 1998 and by mid-June 1999, more than 265 encephalitis cases, including 105 deaths, had been reported in Malaysia, and 11 cases of encephalitis or respiratory illness with one death had been reported in Singapore. Electron microscopic, serologic, and genetic studies indicate that this virus belongs to the family Paramyxoviridae and is most closely related to the recently discovered Hendra virus. We suggest that these two viruses are representative of a new genus within the family Paramyxoviridae. Like Hendra virus, Nipah virus is unusual among the paramyxoviruses in its ability to infect and cause potentially fatal disease in a number of host species, including humans.',\n",
       "  'date': 2000,\n",
       "  'authors': ['K. B. Chua 1, W. J. Bellini 2, P. A. Rota 2, B. H. Harcourt 2, A. Tamin 2, S. K. Lam 1, T. G. Ksiazek 2, P. E. Rollin 2, S. R. Zaki 2, W.-J. Shieh 2, C. S. Goldsmith 2, D. J. Gubler 3, J. T. Roehrig 3, B. Eaton 4, A. R. Gould 4, J. Olson 2, P. Daniels 4, A. E. Ling 5, C. J. Peters 2, L. J. Anderson 2, B. W. J. Mahy 2'],\n",
       "  'related_topics': ['Hendra Virus',\n",
       "   'Tioman virus',\n",
       "   'Henipavirus',\n",
       "   'Veterinary virology',\n",
       "   'Menangle virus',\n",
       "   'Virus',\n",
       "   'Encephalitis',\n",
       "   'Henipavirus Infections',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2094644220',\n",
       "   '1981329413',\n",
       "   '1983097458',\n",
       "   '2130324980',\n",
       "   '2040567164',\n",
       "   '2028665290',\n",
       "   '1699123896',\n",
       "   '2029131064',\n",
       "   '2147898590',\n",
       "   '367527820']},\n",
       " {'id': '1576737979',\n",
       "  'title': 'Microarray-based detection and genotyping of viral pathogens',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '931',\n",
       "  'abstract': 'The detection of viral pathogens is of critical importance in biology, medicine, and agriculture. Unfortunately, existing techniques to screen for a broad spectrum of viruses suffer from severe limitations. To facilitate the comprehensive and unbiased analysis of viral prevalence in a given biological setting, we have developed a genomic strategy for highly parallel viral screening. The cornerstone of this approach is a long oligonucleotide (70-mer) DNA microarray capable of simultaneously detecting hundreds of viruses. Using virally infected cell cultures, we were able to efficiently detect and identify many diverse viruses. Related viral serotypes could be distinguished by the unique pattern of hybridization generated by each virus. Furthermore, by selecting microarray elements derived from highly conserved regions within viral families, individual viruses that were not explicitly represented on the microarray were still detected, raising the possibility that this approach could be used for virus discovery. Finally, by using a random PCR amplification strategy in conjunction with the microarray, we were able to detect multiple viruses in human respiratory specimens without the use of sequence-specific or degenerate primers. This method is versatile and greatly expands the spectrum of detectable viruses in a single assay while simultaneously providing the capability to discriminate among viral subtypes.',\n",
       "  'date': 2002,\n",
       "  'authors': ['David Wang 1, Laurent Coscoy 2, Maxine Zylberberg 2, Pedro C. Avila 2, Homer A. Boushey 2, Don Ganem 2, Joseph L. DeRisi 2'],\n",
       "  'related_topics': ['DNA microarray',\n",
       "   'Genotyping',\n",
       "   'Virus',\n",
       "   'Genome',\n",
       "   'Polymerase chain reaction',\n",
       "   'Microarray',\n",
       "   'Genotype',\n",
       "   'Human genetics',\n",
       "   'Genetics',\n",
       "   'Biology'],\n",
       "  'references': ['2055043387',\n",
       "   '1502936039',\n",
       "   '1549647993',\n",
       "   '2137089963',\n",
       "   '2028318125',\n",
       "   '2099369211',\n",
       "   '2169391021',\n",
       "   '2033380151',\n",
       "   '2037142940',\n",
       "   '2133247102']},\n",
       " {'id': '2128788856',\n",
       "  'title': 'Human Metapneumovirus Infections in Young and Elderly Adults',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '696',\n",
       "  'abstract': 'Human metapneumovirus virus (hMPV) is a newly discovered respiratory pathogen with limited epidemiological data available. Cohorts of young and older adults were prospectively evaluated for hMPV infection during 2 winter seasons. Patients hospitalized for cardiopulmonary conditions during that period were also studied. Overall, 44 (4.5%) of 984 illnesses were associated with hMPV infection, and 9 (4.1%) of 217 asymptomatic subjects were infected. There was a significant difference in rates of hMPV illnesses between years 1 and 2 (7/452 [1.5%] vs. 37/532 [7.0%]; P<.0001). In the second year, 11% of hospitalized patients had evidence of hMPV infection. Infections occurred in all age groups but were most common among young adults. Frail elderly people with hMPV infection frequently sought medical attention. In conclusion, hMPV infection occurs in adults of all ages and may account for a significant portion of persons hospitalized with respiratory infections during some years.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Ann R Falsey 1, Dean Erdman 2, Larry J Anderson 2, Edward E Walsh 1'],\n",
       "  'related_topics': ['Human metapneumovirus',\n",
       "   'Asymptomatic',\n",
       "   'Young adult',\n",
       "   'Epidemiology',\n",
       "   'Metapneumovirus',\n",
       "   'Viral disease',\n",
       "   'Pediatrics',\n",
       "   'Paramyxoviridae Infections',\n",
       "   'Respiratory disease',\n",
       "   'Medicine',\n",
       "   'Immunology'],\n",
       "  'references': ['2170881661',\n",
       "   '2152552492',\n",
       "   '2048618475',\n",
       "   '1589490904',\n",
       "   '2110198546',\n",
       "   '2165127900',\n",
       "   '2076770315',\n",
       "   '2079378048',\n",
       "   '2068094897',\n",
       "   '2065996927']},\n",
       " {'id': '2076620790',\n",
       "  'title': 'A morbillivirus that caused fatal disease in horses and humans',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '800',\n",
       "  'abstract': 'A morbillivirus has been isolated and added to an increasing list of emerging viral diseases. This virus caused an outbreak of fatal respiratory disease in horses and humans. Genetic analyses show it to be only distantly related to the classic morbilliviruses rinderpest, measles, and canine distemper. When seen by electron microscopy, viruses had 10- and 18-nanometer surface projections that gave them a \"double-fringed\" appearance. The virus induced syncytia that developed in the endothelium of blood vessels, particularly the lungs.',\n",
       "  'date': 1995,\n",
       "  'authors': ['K Murray',\n",
       "   'P Selleck',\n",
       "   'P Hooper',\n",
       "   'A Hyatt',\n",
       "   'A Gould',\n",
       "   'L Gleeson',\n",
       "   'H Westbury',\n",
       "   'L Hiley',\n",
       "   'L Selvey',\n",
       "   'B Rodwell'],\n",
       "  'related_topics': ['Morbillivirus',\n",
       "   'Rinderpest virus',\n",
       "   'Phocine distemper virus',\n",
       "   'Paramyxoviridae',\n",
       "   'Canine distemper',\n",
       "   'Rinderpest',\n",
       "   'Hendra Virus',\n",
       "   'Menangle virus',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2034148010',\n",
       "   '2123427059',\n",
       "   '1972759043',\n",
       "   '2020526182',\n",
       "   '2002849005',\n",
       "   '1582863396',\n",
       "   '2078406556',\n",
       "   '2148205933',\n",
       "   '1997387663',\n",
       "   '2106255335']},\n",
       " {'id': '2123324969',\n",
       "  'title': 'Guidelines for the management of adults with community-acquired pneumonia. Diagnosis, assessment of severity, antimicrobial therapy, and prevention.',\n",
       "  'reference_count': '143',\n",
       "  'citation_count': '2,491',\n",
       "  'abstract': '',\n",
       "  'date': 2000,\n",
       "  'authors': ['M. S. Niederman',\n",
       "   'L. A. Mandell',\n",
       "   'A. Anzueto',\n",
       "   'J. B. Bass',\n",
       "   'W. A. Broughton',\n",
       "   'G. D. Campbell',\n",
       "   'N. Dean',\n",
       "   'T. File',\n",
       "   'M. J. Fine',\n",
       "   'P. A. Gross',\n",
       "   'F. Martinez',\n",
       "   'T. J. Marrie',\n",
       "   'J. F. Plouffe',\n",
       "   'J. Ramirez',\n",
       "   'G. A. Sarosi',\n",
       "   'A. Torres',\n",
       "   'R. Wilson',\n",
       "   'V. L. Yu'],\n",
       "  'related_topics': ['Pneumonia severity index',\n",
       "   'Pneumonia',\n",
       "   'CURB-65',\n",
       "   'Community-acquired pneumonia',\n",
       "   'MEDLINE',\n",
       "   'Antimicrobial',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Viral therapy'],\n",
       "  'references': ['2320270386',\n",
       "   '2130141864',\n",
       "   '2004554957',\n",
       "   '2115071132',\n",
       "   '2336287850',\n",
       "   '2035760184',\n",
       "   '2158075347',\n",
       "   '1988729025',\n",
       "   '2339696769',\n",
       "   '2027162482']},\n",
       " {'id': '2130141864',\n",
       "  'title': 'Practice Guidelines for the Management of Community-Acquired Pneumonia in Adults',\n",
       "  'reference_count': '204',\n",
       "  'citation_count': '3,220',\n",
       "  'abstract': \"John G. Bartlett,1 Scott F Dowell,2 Lionel A. Mandell,6 Thomas M. File, Jr.,3 Daniel M. Musher,4 and Michael J. Fine5 'Johns Hopkins University School of Medicine, Baltimore, Maryland, 2Centers for Disease Control and Prevention, Atlanta, Georgia, 3Northeastern Ohio Universities College of Medicine, Cleveland, Ohio, 4Baylor College of Medicine and Veterans Affairs Medical Center, Houston, Texas, and 5University of Pittsburgh, Pennsylvania, USA; and 6McMaster University, Toronto, Canada\",\n",
       "  'date': 2000,\n",
       "  'authors': ['John G Bartlett 1, Scott F Dowell 2, Lionel A Mandell 3, Thomas M File 4, Daniel M Musher 5, Michael J Fine 6'],\n",
       "  'related_topics': ['Antibacterial agent',\n",
       "   'Veterans Affairs',\n",
       "   'Atlanta',\n",
       "   'Epidemiology',\n",
       "   'Library science',\n",
       "   'Community-acquired pneumonia',\n",
       "   'Medicine',\n",
       "   'Intensive care medicine',\n",
       "   'Disease control',\n",
       "   'Lung disease',\n",
       "   'Medical treatment'],\n",
       "  'references': ['1833207062',\n",
       "   '2320270386',\n",
       "   '2109779439',\n",
       "   '2103828083',\n",
       "   '3140139051',\n",
       "   '2202941334',\n",
       "   '2409448230',\n",
       "   '2336287850',\n",
       "   '2037712857',\n",
       "   '2112302527']},\n",
       " {'id': '1991467275',\n",
       "  'title': 'Bronchiolitis Obliterans Organizing Pneumonia',\n",
       "  'reference_count': '82',\n",
       "  'citation_count': '337',\n",
       "  'abstract': 'Bronchiolar disorders can be divided into 2 general categories: (1) airway disorders (cellular bronchiolitis and obliterative bronchiolitis) and (2) parenchymal disorders (respiratory bronchiolitis-interstitial lung disease, which occurs in smokers and is treatable with smoking cessation or corticosteroid therapy, and bronchiolitis obliterans organizing pneumonia, an inflammatory lung disease simultaneously involving the terminal bronchioles and alveoli). This article reviews the clinical findings and therapeutic management of bronchiolitis obliterans organizing pneumonia.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Gary R. Epler'],\n",
       "  'related_topics': ['Bronchiolitis obliterans organizing pneumonia',\n",
       "   'Bronchiolitis',\n",
       "   'Respiratory system',\n",
       "   'Airway',\n",
       "   'Smoking cessation',\n",
       "   'Pathology',\n",
       "   'Parenchyma',\n",
       "   'Medicine',\n",
       "   'Lung disease',\n",
       "   'Terminal Bronchioles'],\n",
       "  'references': ['2626588662',\n",
       "   '3025576394',\n",
       "   '1981681632',\n",
       "   '2030046096',\n",
       "   '2006351935',\n",
       "   '1971694872',\n",
       "   '2076477487',\n",
       "   '1976530706',\n",
       "   '2331116046',\n",
       "   '2037346751']},\n",
       " {'id': '1982444609',\n",
       "  'title': 'Bronchiolitis obliterans organizing pneumonia: CT features in 14 patients.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '271',\n",
       "  'abstract': 'Bronchiolitis obliterans organizing pneumonia is a disease characterized by the presence of granulation tissue within small airways and the presence of areas of organizing pneumonia. We retrospectively reviewed the chest radiographs, CT scans, and biopsy specimens in 14 consecutive patients with proved bronchiolitis obliterans organizing pneumonia. Six patients were immunocompromised because of leukemia or bone-marrow transplantation. In all patients, 10-mm collimation CT scans were available. In 11 of the 14 patients, select 1.5-mm scans were obtained. The CT findings included patchy unilateral (n = 1) or bilateral air-space consolidation (n = 9), small nodular opacities (n = 7), irregular linear opacities (n = 2), bronchial wall thickening and dilatation (n = 6), and small pleural effusions (n = 4). All patients had areas of air-space consolidation, small nodules, or both. A predominantly subpleural distribution of the air-space consolidation was apparent on the radiographs of two patients and on CT scans of six. Pathologically, the nodules and the consolidation represented different degrees of inflammation in bronchioles, alveolar ducts, and alveoli. Although most of the findings were apparent on the radiographs, the CT scans depicted the anatomic distribution and extent of bronchiolitis obliterans organizing pneumonia more accurately than did the plain chest radiographs.',\n",
       "  'date': 1990,\n",
       "  'authors': ['N L Müller', 'C A Staples', 'R R Miller'],\n",
       "  'related_topics': ['Bronchiolitis obliterans organizing pneumonia',\n",
       "   'Bronchiolitis obliterans',\n",
       "   'Pneumonia',\n",
       "   'Transplantation',\n",
       "   'Lung',\n",
       "   'Respiratory disease',\n",
       "   'Thorax',\n",
       "   'Biopsy',\n",
       "   'Radiology',\n",
       "   'Medicine'],\n",
       "  'references': ['2131262274',\n",
       "   '2149661971',\n",
       "   '2125251240',\n",
       "   '2416914730',\n",
       "   '2103258766',\n",
       "   '2111742711',\n",
       "   '2163026870',\n",
       "   '2151996610',\n",
       "   '2114857071',\n",
       "   '2034656398']},\n",
       " {'id': '2107053896',\n",
       "  'title': 'Hospital Outbreak of Middle East Respiratory Syndrome Coronavirus',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '1,270',\n",
       "  'abstract': 'Background In September 2012, the World Health Organization reported the first cases of pneumonia caused by the novel Middle East respiratory syndrome coronavirus (MERS-CoV). We describe a cluster of health care–acquired MERS-CoV infections. Methods Medical records were reviewed for clinical and demographic information and determination of potential contacts and exposures. Case patients and contacts were interviewed. The incubation period and serial interval (the time between the successive onset of symptoms in a chain of transmission) were estimated. Viral RNA was sequenced. Results Between April 1 and May 23, 2013, a total of 23 cases of MERS-CoV infection were reported in the eastern province of Saudi Arabia. Symptoms included fever in 20 patients (87%), cough in 20 (87%), shortness of breath in 11 (48%), and gastrointestinal symptoms in 8 (35%); 20 patients (87%) presented with abnormal chest radiographs. As of June 12, a total of 15 patients (65%) had died, 6 (26%) had recovered, and 2 (9%) remained ...',\n",
       "  'date': 2013,\n",
       "  'authors': ['Abdullah Assiri 1, Allison McGeer 2, Trish M. Perl 3, Connie S. Price 4, Abdullah A. Al Rabeeah 1, Derek A.T. Cummings 3, Zaki N. Alabdullatif 1, Maher Assad 1, Abdulmohsen Almulhim 1, Hatem Makhdoom 1, Hossam Madani 1, Rafat Alhakeem 1, Jaffar A. Al-Tawfiq 3, Matthew Cotten 5, Simon J. Watson 5, Paul Kellam 5, 6, Alimuddin I. Zumla 6, 7, Ziad A. Memish 8'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Pneumonia'],\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2100820722',\n",
       "   '1703839189',\n",
       "   '2147166346',\n",
       "   '2116586125',\n",
       "   '2045002682',\n",
       "   '2113457186',\n",
       "   '2058144955',\n",
       "   '2111412754']},\n",
       " {'id': '2112147913',\n",
       "  'title': 'Middle East respiratory syndrome coronavirus (MERS-CoV): announcement of the Coronavirus Study Group.',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '1,166',\n",
       "  'abstract': 'During the summer of 2012, in Jeddah, Saudi Arabia, a hitherto unknown coronavirus (CoV) was isolated from the sputum of a patient with acute pneumonia and renal failure ([1][1], [2][2]). The isolate was provisionally called human coronavirus Erasmus Medical Center (EMC) ([3][3]). Shortly thereafter',\n",
       "  'date': 2013,\n",
       "  'authors': ['R. J. de Groot 1, S. C. Baker 2, R. S. Baric 3, C. S. Brown 4, C. Drosten 5, L. Enjuanes 6, R. A. M. Fouchier 7, M. Galiano 8, A. E. Gorbalenya 9, Z. A. Memish 10, S. Perlman 11, L. L. M. Poon 12, E. J. Snijder 9, G. M. Stephens 10, P. C. Y. Woo 12, A. M. Zaki 13, M. Zambon 8, J. Ziebuhr 14'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Sputum'],\n",
       "  'references': ['2166867592',\n",
       "   '2113457186',\n",
       "   '1690366459',\n",
       "   '2066347985',\n",
       "   '2105558355',\n",
       "   '1035662337',\n",
       "   '2102229939',\n",
       "   '2060720058',\n",
       "   '2079206979']},\n",
       " {'id': '2045002682',\n",
       "  'title': 'Family Cluster of Middle East Respiratory Syndrome Coronavirus Infections',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '522',\n",
       "  'abstract': 'A human coronavirus, called the Middle East respiratory syndrome coronavirus (MERS-CoV), was first identified in September 2012 in samples obtained from a Saudi Arabian businessman who died from acute respiratory failure. Since then, 49 cases of infections caused by MERS-CoV (previously called a novel coronavirus) with 26 deaths have been reported to date. In this report, we describe a family case cluster of MERS-CoV infection, including the clinical presentation, treatment outcomes, and household relationships of three young men who became ill with MERS-CoV infection after the hospitalization of an elderly male relative, who died of the disease. Twenty-four other family members living in the same household and 124 attending staff members at the hospitals did not become ill. MERS-CoV infection may cause a spectrum of clinical illness. Although an animal reservoir is suspected, none has been discovered. Meanwhile, global concern rests on the ability of MERS-CoV to cause major illness in close contacts of patients.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Ziad A. Memish 1, Alimuddin I. Zumla 2, Rafat F. Al-Hakeem 3, Abdullah A. Al-Rabeeah 3, Gwen M. Stephens 3'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Disease cluster',\n",
       "   'Disease',\n",
       "   'Pediatrics',\n",
       "   'Medicine',\n",
       "   'Immunology',\n",
       "   'Family cluster',\n",
       "   'Human coronavirus',\n",
       "   'Treatment outcome'],\n",
       "  'references': ['2166867592',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '2158773042',\n",
       "   '2130450914',\n",
       "   '2088479029',\n",
       "   '297155885']},\n",
       " {'id': '1852588318',\n",
       "  'title': 'Assays for laboratory confirmation of novel human coronavirus (hCoV-EMC) infections.',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '415',\n",
       "  'abstract': 'We present a rigorously validated and highly sensitive confirmatory real-time RT-PCR assay (1A assay) that can be used in combination with the previously reported upE assay. Two additional RT-PCR assays for sequencing are described, targeting the RdRp gene (RdRpSeq assay) and N gene (NSeq assay), where an insertion/deletion polymorphism might exist among different hCoV-EMC strains. Finally, a simplified and biologically safe protocol for detection of antibody response by immunofluorescence microscopy was developed using convalescent patient serum.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Victor Corman 1, Marcel Müller 1, U. Costabel 2, J. Timm 2, Tabea Binger 1, Bernhard Meyer 1, P. Kreher 3, Erik Lattwein 4, Monika Eschbach-Bludau 1, A. Nitsche 3, T. Bleicker 1, O. Landt 5, Brunhilde Schweiger 3, Jan-Felix Drexler 1, Albert Osterhaus 6, Bart Haagmans 6, U. Dittmer 2, F. Bonin 2, Thorsten Wolff 3, Christian Drosten 1'],\n",
       "  'related_topics': ['Sequence analysis', 'Virology', 'Gene'],\n",
       "  'references': ['2166867592',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '1690366459',\n",
       "   '2167080692',\n",
       "   '2082755732',\n",
       "   '1593955729',\n",
       "   '2145810580',\n",
       "   '2122612816',\n",
       "   '2136039989']},\n",
       " {'id': '2163627712',\n",
       "  'title': 'Clinical features and short-term outcomes of 144 patients with SARS in the greater Toronto area.',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '1,620',\n",
       "  'abstract': 'ContextSevere acute respiratory syndrome (SARS) is an emerging infectious disease that first manifested in humans in China in November 2002 and has subsequently spread worldwide.ObjectivesTo describe the clinical characteristics and short-term outcomes of SARS in the first large group of patients in North America; to describe how these patients were treated and the variables associated with poor outcome.Design, Setting, and PatientsRetrospective case series involving 144 adult patients admitted to 10 academic and community hospitals in the greater Toronto, Ontario, area between March 7 and April 10, 2003, with a diagnosis of suspected or probable SARS. Patients were included if they had fever, a known exposure to SARS, and respiratory symptoms or infiltrates observed on chest radiograph. Patients were excluded if an alternative diagnosis was determined.Main Outcome MeasuresLocation of exposure to SARS; features of the history, physical examination, and laboratory tests at admission to the hospital; and 21-day outcomes such as death or intensive care unit (ICU) admission with or without mechanical ventilation.ResultsOf the 144 patients, 111 (77%) were exposed to SARS in the hospital setting. Features of the clinical examination most commonly found in these patients at admission were self-reported fever (99%), documented elevated temperature (85%), nonproductive cough (69%), myalgia (49%), and dyspnea (42%). Common laboratory features included elevated lactate dehydrogenase (87%), hypocalcemia (60%), and lymphopenia (54%). Only 2% of patients had rhinorrhea. A total of 126 patients (88%) were treated with ribavirin, although its use was associated with significant toxicity, including hemolysis (in 76%) and decrease in hemoglobin of 2 g/dL (in 49%). Twenty-nine patients (20%) were admitted to the ICU with or without mechanical ventilation, and 8 patients died (21-day mortality, 6.5%; 95% confidence interval [CI], 1.9%-11.8%). Multivariable analysis showed that the presence of diabetes (relative risk [RR], 3.1; 95% CI, 1.4-7.2; P = .01) or other comorbid conditions (RR, 2.5; 95% CI, 1.1-5.8; P = .03) were independently associated with poor outcome (death, ICU admission, or mechanical ventilation).ConclusionsThe majority of cases in the SARS outbreak in the greater Toronto area were related to hospital exposure. In the event that contact history becomes unreliable, several features of the clinical presentation will be useful in raising the suspicion of SARS. Although SARS is associated with significant morbidity and mortality, especially in patients with diabetes or other comorbid conditions, the vast majority (93.5%) of patients in our cohort survived.Published online May 6, 2003 (doi:10.1001/jama.289.21.JOC30885).',\n",
       "  'date': 2003,\n",
       "  'authors': ['Christopher M. Booth 1, Larissa M. Matukas 1, George A. Tomlinson 1, Anita R. Rachlis 1, David B. Rose 1, Hy A. Dwosh 1, Sharon L. Walmsley 1, Tony Mazzulli 1, 2, Monica Avendano 1, Peter Derkach 1, Issa E. Ephtimios 1, Ian Kitai 1, Barbara D. Mederski 1, Steven B. Shadowitz 1, Wayne L. Gold 1, 2, Laura A. Hawryluck 1, Elizabeth Rea 1, Jordan S. Chenkin 1, David W. Cescon 1, Susan M. Poutanen 1, 2, 3, Allan S. Detsky 1'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Intensive care unit',\n",
       "   'Relative risk'],\n",
       "  'references': ['2104548316',\n",
       "   '2131262274',\n",
       "   '2115709314',\n",
       "   '2100820722',\n",
       "   '2125251240',\n",
       "   '2136883754',\n",
       "   '2463755683',\n",
       "   '1966714873',\n",
       "   '2136166622',\n",
       "   '2158075347']},\n",
       " {'id': '2140143765',\n",
       "  'title': 'Clinical features and virological analysis of a case of Middle East respiratory syndrome coronavirus infection',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '442',\n",
       "  'abstract': 'Summary Background The Middle East respiratory syndrome coronavirus (MERS-CoV) is an emerging virus involved in cases and case clusters of severe acute respiratory infection in the Arabian Peninsula, T unisia, Morocco, France, Italy, Germany, and the UK. We provide a full description of a fatal case of MERS-CoV infection and associated phylogenetic analyses. Methods We report data for a patient who was admitted to the Klinikum Schwabing (Munich, Germany) for severe acute respiratory infection. We did diagnostic RT -PCR and indirect immunofl uorescence. From time of diagnosis, respiratory, faecal, and urine samples were obtained for virus quantifi cation. We constructed a maximum likelihood tree of the fi ve available complete MERS-CoV genomes.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Christian Drosten 1, Michael Seilmaier 2, Victor M. Corman 1, Wulf Hartmann 2, Gregor Scheible 2, Stefan Sack 2, Wolfgang Guggemos 2, Rene Kallies 1, Doreen Muth 1, Sandra Junglen 1, Marcel A. Müller 1, Walter Haas 3, Hana Guberina 4, Tim Röhnisch 5, Monika Schmid-Wendtner 5, Souhaib Aldabbagh 1, Ulf Dittmer 4, Hermann Gold 6, Petra Graf 6, Frank Bonin 7, Andrew Rambaut 8, 9, Clemens Martin Wendtner 2'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Viral load',\n",
       "   'Respiratory system'],\n",
       "  'references': ['2166867592',\n",
       "   '2103546861',\n",
       "   '2132260239',\n",
       "   '1703839189',\n",
       "   '2119111857',\n",
       "   '2112147913',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2119775949',\n",
       "   '1690366459']},\n",
       " {'id': '2119775949',\n",
       "  'title': 'Clinical features and viral diagnosis of two cases of infection with Middle East Respiratory Syndrome coronavirus: a report of nosocomial transmission',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '476',\n",
       "  'abstract': \"Summary Background Human infection with a novel coronavirus named Middle East Respiratory Syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia and the Middle East in September, 2012, with 44 laboratory-confirmed cases as of May 23, 2013. We report detailed clinical and virological data for two related cases of MERS-CoV disease, after nosocomial transmission of the virus from one patient to another in a French hospital. Methods Patient 1 visited Dubai in April, 2013; patient 2 lives in France and did not travel abroad. Both patients had underlying immunosuppressive disorders. We tested specimens from the upper (nasopharyngeal swabs) or the lower (bronchoalveolar lavage, sputum) respiratory tract and whole blood, plasma, and serum specimens for MERS-CoV by real-time RT-PCR targeting the upE and Orf1A genes of MERS-CoV. Findings Initial clinical presentation included fever, chills, and myalgia in both patients, and for patient 1, diarrhoea. Respiratory symptoms rapidly became predominant with acute respiratory failure leading to mechanical ventilation and extracorporeal membrane oxygenation (ECMO). Both patients developed acute renal failure. MERS-CoV was detected in lower respiratory tract specimens with high viral load (eg, cycle threshold [Ct] values of 22·9 for upE and 24 for Orf1a for a bronchoalveolar lavage sample from patient 1; Ct values of 22·5 for upE and 23·9 for Orf1a for an induced sputum sample from patient 2), whereas nasopharyngeal specimens were weakly positive or inconclusive. The two patients shared the same room for 3 days. The incubation period was estimated at 9–12 days for the second case. No secondary transmission was documented in hospital staff despite the absence of specific protective measures before the diagnosis of MERS-CoV was suspected. Patient 1 died on May 28, due to refractory multiple organ failure. Interpretation Patients with respiratory symptoms returning from the Middle East or exposed to a confirmed case should be isolated and investigated for MERS-CoV with lower respiratory tract sample analysis and an assumed incubation period of 12 days. Immunosuppression should also be taken into account as a risk factor. Funding French Institute for Public Health Surveillance, ANR grant Labex Integrative Biology of Emerging Infectious Diseases, and the European Community's Seventh Framework Programme projects EMPERIE and PREDEMICS.\",\n",
       "  'date': 2013,\n",
       "  'authors': ['Benoit Guery 1, Julien Poissy 1, Loubna El Mansouf 2, Caroline Séjourné 3, Nicolas Ettahar 4, Xavier Lemaire 2, Fanny Vuotto 1, Anne Goffard 1, Sylvie Behillil 5, 6, 7, Vincent Enouf 5, 6, 7, Valérie Caro 5, Alexandra Mailles 8, Didier Che 8, Jean Claude Manuguerra 5, Daniel Mathieu 1, Arnaud Fontanet 5, 9, Sylvie Van Der Werf 5, 6, 7'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Sputum',\n",
       "   'Coronavirus'],\n",
       "  'references': ['2166867592',\n",
       "   '2129542667',\n",
       "   '1703839189',\n",
       "   '2112147913',\n",
       "   '1852588318',\n",
       "   '2113457186',\n",
       "   '2163627712',\n",
       "   '2046153984',\n",
       "   '1690366459',\n",
       "   '1998725525']},\n",
       " {'id': '3010819577',\n",
       "  'title': 'Epidemiology of COVID-19 Among Children in China.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '2,514',\n",
       "  'abstract': 'OBJECTIVE: To identify the epidemiological characteristics and transmission patterns of pediatric patients with the 2019 novel coronavirus disease (COVID-19) in China. METHODS: Nationwide case series of 2135 pediatric patients with COVID-19 reported to the Chinese Center for Disease Control and Prevention from January 16, 2020, to February 8, 2020, were included. The epidemic curves were constructed by key dates of disease onset and case diagnosis. Onset-to-diagnosis curves were constructed by fitting a log-normal distribution to data on both onset and diagnosis dates. RESULTS: There were 728 (34.1%) laboratory-confirmed cases and 1407 (65.9%) suspected cases. The median age of all patients was 7 years (interquartile range: 2–13 years), and 1208 case patients (56.6%) were boys. More than 90% of all patients had asymptomatic, mild, or moderate cases. The median time from illness onset to diagnoses was 2 days (range: 0–42 days). There was a rapid increase of disease at the early stage of the epidemic, and then there was a gradual and steady decrease. The disease rapidly spread from Hubei province to surrounding provinces over time. More children were infected in Hubei province than any other province. CONCLUSIONS: Children of all ages appeared susceptible to COVID-19, and there was no significant sex difference. Although clinical manifestations of children’s COVID-19 cases were generally less severe than those of adult patients, young children, particularly infants, were vulnerable to infection. The distribution of children’s COVID-19 cases varied with time and space, and most of the cases were concentrated in Hubei province and surrounding areas. Furthermore, this study provides strong evidence of human-to-human transmission.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yuanyuan Dong 1, Xi Mo 1, Yabin Hu 1, Xin Qi 2, Fang Jiang 1, Zhongyi Jiang 1, Shilu Tong 1'],\n",
       "  'related_topics': ['Epidemiology', 'Interquartile range', 'Asymptomatic'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '3004318991',\n",
       "   '3007643904',\n",
       "   '3000834295',\n",
       "   '3003951199']},\n",
       " {'id': '3007273493',\n",
       "  'title': 'Radiological findings from 81 patients with COVID-19 pneumonia in Wuhan, China: a descriptive study.',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '2,664',\n",
       "  'abstract': 'Summary Background A cluster of patients with coronavirus disease 2019 (COVID-19) pneumonia caused by infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) were successively reported in Wuhan, China. We aimed to describe the CT findings across different timepoints throughout the disease course. Methods Patients with COVID-19 pneumonia (confirmed by next-generation sequencing or RT-PCR) who were admitted to one of two hospitals in Wuhan and who underwent serial chest CT scans were retrospectively enrolled. Patients were grouped on the basis of the interval between symptom onset and the first CT scan: group 1 (subclinical patients; scans done before symptom onset), group 2 (scans done ≤1 week after symptom onset), group 3 (>1 week to 2 weeks), and group 4 (>2 weeks to 3 weeks). Imaging features and their distribution were analysed and compared across the four groups. Findings 81 patients admitted to hospital between Dec 20, 2019, and Jan 23, 2020, were retrospectively enrolled. The cohort included 42 (52%) men and 39 (48%) women, and the mean age was 49·5 years (SD 11·0). The mean number of involved lung segments was 10·5 (SD 6·4) overall, 2·8 (3·3) in group 1, 11·1 (5·4) in group 2, 13·0 (5·7) in group 3, and 12·1 (5·9) in group 4. The predominant pattern of abnormality observed was bilateral (64 [79%] patients), peripheral (44 [54%]), ill-defined (66 [81%]), and ground-glass opacification (53 [65%]), mainly involving the right lower lobes (225 [27%] of 849 affected segments). In group 1 (n=15), the predominant pattern was unilateral (nine [60%]) and multifocal (eight [53%]) ground-glass opacities (14 [93%]). Lesions quickly evolved to bilateral (19 [90%]), diffuse (11 [52%]) ground-glass opacity predominance (17 [81%]) in group 2 (n=21). Thereafter, the prevalence of ground-glass opacities continued to decrease (17 [57%] of 30 patients in group 3, and five [33%] of 15 in group 4), and consolidation and mixed patterns became more frequent (12 [40%] in group 3, eight [53%] in group 4). Interpretation COVID-19 pneumonia manifests with chest CT imaging abnormalities, even in asymptomatic patients, with rapid evolution from focal unilateral to diffuse bilateral ground-glass opacities that progressed to or co-existed with consolidations within 1–3 weeks. Combining assessment of imaging features with clinical and laboratory findings could facilitate early diagnosis of COVID-19 pneumonia. Funding None.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Heshui Shi 1, Xiaoyu Han 1, Nanchuan Jiang 1, Yukun Cao 1, Osamah Alwalid 1, Jin Gu 1, Yanqing Fan 2, Chuansheng Zheng 1'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Asymptomatic',\n",
       "   'Subclinical infection',\n",
       "   'Cohort',\n",
       "   'Lung',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Radiological weapon',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '2166867592',\n",
       "   '2104548316',\n",
       "   '2999364275',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '3017468735',\n",
       "   '2149661971',\n",
       "   '2125251240',\n",
       "   '2112147913']},\n",
       " {'id': '3007814559',\n",
       "  'title': 'Clinical characteristics of 140 patients infected with SARS-CoV-2 in Wuhan, China.',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '2,500',\n",
       "  'abstract': 'Background Coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection has been widely spread. We aim to investigate the clinical characteristic and allergy status of patients infected with SARS-CoV-2. Methods Electronic medical records including demographics, clinical manifestation, comorbidities, laboratory data, and radiological materials of 140 hospitalized COVID-19 patients, with confirmed result of SARS-CoV-2 viral infection, were extracted and analyzed. Results An approximately 1:1 ratio of male (50.7%) and female COVID-19 patients was found, with an overall median age of 57.0 years. All patients were community-acquired cases. Fever (91.7%), cough (75.0%), fatigue (75.0%), and gastrointestinal symptoms (39.6%) were the most common clinical manifestations, whereas hypertension (30.0%) and diabetes mellitus (12.1%) were the most common comorbidities. Drug hypersensitivity (11.4%) and urticaria (1.4%) were self-reported by several patients. Asthma or other allergic diseases were not reported by any of the patients. Chronic obstructive pulmonary disease (COPD, 1.4%) patients and current smokers (1.4%) were rare. Bilateral ground-glass or patchy opacity (89.6%) was the most common sign of radiological finding. Lymphopenia (75.4%) and eosinopenia (52.9%) were observed in most patients. Blood eosinophil counts correlate positively with lymphocyte counts in severe (r = .486, P Conclusion Detailed clinical investigation of 140 hospitalized COVID-19 cases suggests eosinopenia together with lymphopenia may be a potential indicator for diagnosis. Allergic diseases, asthma, and COPD are not risk factors for SARS-CoV-2 infection. Older age, high number of comorbidities, and more prominent laboratory abnormalities were associated with severe patients.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Jin-jin Zhang 1, Xiang Dong 1, Yi-yuan Cao 1, Ya-dong Yuan 2, Yi-bin Yang 1, You-qin Yan 3, Cezmi A. Akdis 4, Ya-dong Gao 1'],\n",
       "  'related_topics': ['Eosinopenia',\n",
       "   'Asthma',\n",
       "   'COPD',\n",
       "   'Risk factor',\n",
       "   'Allergy',\n",
       "   'Comorbidity',\n",
       "   'Severity of illness',\n",
       "   'Diabetes mellitus',\n",
       "   'Internal medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '2999364275',\n",
       "   '3017468735',\n",
       "   '3002812395',\n",
       "   '2163627712']},\n",
       " {'id': '3002764620',\n",
       "  'title': 'Novel coronavirus 2019-nCoV: early estimation of epidemiological parameters and epidemic predictions',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '679',\n",
       "  'abstract': 'Since first identified, the epidemic scale of the recently emerged novel coronavirus (2019-nCoV) in Wuhan, China, has increased rapidly, with cases arising across China and other countries and regions. using a transmission model, we estimate a basic reproductive number of 3.11 (95%CI, 2.39-4.13); 58-76% of transmissions must be prevented to stop increasing; Wuhan case ascertainment of 5.0% (3.6-7.4); 21022 (11090-33490) total infections in Wuhan 1 to 22 January.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Read Jm 1, Bridgen 1, Cummings Da 2, Ho A 3, Jewell Cp 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Basic reproduction number',\n",
       "   'Transmission (mechanics)',\n",
       "   'Epidemiology',\n",
       "   'Estimation',\n",
       "   'Environmental health',\n",
       "   'Biology',\n",
       "   'Case ascertainment',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '2582743722',\n",
       "   '3002539152',\n",
       "   '3017468735',\n",
       "   '2999612210',\n",
       "   '2147166346',\n",
       "   '3002533591',\n",
       "   '3026046290',\n",
       "   '1998725525',\n",
       "   '3001343166']},\n",
       " {'id': '3005655936',\n",
       "  'title': 'Clinical and biochemical indexes from 2019-nCoV infected patients linked to viral loads and lung injury.',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '1,378',\n",
       "  'abstract': 'The outbreak of the 2019-nCoV infection began in December 2019 in Wuhan, Hubei province, and rapidly spread to many provinces in China as well as other countries. Here we report the epidemiological, clinical, laboratory, and radiological characteristics, as well as potential biomarkers for predicting disease severity in 2019-nCoV-infected patients in Shenzhen, China. All 12 cases of the 2019-nCoV-infected patients developed pneumonia and half of them developed acute respiratory distress syndrome (ARDS). The most common laboratory abnormalities were hypoalbuminemia, lymphopenia, decreased percentage of lymphocytes (LYM) and neutrophils (NEU), elevated C-reactive protein (CRP) and lactate dehydrogenase (LDH), and decreased CD8 count. The viral load of 2019-nCoV detected from patient respiratory tracts was positively linked to lung disease severity. ALB, LYM, LYM (%), LDH, NEU (%), and CRP were highly correlated to the acute lung injury. Age, viral load, lung injury score, and blood biochemistry indexes, albumin (ALB), CRP, LDH, LYM (%), LYM, and NEU (%), may be predictors of disease severity. Moreover, the Angiotensin II level in the plasma sample from 2019-nCoV infected patients was markedly elevated and linearly associated to viral load and lung injury. Our results suggest a number of potential diagnosis biomarkers and angiotensin receptor blocker (ARB) drugs for potential repurposing treatment of 2019-nCoV infection.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yingxia Liu 1, Yang Yang 1, Cong Zhang 2, 3, Fengming Huang 3, Fuxiang Wang 1, Jing Yuan 1, Zhaoqin Wang 1, Jinxiu Li 1, Jianming Li 1, Cheng Feng 1, Zheng Zhang 1, Lifei Wang 1, Ling Peng 1, Li Chen 1, Yuhao Qin 3, Dandan Zhao 3, Shuguang Tan 4, Lu Yin 5, Jun Xu 5, Congzhao Zhou 2, Chengyu Jiang 3, Lei Liu 1'],\n",
       "  'related_topics': ['Lung injury', 'Angiotensin II', 'Viral load'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '2999364275',\n",
       "   '3017468735',\n",
       "   '2091671824',\n",
       "   '1964982019',\n",
       "   '2025672718',\n",
       "   '2005357317']},\n",
       " {'id': '3015190630',\n",
       "  'title': 'Structural and Functional Basis of SARS-CoV-2 Entry by Using Human ACE2.',\n",
       "  'reference_count': '58',\n",
       "  'citation_count': '1,250',\n",
       "  'abstract': 'The recent emergence of a novel coronavirus (SARS-CoV-2) in China has caused significant public health concerns. Recently, ACE2 was reported as an entry receptor for SARS-CoV-2. In this study, we present the crystal structure of the C-terminal domain of SARS-CoV-2 (SARS-CoV-2-CTD) spike (S) protein in complex with human ACE2 (hACE2), which reveals a hACE2-binding mode similar overall to that observed for SARS-CoV. However, atomic details at the binding interface demonstrate that key residue substitutions in SARS-CoV-2-CTD slightly strengthen the interaction and lead to higher affinity for receptor binding than SARS-RBD. Additionally, a panel of murine monoclonal antibodies (mAbs) and polyclonal antibodies (pAbs) against SARS-CoV-S1/receptor-binding domain (RBD) were unable to interact with the SARS-CoV-2 S protein, indicating notable differences in antigenicity between SARS-CoV and SARS-CoV-2. These findings shed light on the viral pathogenesis and provide important structural information regarding development of therapeutic countermeasures against the emerging virus.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Qihui Wang 1, Yanfang Zhang 1, Lili Wu 1, Sheng Niu 1, 2, Chunli Song 1, 3, Zengyuan Zhang 1, Guangwen Lu 4, Chengpeng Qiao 1, Yu Hu 1, 5, Kwok Yung Yuen 6, Qisheng Wang 1, Huan Zhou 1, Jinghua Yan 7, Jianxun Qi 1'],\n",
       "  'related_topics': ['Coronavirus', 'Protein domain', 'Viral protein'],\n",
       "  'references': ['3001897055',\n",
       "   '3004280078',\n",
       "   '2152207030',\n",
       "   '3001465255',\n",
       "   '3009912996',\n",
       "   '3003217347',\n",
       "   '2166867592',\n",
       "   '3007643904',\n",
       "   '2114622716',\n",
       "   '1539796472']},\n",
       " {'id': '3009739970',\n",
       "  'title': 'The deadly coronaviruses: The 2003 SARS pandemic and the 2020 novel coronavirus epidemic in China.',\n",
       "  'reference_count': '113',\n",
       "  'citation_count': '518',\n",
       "  'abstract': 'The 2019-nCoV is officially called SARS-CoV-2 and the disease is named COVID-19. This viral epidemic in China has led to the deaths of over 1800 people, mostly elderly or those with an underlying chronic disease or immunosuppressed state. This is the third serious Coronavirus outbreak in less than 20 years, following SARS in 2002-2003 and MERS in 2012. While human strains of Coronavirus are associated with about 15% of cases of the common cold, the SARS-CoV-2 may present with varying degrees of severity, from flu-like symptoms to death. It is currently believed that this deadly Coronavirus strain originated from wild animals at the Huanan market in Wuhan, a city in Hubei province. Bats, snakes and pangolins have been cited as potential carriers based on the sequence homology of CoV isolated from these animals and the viral nucleic acids of the virus isolated from SARS-CoV-2 infected patients. Extreme quarantine measures, including sealing off large cities, closing borders and confining people to their homes, were instituted in January 2020 to prevent spread of the virus, but by that time much of the damage had been done, as human-human transmission became evident. While these quarantine measures are necessary and have prevented a historical disaster along the lines of the Spanish flu, earlier recognition and earlier implementation of quarantine measures may have been even more effective. Lessons learned from SARS resulted in faster determination of the nucleic acid sequence and a more robust quarantine strategy. However, it is clear that finding an effective antiviral and developing a vaccine are still significant challenges. The costs of the epidemic are not limited to medical aspects, as the virus has led to significant sociological, psychological and economic effects globally. Unfortunately, emergence of SARS-CoV-2 has led to numerous reports of Asians being subjected to racist behavior and hate crimes across the world.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yongshi Yang 1, Fujun Peng 2, Runsheng Wang 3, Kai Guan 1, Taijiao Jiang 2, Guogang Xu 3, Jinlyu Sun 1, Christopher Chang 4, 5'],\n",
       "  'related_topics': ['Pandemic',\n",
       "   'Coronavirus',\n",
       "   'Quarantine',\n",
       "   'Outbreak',\n",
       "   'Transmission (medicine)',\n",
       "   'Common cold',\n",
       "   'Disease',\n",
       "   'Virus',\n",
       "   'Virology'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '3004318991',\n",
       "   '3003465021']},\n",
       " {'id': '2195009776',\n",
       "  'title': 'A SARS-like cluster of circulating bat coronaviruses shows potential for human emergence',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '635',\n",
       "  'abstract': 'The emergence of severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome (MERS)-CoV underscores the threat of cross-species transmission events leading to outbreaks in humans. Here we examine the disease potential of a SARS-like virus, SHC014-CoV, which is currently circulating in Chinese horseshoe bat populations. Using the SARS-CoV reverse genetics system, we generated and characterized a chimeric virus expressing the spike of bat coronavirus SHC014 in a mouse-adapted SARS-CoV backbone. The results indicate that group 2b viruses encoding the SHC014 spike in a wild-type backbone can efficiently use multiple orthologs of the SARS receptor human angiotensin converting enzyme II (ACE2), replicate efficiently in primary human airway cells and achieve in vitro titers equivalent to epidemic strains of SARS-CoV. Additionally, in vivo experiments demonstrate replication of the chimeric virus in mouse lung with notable pathogenesis. Evaluation of available SARS-based immune-therapeutic and prophylactic modalities revealed poor efficacy; both monoclonal antibody and vaccine approaches failed to neutralize and protect from infection with CoVs using the novel spike protein. On the basis of these findings, we synthetically re-derived an infectious full-length SHC014 recombinant virus and demonstrate robust viral replication both in vitro and in vivo. Our work suggests a potential risk of SARS-CoV re-emergence from viruses currently circulating in bat populations.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Vineet D. Menachery 1, Boyd L. Yount 1, Kari Debbink 1, Sudhakar Agnihothram 2, Lisa E. Gralinski 1, Jessica A. Plante 1, Rachel L. Graham 1, Trevor Scobey 1, Xing Yi Ge 3, Eric F. Donaldson 1, Scott H. Randell 1, Antonio Lanzavecchia 4, Wayne A. Marasco 5, Zhengli Li Shi 3, Ralph S. Baric 1'],\n",
       "  'related_topics': ['Virus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Viral replication',\n",
       "   'Recombinant virus',\n",
       "   'Antibody',\n",
       "   'Reverse genetics',\n",
       "   'Monoclonal antibody',\n",
       "   'Virology',\n",
       "   'Horseshoe bat',\n",
       "   'Biology'],\n",
       "  'references': ['1993577573',\n",
       "   '2094993149',\n",
       "   '2126707939',\n",
       "   '2092969802',\n",
       "   '2152528032',\n",
       "   '1995367098',\n",
       "   '2143230291',\n",
       "   '2074618762',\n",
       "   '1963580683',\n",
       "   '2098037373']},\n",
       " {'id': '2115555188',\n",
       "  'title': 'Middle East Respiratory Syndrome Coronavirus: Another Zoonotic Betacoronavirus Causing SARS-Like Disease',\n",
       "  'reference_count': '351',\n",
       "  'citation_count': '729',\n",
       "  'abstract': 'SUMMARY The source of the severe acute respiratory syndrome (SARS) epidemic was traced to wildlife market civets and ultimately to bats. Subsequent hunting for novel coronaviruses (CoVs) led to the discovery of two additional human and over 40 animal CoVs, including the prototype lineage C betacoronaviruses, Tylonycteris bat CoV HKU4 and Pipistrellus bat CoV HKU5; these are phylogenetically closely related to the Middle East respiratory syndrome (MERS) CoV, which has affected more than 1,000 patients with over 35% fatality since its emergence in 2012. All primary cases of MERS are epidemiologically linked to the Middle East. Some of these patients had contacted camels which shed virus and/or had positive serology. Most secondary cases are related to health care-associated clusters. The disease is especially severe in elderly men with comorbidities. Clinical severity may be related to MERS-CoV9s ability to infect a broad range of cells with DPP4 expression, evade the host innate immune response, and induce cytokine dysregulation. Reverse transcription-PCR on respiratory and/or extrapulmonary specimens rapidly establishes diagnosis. Supportive treatment with extracorporeal membrane oxygenation and dialysis is often required in patients with organ failure. Antivirals with potent in vitro activities include neutralizing monoclonal antibodies, antiviral peptides, interferons, mycophenolic acid, and lopinavir. They should be evaluated in suitable animal models before clinical trials. Developing an effective camel MERS-CoV vaccine and implementing appropriate infection control measures may control the continuing epidemic.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Jasper F. W. Chan',\n",
       "   'Susanna K. P. Lau',\n",
       "   'Kelvin K. W. To',\n",
       "   'Vincent C. C. Cheng',\n",
       "   'Patrick C. Y. Woo',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Middle East respiratory syndrome',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Virus',\n",
       "   'Disease',\n",
       "   'Innate immune system',\n",
       "   'Infection control',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Respiratory system',\n",
       "   'Biology'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '2129542667',\n",
       "   '1993577573',\n",
       "   '1703839189',\n",
       "   '2565805236',\n",
       "   '2119111857',\n",
       "   '2112147913']},\n",
       " {'id': '2298153446',\n",
       "  'title': 'SARS-like WIV1-CoV poised for human emergence',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '276',\n",
       "  'abstract': 'Outbreaks from zoonotic sources represent a threat to both human disease as well as the global economy. Despite a wealth of metagenomics studies, methods to leverage these datasets to identify future threats are underdeveloped. In this study, we describe an approach that combines existing metagenomics data with reverse genetics to engineer reagents to evaluate emergence and pathogenic potential of circulating zoonotic viruses. Focusing on the severe acute respiratory syndrome (SARS)-like viruses, the results indicate that the WIV1-coronavirus (CoV) cluster has the ability to directly infect and may undergo limited transmission in human populations. However, in vivo attenuation suggests additional adaptation is required for epidemic disease. Importantly, available SARS monoclonal antibodies offered success in limiting viral infection absent from available vaccine approaches. Together, the data highlight the utility of a platform to identify and prioritize prepandemic strains harbored in animal reservoirs and document the threat posed by WIV1-CoV for emergence in human populations.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Vineet D. Menachery 1, Boyd L. Yount 1, Amy C Sims 1, Kari Debbink 1, Sudhakar S. Agnihothram 2, Lisa E. Gralinski 1, Rachel Lauren Graham 1, Trevor Scobey 1, Jessica A. Plante 1, Scott R. Royal 1, Jesica Swanstrom 1, Timothy Patrick Sheahan 1, Raymond J Pickles 1, 3, Davide Corti 4, Scott H Randell 1, Antonio Lanzavecchia 4, Wayne A. Marasco 5, Ralph S Baric 1, 3'],\n",
       "  'related_topics': ['Transmission (medicine)', 'Metagenomics', 'Outbreak'],\n",
       "  'references': ['1993577573',\n",
       "   '2094993149',\n",
       "   '2126707939',\n",
       "   '2092969802',\n",
       "   '2101176145',\n",
       "   '2152528032',\n",
       "   '1998383538',\n",
       "   '1995367098',\n",
       "   '2143230291',\n",
       "   '1963580683']},\n",
       " {'id': '2525468044',\n",
       "  'title': 'Viral Load Kinetics of MERS Coronavirus Infection.',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '174',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus continues to circulate in the Middle East. During a recent outbreak in Korea, changes in MERS coronavirus viral load were determined during the course of illness in 17 patients.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Myoung Don Oh 1, Wan Beom Park 1, Pyoeng Gyun Choe 1, Su Jin Choi 2, Jong I.I. Kim 1, Jeesoo Chae 1, Sung Sup Park 1, Eui Chong Kim 1, Hong Sang Oh 1, Eun Jung Kim 1, Eun Young Nam 1, Sun Hee Na 1, Dong Ki Kim 1, Sang Min Lee 1, Kyoung Ho Song 1, Ji Hwan Bang 1, Eu Suk Kim 1, Hong Bin Kim 1, Sang Won Park 1, Nam Joong Kim 1'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Viral load',\n",
       "   'Outbreak',\n",
       "   'Virology',\n",
       "   'Severity of illness',\n",
       "   'Medicine',\n",
       "   'Coronavirus Infections',\n",
       "   'Course of illness'],\n",
       "  'references': ['2126707939', '2103118479', '2405185968', '2134527559']},\n",
       " {'id': '1945961678',\n",
       "  'title': 'Treatment With Lopinavir/Ritonavir or Interferon-β1b Improves Outcome of MERS-CoV Infection in a Nonhuman Primate Model of Common Marmoset',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '587',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus (MERS-CoV) causes severe disease in human with an overall case-fatality rate of >35%. Effective antivirals are crucial for improving the clinical outcome of MERS. Although a number of repurposed drugs, convalescent-phase plasma, antiviral peptides, and neutralizing antibodies exhibit anti-MERS-CoV activity in vitro, most are not readily available or have not been evaluated in nonhuman primates. We assessed 3 repurposed drugs with potent in vitro anti-MERS-CoV activity (mycophenolate mofetil [MMF], lopinavir/ritonavir, and interferon-β1b) in common marmosets with severe disease resembling MERS in humans. The lopinavir/ritonavir-treated and interferon-β1b-treated animals had better outcome than the untreated animals, with improved clinical (mean clinical scores ↓50.9%-95.0% and ↓weight loss than the untreated animals), radiological (minimal pulmonary infiltrates), and pathological (mild bronchointerstitial pneumonia) findings, and lower mean viral loads in necropsied lung (↓0.59-1.06 log10 copies/glyceraldehyde 3-phosphate dehydrogenase [GAPDH]; P < .050) and extrapulmonary (↓0.11-1.29 log10 copies/GAPDH; P < .050 in kidney) tissues. In contrast, all MMF-treated animals developed severe and/or fatal disease with higher mean viral loads (↑0.15-0.54 log10 copies/GAPDH) than the untreated animals. The mortality rate at 36 hours postinoculation was 67% (untreated and MMF-treated) versus 0-33% (lopinavir/ritonavir-treated and interferon-β1b-treated). Lopinavir/ritonavir and interferon-β1b alone or in combination should be evaluated in clinical trials. MMF alone may worsen MERS and should not be used.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Jasper Fuk Woo Chan 1, Yanfeng Yao 2, Man Lung Yeung 3, Wei Deng 2, Linlin Bao 2, Lilong Jia 3, Fengdi Li 2, Chong Xiao 2, Hong Gao 2, Pin Yu 2, Jian Piao Cai 3, Hin Chu 3, Jie Zhou 3, Honglin Chen 1, Chuan Qin 2, Kwok Yung Yuen 1'],\n",
       "  'related_topics': ['Lopinavir/ritonavir',\n",
       "   'Lopinavir',\n",
       "   'Ritonavir',\n",
       "   'Viral load',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Immunology',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2166867592',\n",
       "   '2006434809',\n",
       "   '2115555188',\n",
       "   '2034462612',\n",
       "   '1977050884',\n",
       "   '2150120685',\n",
       "   '1947409115',\n",
       "   '2017248106',\n",
       "   '2078121682',\n",
       "   '2096238447']},\n",
       " {'id': '2099941783',\n",
       "  'title': 'Presence of Middle East respiratory syndrome coronavirus antibodies in Saudi Arabia: a nationwide, cross-sectional, serological study',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '270',\n",
       "  'abstract': \"Summary Background Scientific evidence suggests that dromedary camels are the intermediary host for the Middle East respiratory syndrome coronavirus (MERS-CoV). However, the actual number of infections in people who have had contact with camels is unknown and most index patients cannot recall any such contact. We aimed to do a nationwide serosurvey in Saudi Arabia to establish the prevalence of MERS-CoV antibodies, both in the general population and in populations of individuals who have maximum exposure to camels. Methods In the cross-sectional serosurvey, we tested human serum samples obtained from healthy individuals older than 15 years who attended primary health-care centres or participated in a national burden-of-disease study in all 13 provinces of Saudi Arabia. Additionally, we tested serum samples from shepherds and abattoir workers with occupational exposure to camels. Samples were screened by recombinant ELISA and MERS-CoV seropositivity was confirmed by recombinant immunofluorescence and plaque reduction neutralisation tests. We used two-tailed Mann Whitney U exact tests, χ 2 , and Fisher's exact tests to analyse the data. Findings Between Dec 1, 2012, and Dec 1, 2013, we obtained individual serum samples from 10\\u2008009 individuals. Anti-MERS-CoV antibodies were confirmed in 15 (0·15%; 95% CI 0·09–0·24) of 10\\u2008009 people in six of the 13 provinces. The mean age of seropositive individuals was significantly younger than that of patients with reported, laboratory-confirmed, primary Middle Eastern respiratory syndrome (43·5 years [SD 17·3] vs 53·8 years [17·5]; p=0·008). Men had a higher antibody prevalence than did women (11 [0·25%] of 4341 vs two [0·05%] of 4378; p=0·028) and antibody prevalence was significantly higher in central versus coastal provinces (14 [0·26%] of 5479 vs one [0·02%] of 4529; p=0·003). Compared with the general population, seroprevalence of MERS-CoV antibodies was significantly increased by 15 times in shepherds (two [2·3%] of 87, p=0·0004) and by 23 times in slaughterhouse workers (five [3·6%] of 140; p Interpretation Seroprevalence of MERS-CoV antibodies was significantly higher in camel-exposed individuals than in the general population. By simple multiplication, a projected 44\\u2008951 (95% CI 26\\u2008971–71\\u2008922) individuals older than 15 years might be seropositive for MERS-CoV in Saudi Arabia. These individuals might be the source of infection for patients with confirmed MERS who had no previous exposure to camels. Funding European Union, German Centre for Infection Research, Federal Ministry of Education and Research, German Research Council, and Ministry of Health of Saudi Arabia.\",\n",
       "  'date': 2015,\n",
       "  'authors': ['Marcel A. Müller 1, Benjamin Meyer 1, Victor M. Corman 1, Malak Al-Masri 2, Abdulhafeez Turkestani 3, Daniel Ritz 1, Andrea Sieberg 1, Souhaib Aldabbagh 1, Berend J. Bosch 4, Erik Lattwein 5, Raafat F. Alhakeem 2, Abdullah M. Assiri 2, Ali M. Albarrak 6, Ali M. Al-Shangiti 7, Jaffar A. Al-Tawfiq 8, 9, Paul Wikramaratna 10, Abdullah A. Alrabeeah 11, Christian Drosten 1, Ziad A. Memish 12'],\n",
       "  'related_topics': ['Seroprevalence', 'European union', 'Population'],\n",
       "  'references': ['2107053896',\n",
       "   '2119111857',\n",
       "   '2160011624',\n",
       "   '1852588318',\n",
       "   '2145441153',\n",
       "   '2128886090',\n",
       "   '2119837294',\n",
       "   '2108756402',\n",
       "   '1968393246',\n",
       "   '2130227690']},\n",
       " {'id': '1948751323',\n",
       "  'title': 'Hypercolumns for object segmentation and fine-grained localization',\n",
       "  'reference_count': '42',\n",
       "  'citation_count': '1,486',\n",
       "  'abstract': 'Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Bharath Hariharan 1, Pablo Arbelaez 2, Ross Girshick 3, Jitendra Malik 1'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Pixel',\n",
       "   'Feature (computer vision)',\n",
       "   'Feature extraction',\n",
       "   'Segmentation',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Artificial neural network',\n",
       "   'Computer vision',\n",
       "   'Point (geometry)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2102605133',\n",
       "   '1903029394',\n",
       "   '2168356304',\n",
       "   '2109255472',\n",
       "   '2156303437',\n",
       "   '2022508996',\n",
       "   '2118585731',\n",
       "   '1507506748']},\n",
       " {'id': '2167510172',\n",
       "  'title': 'Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '1,463',\n",
       "  'abstract': 'We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 × 512 × 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Dan Ciresan',\n",
       "   'Alessandro Giusti',\n",
       "   'Luca M. Gambardella',\n",
       "   'Jürgen Schmidhuber'],\n",
       "  'related_topics': ['Pixel',\n",
       "   'Ground truth',\n",
       "   'Artificial neural network',\n",
       "   'Segmentation',\n",
       "   'Gradient descent',\n",
       "   'Image warping',\n",
       "   'Computer vision',\n",
       "   'Classifier (UML)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '2141125852',\n",
       "   '2143516773',\n",
       "   '2156163116',\n",
       "   '2148461049',\n",
       "   '1624854622',\n",
       "   '2132424367',\n",
       "   '1969013163',\n",
       "   '1523493493',\n",
       "   '2149194912']},\n",
       " {'id': '1893585201',\n",
       "  'title': 'Learning to generate chairs with convolutional neural networks',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '688',\n",
       "  'abstract': 'We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Alexey Dosovitskiy', 'Jost Tobias Springenberg', 'Thomas Brox'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Similarity (geometry)',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2099471712',\n",
       "   '2155893237',\n",
       "   '2136922672',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '1959608418',\n",
       "   '2100495367',\n",
       "   '2155541015']},\n",
       " {'id': '2148349024',\n",
       "  'title': 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '667',\n",
       "  'abstract': \"Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).\",\n",
       "  'date': 2014,\n",
       "  'authors': ['Alexey Dosovitskiy',\n",
       "   'Jost Tobias Springenberg',\n",
       "   'Martin Riedmiller',\n",
       "   'Thomas Brox'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Unsupervised learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Convolutional neural network',\n",
       "   'Feature learning',\n",
       "   'Feature (machine learning)',\n",
       "   'Discriminative model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2155893237',\n",
       "   '1849277567',\n",
       "   '2109255472',\n",
       "   '3118608800',\n",
       "   '1904365287',\n",
       "   '2158899491',\n",
       "   '2155541015',\n",
       "   '2963911037']},\n",
       " {'id': '1909499787',\n",
       "  'title': 'MERS, SARS, and Ebola: The Role of Super-Spreaders in Infectious Disease.',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '286',\n",
       "  'abstract': 'Super-spreading occurs when a single patient infects a disproportionate number of contacts. The 2015 MERS-CoV, 2003 SARS-CoV, and to a lesser extent 2014-15 Ebola virus outbreaks were driven by super-spreaders. We summarize documented super-spreading in these outbreaks, explore contributing factors, and suggest studies to better understand super-spreading.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Gary Wong 1, Wenjun Liu 1, Yingxia Liu 2, Boping Zhou 2, Yuhai Bi 1, George F. Gao 3'],\n",
       "  'related_topics': ['Ebola virus',\n",
       "   'Infectious disease (medical specialty)',\n",
       "   'Outbreak',\n",
       "   'Virology',\n",
       "   'Bioinformatics',\n",
       "   'Biology',\n",
       "   'Disease transmission',\n",
       "   'Single patient'],\n",
       "  'references': ['2115102869',\n",
       "   '1975375203',\n",
       "   '2096145431',\n",
       "   '2227495319',\n",
       "   '1994871753',\n",
       "   '1979807576',\n",
       "   '1942680103',\n",
       "   '2024845268',\n",
       "   '2135540513',\n",
       "   '2089797630']},\n",
       " {'id': '3027518954',\n",
       "  'title': 'Pathogen genomics in public health',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '82',\n",
       "  'abstract': 'Summary Rapid advances in DNA sequencing technology (“next-generation sequencing”) have inspired optimism about the potential of human genomics for “precision medicine.” Meanwhile, pathogen genomic...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Gregory L. Armstrong 1, Duncan R. MacCannell 1, Jill Taylor 2, Heather A. Carleton 1, Elizabeth B. Neuhaus 1, Richard S. Bradbury 3, James E. Posey 4, Marta Gwinn 1'],\n",
       "  'related_topics': ['Genomics',\n",
       "   'DNA sequencing',\n",
       "   'Pathogen',\n",
       "   'Public health',\n",
       "   'Computational biology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Human genomics',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001897055',\n",
       "   '3012538234',\n",
       "   '3036563213',\n",
       "   '3013266235',\n",
       "   '3096084589',\n",
       "   '3095124337',\n",
       "   '3023674326',\n",
       "   '3009375872',\n",
       "   '3047108013',\n",
       "   '3096165595']},\n",
       " {'id': '2792024998',\n",
       "  'title': 'From “A”IV to “Z”IKV: Attacks from Emerging and Re-emerging Pathogens',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '167',\n",
       "  'abstract': '100 years after the infamous “Spanish flu” pandemic, the 2017–2018 flu season has been severe, with numerous infections worldwide. In between, there have been continuous, relentless attacks from (re-)emerging viruses. To fully understand viral pathogenesis and develop effective medical countermeasures, we must strengthen current surveillance and basic research efforts.',\n",
       "  'date': 2018,\n",
       "  'authors': ['George F. Gao'],\n",
       "  'related_topics': ['Pandemic',\n",
       "   'Flu season',\n",
       "   'Viral pathogenesis',\n",
       "   'Influenza A virus',\n",
       "   'Intensive care medicine',\n",
       "   'Biology',\n",
       "   'Basic research'],\n",
       "  'references': ['1975375203',\n",
       "   '2116682907',\n",
       "   '2788045019',\n",
       "   '1783641736',\n",
       "   '1942680103',\n",
       "   '1967283148',\n",
       "   '2793181185',\n",
       "   '2081635462',\n",
       "   '2473338860',\n",
       "   '2782496877']},\n",
       " {'id': '2955025503',\n",
       "  'title': 'Viral and Bacterial Etiology of Acute Febrile Respiratory Syndrome among Patients in Qinghai, China',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '9',\n",
       "  'abstract': 'Objective This study was conducted to investigate the viral and bacterial etiology and epidemiology of patients with acute febrile respiratory syndrome (AFRS) in Qinghai using a commercial routine multiplex-ligation-nucleic acid amplification test (NAT)-based assay. Methods A total of 445 nasopharyngeal swabs specimens from patients with AFRS were analyzed using the RespiFinderSmart22kit (PathoFinder BV, Netherlands) and the LightCycler 480 real-time PCR system. Results Among the 225 (225/445, 51%) positive specimens, 329 positive pathogens were detected, including 298 (90.58%) viruses and 31 (9%) bacteria. The most commonly detected pathogens were infiuenza virus (IFV; 37.39%; 123/329), adenovirus (AdV; 17.02%; 56/329), human coronaviruses (HCoVs; 10.94%; 36/329), rhinovirus/enterovirus (RV/EV; 10.03%; 33/329), parainfiuenza viruses (PIVs; 8.51%; 28/329), and Mycoplasma pneumoniae (M. pneu; 8.51%; 28/329), respectively. Among the co-infected cases (17.53%; 78/445), IFV/AdV and IFV/M. pneu were the most common co-infections. Most of the respiratory viruses were detected in summer and fall. Conclusion In our study, IFV-A was the most common respiratory pathogen among 22 detected pathogens, followed by AdV, HCoV, RV/EV, PIV, and M. pneu. Bacteria appeared less frequently than viruses, and co-infection was the most common phenomenon among viral pathogens. Pathogens were distributed among different age groups and respiratory viruses were generally active in July, September, and November. Enhanced surveillance and early detection can be useful in the diagnosis, treatment, and prevention of AFRS, as well as for guiding the development of appropriate public health strategies.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Gao Shan Liu 1, Hong Li 2, Sheng Cang Zhao 2, Rou Jian Lu 1, Pei Hua Niu 1, Wen Jie Tan 1'],\n",
       "  'related_topics': ['Enterovirus',\n",
       "   'Rhinovirus',\n",
       "   'Mycoplasma pneumoniae',\n",
       "   'Virus',\n",
       "   'Respiratory system',\n",
       "   'Virology',\n",
       "   'Epidemiology',\n",
       "   'Bacteria',\n",
       "   'Medicine',\n",
       "   'Bacterial etiology'],\n",
       "  'references': ['2078917493',\n",
       "   '2287076968',\n",
       "   '2120839730',\n",
       "   '2412526156',\n",
       "   '2081835188',\n",
       "   '2083870139',\n",
       "   '2131338055',\n",
       "   '2097213674',\n",
       "   '2016253387',\n",
       "   '2159773954']},\n",
       " {'id': '2257005270',\n",
       "  'title': 'Coronaviruses and the human airway: a universal system for virus-host interaction studies.',\n",
       "  'reference_count': '110',\n",
       "  'citation_count': '74',\n",
       "  'abstract': 'Human coronaviruses (HCoVs) are large RNA viruses that infect the human respiratory tract. The emergence of both Severe Acute Respiratory Syndrome and Middle East Respiratory syndrome CoVs as well as the yearly circulation of four common CoVs highlights the importance of elucidating the different mechanisms employed by these viruses to evade the host immune response, determine their tropism and identify antiviral compounds. Various animal models have been established to investigate HCoV infection, including mice and non-human primates. To establish a link between the research conducted in animal models and humans, an organotypic human airway culture system, that recapitulates the human airway epithelium, has been developed. Currently, different cell culture systems are available to recapitulate the human airways, including the Air-Liquid Interface (ALI) human airway epithelium (HAE) model. Tracheobronchial HAE cultures recapitulate the primary entry point of human respiratory viruses while the alveolar model allows for elucidation of mechanisms involved in viral infection and pathogenesis in the alveoli. These organotypic human airway cultures represent a universal platform to study respiratory virus-host interaction by offering more detailed insights compared to cell lines. Additionally, the epidemic potential of this virus family highlights the need for both vaccines and antivirals. No commercial vaccine is available but various effective antivirals have been identified, some with potential for human treatment. These morphological airway cultures are also well suited for the identification of antivirals, evaluation of compound toxicity and viral inhibition.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Hulda Run Jonsdottir 1, 2, Ronald Dijkman 1, 2'],\n",
       "  'related_topics': ['Tissue tropism',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Respiratory epithelium',\n",
       "   'Respiratory tract',\n",
       "   'Virus',\n",
       "   'Tropism',\n",
       "   'Immune system',\n",
       "   'Immunity',\n",
       "   'Virology',\n",
       "   'Immunology',\n",
       "   'Biology'],\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '1993577573',\n",
       "   '2119111857',\n",
       "   '2116586125',\n",
       "   '2195009776',\n",
       "   '311927316',\n",
       "   '2167384912',\n",
       "   '2111412754',\n",
       "   '2170933940']},\n",
       " {'id': '2999409984',\n",
       "  'title': 'The continuing 2019-nCoV epidemic threat of novel coronaviruses to global health - The latest 2019 novel coronavirus outbreak in Wuhan, China.',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '1,945',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['David S. Hui 1, Esam Ei Azhar 2, Tariq A. Madani 2, Francine Ntoumi 3, Richard Kock 4, Osman Dar 5, Giuseppe Ippolito 6, Timothy D. Mchugh 7, Ziad A. Memish 8, Christian Drosten 9, Alimuddin Zumla 7, Eskild Petersen 10'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Global health',\n",
       "   'Medicine',\n",
       "   'China',\n",
       "   'Environmental health',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3027264380',\n",
       "   '2981657433',\n",
       "   '2981752008',\n",
       "   '3000092258',\n",
       "   '3029903408',\n",
       "   '2442480670',\n",
       "   '3031559976',\n",
       "   '3023268903',\n",
       "   '3029135544',\n",
       "   '2414957595']},\n",
       " {'id': '2999318660',\n",
       "  'title': 'Outbreak of pneumonia of unknown etiology in Wuhan, China: The mystery and the miracle.',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '2,194',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Hongzhou Lu 1, Charles W. Stratton 2, Yi Wei Tang 3'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Outbreak',\n",
       "   'Etiology',\n",
       "   'Medicine',\n",
       "   'Miracle',\n",
       "   'Virology',\n",
       "   'China',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['2470646526',\n",
       "   '2132260239',\n",
       "   '2255243349',\n",
       "   '2766931063',\n",
       "   '2103503670',\n",
       "   '2134061616',\n",
       "   '1997954607',\n",
       "   '2158887145',\n",
       "   '2121494157']},\n",
       " {'id': '2999364275',\n",
       "  'title': 'Evolution of the novel coronavirus from the ongoing Wuhan outbreak and modeling of its spike protein for risk of human transmission',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '1,532',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Xintian Xu 1, Ping Chen 1, Jingfang Wang 2, Jiannan Feng 3, Hui Zhou 1, Xuan Li 1, Wu Zhong 3, Pei Hao 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Betacoronavirus',\n",
       "   'Transmission (mechanics)',\n",
       "   'Viral Epidemiology',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Spike Protein'],\n",
       "  'references': ['2605343262',\n",
       "   '2775086803',\n",
       "   '2119111857',\n",
       "   '2404280981',\n",
       "   '2060809301',\n",
       "   '1982533785',\n",
       "   '3021832855',\n",
       "   '2126080553',\n",
       "   '3000376083']},\n",
       " {'id': '2909194930',\n",
       "  'title': 'From SARS to MERS, Thrusting Coronaviruses into the Spotlight',\n",
       "  'reference_count': '209',\n",
       "  'citation_count': '810',\n",
       "  'abstract': 'Coronaviruses (CoVs) have formerly been regarded as relatively harmless respiratory pathogens to humans. However, two outbreaks of severe respiratory tract infection, caused by the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV), as a result of zoonotic CoVs crossing the species barrier, caused high pathogenicity and mortality rates in human populations. This brought CoVs global attention and highlighted the importance of controlling infectious pathogens at international borders. In this review, we focus on our current understanding of the epidemiology, pathogenesis, prevention, and treatment of SARS-CoV and MERS-CoV, as well as provides details on the pivotal structure and function of the spike proteins (S proteins) on the surface of each of these viruses. For building up more suitable animal models, we compare the current animal models recapitulating pathogenesis and summarize the potential role of host receptors contributing to diverse host affinity in various species. We outline the research still needed to fully elucidate the pathogenic mechanism of these viruses, to construct reproducible animal models, and ultimately develop countermeasures to conquer not only SARS-CoV and MERS-CoV, but also these emerging coronaviral diseases.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Zhiqi Song 1, Yanfeng Xu 1, Linlin Bao 1, Ling Zhang 1, Pin Yu 1, Yajin Qu 1, Hua Zhu 1, Wenjie Zhao 1, Yunlin Han 1, Chuan Qin 2'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Outbreak'],\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '2470646526',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2107053896',\n",
       "   '2131262274',\n",
       "   '2006434809',\n",
       "   '1993577573',\n",
       "   '2138324310']},\n",
       " {'id': '2991899552',\n",
       "  'title': 'Clinical Features Predicting Mortality Risk in Patients With Viral Pneumonia: The MuLBSTA Score.',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '277',\n",
       "  'abstract': 'Objective The aim of this study was to further clarify clinical characteristics and predict mortality risk among patients with viral pneumonia. Methods A total of 528 patients with viral pneumonia at RuiJin hospital in Shanghai from May 2015 to May 2019 were recruited. Multiplex real-time RT-PCR was used to detect respiratory viruses. Demographic information, comorbidities, routine laboratory examinations, immunological indexes, etiological detections, radiological images and treatment were collected on admission. Results 76 (14.4%) patients died within 90 days in hospital. A predictive MuLBSTA score was calculated on the basis of a multivariate logistic regression model in order to predict mortality with a weighted score that included multilobular infiltrates (OR = 5.20, 95% CI 1.41-12.52, p = 0.010; 5 points), lymphocyte ≤ 0.8∗109/L (OR = 4.53, 95% CI 2.55-8.05, p < 0.001; 4 points), bacterial coinfection (OR = 3.71, 95% CI 2.11-6.51, p < 0.001; 4 points), acute-smoker (OR = 3.19, 95% CI 1.34-6.26, p = 0.001; 3 points), quit-smoker (OR = 2.18, 95% CI 0.99-4.82, p = 0.054; 2 points), hypertension (OR = 2.39, 95% CI 1.55-4.26, p = 0.003; 2 points) and age ≥60 years (OR = 2.14, 95% CI 1.04-4.39, p = 0.038; 2 points). 12 points was used as a cut-off value for mortality risk stratification. This model showed sensitivity of 0.776, specificity of 0.778 and a better predictive ability than CURB-65 (AUROC = 0.773 vs. 0.717, p < 0.001). Conclusion Here, we designed an easy-to-use clinically predictive tool for assessing 90-day mortality risk of viral pneumonia. It can accurately stratify hospitalized patients with viral pneumonia into relevant risk categories and could provide guidance to make further clinical decisions.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Lingxi Guo 1, Dong Wei 1, Xinxin Zhang 1, Yurong Wu 2, Qingyun Li 1, Min Zhou 1, Jieming Qu 1'],\n",
       "  'related_topics': ['Viral pneumonia',\n",
       "   'Etiology',\n",
       "   'Internal medicine',\n",
       "   'Coinfection',\n",
       "   'Medicine',\n",
       "   'Hospitalized patients',\n",
       "   'In patient',\n",
       "   'Multivariate logistic regression model',\n",
       "   'Routine laboratory',\n",
       "   'Weighted score'],\n",
       "  'references': ['2133979383',\n",
       "   '2065974896',\n",
       "   '2159340685',\n",
       "   '2091139031',\n",
       "   '2103645914',\n",
       "   '2948483377',\n",
       "   '2021046603',\n",
       "   '2155020492',\n",
       "   '1975461687',\n",
       "   '2900850632']},\n",
       " {'id': '3000834295',\n",
       "  'title': 'Coronavirus Infections-More Than Just the Common Cold.',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '1,469',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Catharine I. Paules 1, Hilary D. Marston 2, Anthony S. Fauci 2'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Common cold',\n",
       "   'Betacoronavirus',\n",
       "   'Medicine',\n",
       "   'Virology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3000413850',\n",
       "   '2470646526',\n",
       "   '2909194930',\n",
       "   '3027659922',\n",
       "   '3027264380',\n",
       "   '2718090702',\n",
       "   '1993435091',\n",
       "   '2792208289',\n",
       "   '3030422584',\n",
       "   '3023275846']},\n",
       " {'id': '3003951199',\n",
       "  'title': 'Importation and Human-to-Human Transmission of a Novel Coronavirus in Vietnam.',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '1,057',\n",
       "  'abstract': 'Human-to-Human Coronavirus Transmission in Vietnam The authors describe transmission of 2019-nCoV from a father, who had flown with his wife from Wuhan to Hanoi, to the son, who met his father and ...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Lan T. Phan 1, Thuong V. Nguyen 1, Quang C. Luong 1, Thinh V. Nguyen 1, Hieu T. Nguyen 1, Hung Q. Le 2, Thuc T. Nguyen 2, Thang M. Cao 3, Quang D. Pham 3'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Transmission (mechanics)',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Lung pathology',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral transmission'],\n",
       "  'references': ['3001897055']},\n",
       " {'id': '1803784511',\n",
       "  'title': 'Acute respiratory distress syndrome: the Berlin Definition.',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '8,639',\n",
       "  'abstract': 'The acute respiratory distress syndrome (ARDS) was defined in 1994 by the American-European Consensus Conference (AECC); since then, issues regarding the reliability and validity of this definition have emerged. Using a consensus process, a panel of experts convened in 2011 (an initiative of the European Society of Intensive Care Medicine endorsed by the American Thoracic Society and the Society of Critical Care Medicine) developed the Berlin Definition, focusing on feasibility, reliability, validity, and objective evaluation of its performance. A draft definition proposed 3 mutually exclusive categories of ARDS based on degree of hypoxemia: mild (200 mm Hg < PaO2/FIO2 ≤ 300 mm Hg), moderate (100 mm Hg < PaO2/FIO2 ≤ 200 mm Hg), and severe (PaO2/FIO2 ≤ 100 mm Hg) and 4 ancillary variables for severe ARDS: radiographic severity, respiratory system compliance (≤40 mL/cm H2O), positive end-expiratory pressure (≥10 cm H2O), and corrected expired volume per minute (≥10 L/min). The draft Berlin Definition was empirically evaluated using patient-level meta-analysis of 4188 patients with ARDS from 4 multicenter clinical data sets and 269 patients with ARDS from 3 single-center data sets containing physiologic information. The 4 ancillary variables did not contribute to the predictive validity of severe ARDS for mortality and were removed from the definition. Using the Berlin Definition, stages of mild, moderate, and severe ARDS were associated with increased mortality (27%; 95% CI, 24%-30%; 32%; 95% CI, 29%-34%; and 45%; 95% CI, 42%-48%, respectively; P < .001) and increased median duration of mechanical ventilation in survivors (5 days; interquartile [IQR], 2-11; 7 days; IQR, 4-14; and 9 days; IQR, 5-17, respectively; P < .001). Compared with the AECC definition, the final Berlin Definition had better predictive validity for mortality, with an area under the receiver operating curve of 0.577 (95% CI, 0.561-0.593) vs 0.536 (95% CI, 0.520-0.553; P < .001). This updated and revised Berlin Definition for ARDS addresses a number of the limitations of the AECC definition. The approach of combining consensus discussions with empirical evaluation may serve as a model to create more accurate, evidence-based, critical illness syndrome definitions and to better inform clinical care, research, and health services planning.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Ards Definition Task Force 1, V Marco Ranieri 1, Gordon D Rubenfeld 2, B Taylor Thompson 2, Niall D Ferguson 3, Ellen Caldwell 2, Eddy Fan 4, Luigi Camporota 2, 5, Arthur S Slutsky 2'],\n",
       "  'related_topics': ['ARDS', 'Prone ventilation', 'Interquartile range'],\n",
       "  'references': ['2161328469',\n",
       "   '2068854215',\n",
       "   '2328176404',\n",
       "   '2326364273',\n",
       "   '1823772832',\n",
       "   '1979469936',\n",
       "   '2113752525',\n",
       "   '2070070465',\n",
       "   '2168829312',\n",
       "   '2074935456']},\n",
       " {'id': '3002533507',\n",
       "  'title': 'A Novel Coronavirus Emerging in China - Key Questions for Impact Assessment.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,083',\n",
       "  'abstract': 'A Novel Coronavirus Emerging in China A novel coronavirus, designated as 2019-nCoV, emerged in Wuhan, China, at the end of 2019. Although many details of the emergence of this virus remain unknown,...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Vincent J. Munster 1, Marion Koopmans 2, Neeltje van Doremalen 1, Debby van Riel 2, Emmie de Wit 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'China',\n",
       "   'Impact assessment',\n",
       "   'Economic growth',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3003668884',\n",
       "   '3012099172',\n",
       "   '3009912996',\n",
       "   '3012284084',\n",
       "   '3006645647',\n",
       "   '3015792206',\n",
       "   '3009834387',\n",
       "   '3012731499',\n",
       "   '3003901880',\n",
       "   '3012454642']},\n",
       " {'id': '3002715510',\n",
       "  'title': 'Another Decade, Another Coronavirus.',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '750',\n",
       "  'abstract': 'For the third time in as many decades, a zoonotic coronavirus has crossed species to infect human populations. This virus, provisionally called 2019-nCoV, was first identified in Wuhan, China, in p...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Stanley Perlman'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Virus',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus Infections',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001897055',\n",
       "   '2801339009',\n",
       "   '2126707939',\n",
       "   '2156273941',\n",
       "   '2217313808',\n",
       "   '2538584349',\n",
       "   '96734778',\n",
       "   '2002481497']},\n",
       " {'id': '3001971765',\n",
       "  'title': 'Real-time tentative assessment of the epidemiological characteristics of novel coronavirus infections in Wuhan, China, as at 22 January 2020.',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '408',\n",
       "  'abstract': 'A novel coronavirus (2019-nCoV) causing severe acute respiratory disease emerged recently in Wuhan, China. Information on reported cases strongly indicates human-to-human spread, and the most recent information is increasingly indicative of sustained human-to-human transmission. While the overall severity profile among cases may change as more mild cases are identified, we estimate a risk of fatality among hospitalised cases at 14% (95% confidence interval: 3.9–32%).',\n",
       "  'date': 2020,\n",
       "  'authors': ['Peng Wu',\n",
       "   'Xinxin Hao',\n",
       "   'Eric H Y Lau',\n",
       "   'Jessica Y Wong',\n",
       "   'Kathy S M Leung',\n",
       "   'Joseph T Wu',\n",
       "   'Benjamin J Cowling',\n",
       "   'Gabriel M Leung'],\n",
       "  'related_topics': ['Communicable disease',\n",
       "   'Coronavirus',\n",
       "   'Epidemiology',\n",
       "   'Risk assessment',\n",
       "   'Confidence interval',\n",
       "   'Transmission (medicine)',\n",
       "   'Pediatrics',\n",
       "   'Public health',\n",
       "   'Medicine',\n",
       "   'China'],\n",
       "  'references': ['2306794997',\n",
       "   '1909499787',\n",
       "   '2918873120',\n",
       "   '2801339009',\n",
       "   '2227495319',\n",
       "   '2534644646',\n",
       "   '1042757214',\n",
       "   '2156614913',\n",
       "   '2109088393',\n",
       "   '2127974353']},\n",
       " {'id': '2147166346',\n",
       "  'title': 'Transmission Dynamics and Control of Severe Acute Respiratory Syndrome',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '1,565',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) is a recently described illness of humans that has spread widely over the past 6 months. With the use of detailed epidemiologic data from Singapore and epidemic curves from other settings, we estimated the reproductive number for SARS in the absence of interventions and in the presence of control efforts. We estimate that a single infectious case of SARS will infect about three secondary cases in a population that has not yet instituted control measures. Public-health efforts to reduce transmission are expected to have a substantial impact on reducing the size of the epidemic.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Marc Lipsitch 1, Ted Cohen 1, Ben Cooper 1, James M. Robins 1, Stefan Ma 2, Lyn James 2, Gowri Gopalakrishna 2, Suok Kai Chew 2, Chorh Chuan Tan 2, Matthew H. Samore 3, David Fisman 4, Megan Murray 1'],\n",
       "  'related_topics': ['Population',\n",
       "   'Serial interval',\n",
       "   'Transmission (mechanics)',\n",
       "   'Epidemiology',\n",
       "   'Contact tracing',\n",
       "   'Intensive care medicine',\n",
       "   'Viral disease',\n",
       "   'Respiratory disease',\n",
       "   'Psychological intervention'],\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '1606697907',\n",
       "   '2011756067',\n",
       "   '2318510691',\n",
       "   '1965399019',\n",
       "   '1979065938']},\n",
       " {'id': '2149508011',\n",
       "  'title': 'Evidence for camel-to-human transmission of MERS coronavirus',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '774',\n",
       "  'abstract': 'We describe the isolation and sequencing of Middle East respiratory syndrome coronavirus (MERS-CoV) obtained from a dromedary camel and from a patient who died of laboratory-confirmed MERS-CoV infection after close contact with camels that had rhinorrhea. Nasal swabs collected from the patient and from one of his nine camels were positive for MERS-CoV RNA. In addition, MERS-CoV was isolated from the patient and the camel. The full genome sequences of the two isolates were identical. Serologic data indicated that MERS-CoV was circulating in the camels but not in the patient before the human infection occurred. These data suggest that this fatal case of human MERS-CoV infection was transmitted through close contact with an infected camel.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Esam I. Azhar',\n",
       "   'Sherif A. El-Kafrawy',\n",
       "   'Suha A. Farraj',\n",
       "   'Ahmed M. Hassan',\n",
       "   'Muneera S. Al-Saeed',\n",
       "   'Anwar M. Hashem',\n",
       "   'Tariq A. Madani'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Serology',\n",
       "   'Transmission (medicine)',\n",
       "   'rhinorrhea',\n",
       "   'Isolation (health care)',\n",
       "   'Virology',\n",
       "   'Nasal Swab',\n",
       "   'Biology',\n",
       "   'Close contact',\n",
       "   'Dromedary camel'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2160011624',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2113457186',\n",
       "   '2145441153',\n",
       "   '2119775949',\n",
       "   '1690366459',\n",
       "   '2049975503']},\n",
       " {'id': '2103503670',\n",
       "  'title': 'Bats are natural reservoirs of SARS-like coronaviruses.',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '2,301',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) emerged in 2002 to 2003 in southern China. The origin of its etiological agent, the SARS coronavirus (SARS-CoV), remains elusive. Here we report that species of bats are a natural host of coronaviruses closely related to those responsible for the SARS outbreak. These viruses, termed SARS-like coronaviruses (SL-CoVs), display greater genetic variation than SARS-CoV isolated from humans or from civets. The human and civet isolates of SARS-CoV nestle phylogenetically within the spectrum of SL-CoVs, indicating that the virus responsible for the SARS outbreak was a member of this coronavirus group.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Wendong Li',\n",
       "   'Zhengli Shi',\n",
       "   'Meng Yu',\n",
       "   'Wuze Ren',\n",
       "   'Craig Smith',\n",
       "   'Jonathan H. Epstein',\n",
       "   'Hanzhong Wang',\n",
       "   'Gary Crameri',\n",
       "   'Zhihong Hu',\n",
       "   'Huajun Zhang',\n",
       "   'Jianhong Zhang',\n",
       "   'Jennifer McEachern',\n",
       "   'Hume Field',\n",
       "   'Peter Daszak',\n",
       "   'Bryan T. Eaton',\n",
       "   'Shuyi Zhang',\n",
       "   'Lin-Fa Wang'],\n",
       "  'related_topics': ['Coronavirus', 'Alphacoronavirus', 'Disease reservoir'],\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2134061616',\n",
       "   '1990059132',\n",
       "   '2127949919',\n",
       "   '96734778',\n",
       "   '2076620790',\n",
       "   '2042499956']},\n",
       " {'id': '2807736175',\n",
       "  'title': 'Saliva as a diagnostic specimen for testing respiratory virus by a point-of-care molecular assay: a diagnostic validity study.',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '113',\n",
       "  'abstract': 'Abstract Objectives Automated point-of-care molecular assays have greatly shortened the turnaround time of respiratory virus testing. One of the major bottlenecks now lies at the specimen collection step, especially in a busy clinical setting. Saliva is a convenient specimen type that can be provided easily by adult patients. This study assessed the diagnostic validity, specimen collection time and cost associated with the use of saliva. Methods This was a prospective diagnostic validity study comparing the detection rate of respiratory viruses between saliva and nasopharyngeal aspirate (NPA) among adult hospitalized patients using Xpert® Xpress Flu/RSV. The cost and time associated with the collection of saliva and nasopharyngeal specimens were also estimated. Results Between July and October 2017, 214 patients were recruited. The overall agreement between saliva and NPA was 93.3% (196/210, κ 0.851, 95% CI 0.776–0.926). There was no significant difference in the detection rate of respiratory viruses between saliva and NPA (32.9% (69/210) versus 35.7% (75/210); p 0.146). The overall sensitivity and specificity were 90.8% (81.9%–96.2%) and 100% (97.3%–100%), respectively, for saliva, and were 96.1% (88.9%–99.2%) and 98.5% (94.7%–99.8%), respectively, for NPA. The time and cost associated with the collection of saliva were 2.26-fold and 2.59-fold lower, respectively, than those of NPA. Conclusions Saliva specimens have high sensitivity and specificity in the detection of respiratory viruses by an automated multiplex Clinical Laboratory Improvement Amendments-waived point-of-care molecular assay when compared with those of NPA. The use of saliva also reduces the time and cost associated with specimen collection.',\n",
       "  'date': 2019,\n",
       "  'authors': ['K.K.W. To 1, C.C.Y. Yip 2, 3, C.Y.W. Lai 2, 3, C.K.H. Wong 3, D.T.Y. Ho 3, P.K.P. Pang 3, A.C.K. Ng 3, K.-H. Leung 2, R.W.S. Poon 2, 3, K.-H. Chan 3, V.C.C. Cheng 2, 3, I.F.N. Hung 3, 4, K.-Y. Yuen 1'],\n",
       "  'related_topics': ['Specimen collection', 'Respiratory virus', 'Saliva'],\n",
       "  'references': ['2000714505',\n",
       "   '2164777277',\n",
       "   '2102263282',\n",
       "   '2103338644',\n",
       "   '2752904261',\n",
       "   '2605374894',\n",
       "   '2101172433',\n",
       "   '2623181050',\n",
       "   '2752140764',\n",
       "   '2548288333']},\n",
       " {'id': '2105637133',\n",
       "  'title': 'Discovery of Seven Novel Mammalian and Avian Coronaviruses in the Genus Deltacoronavirus Supports Bat Coronaviruses as the Gene Source of Alphacoronavirus and Betacoronavirus and Avian Coronaviruses as the Gene Source of Gammacoronavirus and Deltacoronavirus',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '1,124',\n",
       "  'abstract': 'Recently, we reported the discovery of three novel coronaviruses, bulbul coronavirus HKU11, thrush coronavirus HKU12, and munia coronavirus HKU13, which were identified as representatives of a novel genus, Deltacoronavirus, in the subfamily Coronavirinae. In this territory-wide molecular epidemiology study involving 3,137 mammals and 3,298 birds, we discovered seven additional novel deltacoronaviruses in pigs and birds, which we named porcine coronavirus HKU15, white-eye coronavirus HKU16, sparrow coronavirus HKU17, magpie robin coronavirus HKU18, night heron coronavirus HKU19, wigeon coronavirus HKU20, and common moorhen coronavirus HKU21. Complete genome sequencing and comparative genome analysis showed that the avian and mammalian deltacoronaviruses have similar genome characteristics and structures. They all have relatively small genomes (25.421 to 26.674 kb), the smallest among all coronaviruses. They all have a single papain-like protease domain in the nsp3 gene; an accessory gene, NS6 open reading frame (ORF), located between the M and N genes; and a variable number of accessory genes (up to four) downstream of the N gene. Moreover, they all have the same putative transcription regulatory sequence of ACACCA. Molecular clock analysis showed that the most recent common ancestor of all coronaviruses was estimated at approximately 8100 BC, and those of Alphacoronavirus, Betacoronavirus, Gammacoronavirus, and Deltacoronavirus were at approximately 2400 BC, 3300 BC, 2800 BC, and 3000 BC, respectively. From our studies, it appears that bats and birds, the warm blooded flying vertebrates, are ideal hosts for the coronavirus gene source, bats for Alphacoronavirus and Betacoronavirus and birds for Gammacoronavirus and Deltacoronavirus, to fuel coronavirus evolution and dissemination.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Patrick C. Y. Woo 1, Susanna K. P. Lau 2, Carol S. F. Lam 3, Candy C. Y. Lau 3, Alan K. L. Tsang 3, John H. N. Lau 3, Ru Bai 3, Jade L. L. Teng 3, Chris C. C. Tsang 3, Ming Wang 1, Bo-Jian Zheng 2, Kwok-Hung Chan 3, Kwok-Yung Yuen 2'],\n",
       "  'related_topics': ['Coronavirinae',\n",
       "   'Bulbul coronavirus HKU11',\n",
       "   'Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Alphacoronavirus',\n",
       "   'Deltacoronavirus',\n",
       "   'Gammacoronavirus',\n",
       "   'Gene',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2141885858',\n",
       "   '2025170735',\n",
       "   '2110835349',\n",
       "   '2116586125',\n",
       "   '2169198329',\n",
       "   '2103503670',\n",
       "   '2134061616',\n",
       "   '2056584399',\n",
       "   '2111412754',\n",
       "   '2140338292']},\n",
       " {'id': '2889758689',\n",
       "  'title': 'Genomic characterization and infectivity of a novel SARS-like coronavirus in Chinese bats',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '198',\n",
       "  'abstract': 'SARS coronavirus (SARS-CoV), the causative agent of the large SARS outbreak in 2003, originated in bats. Many SARS-like coronaviruses (SL-CoVs) have been detected in bats, particularly those that reside in China, Europe, and Africa. To further understand the evolutionary relationship between SARS-CoV and its reservoirs, 334 bats were collected from Zhoushan city, Zhejiang province, China, between 2015 and 2017. PCR amplification of the conserved coronaviral protein RdRp detected coronaviruses in 26.65% of bats belonging to this region, and this number was influenced by seasonal changes. Full genomic analyses of the two new SL-CoVs from Zhoushan (ZXC21 and ZC45) showed that their genomes were 29,732 nucleotides (nt) and 29,802 nt in length, respectively, with 13 open reading frames (ORFs). These results revealed 81% shared nucleotide identity with human/civet SARS CoVs, which was more distant than that observed previously for bat SL-CoVs in China. Importantly, using pathogenic tests, we found that the virus can reproduce and cause disease in suckling rats, and further studies showed that the virus-like particles can be observed in the brains of suckling rats by electron microscopy. Thus, this study increased our understanding of the genetic diversity of the SL-CoVs carried by bats and also provided a new perspective to study the possibility of cross-species transmission of SL-CoVs using suckling rats as an animal model.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Dan Hu 1, Changqiang Zhu 2, Lele Ai 2, Ting He 2, Yi Wang 3, Fuqiang Ye 2, Lu Yang 2, Chenxi Ding 2, Xuhui Zhu 2, Ruicheng Lv 2, Jin Zhu 2, Bachar Hassan 4, Youjun Feng 5, Weilong Tan 2, Changjun Wang 1'],\n",
       "  'related_topics': ['ORFS',\n",
       "   'Infectivity',\n",
       "   'Phylogenetics',\n",
       "   'Civet',\n",
       "   'Outbreak',\n",
       "   'Virus',\n",
       "   'Polymerase chain reaction',\n",
       "   'Virulence',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2311203695',\n",
       "   '2104548316',\n",
       "   '1993577573',\n",
       "   '2775086803',\n",
       "   '2169198329',\n",
       "   '2298153446',\n",
       "   '2046153984',\n",
       "   '2141877163',\n",
       "   '2140338292',\n",
       "   '2141008678']},\n",
       " {'id': '2769543984',\n",
       "  'title': 'Human intestinal tract serves as an alternative infection route for Middle East respiratory syndrome coronavirus',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '280',\n",
       "  'abstract': 'Middle East respiratory syndrome coronavirus (MERS-CoV) has caused human respiratory infections with a high case fatality rate since 2012. However, the mode of virus transmission is not well understood. The findings of epidemiological and virological studies prompted us to hypothesize that the human gastrointestinal tract could serve as an alternative route to acquire MERS-CoV infection. We demonstrated that human primary intestinal epithelial cells, small intestine explants, and intestinal organoids were highly susceptible to MERS-CoV and can sustain robust viral replication. We also identified the evidence of enteric MERS-CoV infection in the stool specimen of a clinical patient. MERS-CoV was considerably resistant to fed-state gastrointestinal fluids but less tolerant to highly acidic fasted-state gastric fluid. In polarized Caco-2 cells cultured in Transwell inserts, apical MERS-CoV inoculation was more effective in establishing infection than basolateral inoculation. Notably, direct intragastric inoculation of MERS-CoV caused a lethal infection in human DPP4 transgenic mice. Histological examination revealed MERS-CoV enteric infection in all inoculated mice, as shown by the presence of virus-positive cells, progressive inflammation, and epithelial degeneration in small intestines, which were exaggerated in the mice pretreated with the proton pump inhibitor pantoprazole. With the progression of the enteric infection, inflammation, virus-positive cells, and live viruses emerged in the lung tissues, indicating the development of sequential respiratory infection. Taken together, these data suggest that the human intestinal tract may serve as an alternative infection route for MERS-CoV.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Jie Zhou 1, Cun Li 1, Guangyu Zhao 2, Hin Chu 1, Dong Wang 1, Helen Hoi-Ning Yan 1, Vincent Kwok-Man Poon 1, Lei Wen 1, Bosco Ho-Yin Wong 1, Xiaoyu Zhao 1, Man Chun Chiu 1, Dong Yang 1, Yixin Wang 1, Rex K. H. Au-Yeung 1, Ivy Hau-Yee Chan 3, Shihui Sun 2, Jasper Fuk-Woo Chan 4, Kelvin Kai-Wang To 4, Ziad Ahmed Memish 5, 6, Victor M. Corman 7, Christian Drosten 7, Ivan Fan-Ngai Hung 1, Yusen Zhou 2, Suet Yi Leung 1, Kwok-Yung Yuen 4'],\n",
       "  'related_topics': ['Respiratory infection',\n",
       "   'Human gastrointestinal tract',\n",
       "   'Small intestine'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '2115555188',\n",
       "   '2045002682',\n",
       "   '2002513358',\n",
       "   '2144410942',\n",
       "   '1757215199',\n",
       "   '1947409115']},\n",
       " {'id': '2140338292',\n",
       "  'title': 'Severe acute respiratory syndrome coronavirus-like virus in Chinese horseshoe bats',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '1,733',\n",
       "  'abstract': 'Although the finding of severe acute respiratory syndrome coronavirus (SARS-CoV) in caged palm civets from live animal markets in China has provided evidence for interspecies transmission in the genesis of the SARS epidemic, subsequent studies suggested that the civet may have served only as an amplification host for SARS-CoV. In a surveillance study for CoV in noncaged animals from the wild areas of the Hong Kong Special Administration Region, we identified a CoV closely related to SARS-CoV (bat-SARS-CoV) from 23 (39%) of 59 anal swabs of wild Chinese horseshoe bats (Rhinolophus sinicus) by using RT-PCR. Sequencing and analysis of three bat-SARS-CoV genomes from samples collected at different dates showed that bat-SARS-CoV is closely related to SARS-CoV from humans and civets. Phylogenetic analysis showed that bat-SARS-CoV formed a distinct cluster with SARS-CoV as group 2b CoV, distantly related to known group 2 CoV. Most differences between the bat-SARS-CoV and SARS-CoV genomes were observed in the spike genes, ORF 3 and ORF 8, which are the regions where most variations also were observed between human and civet SARS-CoV genomes. In addition, the presence of a 29-bp insertion in ORF 8 of bat-SARS-CoV genome, not in most human SARS-CoV genomes, suggests that it has a common ancestor with civet SARS-CoV. Antibody against recombinant bat-SARS-CoV nucleocapsid protein was detected in 84% of Chinese horseshoe bats by using an enzyme immunoassay. Neutralizing antibody to human SARS-CoV also was detected in bats with lower viral loads. Precautions should be exercised in the handling of these animals.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Susanna K. P. Lau',\n",
       "   'Patrick C. Y. Woo',\n",
       "   'Kenneth S. M. Li',\n",
       "   'Yi Huang',\n",
       "   'Hoi-Wah Tsoi',\n",
       "   'Beatrice H. L. Wong',\n",
       "   'Samson S. Y. Wong',\n",
       "   'Suet-Yi Leung',\n",
       "   'Kwok-Hung Chan',\n",
       "   'Kwok-Yung Yuen'],\n",
       "  'related_topics': ['Civet',\n",
       "   'Alphacoronavirus',\n",
       "   'Rhinolophus sinicus',\n",
       "   'Phylogenetics',\n",
       "   'Genome',\n",
       "   'Virus',\n",
       "   'Phylogenetic tree',\n",
       "   'Sequence analysis',\n",
       "   'Virology',\n",
       "   'Genetics',\n",
       "   'Biology'],\n",
       "  'references': ['2141885858',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2116586125',\n",
       "   '1966238900',\n",
       "   '2169198329',\n",
       "   '2171091522',\n",
       "   '2134061616',\n",
       "   '2111412754']},\n",
       " {'id': '3004280078',\n",
       "  'title': 'A pneumonia outbreak associated with a new coronavirus of probable bat origin',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '11,692',\n",
       "  'abstract': 'Since the outbreak of severe acute respiratory syndrome (SARS) 18 years ago, a large number of SARS-related coronaviruses (SARSr-CoVs) have been discovered in their natural reservoir host, bats1-4. Previous studies have shown that some bat SARSr-CoVs have the potential to infect humans5-7. Here we report the identification and characterization of a new coronavirus (2019-nCoV), which caused an epidemic of acute respiratory syndrome in humans in Wuhan, China. The epidemic, which started on 12 December 2019, had caused 2,794 laboratory-confirmed infections including 80 deaths by 26 January 2020. Full-length genome sequences were obtained from five patients at an early stage of the outbreak. The sequences are almost identical and share 79.6% sequence identity to SARS-CoV. Furthermore, we show that 2019-nCoV is 96% identical at the whole-genome level to a bat coronavirus. Pairwise protein sequence analysis of seven conserved non-structural proteins domains show that this virus belongs to the species of SARSr-CoV. In addition, 2019-nCoV virus isolated from the bronchoalveolar lavage fluid of a critically ill patient could be neutralized by sera from several patients. Notably, we confirmed that 2019-nCoV uses the same cell entry receptor-angiotensin converting enzyme II (ACE2)-as SARS-CoV.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Peng Zhou 1, Xing Lou Yang 1, Xian Guang Wang 2, Ben Hu 1, Lei Zhang 1, Wei Zhang 1, Hao Rui Si 1, Yan Zhu 1, Bei Li 1, Chao Lin Huang 2, Hui Dong Chen 2, Jing Chen 1, Yun Luo 1, Hua Guo 1, Ren Di Jiang 1, Mei Qin Liu 1, Ying Chen 1, Xu Rui Shen 1, Xi Wang 1, Xiao Shuang Zheng 1, Kai Zhao 1, Quan Jiao Chen 1, Fei Deng 1, Lin Lin Liu 3, Bing Yan 1, Fa Xian Zhan 3, Yan Yi Wang 1, Geng Fu Xiao 1, Zheng Li Shi 1'],\n",
       "  'related_topics': ['Coronavirus', 'Betacoronavirus', 'Outbreak'],\n",
       "  'references': ['2903899730',\n",
       "   '2166867592',\n",
       "   '2132260239',\n",
       "   '1993577573',\n",
       "   '2775086803',\n",
       "   '1966238900',\n",
       "   '2195009776',\n",
       "   '2918873120',\n",
       "   '2103503670',\n",
       "   '2298153446']},\n",
       " {'id': '2103441770',\n",
       "  'title': 'Fast and accurate short read alignment with Burrows–Wheeler transform',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '30,028',\n",
       "  'abstract': 'Motivation: The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals. Results: We implemented Burrows-Wheeler Alignment tool (BWA), a new read alignment package that is based on backward search with Burrows–Wheeler Transform (BWT), to efficiently align short sequencing reads against a large reference sequence such as the human genome, allowing mismatches and gaps. BWA supports both base space reads, e.g. from Illumina sequencing machines, and color space reads from AB SOLiD machines. Evaluations on both simulated and real data suggest that BWA is ~10–20× faster than MAQ, while achieving similar accuracy. In addition, BWA outputs alignment in the new standard SAM (Sequence Alignment/Map) format. Variant calling and other downstream analyses after the alignment can be achieved with the open source SAMtools software package. Availability: http://maq.sourceforge.net Contact: [email protected]',\n",
       "  'date': 2009,\n",
       "  'authors': ['Heng Li', 'Richard Durbin'],\n",
       "  'related_topics': ['Hybrid genome assembly',\n",
       "   'Sequence assembly',\n",
       "   '2 base encoding',\n",
       "   'Peak calling',\n",
       "   'DNA sequencing theory',\n",
       "   'Paired-end tag',\n",
       "   'Burrows–Wheeler transform',\n",
       "   'FASTQ format',\n",
       "   'Variant Call Format',\n",
       "   'Deep sequencing',\n",
       "   'Reference genome',\n",
       "   'ABI Solid Sequencing',\n",
       "   'Minion',\n",
       "   'Nanopore sequencing',\n",
       "   'Shotgun sequencing',\n",
       "   'Massive parallel sequencing',\n",
       "   'ChIP-exo',\n",
       "   'Algorithm',\n",
       "   'Sequence alignment',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Cancer genome sequencing',\n",
       "   'Genomics',\n",
       "   'Sequence analysis',\n",
       "   'DNA sequencing',\n",
       "   'MALBAC',\n",
       "   'Illumina dye sequencing',\n",
       "   'Structural variation',\n",
       "   'Genomic Structural Variation',\n",
       "   'Exome sequencing',\n",
       "   'DNase-Seq',\n",
       "   'Human genome',\n",
       "   'Genome',\n",
       "   'INDEL Mutation',\n",
       "   'Variome',\n",
       "   'ATAC-seq',\n",
       "   'PAR-CLIP',\n",
       "   'Indel',\n",
       "   'Population genomics',\n",
       "   'Exome',\n",
       "   'Ancient DNA',\n",
       "   'Selective sweep',\n",
       "   'PRDM9',\n",
       "   'Kataegis',\n",
       "   'Candidate Gene Identification',\n",
       "   'DNA',\n",
       "   'POLD1',\n",
       "   'Mutation Accumulation',\n",
       "   'Genome resequencing',\n",
       "   'Genotyping by sequencing',\n",
       "   'Structural variant'],\n",
       "  'references': ['2108234281',\n",
       "   '2158714788',\n",
       "   '2124985265',\n",
       "   '2112113834',\n",
       "   '2136145671',\n",
       "   '2139760555',\n",
       "   '2015292449',\n",
       "   '2132341951',\n",
       "   '2055666215',\n",
       "   '2142619120']},\n",
       " {'id': '2141052558',\n",
       "  'title': 'RAxML version 8: a tool for phylogenetic analysis and post-analysis of large phylogenies.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '18,694',\n",
       "  'abstract': 'Motivation: Phylogenies are increasingly used in all fields of medical and biological research. Moreover, because of the next-generation sequencing revolution, datasets used for conducting phylogenetic analyses grow at an unprecedented pace. RAxML (Randomized Axelerated Maximum Likelihood) is a popular program for phylogenetic analyses of large datasets under maximum likelihood. Since the last RAxML paper in 2006, it has been continuously maintained and extended to accommodate the increasingly growing input datasets and to serve the needs of the user community. Results: I present some of the most notable new features and extensions of RAxML, such as a substantial extension of substitution models and supported data types, the introduction of SSE3, AVX and AVX2 vector intrinsics, techniques for reducing the memory requirements of the code and a plethora of operations for conducting post-analyses on sets of trees. In addition, an up-to-date 50-page user manual covering all new RAxML options is available. Availability and implementation: The code is available under GNU GPL at https://github.com/stamatak/standard-RAxML. Contact: gro.sti-h@sikatamats.sordnaxela Supplementary information: Supplementary data are available at Bioinformatics online.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Alexandros Stamatakis'],\n",
       "  'related_topics': ['Intrinsics',\n",
       "   'Data type',\n",
       "   'Supermatrix',\n",
       "   'SSE3',\n",
       "   'Phylogenomics',\n",
       "   'Data mining',\n",
       "   'Code (cryptography)',\n",
       "   'Phylogenetic tree',\n",
       "   'Software',\n",
       "   'Biology'],\n",
       "  'references': ['2111211467',\n",
       "   '2168696662',\n",
       "   '2127847431',\n",
       "   '1794270752',\n",
       "   '2012220164',\n",
       "   '2068187483',\n",
       "   '2151736966',\n",
       "   '2100030044',\n",
       "   '2122082385',\n",
       "   '2156921764']},\n",
       " {'id': '2804822363',\n",
       "  'title': 'SWISS-MODEL: homology modelling of protein structures and complexes.',\n",
       "  'reference_count': '69',\n",
       "  'citation_count': '3,503',\n",
       "  'abstract': 'Homology modelling has matured into an important technique in structural biology, significantly contributing to narrowing the gap between known protein sequences and experimentally determined structures. Fully automated workflows and servers simplify and streamline the homology modelling process, also allowing users without a specific computational expertise to generate reliable protein models and have easy access to modelling results, their visualization and interpretation. Here, we present an update to the SWISS-MODEL server, which pioneered the field of automated modelling 25 years ago and been continuously further developed. Recently, its functionality has been extended to the modelling of homo- and heteromeric complexes. Starting from the amino acid sequences of the interacting proteins, both the stoichiometry and the overall structure of the complex are inferred by homology modelling. Other major improvements include the implementation of a new modelling engine, ProMod3 and the introduction a new local model quality estimation method, QMEANDisCo. SWISS-MODEL is freely available at https://swissmodel.expasy.org.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Andrew Waterhouse 1, 2, Martino Bertoni 1, 2, Stefan Bienert 1, 2, Gabriel Studer 1, 2, Gerardo Tauriello 1, 2, Rafal Gumienny 1, 2, Florian T Heer 1, 2, Tjaart A P de Beer 1, 2, Christine Rempfer 1, 2, Lorenza Bordoli 1, 2, Rosalba Lepore 1, 2, Torsten Schwede 1, 2'],\n",
       "  'related_topics': ['Structural biology', 'Server', 'Visualization'],\n",
       "  'references': ['2158714788',\n",
       "   '2142678478',\n",
       "   '2149525061',\n",
       "   '2152301430',\n",
       "   '2154139219',\n",
       "   '2015642465',\n",
       "   '2060809301',\n",
       "   '2065283382',\n",
       "   '2051210555',\n",
       "   '2159614853']},\n",
       " {'id': '2991491848',\n",
       "  'title': 'A Randomized, Controlled Trial of Ebola Virus Disease Therapeutics.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '903',\n",
       "  'abstract': 'Abstract Background Although several experimental therapeutics for Ebola virus disease (EVD) have been developed, the safety and efficacy of the most promising therapies need to be assessed in the ...',\n",
       "  'date': 2019,\n",
       "  'authors': ['Mulangu S',\n",
       "   'Dodd Le 1, Davey Rt',\n",
       "   'Tshiani Mbaya O 2, Proschan M 3, Mukadi D 4, Lusakibanza Manzo M 5, Nzolo D 6, Tshomba Oloma A',\n",
       "   'Ibanda A',\n",
       "   'Ali R 7, Coulibaly S',\n",
       "   'Levine Ac',\n",
       "   'Grais R',\n",
       "   'Diaz J',\n",
       "   'Lane Hc',\n",
       "   'Muyembe-Tamfum Jj 8, Sivahera B',\n",
       "   'Camara M',\n",
       "   'Kojan R',\n",
       "   'Walker R',\n",
       "   'Dighero-Kemp B',\n",
       "   'Cao H',\n",
       "   'Mukumbayi P',\n",
       "   'Mbala-Kingebeni P',\n",
       "   'Ahuka S',\n",
       "   'Albert S',\n",
       "   'Bonnett T',\n",
       "   'Crozier I',\n",
       "   'Duvenhage M',\n",
       "   'Proffitt C',\n",
       "   'Teitelbaum M',\n",
       "   'Moench T',\n",
       "   'Aboulhab J',\n",
       "   'Barrett K',\n",
       "   'Cahill K',\n",
       "   'Cone K',\n",
       "   'Eckes R',\n",
       "   'Hensley L',\n",
       "   'Herpin B',\n",
       "   'Higgs E',\n",
       "   'Ledgerwood J',\n",
       "   'Pierson J',\n",
       "   'Smolskis M',\n",
       "   'Sow Y',\n",
       "   'Tierney J',\n",
       "   'Sivapalasingam S',\n",
       "   'Holman W',\n",
       "   'Gettinger N',\n",
       "   'Vallée D +1'],\n",
       "  'related_topics': ['Ebola virus', 'Randomized controlled trial', 'MEDLINE'],\n",
       "  'references': ['3003465021',\n",
       "   '3005212621',\n",
       "   '3013393665',\n",
       "   '3018023298',\n",
       "   '3015490759',\n",
       "   '3010848803',\n",
       "   '3014604938',\n",
       "   '3013653866',\n",
       "   '3006564542']},\n",
       " {'id': '2605343262',\n",
       "  'title': 'GISAID: Global initiative on sharing all influenza data - from vision to reality.',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '983',\n",
       "  'abstract': '',\n",
       "  'date': 2017,\n",
       "  'authors': ['Yuelong Shu 1, John McCauley 2'],\n",
       "  'related_topics': ['Public health informatics',\n",
       "   'Data sharing',\n",
       "   'Global health',\n",
       "   'Data collection',\n",
       "   'Information Dissemination',\n",
       "   'MEDLINE',\n",
       "   'Pandemic',\n",
       "   'Knowledge management',\n",
       "   'Influenza A virus subtype H5N1',\n",
       "   'Business'],\n",
       "  'references': ['2106173155',\n",
       "   '2259815689',\n",
       "   '2587970647',\n",
       "   '2063651055',\n",
       "   '2222043208',\n",
       "   '2532120756',\n",
       "   '2557499142',\n",
       "   '2104424333',\n",
       "   '2033459705',\n",
       "   '2096039130']},\n",
       " {'id': '3001388158',\n",
       "  'title': 'China coronavirus: Six questions scientists are asking',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '65',\n",
       "  'abstract': 'Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond. Researchers are racing to find out more about the epidemiology and genetic sequence of the coronavirus spreading in Asia and beyond.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ewen Callaway', 'David Cyranoski'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'China',\n",
       "   'Geography',\n",
       "   'Genealogy',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3002539152', '3000771439']},\n",
       " {'id': '3004397688',\n",
       "  'title': 'Preliminary estimation of the basic reproduction number of novel coronavirus (2019-nCoV) in China, from 2019 to 2020: A data-driven analysis in the early phase of the outbreak.',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '1,440',\n",
       "  'abstract': 'Abstract Backgrounds An ongoing outbreak of a novel coronavirus (2019-nCoV) pneumonia hit a major city in China, Wuhan, December 2019 and subsequently reached other provinces/regions of China and other countries. We present estimates of the basic reproduction number, R0, of 2019-nCoV in the early phase of the outbreak. Methods Accounting for the impact of the variations in disease reporting rate, we modelled the epidemic curve of 2019-nCoV cases time series, in mainland China from January 10 to January 24, 2020, through the exponential growth. With the estimated intrinsic growth rate (γ), we estimated R0 by using the serial intervals (SI) of two other well-known coronavirus diseases, MERS and SARS, as approximations for the true unknown SI. Findings The early outbreak data largely follows the exponential growth. We estimated that the mean R0 ranges from 2.24 (95%CI: 1.96–2.55) to 3.58 (95%CI: 2.89–4.39) associated with 8-fold to 2-fold increase in the reporting rate. We demonstrated that changes in reporting rate substantially affect estimates of R0. Conclusion The mean estimate of R0 for the 2019-nCoV ranges from 2.24 to 3.58, and is significantly larger than 1. Our findings indicate the potential of 2019-nCoV to cause outbreaks.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Shi Zhao 1, Qianyin Lin 2, Jinjun Ran 3, Salihu S Musa 4, Guangpu Yang 1, Weiming Wang 5, Yijun Lou 3, Daozhou Gao 6, Lin Yang 4, Daihai He 4, Maggie H Wang 1'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Serial interval',\n",
       "   'Basic reproduction number',\n",
       "   'Coronavirus',\n",
       "   'Mainland China',\n",
       "   'Estimation',\n",
       "   'Pandemic',\n",
       "   'Exponential growth',\n",
       "   'Demography',\n",
       "   'Biology'],\n",
       "  'references': ['2107053896',\n",
       "   '3002764620',\n",
       "   '3004026249',\n",
       "   '2999612210',\n",
       "   '2147166346',\n",
       "   '3026046290',\n",
       "   '1990049863',\n",
       "   '2117002055',\n",
       "   '2102187991',\n",
       "   '3002747665']},\n",
       " {'id': '3002533591',\n",
       "  'title': 'Transmission Dynamics of 2019 Novel Coronavirus (2019-nCoV)',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '312',\n",
       "  'abstract': 'Background: Since December 29, 2019, pneumonia infection with 2019-nCoV has rapidly spread out from Wuhan, HubeProvince, China to most others provinces and ot',\n",
       "  'date': 2020,\n",
       "  'authors': ['Tao Liu',\n",
       "   'Jianxiong Hu',\n",
       "   'Min Kang',\n",
       "   'Lifeng Lin',\n",
       "   'Haojie Zhong',\n",
       "   'Jianpeng Xiao',\n",
       "   'Guanhao He',\n",
       "   'Tie Song',\n",
       "   'Qiong Huang',\n",
       "   'Zuhua Rong',\n",
       "   'Aiping Deng',\n",
       "   'Weilin Zeng',\n",
       "   'Xiaohua Tan',\n",
       "   'Siqing Zeng',\n",
       "   'Zhihua Zhu',\n",
       "   'Jiansen Li',\n",
       "   'Donghua Wan',\n",
       "   'Jing Lu',\n",
       "   'Huihong Deng',\n",
       "   'Jianfeng He',\n",
       "   'Wenjun Ma'],\n",
       "  'related_topics': ['Pneumonia', 'Transmission (mechanics)', 'Virology'],\n",
       "  'references': ['3001118548',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3003573988',\n",
       "   '3001465255',\n",
       "   '3004397688',\n",
       "   '2147166346',\n",
       "   '2117002055',\n",
       "   '2140763962',\n",
       "   '2102187991']},\n",
       " {'id': '1815575713',\n",
       "  'title': 'Transmission characteristics of MERS and SARS in the healthcare setting: a comparative study',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '413',\n",
       "  'abstract': 'The Middle East respiratory syndrome (MERS) coronavirus has caused recurrent outbreaks in the Arabian Peninsula since 2012. Although MERS has low overall human-to-human transmission potential, there is occasional amplification in the healthcare setting, a pattern reminiscent of the dynamics of the severe acute respiratory syndrome (SARS) outbreaks in 2003. Here we provide a head-to-head comparison of exposure patterns and transmission dynamics of large hospital clusters of MERS and SARS, including the most recent South Korean outbreak of MERS in 2015.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Gerardo Chowell 1, 2, Fatima Abdirizak 1, Sunmi Lee 3, Jonggul Lee 4, Eunok Jung 4, Hiroshi Nishiura 5, Cécile Viboud 2'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'Coronavirus Infections',\n",
       "   'Cross infection',\n",
       "   'Transmission potential'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2006434809',\n",
       "   '2138324310',\n",
       "   '2147166346',\n",
       "   '1990049863',\n",
       "   '2069251911',\n",
       "   '2096145431',\n",
       "   '1968393246',\n",
       "   '2130227690']},\n",
       " {'id': '2069251911',\n",
       "  'title': 'Superspreading and the effect of individual variation on disease emergence',\n",
       "  'reference_count': '96',\n",
       "  'citation_count': '1,992',\n",
       "  'abstract': \"Population-level analyses often use average quantities to describe heterogeneous systems, particularly when variation does not arise from identifiable groups. A prominent example, central to our current understanding of epidemic spread, is the basic reproductive number, R(0), which is defined as the mean number of infections caused by an infected individual in a susceptible population. Population estimates of R(0) can obscure considerable individual variation in infectiousness, as highlighted during the global emergence of severe acute respiratory syndrome (SARS) by numerous 'superspreading events' in which certain individuals infected unusually large numbers of secondary cases. For diseases transmitted by non-sexual direct contacts, such as SARS or smallpox, individual variation is difficult to measure empirically, and thus its importance for outbreak dynamics has been unclear. Here we present an integrated theoretical and statistical analysis of the influence of individual variation in infectiousness on disease emergence. Using contact tracing data from eight directly transmitted diseases, we show that the distribution of individual infectiousness around R(0) is often highly skewed. Model predictions accounting for this variation differ sharply from average-based approaches, with disease extinction more likely and outbreaks rarer but more explosive. Using these models, we explore implications for outbreak control, showing that individual-specific control measures outperform population-wide measures. Moreover, the dramatic improvements achieved through targeted control policies emphasize the need to identify predictive correlates of higher infectiousness. Our findings indicate that superspreading is a normal feature of disease spread, and to frame ongoing discussion we propose a rigorous definition for superspreading events and a method to predict their frequency.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['J. O. Lloyd-Smith 1, S. J. Schreiber 2, P. E. Kopp 3, W. M. Getz 1'],\n",
       "  'related_topics': ['Susceptible individual',\n",
       "   'Super-spreader',\n",
       "   'Basic reproduction number',\n",
       "   'Outbreak',\n",
       "   'Disease',\n",
       "   'Contact tracing',\n",
       "   'Variation (linguistics)',\n",
       "   'Extinction',\n",
       "   'Demography',\n",
       "   'Biology'],\n",
       "  'references': ['1995945562',\n",
       "   '2009435671',\n",
       "   '2131262274',\n",
       "   '2147166346',\n",
       "   '1606697907',\n",
       "   '2146272590',\n",
       "   '1965499304',\n",
       "   '2096145431',\n",
       "   '2104595316',\n",
       "   '2463755683']},\n",
       " {'id': '2096145431',\n",
       "  'title': 'Transmission dynamics of the etiological agent of SARS in Hong Kong: impact of public health interventions.',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '1,330',\n",
       "  'abstract': 'We present an analysis of the first 10 weeks of the severe acute respiratory syndrome (SARS) epidemic in Hong Kong. The epidemic to date has been characterized by two large clusters-initiated by two separate \"super-spread\" events (SSEs)-and by ongoing community transmission. By fitting a stochastic model to data on 1512 cases, including these clusters, we show that the etiological agent of SARS is moderately transmissible. Excluding SSEs, we estimate that 2.7 secondary infections were generated per case on average at the start of the epidemic, with a substantial contribution from hospital transmission. Transmission rates fell during the epidemic, primarily as a result of reductions in population contact rates and improved hospital infection control, but also because of more rapid hospital attendance by symptomatic individuals. As a result, the epidemic is now in decline, although continued vigilance is necessary for this to be maintained. Restrictions on longer range population movement are shown to be a potentially useful additional control measure in some contexts. We estimate that most currently infected persons are now hospitalized, which highlights the importance of control of nosocomial transmission.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Steven Riley 1, Christophe Fraser 1, Christl A. Donnelly 1, Azra C. Ghani 1, Laith J. Abu-Raddad 1, Anthony J. Hedley 2, Gabriel M. Leung 2, Lai Ming Ho 2, Tai Hing Lam 2, Thuan Q. Thach 2, Patsy Chau 2, King Pan Chan 2, Su Vui Lo 3, Pak Yin Leung 4, Thomas Tsang 4, William Ho 5, Koon Hung Lee 5, Edith M.C. Lau 6, Neil M. Ferguson 1, Roy M. Anderson 1'],\n",
       "  'related_topics': ['Population',\n",
       "   'Secondary infection',\n",
       "   'Global health',\n",
       "   'Transmission (mechanics)',\n",
       "   'Infection control',\n",
       "   'Contact tracing',\n",
       "   'Epidemiology',\n",
       "   'Attendance',\n",
       "   'Environmental health'],\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2169198329',\n",
       "   '1606697907',\n",
       "   '2104595316',\n",
       "   '2124853344']},\n",
       " {'id': '2104595316',\n",
       "  'title': 'Mathematical Epidemiology of Infectious Diseases: Model Building, Analysis and Interpretation',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '3,624',\n",
       "  'abstract': 'Provides systematic coverage of the mathematical theory of modelling epidemics in populations, with a clear and coherent discussion of the issues, concepts and phenomena. Mathematical modelling of epidemics is a vast and important area of study and this book helps the reader to translate, model, analyse and interpret, with numerous applications, examples and exercises to aid understanding.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Odo Diekmann', 'J. A. P Heesterbeek'],\n",
       "  'related_topics': ['Mathematical theory',\n",
       "   'Mathematical modelling of infectious disease',\n",
       "   'Model building',\n",
       "   'Interpretation (philosophy)',\n",
       "   'Management science',\n",
       "   'Mathematical model',\n",
       "   'Next-generation matrix',\n",
       "   'Area studies',\n",
       "   'Operations research',\n",
       "   'Medicine',\n",
       "   'Basic Reproduction Ratio'],\n",
       "  'references': ['3003573988',\n",
       "   '2070722739',\n",
       "   '1878853999',\n",
       "   '2112680994',\n",
       "   '2159301256',\n",
       "   '3018782651',\n",
       "   '3033562259',\n",
       "   '2133131640',\n",
       "   '2069251911',\n",
       "   '2096145431']},\n",
       " {'id': '1968393246',\n",
       "  'title': 'Middle East respiratory syndrome coronavirus: quantification of the extent of the epidemic, surveillance biases, and transmissibility',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '346',\n",
       "  'abstract': 'Summary Background The novel Middle East respiratory syndrome coronavirus (MERS-CoV) had, as of Aug 8, 2013, caused 111 virologically confirmed or probable human cases of infection worldwide. We analysed epidemiological and genetic data to assess the extent of human infection, the performance of case detection, and the transmission potential of MERS-CoV with and without control measures. Methods We assembled a comprehensive database of all confirmed and probable cases from public sources and estimated the incubation period and generation time from case cluster data. Using data of numbers of visitors to the Middle East and their duration of stay, we estimated the number of symptomatic cases in the Middle East. We did independent analyses, looking at the growth in incident clusters, the growth in viral population, the reproduction number of cluster index cases, and cluster sizes to characterise the dynamical properties of the epidemic and the transmission scenario. Findings The estimated number of symptomatic cases up to Aug 8, 2013, is 940 (95% CI 290–2200), indicating that at least 62% of human symptomatic cases have not been detected. We find that the case-fatality ratio of primary cases detected via routine surveillance (74%; 95% CI 49–91) is biased upwards because of detection bias; the case-fatality ratio of secondary cases was 20% (7–42). Detection of milder cases (or clinical management) seemed to have improved in recent months. Analysis of human clusters indicated that chains of transmission were not self-sustaining when infection control was implemented, but that R in the absence of controls was in the range 0·8–1·3. Three independent data sources provide evidence that R cannot be much above 1, with an upper bound of 1·2–1·5. Interpretation By showing that a slowly growing epidemic is underway either in human beings or in an animal reservoir, quantification of uncertainty in transmissibility estimates, and provision of the first estimates of the scale of the epidemic and extent of case detection biases, we provide valuable information for more informed risk assessment. Funding Medical Research Council, Bill & Melinda Gates Foundation, EU FP7, and National Institute of General Medical Sciences.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Simon Cauchemez 1, Christophe Fraser 1, Maria D Van Kerkhove 1, Christl A Donnelly 1, Steven Riley 1, Andrew Rambaut 2, Vincent Enouf 3, Sylvie van der Werf 3, Neil M Ferguson 1'],\n",
       "  'related_topics': ['Population',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Basic reproduction number'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2157725602',\n",
       "   '2147166346',\n",
       "   '2160011624',\n",
       "   '2045002682',\n",
       "   '2113457186',\n",
       "   '2130227690',\n",
       "   '2140763962',\n",
       "   '2102187991']},\n",
       " {'id': '2158899491',\n",
       "  'title': 'Natural Language Processing (Almost) from Scratch',\n",
       "  'reference_count': '95',\n",
       "  'citation_count': '7,454',\n",
       "  'abstract': 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Ronan Collobert',\n",
       "   'Jason Weston 1, Léon Bottou',\n",
       "   'Michael Karlen',\n",
       "   'Koray Kavukcuoglu 2, Pavel Kuksa 3'],\n",
       "  'related_topics': ['Sequence labeling',\n",
       "   'Named-entity recognition',\n",
       "   'Chunking (psychology)',\n",
       "   'Semantic role labeling',\n",
       "   'Tag system',\n",
       "   'Task (computing)',\n",
       "   'Machine learning',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Basis (linear algebra)',\n",
       "   'Scratch',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2147880316',\n",
       "   '2125838338',\n",
       "   '2110798204',\n",
       "   '2158139315',\n",
       "   '2159080219',\n",
       "   '2098162425',\n",
       "   '2150102617',\n",
       "   '2296073425']},\n",
       " {'id': '2110158442',\n",
       "  'title': 'Contour Detection and Hierarchical Image Segmentation',\n",
       "  'reference_count': '76',\n",
       "  'citation_count': '4,663',\n",
       "  'abstract': 'This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.',\n",
       "  'date': 2011,\n",
       "  'authors': ['P Arbeláez 1, M Maire 2, C Fowlkes 3, J Malik 1'],\n",
       "  'related_topics': ['Scale-space segmentation',\n",
       "   'Segmentation-based object categorization',\n",
       "   'Image segmentation',\n",
       "   'Active contour model',\n",
       "   'Object detection',\n",
       "   'Edge detection',\n",
       "   'Segmentation',\n",
       "   'Image processing',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2067191022',\n",
       "   '2116040950',\n",
       "   '2124351162',\n",
       "   '1999478155',\n",
       "   '2145023731',\n",
       "   '1578099820',\n",
       "   '2169551590',\n",
       "   '2121927366',\n",
       "   '1528789833']},\n",
       " {'id': '1999478155',\n",
       "  'title': 'Efficient Graph-Based Image Segmentation',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '7,281',\n",
       "  'abstract': 'This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Pedro F. Felzenszwalb 1, Daniel P. Huttenlocher 2'],\n",
       "  'related_topics': ['Segmentation-based object categorization',\n",
       "   'Image segmentation',\n",
       "   'Range segmentation',\n",
       "   'Connected-component labeling',\n",
       "   'Scale-space segmentation',\n",
       "   'Graph (abstract data type)',\n",
       "   'Minimum spanning tree-based segmentation',\n",
       "   'Image texture',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2752885492',\n",
       "   '1971784203',\n",
       "   '1964443764',\n",
       "   '2160167256',\n",
       "   '2137560895',\n",
       "   '2167077256',\n",
       "   '2132603077',\n",
       "   '1640070940',\n",
       "   '2109562068']},\n",
       " {'id': '2143516773',\n",
       "  'title': 'Fast approximate energy minimization via graph cuts',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '10,570',\n",
       "  'abstract': 'Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Y. Boykov 1, O. Veksler 1, R. Zabih 2'],\n",
       "  'related_topics': ['Graph cuts in computer vision',\n",
       "   'Approximation algorithm',\n",
       "   'Minimum cut',\n",
       "   'Simulated annealing',\n",
       "   'Cut',\n",
       "   'Standard algorithms',\n",
       "   'Energy minimization',\n",
       "   'Computational complexity theory',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['2121947440',\n",
       "   '2104095591',\n",
       "   '1997063559',\n",
       "   '1977545325',\n",
       "   '2113137767',\n",
       "   '2620619910',\n",
       "   '2121781154',\n",
       "   '2913192828',\n",
       "   '1554544485',\n",
       "   '1649464328']},\n",
       " {'id': '2169551590',\n",
       "  'title': 'Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '6,159',\n",
       "  'abstract': 'In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Y.Y. Boykov 1, M.-P. Jolly 2'],\n",
       "  'related_topics': ['Scale-space segmentation',\n",
       "   'Segmentation-based object categorization',\n",
       "   'Image segmentation',\n",
       "   'Minimum spanning tree-based segmentation',\n",
       "   'Region growing',\n",
       "   'Image texture',\n",
       "   'Connected-component labeling',\n",
       "   'GrabCut',\n",
       "   'Graph cuts in computer vision',\n",
       "   'Segmentation',\n",
       "   'Simple interactive object extraction',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2104095591',\n",
       "   '2113137767',\n",
       "   '1991113069',\n",
       "   '1564419782',\n",
       "   '2098152234',\n",
       "   '2086921140',\n",
       "   '2096139825',\n",
       "   '1987983010',\n",
       "   '2132603077']},\n",
       " {'id': '2031342017',\n",
       "  'title': 'Unbiased look at dataset bias',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '1,670',\n",
       "  'abstract': 'Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Antonio Torralba 1, Alexei A. Efros 2'],\n",
       "  'related_topics': ['Automatic identification and data capture',\n",
       "   'Pascal (programming language)',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Training set'],\n",
       "  'references': ['2108598243',\n",
       "   '2161969291',\n",
       "   '2031489346',\n",
       "   '2110764733',\n",
       "   '1576445103',\n",
       "   '2145607950',\n",
       "   '2217896605',\n",
       "   '2120419212',\n",
       "   '1566135517',\n",
       "   '2166049352']},\n",
       " {'id': '2913932916',\n",
       "  'title': 'Semantic hashing',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '1,219',\n",
       "  'abstract': \"We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs ''semantic hashing'': Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.\",\n",
       "  'date': 2009,\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Geoffrey Hinton'],\n",
       "  'related_topics': ['Feature hashing',\n",
       "   'Latent semantic analysis',\n",
       "   'Locality-sensitive hashing',\n",
       "   'Hopscotch hashing',\n",
       "   'Graphical model',\n",
       "   'Set (abstract data type)',\n",
       "   'Memory address',\n",
       "   'Unsupervised learning',\n",
       "   'Information retrieval',\n",
       "   'Computer science'],\n",
       "  'references': ['2136922672',\n",
       "   '1880262756',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2147152072',\n",
       "   '2150102617',\n",
       "   '2162006472',\n",
       "   '2038276547',\n",
       "   '2157364932',\n",
       "   '1978394996']},\n",
       " {'id': '2103359087',\n",
       "  'title': 'Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '402',\n",
       "  'abstract': 'Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.',\n",
       "  'date': 2010,\n",
       "  'authors': ['George Dahl',\n",
       "   \"Marc'aurelio Ranzato\",\n",
       "   'Abdel-rahman Mohamed',\n",
       "   'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'TIMIT',\n",
       "   'Word error rate',\n",
       "   'Covariance',\n",
       "   'Conditional independence',\n",
       "   'Conditional probability distribution',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '1498436455',\n",
       "   '1567512734',\n",
       "   '137106866',\n",
       "   '1983334819',\n",
       "   '2161000554',\n",
       "   '2161893161',\n",
       "   '2083380015',\n",
       "   '2131700150',\n",
       "   '2110871230']},\n",
       " {'id': '2033819227',\n",
       "  'title': 'Multiple view geometry in computer vision',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '28,613',\n",
       "  'abstract': 'From the Publisher: A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Richard Hartley 1, Andrew Zisserman 2'],\n",
       "  'related_topics': ['Structure from motion',\n",
       "   'Epipolar geometry',\n",
       "   'Computer graphics',\n",
       "   'Trifocal tensor',\n",
       "   'RANSAC',\n",
       "   'Bundle adjustment',\n",
       "   'Visual odometry',\n",
       "   'Eight-point algorithm',\n",
       "   'Computer science',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2177274842',\n",
       "   '2104974755',\n",
       "   '2123487311',\n",
       "   '2171740948',\n",
       "   '2151290401',\n",
       "   '2141362318',\n",
       "   '1980911747']},\n",
       " {'id': '2124386111',\n",
       "  'title': 'Object recognition from local scale-invariant features',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '21,546',\n",
       "  'abstract': 'An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.',\n",
       "  'date': 1999,\n",
       "  'authors': ['D.G. Lowe'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Haar-like features',\n",
       "   'Scale space',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Implicit Shape Model',\n",
       "   'Feature extraction',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2914885528',\n",
       "   '2124087378',\n",
       "   '2123977795',\n",
       "   '2011891945',\n",
       "   '22745672',\n",
       "   '2096077837',\n",
       "   '2096600681',\n",
       "   '2131806657',\n",
       "   '2042243448',\n",
       "   '1553558465']},\n",
       " {'id': '2154422044',\n",
       "  'title': 'Object class recognition by unsupervised scale-invariant learning',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '3,001',\n",
       "  'abstract': 'We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals).',\n",
       "  'date': 2003,\n",
       "  'authors': ['R. Fergus 1, P. Perona 2, A. Zisserman 1'],\n",
       "  'related_topics': ['Object model',\n",
       "   'Implicit Shape Model',\n",
       "   'Constellation model',\n",
       "   'Feature extraction',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Caltech 101',\n",
       "   'Contextual image classification',\n",
       "   'Naive Bayes classifier',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2217896605',\n",
       "   '2049633694',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2159686933',\n",
       "   '2155511848',\n",
       "   '1949116567',\n",
       "   '2160225842',\n",
       "   '1699734612']},\n",
       " {'id': '2012778485',\n",
       "  'title': 'Invariant Features from Interest Point Groups',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '1,141',\n",
       "  'abstract': \"This paper approaches the problem of ¯nding correspondences between images in which there are large changes in viewpoint, scale and illumi- nation. Recent work has shown that scale-space `interest points' may be found with good repeatability in spite of such changes. Further- more, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descrip- tors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process. In this work we introduce a family of features which use groups of interest points to form geometrically invariant descriptors of image regions. Feature descriptors are formed by resampling the image rel- ative to canonical frames de¯ned by the points. In addition to robust matching, a key advantage of this approach is that each match implies a hypothesis of the local 2D (projective) transformation. This allows us to immediately reject most of the false matches using a Hough trans- form. We reject remaining outliers using RANSAC and the epipolar constraint. Results show that dense feature matching can be achieved in a few seconds of computation on 1GHz Pentium III machines.\",\n",
       "  'date': 2002,\n",
       "  'authors': ['Matthew Brown', 'David G. Lowe'],\n",
       "  'related_topics': ['RANSAC',\n",
       "   'Epipolar geometry',\n",
       "   'Invariant (mathematics)',\n",
       "   'Discriminative model',\n",
       "   'Pattern recognition',\n",
       "   'Outlier',\n",
       "   'Resampling',\n",
       "   'Computer vision',\n",
       "   'Computation',\n",
       "   'Pentium',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2124386111',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2165497495',\n",
       "   '2103504761',\n",
       "   '1505641881',\n",
       "   '2011891945',\n",
       "   '22745672',\n",
       "   '2005433550']},\n",
       " {'id': '2124404372',\n",
       "  'title': 'Robust wide-baseline stereo from maximally stable extremal regions',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '7,055',\n",
       "  'abstract': 'Abstract The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions , is introduced. Extremal regions possess highly desirable properties: the set is closed under (1) continuous (and thus projective) transformation of image coordinates and (2) monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5×), illumination conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Jiri Matas 1, Ondrej Chum 2, Martin Urban 2, Tomás Pajdla 2'],\n",
       "  'related_topics': ['Maximally stable extremal regions',\n",
       "   'Epipolar geometry',\n",
       "   'Harris affine region detector',\n",
       "   'Invariant (mathematics)',\n",
       "   'Hessian affine region detector',\n",
       "   'Similarity measure',\n",
       "   'Robustness (computer science)',\n",
       "   'Principal curvature-based region detector',\n",
       "   'Algorithm',\n",
       "   'Topology',\n",
       "   'Mathematics'],\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2165497495',\n",
       "   '2124260943',\n",
       "   '1541642243',\n",
       "   '2132332894',\n",
       "   '2143753158']},\n",
       " {'id': '1676552347',\n",
       "  'title': 'An Affine Invariant Interest Point Detector',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '2,127',\n",
       "  'abstract': 'This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas : 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.',\n",
       "  'date': 2002,\n",
       "  'authors': ['K. Mikolajczyk', 'C. Schmid'],\n",
       "  'related_topics': ['Affine shape adaptation',\n",
       "   'Harris affine region detector',\n",
       "   'Affine transformation',\n",
       "   'Affine combination',\n",
       "   'Affine coordinate system',\n",
       "   'Affine hull',\n",
       "   'Hessian affine region detector',\n",
       "   'Affine group',\n",
       "   'Topology',\n",
       "   'Mathematics'],\n",
       "  'references': ['2124386111',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2111308925',\n",
       "   '2165497495',\n",
       "   '1991605728',\n",
       "   '2112328181',\n",
       "   '2005433550',\n",
       "   '1970269179']},\n",
       " {'id': '2124087378',\n",
       "  'title': 'Local grayvalue invariants for image retrieval',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '2,336',\n",
       "  'abstract': 'This paper addresses the problem of retrieving images from large image databases. The method is based on local grayvalue invariants which are computed at automatically detected interest points. A voting algorithm and semilocal constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1,000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.',\n",
       "  'date': 1997,\n",
       "  'authors': ['C. Schmid 1, R. Mohr 2'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Search engine indexing',\n",
       "   'Computer vision',\n",
       "   'Robustness (computer science)',\n",
       "   'Autocorrelation',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Image matching',\n",
       "   'Voting algorithm'],\n",
       "  'references': ['2914885528',\n",
       "   '2111308925',\n",
       "   '2098693229',\n",
       "   '2123977795',\n",
       "   '2095757522',\n",
       "   '2011891945',\n",
       "   '2109863423',\n",
       "   '2112328181',\n",
       "   '2022735534',\n",
       "   '2160835070']},\n",
       " {'id': '2111308925',\n",
       "  'title': 'A COMBINED CORNER AND EDGE DETECTOR',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '19,238',\n",
       "  'abstract': 'The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.',\n",
       "  'date': 1987,\n",
       "  'authors': ['Christopher G. Harris', 'Mike Stephens'],\n",
       "  'related_topics': ['Motion analysis',\n",
       "   'Corner detection',\n",
       "   'Interest point detection',\n",
       "   'Alvey',\n",
       "   'Principal curvature-based region detector',\n",
       "   'Harris affine region detector',\n",
       "   'GLOH',\n",
       "   'Hessian affine region detector',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1639227073',\n",
       "   '1756736144',\n",
       "   '2063599328',\n",
       "   '2048192053',\n",
       "   '2039106392',\n",
       "   '2997169974']},\n",
       " {'id': '2165497495',\n",
       "  'title': 'Reliable feature matching across widely separated views',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '915',\n",
       "  'abstract': 'We present a robust method for automatically matching features in images corresponding to the same physical point on an object seen from two arbitrary viewpoints. Unlike conventional stereo matching approaches we assume no prior knowledge about the relative camera positions and orientations. In fact in our application this is the information we wish to determine from the image feature matches. Features are detected in two or more images and characterised using affine texture invariants. The problem of window effects is explicitly addressed by our method-our feature characterisation is invariant to linear transformations of the image data including rotation, stretch and skew. The feature matching process is optimised for a structure-from-motion application where we wish to ignore unreliable matches at the expense of reducing the number of feature matches.',\n",
       "  'date': 2000,\n",
       "  'authors': ['A. Baumberg'],\n",
       "  'related_topics': ['Feature extraction',\n",
       "   'Affine transformation',\n",
       "   'Hessian affine region detector',\n",
       "   'Motion estimation',\n",
       "   'Invariant (mathematics)',\n",
       "   'Skew',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Electrical capacitance tomography',\n",
       "   'Linear map',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2124386111',\n",
       "   '2130103520',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2085261163',\n",
       "   '2112328181',\n",
       "   '3022352042',\n",
       "   '1970269179',\n",
       "   '2143753158',\n",
       "   '1549739843']},\n",
       " {'id': '1949116567',\n",
       "  'title': 'Unsupervised Learning of Models for Recognition',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '947',\n",
       "  'abstract': 'We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Markus Weber 1, Max Welling 1, Pietro Perona 1, 2'],\n",
       "  'related_topics': ['Constellation model',\n",
       "   'Unsupervised learning',\n",
       "   'Cluster analysis',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Contextual image classification',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Focus (optics)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2049633694',\n",
       "   '3017143921',\n",
       "   '1564419782',\n",
       "   '2095757522',\n",
       "   '1958762911',\n",
       "   '2124722975',\n",
       "   '2117138270',\n",
       "   '2125791971',\n",
       "   '2029727948',\n",
       "   '1628541567']},\n",
       " {'id': '2160660844',\n",
       "  'title': 'Mining and summarizing customer reviews',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '8,447',\n",
       "  'abstract': 'Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Minqing Hu', 'Bing Liu'],\n",
       "  'related_topics': ['Automatic summarization',\n",
       "   'Product (category theory)',\n",
       "   'Sentiment analysis',\n",
       "   'Task (project management)',\n",
       "   'Sentence',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Data mining',\n",
       "   'Text mining',\n",
       "   'Customer reviews'],\n",
       "  'references': ['2038721957',\n",
       "   '2166706824',\n",
       "   '1574901103',\n",
       "   '3146306708',\n",
       "   '1506285740',\n",
       "   '2115023510',\n",
       "   '2102381086',\n",
       "   '1581485226',\n",
       "   '2155328222',\n",
       "   '2199803028']},\n",
       " {'id': '2952122856',\n",
       "  'title': 'Microsoft COCO: Common Objects in Context',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '5,892',\n",
       "  'abstract': 'We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Tsung-Yi Lin',\n",
       "   'Michael Maire',\n",
       "   'Serge Belongie',\n",
       "   'Lubomir Bourdev',\n",
       "   'Ross Girshick',\n",
       "   'James Hays',\n",
       "   'Pietro Perona',\n",
       "   'Deva Ramanan',\n",
       "   'C. Lawrence Zitnick',\n",
       "   'Piotr Dollár'],\n",
       "  'related_topics': ['Cognitive neuroscience of visual object recognition',\n",
       "   'Minimum bounding box',\n",
       "   'Segmentation'],\n",
       "  'references': ['2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '1861492603',\n",
       "   '2963542991',\n",
       "   '3118608800',\n",
       "   '2031489346',\n",
       "   '2110158442',\n",
       "   '2038721957']},\n",
       " {'id': '2022166150',\n",
       "  'title': 'Yago: a core of semantic knowledge',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '4,078',\n",
       "  'abstract': 'We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Fabian M. Suchanek', 'Gjergji Kasneci', 'Gerhard Weikum'],\n",
       "  'related_topics': ['Ontology (information science)',\n",
       "   'WordNet',\n",
       "   'Knowledge extraction',\n",
       "   'Infobox',\n",
       "   'Ontology',\n",
       "   'Knowledge base',\n",
       "   'RDF Schema',\n",
       "   'Information extraction',\n",
       "   'Correctness',\n",
       "   'Information retrieval',\n",
       "   'Hierarchy',\n",
       "   'Information system',\n",
       "   'Computer science'],\n",
       "  'references': ['2038721957',\n",
       "   '2122410182',\n",
       "   '4508078',\n",
       "   '1548663377',\n",
       "   '1529522905',\n",
       "   '2075123415',\n",
       "   '1583295953',\n",
       "   '2103931177',\n",
       "   '1678959094',\n",
       "   '2137023796']},\n",
       " {'id': '2177274842',\n",
       "  'title': 'A performance evaluation of local descriptors',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '20,408',\n",
       "  'abstract': 'In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.',\n",
       "  'date': 2005,\n",
       "  'authors': ['K. Mikolajczyk 1, C. Schmid 2'],\n",
       "  'related_topics': ['Principal curvature-based region detector',\n",
       "   'Hessian affine region detector',\n",
       "   'GLOH',\n",
       "   'Interest point detection',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Shape context',\n",
       "   'Contextual image classification',\n",
       "   'Implicit Shape Model',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2033819227',\n",
       "   '2124386111',\n",
       "   '2163352848',\n",
       "   '2131846894',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2145023731',\n",
       "   '1980911747',\n",
       "   '2145072179']},\n",
       " {'id': '2131846894',\n",
       "  'title': 'Video Google: a text retrieval approach to object matching in videos',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '7,805',\n",
       "  'abstract': 'We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Sivic', 'Zisserman'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Video copy detection',\n",
       "   'Inverted index',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Vector quantization',\n",
       "   'Visual dictionary',\n",
       "   'Caltech 101',\n",
       "   'Computer vision',\n",
       "   'Pattern recognition',\n",
       "   'Invariant (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3013264884',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '1660390307',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2165497495',\n",
       "   '2160484851',\n",
       "   '1541642243']},\n",
       " {'id': '1980911747',\n",
       "  'title': 'A Comparison of Affine Region Detectors',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '4,250',\n",
       "  'abstract': \"The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris (Mikolajczyk and Schmid, 2002; Schaffalitzky and Zisserman, 2002) and Hessian points (Mikolajczyk and Schmid, 2002), a detector of `maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of `salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['K. Mikolajczyk 1, T. Tuytelaars 2, C. Schmid 3, A. Zisserman 1, J. Matas 4, F. Schaffalitzky 1, T. Kadir 1, L. Van Gool 2'],\n",
       "  'related_topics': ['Hessian affine region detector',\n",
       "   'Harris affine region detector',\n",
       "   'Kadir–Brady saliency detector',\n",
       "   'Affine shape adaptation',\n",
       "   'Affine transformation',\n",
       "   'Principal curvature-based region detector',\n",
       "   'Maximally stable extremal regions',\n",
       "   'Detector',\n",
       "   'Algorithm',\n",
       "   'Geometry',\n",
       "   'Mathematics'],\n",
       "  'references': ['2151103935',\n",
       "   '2033819227',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2131846894',\n",
       "   '2154422044',\n",
       "   '2145023731',\n",
       "   '1625255723',\n",
       "   '2172188317',\n",
       "   '2124404372']},\n",
       " {'id': '2104978738',\n",
       "  'title': 'The pyramid match kernel: discriminative classification with sets of image features',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '1,993',\n",
       "  'abstract': 'Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches',\n",
       "  'date': 2005,\n",
       "  'authors': ['K. Grauman', 'T. Darrell'],\n",
       "  'related_topics': ['Variable kernel density estimation',\n",
       "   'Radial basis function kernel',\n",
       "   'String kernel',\n",
       "   'Tree kernel',\n",
       "   'Kernel method',\n",
       "   'Kernel embedding of distributions',\n",
       "   'Polynomial kernel',\n",
       "   'Kernel (statistics)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2153635508',\n",
       "   '3145128584',\n",
       "   '2148603752',\n",
       "   '1563088657',\n",
       "   '2131846894',\n",
       "   '2057175746',\n",
       "   '1510073064',\n",
       "   '2145072179',\n",
       "   '2914885528']},\n",
       " {'id': '2172188317',\n",
       "  'title': 'Scale & Affine Invariant Interest Point Detectors',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '5,436',\n",
       "  'abstract': 'In this paper we propose a novel approach for detecting interest points invariant to scale and affine transformations. Our scale and affine invariant detectors are based on the following recent results: (1) Interest points extracted with the Harris detector can be adapted to affine transformations and give repeatable results (geometrically stable). (2) The characteristic scale of a local structure is indicated by a local extremum over scale of normalized derivatives (the Laplacian). (3) The affine shape of a point neighborhood is estimated based on the second moment matrix. Our scale invariant detector computes a multi-scale representation for the Harris interest point detector and then selects points at which a local measure (the Laplacian) is maximal over scales. This provides a set of distinctive points which are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. The characteristic scale determines a scale invariant region for each point. We extend the scale invariant detector to affine invariance by estimating the affine shape of a point neighborhood. An iterative algorithm modifies location, scale and neighborhood of each point and converges to affine invariant points. This method can deal with significant affine transformations including large scale changes. The characteristic scale and the affine shape of neighborhood determine an affine invariant region for each point. We present a comparative evaluation of different detectors and show that our approach provides better results than existing methods. The performance of our detector is also confirmed by excellent matching resultss the image is described by a set of scale/affine invariant descriptors computed on the regions associated with our points.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Krystian Mikolajczyk', 'Cordelia Schmid'],\n",
       "  'related_topics': ['Harris affine region detector',\n",
       "   'Affine shape adaptation',\n",
       "   'Hessian affine region detector',\n",
       "   'Affine transformation',\n",
       "   'Affine combination',\n",
       "   'Affine hull',\n",
       "   'Affine coordinate system',\n",
       "   'Affine group',\n",
       "   'Algorithm',\n",
       "   'Topology',\n",
       "   'Mathematics'],\n",
       "  'references': ['2033819227',\n",
       "   '2124386111',\n",
       "   '2012778485',\n",
       "   '2124404372',\n",
       "   '1676552347',\n",
       "   '2124087378',\n",
       "   '2119747362',\n",
       "   '2109200236',\n",
       "   '2111308925',\n",
       "   '2165497495']},\n",
       " {'id': '2147717514',\n",
       "  'title': 'Approximate nearest neighbors: towards removing the curse of dimensionality',\n",
       "  'reference_count': '66',\n",
       "  'citation_count': '5,030',\n",
       "  'abstract': \"We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in R d , the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authors' STOC'98 and FOCS'01 papers. It unifies, generalizes and simplifies the results from those papers.\",\n",
       "  'date': 1998,\n",
       "  'authors': ['Piotr Indyk', 'Rajeev Motwani'],\n",
       "  'related_topics': ['Nearest neighbor search',\n",
       "   'Best bin first',\n",
       "   'Ball tree',\n",
       "   'Fixed-radius near neighbors',\n",
       "   'Cover tree',\n",
       "   'Minimum spanning tree',\n",
       "   'Curse of dimensionality',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2752885492',\n",
       "   '2147152072',\n",
       "   '1634005169',\n",
       "   '2295428206',\n",
       "   '1956559956',\n",
       "   '2160066518',\n",
       "   '1502916507',\n",
       "   '3017143921',\n",
       "   '2427881153',\n",
       "   '2152565070']},\n",
       " {'id': '2162006472',\n",
       "  'title': 'Locality-sensitive hashing scheme based on p-stable distributions',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '3,261',\n",
       "  'abstract': 'We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p',\n",
       "  'date': 2004,\n",
       "  'authors': ['Mayur Datar 1, Nicole Immorlica 2, Piotr Indyk 2, Vahab S. Mirrokni 2'],\n",
       "  'related_topics': ['Locality-sensitive hashing',\n",
       "   'Dynamic perfect hashing',\n",
       "   'Universal hashing',\n",
       "   'Nearest neighbor search',\n",
       "   'K-independent hashing',\n",
       "   '2-choice hashing',\n",
       "   'Hopscotch hashing',\n",
       "   'Open addressing',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2147717514',\n",
       "   '1502916507',\n",
       "   '1541459201',\n",
       "   '2520931985',\n",
       "   '2165533158',\n",
       "   '2045533739',\n",
       "   '2169351022',\n",
       "   '2109034006',\n",
       "   '1595303882',\n",
       "   '2048779798']},\n",
       " {'id': '2138451337',\n",
       "  'title': 'Eigenfaces for recognition',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '20,218',\n",
       "  'abstract': 'We have developed a near-real-time computer system that can locate and track a subject\\'s head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Matthew Turk', 'Alex Pentland'],\n",
       "  'related_topics': ['Eigenface',\n",
       "   'Three-dimensional face recognition',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Face detection',\n",
       "   'Face hallucination',\n",
       "   'Facial recognition system',\n",
       "   'Face perception',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Psychology',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2135463994',\n",
       "   '2125848778',\n",
       "   '2130259898',\n",
       "   '2055712799',\n",
       "   '2125999363',\n",
       "   '1509703770',\n",
       "   '1526492552',\n",
       "   '1507699566',\n",
       "   '2032361618',\n",
       "   '1986450498']},\n",
       " {'id': '2107034620',\n",
       "  'title': 'A Bayesian hierarchical model for learning natural scene categories',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '4,729',\n",
       "  'abstract': 'We propose a novel approach to learn and recognize natural scene categories. Unlike previous work, it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a \"theme\". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.',\n",
       "  'date': 2005,\n",
       "  'authors': ['L. Fei-Fei', 'P. Perona'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Categorization',\n",
       "   'Bag-of-words model in computer vision',\n",
       "   'Caltech 101',\n",
       "   'Theme (narrative)',\n",
       "   'Visual dictionary',\n",
       "   'Dynamic topic model',\n",
       "   'LabelMe',\n",
       "   'Contextual image classification',\n",
       "   'Natural language processing',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Training set'],\n",
       "  'references': ['1880262756',\n",
       "   '2124386111',\n",
       "   '2045656233',\n",
       "   '1566135517',\n",
       "   '1484228140',\n",
       "   '2127006916',\n",
       "   '2171188998',\n",
       "   '1699734612',\n",
       "   '2104924585',\n",
       "   '2094414211']},\n",
       " {'id': '2156598602',\n",
       "  'title': 'Photo tourism: exploring photo collections in 3D',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '4,029',\n",
       "  'abstract': 'We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Noah Snavely 1, Steven M. Seitz 1, Richard Szeliski 2'],\n",
       "  'related_topics': ['Digital photo frame',\n",
       "   'Rendering (computer graphics)',\n",
       "   'Rephotography',\n",
       "   'Computer graphics (images)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Tourism',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '3029645440',\n",
       "   '2033819227',\n",
       "   '2131846894',\n",
       "   '2110764733',\n",
       "   '1980911747',\n",
       "   '2141282920',\n",
       "   '2119781527',\n",
       "   '2063366997',\n",
       "   '2085261163']},\n",
       " {'id': '3097096317',\n",
       "  'title': 'Robust Real-Time Face Detection',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '18,902',\n",
       "  'abstract': 'This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Paul Viola 1, Michael J. Jones 2'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Face detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Haar-like features',\n",
       "   'AdaBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'Cascading classifiers',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3124955340',\n",
       "   '2128272608',\n",
       "   '2217896605',\n",
       "   '2149706766',\n",
       "   '2115763357',\n",
       "   '1975846642',\n",
       "   '2124351082',\n",
       "   '2159686933',\n",
       "   '2155511848',\n",
       "   '2101522199']},\n",
       " {'id': '2121647436',\n",
       "  'title': 'Eigenfaces vs. Fisherfaces: recognition using class specific linear projection',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '16,894',\n",
       "  'abstract': 'We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher\\'s linear discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed \"Fisherface\" method has error rates that are lower than those of the eigenface technique for tests on the Harvard and Yale face databases.',\n",
       "  'date': 1997,\n",
       "  'authors': ['P.N. Belhumeur', 'J.P. Hespanha', 'D.J. Kriegman'],\n",
       "  'related_topics': ['Eigenface',\n",
       "   'Three-dimensional face recognition',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Face hallucination',\n",
       "   'Facial recognition system',\n",
       "   'Multilinear subspace learning',\n",
       "   'Linear subspace',\n",
       "   'Linear discriminant analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2098693229',\n",
       "   '2123977795',\n",
       "   '2115689562',\n",
       "   '3017143921',\n",
       "   '2098947662',\n",
       "   '2113341759',\n",
       "   '2740373864',\n",
       "   '2130259898',\n",
       "   '2159173611']},\n",
       " {'id': '2033419168',\n",
       "  'title': 'The FERET evaluation methodology for face-recognition algorithms',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '6,296',\n",
       "  'abstract': 'Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1,199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to 1) assess the state of the art, 2) identify future areas of research, and 3) measure algorithm performance.',\n",
       "  'date': 2000,\n",
       "  'authors': ['P.J. Phillips 1, Hyeonjoon Moon 2, S.A. Rizvi 3, P.J. Rauss 4'],\n",
       "  'related_topics': ['FERET database',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Facial recognition system',\n",
       "   'Biometrics',\n",
       "   'Pattern recognition',\n",
       "   'Data mining',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'FERET'],\n",
       "  'references': ['2138451337',\n",
       "   '2120954940',\n",
       "   '1997011019',\n",
       "   '2128716185',\n",
       "   '2012352340',\n",
       "   '2131273085',\n",
       "   '1761337995',\n",
       "   '2132234724',\n",
       "   '3094217134',\n",
       "   '2689627924']},\n",
       " {'id': '2123921160',\n",
       "  'title': 'From few to many: illumination cone models for face recognition under variable lighting and pose',\n",
       "  'reference_count': '82',\n",
       "  'citation_count': '5,455',\n",
       "  'abstract': 'We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions.',\n",
       "  'date': 2001,\n",
       "  'authors': ['A.S. Georghiades 1, P.N. Belhumeur 1, D.J. Kriegman 2'],\n",
       "  'related_topics': ['Illumination problem',\n",
       "   'Image-based modeling and rendering',\n",
       "   'Generative model',\n",
       "   'Face (geometry)',\n",
       "   'Standard test image',\n",
       "   'Facial recognition system',\n",
       "   'Convex cone',\n",
       "   'Iterative reconstruction',\n",
       "   'Lambertian reflectance',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2217896605',\n",
       "   '2121647436',\n",
       "   '2152826865',\n",
       "   '2033419168',\n",
       "   '2159686933',\n",
       "   '2123977795',\n",
       "   '2115689562',\n",
       "   '2120954940',\n",
       "   '2098947662']},\n",
       " {'id': '2137659841',\n",
       "  'title': 'Overview of the face recognition grand challenge',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '2,767',\n",
       "  'abstract': 'Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.',\n",
       "  'date': 2005,\n",
       "  'authors': ['P.J. Phillips 1, P.J. Flynn 2, T. Scruggs 3, K.W. Bowyer 2, Jin Chang 2, K. Hoffman 3, J. Marques 4, Jaesik Min 2, W. Worek 3'],\n",
       "  'related_topics': ['Face Recognition Grand Challenge',\n",
       "   'Face detection',\n",
       "   'Facial recognition system',\n",
       "   'Word error rate',\n",
       "   'Biometrics',\n",
       "   'Machine learning',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Baseline (configuration management)',\n",
       "   'NIST'],\n",
       "  'references': ['2138451337',\n",
       "   '2102773363',\n",
       "   '2143542740',\n",
       "   '2137385871',\n",
       "   '2120838001',\n",
       "   '1555969862',\n",
       "   '138943044']},\n",
       " {'id': '2098693229',\n",
       "  'title': 'Face recognition using eigenfaces',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '8,478',\n",
       "  'abstract': \"An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner. >\",\n",
       "  'date': 1991,\n",
       "  'authors': ['M.A. Turk', 'A.P. Pentland'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Eigenface',\n",
       "   'Face detection',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Feature vector',\n",
       "   'Unsupervised learning',\n",
       "   'Set (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Face space'],\n",
       "  'references': ['2138451337',\n",
       "   '2125848778',\n",
       "   '2130259898',\n",
       "   '2055712799',\n",
       "   '2125999363',\n",
       "   '1507699566',\n",
       "   '1998186877',\n",
       "   '2169718527']},\n",
       " {'id': '2125310925',\n",
       "  'title': 'Recovering Surface Layout from an Image',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '872',\n",
       "  'abstract': 'Humans have an amazing ability to instantly grasp the overall 3D structure of a scene--ground orientation, relative positions of major landmarks, etc.--even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \"surface layout\" of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis. In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Derek Hoiem', 'Alexei A. Efros', 'Martial Hebert'],\n",
       "  'related_topics': ['Orientation (computer vision)',\n",
       "   'Image processing',\n",
       "   'GRASP',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'View synthesis',\n",
       "   'Color image',\n",
       "   'Object detection',\n",
       "   'Perspective (graphical)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2033819227',\n",
       "   '2147880316',\n",
       "   '2121947440',\n",
       "   '1999478155',\n",
       "   '2143516773',\n",
       "   '1566135517',\n",
       "   '2024046085',\n",
       "   '2209124607',\n",
       "   '2032210760',\n",
       "   '1484228140']},\n",
       " {'id': '2994340921',\n",
       "  'title': 'The AR face databasae',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,961',\n",
       "  'abstract': '',\n",
       "  'date': 1997,\n",
       "  'authors': ['A. M. Martinez'],\n",
       "  'related_topics': ['Face (sociological concept)',\n",
       "   'Computer science',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2129812935',\n",
       "   '1782590233',\n",
       "   '2121601095',\n",
       "   '2102544846',\n",
       "   '2132467081',\n",
       "   '2157364932',\n",
       "   '2134262590',\n",
       "   '1963932623',\n",
       "   '3102431071',\n",
       "   '2912990735']},\n",
       " {'id': '2006793117',\n",
       "  'title': 'The CMU pose, illumination, and expression database',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '2,512',\n",
       "  'abstract': 'In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.',\n",
       "  'date': 2003,\n",
       "  'authors': ['T. Sim 1, S. Baker 2, M. Bsat 2'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Expression (mathematics)',\n",
       "   'Computer vision',\n",
       "   'Computer graphics (images)',\n",
       "   'Computer science',\n",
       "   'Database',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2155759509',\n",
       "   '2118774738',\n",
       "   '2102760078',\n",
       "   '2120420721',\n",
       "   '2110822444',\n",
       "   '2121114545',\n",
       "   '2106143125',\n",
       "   '2141503314',\n",
       "   '2144855601']},\n",
       " {'id': '2963173190',\n",
       "  'title': 'Return of the Devil in the Details: Delving Deep into Convolutional Nets',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '3,184',\n",
       "  'abstract': 'The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Ken Chatfield',\n",
       "   'Karen Simonyan',\n",
       "   'Andrea Vedaldi',\n",
       "   'Andrew Zisserman'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Object detection',\n",
       "   'Source code',\n",
       "   'Pattern recognition',\n",
       "   'Curse of dimensionality',\n",
       "   'Computer science',\n",
       "   'Common ground',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Raising (linguistics)',\n",
       "   'Artificial intelligence',\n",
       "   'Fisher vector'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2109255472',\n",
       "   '2031489346',\n",
       "   '2155541015',\n",
       "   '2162915993',\n",
       "   '2131846894']},\n",
       " {'id': '2295107390',\n",
       "  'title': 'Learning Deep Features for Discriminative Localization',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '3,911',\n",
       "  'abstract': 'In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Bolei Zhou',\n",
       "   'Aditya Khosla',\n",
       "   'Agata Lapedriza',\n",
       "   'Aude Oliva',\n",
       "   'Antonio Torralba'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Discriminative model',\n",
       "   'Pooling',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Representation (mathematics)',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Object (computer science)',\n",
       "   'Simplicity',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2155541015',\n",
       "   '2963911037',\n",
       "   '2162915993']},\n",
       " {'id': '2111993661',\n",
       "  'title': 'Image retrieval: Ideas, influences, and trends of the new age',\n",
       "  'reference_count': '282',\n",
       "  'citation_count': '4,570',\n",
       "  'abstract': 'We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Ritendra Datta', 'Dhiraj Joshi', 'Jia Li', 'James Z. Wang'],\n",
       "  'related_topics': ['Content-based image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Image retrieval',\n",
       "   'Emerging technologies',\n",
       "   'Process (engineering)',\n",
       "   'Adaptation (computer science)',\n",
       "   'Information retrieval',\n",
       "   'Documentation',\n",
       "   'Computer science',\n",
       "   'Annotation'],\n",
       "  'references': ['1480376833',\n",
       "   '2177274842',\n",
       "   '2121947440',\n",
       "   '2067191022',\n",
       "   '2119479037',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '1989702938',\n",
       "   '2130660124',\n",
       "   '1579271636']},\n",
       " {'id': '1666447063',\n",
       "  'title': 'Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '2,202',\n",
       "  'abstract': 'We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.',\n",
       "  'date': 2002,\n",
       "  'authors': ['P. Duygulu 1, Kobus Barnard 1, J. F. G. de Freitas 2, David A. Forsyth 1'],\n",
       "  'related_topics': ['Vocabulary',\n",
       "   'Lexicon',\n",
       "   'Machine translation',\n",
       "   'Test set',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Set (abstract data type)',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Noun',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '1574901103',\n",
       "   '1508960934',\n",
       "   '2006969979',\n",
       "   '1579838312',\n",
       "   '1934863104',\n",
       "   '2129765547',\n",
       "   '2293605478',\n",
       "   '1540386283',\n",
       "   '1585814348']},\n",
       " {'id': '1934863104',\n",
       "  'title': 'Learning the semantics of words and pictures',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '771',\n",
       "  'abstract': 'We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition.',\n",
       "  'date': 2001,\n",
       "  'authors': ['K. Barnard', 'D. Forsyth'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Unsupervised learning',\n",
       "   'Feature (computer vision)',\n",
       "   'Human–computer information retrieval',\n",
       "   'Semantics',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Information retrieval',\n",
       "   'Statistical model',\n",
       "   'Computer science'],\n",
       "  'references': ['2049633694',\n",
       "   '2160066518',\n",
       "   '2135705692',\n",
       "   '2125101937',\n",
       "   '2062270497',\n",
       "   '2155099190',\n",
       "   '1587328194',\n",
       "   '2099251025',\n",
       "   '2011549082',\n",
       "   '2117086609']},\n",
       " {'id': '2166770390',\n",
       "  'title': 'Object Detection Using the Statistics of Parts',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '496',\n",
       "  'abstract': 'In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively. Each classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated class-conditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation—labeling the input as non-object—or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Henry Schneiderman', 'Takeo Kanade'],\n",
       "  'related_topics': ['Margin classifier',\n",
       "   'Quadratic classifier',\n",
       "   'Object detection',\n",
       "   'Classifier (UML)',\n",
       "   'AdaBoost',\n",
       "   'Face detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Likelihood-ratio test',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '2217896605',\n",
       "   '2914885528',\n",
       "   '2124351082',\n",
       "   '1658679052',\n",
       "   '2159686933',\n",
       "   '2032210760',\n",
       "   '2140785063']},\n",
       " {'id': '1587328194',\n",
       "  'title': 'Finding Naked People',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '687',\n",
       "  'abstract': 'This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image. No manual intervention is required. The approach combines color and texture properties to obtain an effective mask for skin regions. The skin mask is shown to be effective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture. The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn from a wide collection of sources.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Margaret M. Fleck 1, David A. Forsyth 2, Chris Bregler 2'],\n",
       "  'related_topics': ['Object model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Texture (music)',\n",
       "   'Computer vision',\n",
       "   'Test set',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Image (mathematics)',\n",
       "   'Range (mathematics)',\n",
       "   'Computer science',\n",
       "   'World Wide Web',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2145023731',\n",
       "   '2123977795',\n",
       "   '2093191240',\n",
       "   '2008297189',\n",
       "   '2068272887',\n",
       "   '2053197265',\n",
       "   '2102475035',\n",
       "   '1530454533',\n",
       "   '2037732452',\n",
       "   '2069266228']},\n",
       " {'id': '2293605478',\n",
       "  'title': 'Clustering art',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '226',\n",
       "  'abstract': 'We extend a recently developed method (K. Barnard and D. Forsyth, 2001) for learning the semantics of image databases using text and pictures. We incorporate statistical natural language processing in order to deal with free text. We demonstrate the current system on a difficult dataset, namely 10000 images of work from the Fine Arts Museum of San Francisco. The images include line drawings, paintings, and pictures of sculpture and ceramics. Many of the images have associated free text which varies greatly from physical description to interpretation and mood. We use WordNet to provide semantic grouping information and to help disambiguate word senses, as well as emphasize the hierarchical nature of semantic relationships. This allows us to impose a natural structure on the image collection that reflects semantics to a considerable degree. Our method produces a joint probability distribution for words and picture elements. We demonstrate that this distribution can be used: (a) to provide illustrations for given captions, and (b) to generate words for images outside the training set. Results from this annotation process yield a quantitative study of our method. Finally, the annotation process can be seen as a form of object recognizer that has been learned through a partially supervised process.',\n",
       "  'date': 2000,\n",
       "  'authors': ['K. Barnard', 'P. Duygulu', 'D. Forsyth'],\n",
       "  'related_topics': ['WordNet',\n",
       "   'Semantics',\n",
       "   'Natural language',\n",
       "   'Computational linguistics',\n",
       "   'Cluster analysis',\n",
       "   'Object (computer science)',\n",
       "   'Annotation',\n",
       "   'Natural language processing',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Computer science',\n",
       "   'Text mining',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2049633694',\n",
       "   '2102381086',\n",
       "   '1934863104',\n",
       "   '2081687495',\n",
       "   '2117086609',\n",
       "   '2166447979',\n",
       "   '2004690028',\n",
       "   '1972812142',\n",
       "   '1504061712']},\n",
       " {'id': '2055225264',\n",
       "  'title': 'PicASHOW: pictorial authority search by hyperlinks on the web.',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '121',\n",
       "  'abstract': 'We describe PicASHOW, a fully automated WWW image retrieval system that is based on several link-structure analyzing algorithms. Our basic premise is that a page p displays (or links to) an image when the author of p considers the image to be of value to the viewers of the page. We thus extend some well known link-based WWW page retrieval schemes to the context of image retrieval. PicASHOW’s analysis of the link structure enables it to retrieve relevant images even when those are stored in files with meaningless names. The same analysis also allows it to identify image containers and image hubs. We define these as Web pages that are rich in relevant images, or from which many images are readily accessible. PicASHOW requires no image analysis whatsoever and no creation of taxonomies for preclassification of the Web’s images. It can be implemented by standard WWW search engines with reasonable overhead, in terms of both computations and storage, and with no change to user query formats. It can thus be used to easily add image retrieving capabilities to standard search engines. Our results demonstrate that PicASHOW, while relying almost exclusively on link analysis, compares well with dedicated WWW image retrieval systems. We conclude that link analysis, a proven effective technique for Web page search, can improve the performance of Web image retrieval, as well as extend its definition to include the retrieval of image hubs and containers.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Ronny Lempel', 'Aya Soffer'],\n",
       "  'related_topics': ['Image retrieval',\n",
       "   'Static web page',\n",
       "   'Web page',\n",
       "   'Web search engine',\n",
       "   'Printer-friendly',\n",
       "   'Web search query',\n",
       "   'Web crawler',\n",
       "   'Hyperlink',\n",
       "   'Information retrieval',\n",
       "   'World Wide Web',\n",
       "   'Computer science'],\n",
       "  'references': ['3013264884',\n",
       "   '2138621811',\n",
       "   '2006119904',\n",
       "   '2008297189',\n",
       "   '2079672501',\n",
       "   '1987777228',\n",
       "   '2089199911',\n",
       "   '2099251025',\n",
       "   '2117086609',\n",
       "   '2140350208']},\n",
       " {'id': '2050457084',\n",
       "  'title': 'Categories, Photographs & Predicaments: Exploratory Research on Representing Pictures for Access',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '5',\n",
       "  'abstract': '',\n",
       "  'date': 2005,\n",
       "  'authors': [\"Brian C. O'Connor\", \"Mary Keeney O'Connor\"],\n",
       "  'related_topics': ['Exploratory research',\n",
       "   'Applied psychology',\n",
       "   'Psychology',\n",
       "   'Multimedia'],\n",
       "  'references': ['2141282920',\n",
       "   '2001574834',\n",
       "   '2152027810',\n",
       "   '2273590290',\n",
       "   '2252966386']},\n",
       " {'id': '181417509',\n",
       "  'title': 'Storage and Retrieval of Feature Data for a Very Large Online Image Collection.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '84',\n",
       "  'abstract': '',\n",
       "  'date': 1995,\n",
       "  'authors': ['T. K. Rengarajan', 'Lucien A. Dimino', 'Dwayne Chung'],\n",
       "  'related_topics': ['Visual Word',\n",
       "   'Image retrieval',\n",
       "   'Automatic image annotation',\n",
       "   'Feature detection (computer vision)',\n",
       "   'Image texture',\n",
       "   'Feature data',\n",
       "   'Image processing',\n",
       "   'Digital image processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science'],\n",
       "  'references': ['2141282920',\n",
       "   '2134814621',\n",
       "   '2153166546',\n",
       "   '2086174602',\n",
       "   '2816429464',\n",
       "   '2079855258',\n",
       "   '1978530472',\n",
       "   '2849837516',\n",
       "   '2852735096']},\n",
       " {'id': '2612148268',\n",
       "  'title': 'Categories, photographs & predicaments : Exploratory research on representing pictures for access : Theory and practice in the organization of images and other visuo-spatial data for retrieval',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2',\n",
       "  'abstract': '',\n",
       "  'date': 1998,\n",
       "  'authors': [\"B. C. O'connor\", \"M. K. O'connor\"],\n",
       "  'related_topics': ['Exploratory research',\n",
       "   'Spatial analysis',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2141282920', '2152027810']},\n",
       " {'id': '2217896605',\n",
       "  'title': 'Neural network-based face detection',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '6,478',\n",
       "  'abstract': 'We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates.',\n",
       "  'date': 1997,\n",
       "  'authors': ['H.A. Rowley 1, S. Baluja 2, T. Kanade 1'],\n",
       "  'related_topics': ['Face detection',\n",
       "   'Object-class detection',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Artificial neural network',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Pixel',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2139212933',\n",
       "   '2981264952',\n",
       "   '2133671888',\n",
       "   '2124351082',\n",
       "   '2147800946',\n",
       "   '2098947662',\n",
       "   '1997011019',\n",
       "   '2173629880',\n",
       "   '2042371054',\n",
       "   '2159173611']},\n",
       " {'id': '2045656233',\n",
       "  'title': 'Bayesian Data Analysis',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '30,525',\n",
       "  'abstract': 'FUNDAMENTALS OF BAYESIAN INFERENCE Probability and Inference Single-Parameter Models Introduction to Multiparameter Models Asymptotics and Connections to Non-Bayesian Approaches Hierarchical Models FUNDAMENTALS OF BAYESIAN DATA ANALYSIS Model Checking Evaluating, Comparing, and Expanding Models Modeling Accounting for Data Collection Decision Analysis ADVANCED COMPUTATION Introduction to Bayesian Computation Basics of Markov Chain Simulation Computationally Efficient Markov Chain Simulation Modal and Distributional Approximations REGRESSION MODELS Introduction to Regression Models Hierarchical Linear Models Generalized Linear Models Models for Robust Inference Models for Missing Data NONLINEAR AND NONPARAMETRIC MODELS Parametric Nonlinear Models Basic Function Models Gaussian Process Models Finite Mixture Models Dirichlet Process Models APPENDICES A: Standard Probability Distributions B: Outline of Proofs of Asymptotic Theorems C: Computation in R and Stan Bibliographic Notes and Exercises appear at the end of each chapter.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Andrew Gelman',\n",
       "   'John B. Carlin',\n",
       "   'Hal S. Stern',\n",
       "   'David B. Dunson',\n",
       "   'Aki Vehtari',\n",
       "   'Donald B. Rubin'],\n",
       "  'related_topics': ['Variable-order Bayesian network',\n",
       "   'Bayesian statistics',\n",
       "   'Dynamic Bayesian network',\n",
       "   'Bayesian inference',\n",
       "   'Dirichlet process',\n",
       "   'Probabilistic programming language',\n",
       "   'Markov chain',\n",
       "   'Bayesian probability',\n",
       "   'Applied mathematics',\n",
       "   'Data mining',\n",
       "   'Computer science'],\n",
       "  'references': ['2159080219', '2156273867', '2126163471', '1533179050']},\n",
       " {'id': '2130416410',\n",
       "  'title': 'Markov Chain Monte Carlo in Practice',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '11,467',\n",
       "  'abstract': 'INTRODUCING MARKOV CHAIN MONTE CARLO Introduction The Problem Markov Chain Monte Carlo Implementation Discussion HEPATITIS B: A CASE STUDY IN MCMC METHODS Introduction Hepatitis B Immunization Modelling Fitting a Model Using Gibbs Sampling Model Elaboration Conclusion MARKOV CHAIN CONCEPTS RELATED TO SAMPLING ALGORITHMS Markov Chains Rates of Convergence Estimation The Gibbs Sampler and Metropolis-Hastings Algorithm INTRODUCTION TO GENERAL STATE-SPACE MARKOV CHAIN THEORY Introduction Notation and Definitions Irreducibility, Recurrence, and Convergence Harris Recurrence Mixing Rates and Central Limit Theorems Regeneration Discussion FULL CONDITIONAL DISTRIBUTIONS Introduction Deriving Full Conditional Distributions Sampling from Full Conditional Distributions Discussion STRATEGIES FOR IMPROVING MCMC Introduction Reparameterization Random and Adaptive Direction Sampling Modifying the Stationary Distribution Methods Based on Continuous-Time Processes Discussion IMPLEMENTING MCMC Introduction Determining the Number of Iterations Software and Implementation Output Analysis Generic Metropolis Algorithms Discussion INFERENCE AND MONITORING CONVERGENCE Difficulties in Inference from Markov Chain Simulation The Risk of Undiagnosed Slow Convergence Multiple Sequences and Overdispersed Starting Points Monitoring Convergence Using Simulation Output Output Analysis for Inference Output Analysis for Improving Efficiency MODEL DETERMINATION USING SAMPLING-BASED METHODS Introduction Classical Approaches The Bayesian Perspective and the Bayes Factor Alternative Predictive Distributions How to Use Predictive Distributions Computational Issues An Example Discussion HYPOTHESIS TESTING AND MODEL SELECTION Introduction Uses of Bayes Factors Marginal Likelihood Estimation by Importance Sampling Marginal Likelihood Estimation Using Maximum Likelihood Application: How Many Components in a Mixture? Discussion Appendix: S-PLUS Code for the Laplace-Metropolis Estimator MODEL CHECKING AND MODEL IMPROVEMENT Introduction Model Checking Using Posterior Predictive Simulation Model Improvement via Expansion Example: Hierarchical Mixture Modelling of Reaction Times STOCHASTIC SEARCH VARIABLE SELECTION Introduction A Hierarchical Bayesian Model for Variable Selection Searching the Posterior by Gibbs Sampling Extensions Constructing Stock Portfolios With SSVS Discussion BAYESIAN MODEL COMPARISON VIA JUMP DIFFUSIONS Introduction Model Choice Jump-Diffusion Sampling Mixture Deconvolution Object Recognition Variable Selection Change-Point Identification Conclusions ESTIMATION AND OPTIMIZATION OF FUNCTIONS Non-Bayesian Applications of MCMC Monte Carlo Optimization Monte Carlo Likelihood Analysis Normalizing-Constant Families Missing Data Decision Theory Which Sampling Distribution? Importance Sampling Discussion STOCHASTIC EM: METHOD AND APPLICATION Introduction The EM Algorithm The Stochastic EM Algorithm Examples GENERALIZED LINEAR MIXED MODELS Introduction Generalized Linear Models (GLMs) Bayesian Estimation of GLMs Gibbs Sampling for GLMs Generalized Linear Mixed Models (GLMMs) Specification of Random-Effect Distributions Hyperpriors and the Estimation of Hyperparameters Some Examples Discussion HIERARCHICAL LONGITUDINAL MODELLING Introduction Clinical Background Model Detail and MCMC Implementation Results Summary and Discussion MEDICAL MONITORING Introduction Modelling Medical Monitoring Computing Posterior Distributions Forecasting Model Criticism Illustrative Application Discussion MCMC FOR NONLINEAR HIERARCHICAL MODELS Introduction Implementing MCMC Comparison of Strategies A Case Study from Pharmacokinetics-Pharmacodynamics Extensions and Discussion BAYESIAN MAPPING OF DISEASE Introduction Hypotheses and Notation Maximum Likelihood Estimation of Relative Risks Hierarchical Bayesian Model of Relative Risks Empirical Bayes Estimation of Relative Risks Fully Bayesian Estimation of Relative Risks Discussion MCMC IN IMAGE ANALYSIS Introduction The Relevance of MCMC to Image Analysis Image Models at Different Levels Methodological Innovations in MCMC Stimulated by Imaging Discussion MEASUREMENT ERROR Introduction Conditional-Independence Modelling Illustrative examples Discussion GIBBS SAMPLING METHODS IN GENETICS Introduction Standard Methods in Genetics Gibbs Sampling Approaches MCMC Maximum Likelihood Application to a Family Study of Breast Cancer Conclusions MIXTURES OF DISTRIBUTIONS: INFERENCE AND ESTIMATION Introduction The Missing Data Structure Gibbs Sampling Implementation Convergence of the Algorithm Testing for Mixtures Infinite Mixtures and Other Extensions AN ARCHAEOLOGICAL EXAMPLE: RADIOCARBON DATING Introduction Background to Radiocarbon Dating Archaeological Problems and Questions Illustrative Examples Discussion Index',\n",
       "  'date': 1997,\n",
       "  'authors': ['W.R. Gilks', 'S. Richardson', 'David Spiegelhalter'],\n",
       "  'related_topics': ['Gibbs sampling',\n",
       "   'Slice sampling',\n",
       "   'Metropolis–Hastings algorithm',\n",
       "   'Markov chain Monte Carlo',\n",
       "   'Marginal likelihood',\n",
       "   'Bayes factor',\n",
       "   'Importance sampling',\n",
       "   'Bayesian inference',\n",
       "   'Mathematical optimization',\n",
       "   'Econometrics',\n",
       "   'Computer science'],\n",
       "  'references': ['2108207895', '2163738067']},\n",
       " {'id': '2030536784',\n",
       "  'title': 'Pictorial Structures for Object Recognition',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '2,824',\n",
       "  'abstract': 'In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Pedro F. Felzenszwalb 1, Daniel P. Huttenlocher 2'],\n",
       "  'related_topics': ['3D single-object recognition',\n",
       "   'Object model',\n",
       "   'Method',\n",
       "   'Visual appearance',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Object (computer science)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Articulated body pose estimation',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3145128584',\n",
       "   '2138451337',\n",
       "   '2752885492',\n",
       "   '2143516773',\n",
       "   '2159080219',\n",
       "   '1560013842',\n",
       "   '1997063559',\n",
       "   '301824129',\n",
       "   '2123977795',\n",
       "   '2085261163']},\n",
       " {'id': '2124351162',\n",
       "  'title': '\"GrabCut\": interactive foreground extraction using iterated graph cuts',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '6,976',\n",
       "  'abstract': 'The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for \"border matting\" has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Carsten Rother', 'Vladimir Kolmogorov', 'Andrew Blake'],\n",
       "  'related_topics': ['GrabCut',\n",
       "   'Image segmentation',\n",
       "   'Simple interactive object extraction',\n",
       "   'Graph cuts in computer vision',\n",
       "   'Image editing',\n",
       "   'Cut',\n",
       "   'Iterative method',\n",
       "   'Segmentation',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2104095591',\n",
       "   '2169551590',\n",
       "   '2049633694',\n",
       "   '2101309634',\n",
       "   '2134820502',\n",
       "   '2077786999',\n",
       "   '2103917701',\n",
       "   '2740373864',\n",
       "   '2103334940',\n",
       "   '1785730614']},\n",
       " {'id': '2024046085',\n",
       "  'title': 'Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors)',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '8,356',\n",
       "  'abstract': 'Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Jerome Friedman', 'Trevor Hastie', 'Robert Tibshirani'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'Gradient boosting',\n",
       "   'Boosting (machine learning)',\n",
       "   'LPBoost',\n",
       "   'LogitBoost',\n",
       "   'AdaBoost',\n",
       "   'Statistical classification',\n",
       "   'Decision rule',\n",
       "   'Machine learning',\n",
       "   'Econometrics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1678356000',\n",
       "   '2102201073',\n",
       "   '1540007258',\n",
       "   '2099968818',\n",
       "   '1881647329',\n",
       "   '2141518341']},\n",
       " {'id': '2168002178',\n",
       "  'title': 'Shape matching and object recognition using low distortion correspondences',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '1,117',\n",
       "  'abstract': \"We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['A.C. Berg', 'T.L. Berg', 'J. Malik'],\n",
       "  'related_topics': ['Caltech 101',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Thin plate spline',\n",
       "   'Facial recognition system',\n",
       "   'Face detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Quadratic programming',\n",
       "   'Spline (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '3097096317',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2154422044',\n",
       "   '2124087378',\n",
       "   '2119823327',\n",
       "   '2155511848',\n",
       "   '2101522199',\n",
       "   '2295106276']},\n",
       " {'id': '1484228140',\n",
       "  'title': 'Representing and Recognizing the Visual Appearance of Materials using Three-dimensional Textons',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '2,049',\n",
       "  'abstract': 'We study the recognition of surfaces made from different materials such as concrete, rug, marble, or leather on the basis of their textural appearance. Such natural textures arise from spatial variation of two surface attributes: (1) reflectance and (2) surface normal. In this paper, we provide a unified model to address both these aspects of natural texture. The main idea is to construct a vocabulary of prototype tiny surface patches with associated local geometric and photometric properties. We call these 3D textons. Examples might be ridges, grooves, spots or stripes or combinations thereof. Associated with each texton is an appearance vector, which characterizes the local irradiance distribution, represented as a set of linear Gaussian derivative filter outputs, under different lighting and viewing conditions. Given a large collection of images of different materials, a clustering approach is used to acquire a small (on the order of 100) 3D texton vocabulary. Given a few (1 to 4) images of any material, it can be characterized using these textons. We demonstrate the application of this representation for recognition of the material viewed under novel lighting and viewing conditions. We also illustrate how the 3D texton model can be used to predict the appearance of materials under novel conditions.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Thomas Leung', 'Jitendra Malik'],\n",
       "  'related_topics': ['Texton',\n",
       "   'Visual appearance',\n",
       "   'Texture synthesis',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Texture (geology)',\n",
       "   'Cluster analysis',\n",
       "   'Basis (linear algebra)',\n",
       "   'Normal',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2170120409',\n",
       "   '2117812871',\n",
       "   '2138451337',\n",
       "   '2130416410',\n",
       "   '1997063559',\n",
       "   '1634005169',\n",
       "   '2116013899',\n",
       "   '1481646516',\n",
       "   '2123977795',\n",
       "   '3017143921']},\n",
       " {'id': '3035018050',\n",
       "  'title': 'Early Detection and Assessment of Covid-19',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '8',\n",
       "  'abstract': \"Background: Since the Covid-19 global pandemic emerged, developing countries have been facing multiple challenges over its diagnosis. We aimed to establish a relationship between the signs and symptoms of COVID-19 for early detection and assessment to reduce the transmission rate of SARS-Cov-2. Methods: We collected published data on the clinical features of Covid-19 retrospectively and categorized them into physical and blood biomarkers. Common features were assigned scores by the Borg scoring method with slight modifications and were incorporated into a newly-developed Hashmi-Asif Covid-19 assessment Chart. Correlations between signs and symptoms with the development of Covid-19 was assessed by Pearson correlation and Spearman Correlation coefficient (rho). Linear regression analysis was employed to assess the highest correlating features. The frequency of signs and symptoms in developing Covid-19 was assessed through Chi-square test two tailed with Cramer's V strength. Changes in signs and symptoms were incorporated into a chart that consisted of four tiers representing disease stages. Results: Data from 10,172 Covid-19 laboratory confirmed cases showed a correlation with Fever in 43.9% (P = 0.000) cases, cough 54.08% and dry mucus 25.68% equally significant (P = 0.000), Hyperemic pharyngeal mucus membrane 17.92% (P = 0.005), leukopenia 28.11% (P = 0.000), lymphopenia 64.35% (P = 0.000), thrombopenia 35.49% (P = 0.000), elevated Alanine aminotransferase 50.02% (P = 0.000), and Aspartate aminotransferase 34.49% (P = 0.000). The chart exhibited a maximum scoring of 39. Normal tier scoring was ≤ 12/39, mild state scoring was 13-22/39, and star values scoring was ≥7/15; this latter category on the chart means Covid-19 is progressing and quarantine should be adopted. Moderate stage scored 23-33 and severe scored 34-39 in the chart. Conclusion: The Hashmi-Asif Covid-19 Chart is significant in assessing subclinical and clinical stages of Covid-19 to reduce the transmission rate.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Hafiz Abdul Sattar Hashmi', 'Hafiz Muhammad Asif'],\n",
       "  'related_topics': ['Chart',\n",
       "   'Subclinical infection',\n",
       "   \"Spearman's rank correlation coefficient\",\n",
       "   'Linear regression',\n",
       "   'Internal medicine',\n",
       "   'Pearson product-moment correlation coefficient',\n",
       "   'Stage (cooking)',\n",
       "   'Leukopenia',\n",
       "   'Correlation',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3003668884',\n",
       "   '3003465021',\n",
       "   '3009912996',\n",
       "   '3008696669',\n",
       "   '3008818676',\n",
       "   '3016535995',\n",
       "   '3012747666']},\n",
       " {'id': '3037451072',\n",
       "  'title': 'Analysis of Risk Perceptions and Related Factors Concerning COVID-19 Epidemic in Chongqing, China.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '15',\n",
       "  'abstract': 'To assess perceptions of risk and related factors concerning COVID-19 epidemic among residents in Chongqing city, China. With convenience sampling, a web questionnaire survey was conducted among 476 residents living in Chongqing on February 13rd to 14th in 2020, when citizens just started to get back to work. Residents’ estimated perceived risks were (4.63\\u2009±\\u20090.57), (4.19\\u2009±\\u20090.76), (3.23\\u2009±\\u20090.91) and (2.29\\u2009±\\u20090.96) for the infectivity, pathogenicity, lethality and self-rated infection possibility of COVID-19, respectively. Females (OR\\u2009=\\u20094.234), people with income\\u2009≥\\u20092000 yuan (2000–4999 yuan: OR\\u2009=\\u20095.052, 5000–9999 yuan: OR\\u2009=\\u20094.301,\\u2009≥\\u200910,000 yuan: OR\\u2009=\\u200923.459), the married status (OR\\u2009=\\u20091.811), the divorced status, widows or widowers (OR\\u2009=\\u20093.038), people living with families including children (OR\\u2009=\\u20095.085) or chronic patients (OR\\u2009=\\u20092.423) had a higher perceived risk level, as well as people who used free media websites (OR\\u2009=\\u20091.756), community workers (OR\\u2009=\\u20094.064) or community information platforms (OR\\u2009=\\u20092.235) as main media information sources. The perceived risk increased by 4.9% for every one-year increase of age. People who used WeChat contacts (OR\\u2009=\\u20090.196) as the main media information source, reported a lower perceived risk. Residents reported a high level of risk perception towards COVID-19 in Chongqing and it was impacted by the population demographic characteristics. Media information sources, including community information platforms and community workers may cause the increase of public risk perceptions.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Shan He 1, Siyu Chen 2, Lingna Kong 1, Weiwei Liu 1'],\n",
       "  'related_topics': ['Population',\n",
       "   'Risk perception',\n",
       "   'Questionnaire',\n",
       "   'Environmental health',\n",
       "   'China',\n",
       "   'Information source',\n",
       "   'Psychology',\n",
       "   'Perception',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Related factors'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3003668884',\n",
       "   '3001465255',\n",
       "   '3008818676',\n",
       "   '2792024998',\n",
       "   '3016902371',\n",
       "   '3022459584',\n",
       "   '2091069417',\n",
       "   '3134923022']},\n",
       " {'id': '3034593359',\n",
       "  'title': 'Epidemiological and Clinical Characteristics of Cases During the Early Phase of COVID-19 Pandemic: A Systematic Review and Meta-Analysis',\n",
       "  'reference_count': '66',\n",
       "  'citation_count': '22',\n",
       "  'abstract': 'Background: On 29th December 2019, a cluster of cases displaying the symptoms of a \"pneumonia of unknown cause\" was identified in Wuhan, Hubei province of China. This systematic review and meta-analysis aims to review the epidemiological and clinical characteristics of COVID-19 cases in the early phase of the COVID-19 pandemic. Methods: The search strategy involved peer-reviewed studies published between 1st January and 11th February 2020 in Pubmed, Google scholar and China Knowledge Resource Integrated database. Publications identified were screened for their title and abstracts according to the eligibility criteria, and further shortlisted by full-text screening. Three independent reviewers extracted data from these studies, and studies were assessed for potential risk of bias. Studies comprising non-overlapping patient populations, were included for qualitative and quantitative synthesis of results. Pooled prevalence with 95% confidence intervals were calculated for patient characteristics. Results: A total of 29 publications were selected after full-text review. This comprised of 18 case reports, three case series and eight cross-sectional studies on patients admitted from mid-December of 2019 to early February of 2020. A total of 533 adult patients with pooled median age of 56 (95% CI: 49-57) and a pooled prevalence of male of 60% (95% CI: 52-68%) were admitted to hospital at a pooled median of 7 days (95% CI: 7-7) post-onset of symptoms. The most common symptoms at admission were fever, cough and fatigue, with a pooled prevalence of 90% (95% CI: 81-97%), 58% (95% CI: 47-68%), and 50% (95% CI: 29-71%), respectively. Myalgia, shortness of breath, headache, diarrhea and sore throat were less common with pooled prevalence of 27% (95% CI: 20-36%), 25% (95% CI: 15-35%), 10% (95% CI: 7-13%), 8% (95% CI: 5-13%), and 7% (95% CI: 1-15%), respectively. ICU patients had a higher proportion of shortness of breath at presentation, as well as pre-existing hypertension, cardiovascular disease and COPD, compared to non-ICU patients in 2 studies (n = 179). Conclusion: This study highlights the key epidemiological and clinical features of COVID-19 cases during the early phase of the COVID-19 pandemic.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Jiayun Koh',\n",
       "   'Shimoni Urvish Shah',\n",
       "   'Pearleen Ee Yong Chua',\n",
       "   'Hao Gui',\n",
       "   'Junxiong Pang'],\n",
       "  'related_topics': ['Epidemiology',\n",
       "   'Meta-analysis',\n",
       "   'Sore throat',\n",
       "   'Confidence interval',\n",
       "   'COPD',\n",
       "   'Internal medicine',\n",
       "   'myalgia',\n",
       "   'Disease cluster',\n",
       "   'Pneumonia',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3008028633',\n",
       "   '3002539152',\n",
       "   '3001195213',\n",
       "   '3003465021']},\n",
       " {'id': '3033301213',\n",
       "  'title': 'Does Early Childhood Vaccination Protect Against COVID-19?',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '16',\n",
       "  'abstract': 'The coronavirus disease 2019 (COVID-19) is an on-going pandemic caused by the SARS-coronavirus-2 (SARS-CoV-2) which targets the respiratory system of humans. The published data show that children, unlike adults, are less susceptible to contracting the disease. This article aims at understanding why children constitute a minor group among hospitalized COVID-19 patients. Here, we hypothesize that the measles, mumps, and rubella (MMR) vaccine could provide a broad neutralizing antibody against numbers of diseases, including COVID-19. Our hypothesis is based on the 30 amino acid sequence homology between the SARS-CoV-2 Spike (S) glycoprotein (PDB: 6VSB) of both the measles virus fusion (F1) glycoprotein (PDB: 5YXW_B) and the rubella virus envelope (E1) glycoprotein (PDB: 4ADG_A). Computational analysis of the homologous region detected the sequence as antigenic epitopes in both measles and rubella. Therefore, we believe that humoral immunity, created through the MMR vaccination, provides children with advantageous protection against COVID-19 as well, however, an experimental analysis is required.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Karzan R Sidiq 1, Dana Khdr Sabir 2, Shakhawan M Ali 3, Rimantas Kodzius 4'],\n",
       "  'related_topics': ['Rubella',\n",
       "   'Rubella virus',\n",
       "   'Measles virus',\n",
       "   'Vaccination',\n",
       "   'Measles',\n",
       "   'Immunization',\n",
       "   'Virus',\n",
       "   'Neutralizing antibody',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'references': ['3004318991',\n",
       "   '3003217347',\n",
       "   '3008818676',\n",
       "   '3007643904',\n",
       "   '3011242477',\n",
       "   '3010819577',\n",
       "   '3012310845',\n",
       "   '3004348779',\n",
       "   '3035011439',\n",
       "   '3004896487']},\n",
       " {'id': '3021916232',\n",
       "  'title': 'Knowledge, awareness and practice of health care professionals amid sars-cov-2, corona virus disease outbreak',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '15',\n",
       "  'abstract': 'Objective: To assess the knowledge, awareness and practice level of health care workers towards Corona Virus disease - 2019 (COVID-19). Methods: A cross sectional study was conducted by administering a well-structured questionnaire comprising of three sections including knowledge, attitude and practice amongst health care professionals in various hospitals and clinics, over a duration of two months ‘Feb-March’ 2020. The data from 810 participants were collected manually as well as through online survey registered on www.surveys.google.com, using a validated questionnaire. The questionnaire comprised of three sections assessing knowledge, awareness and practice of participants. The descriptive analysis was carried out for demographics and dependent variables with statistical program for social sciences. Spearman test was used to detect any relationship between the health care professional response with respect to their gender and level of education. A p-value of < 0.05 was considered statistically significant. Results: More than half (57.2%) of the health care professionals were working in a hospital setting. Fifty two percent of health care professionals had awareness and 72% were practicing adequate measures to combat COVID-19. The majority (81.9%) believed that the sign and symptoms are similar to a common flu and the main strata of population that could be affected by COVID-19 are elderly (79%). Seventy three percent of participants did not attend any lecture, workshop or seminar on COVID-19 for awareness purpose. Sixty seven percent of health care professionals were practicing universal precaution for infection control and 57.4% were using sodium hypochlorite as a surface disinfectant in dental surgeries. There was no significant relationship (p > 0.05) between the health care professionals’ responses with gender and their education level. Conclusion: The study suggests that the vast majority of the health care professionals have adequate knowledge and awareness related to COVID-19. However some aspects of practice of health care professionals were found to be deficient including, following CDC guidelines during patient care, acquiring verified knowledge related to COVID-19, disinfection protocol and the use of N-95 mask. Mandatory Continued professional development programs including lectures and workshops on COVID-19 for all health care professionals are the need of the hour, to manage the pandemic and limiting the morbidity and mortality related to it. doi: https://doi.org/10.12669/pjms.36.COVID19-S4.2704 How to cite this:Ahmed N, Shakoor M, Vohra F, Abduljabbar T, Mariam Q, Rehman MA. Knowledge, Awareness and Practice of Health care Professionals amid SARS-CoV-2, Corona Virus Disease Outbreak. Pak J Med Sci. 2020;36(COVID19-S4):COVID19-S49-S56.  doi: https://doi.org/10.12669/pjms.36.COVID19-S4.2704 This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Naseer Ahmed 1, Maria Shakoor 2, Fahim Vohra 3, Tariq Abduljabbar 3, Quratulain Mariam 4, Mariam Abdul Rehman 5'],\n",
       "  'related_topics': ['Health care', 'Population', 'Universal precautions'],\n",
       "  'references': ['3008827533',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3008696669',\n",
       "   '3008818676',\n",
       "   '3006645647',\n",
       "   '3011176674',\n",
       "   '3009607814',\n",
       "   '3011802905',\n",
       "   '3005798348']},\n",
       " {'id': '3031029566',\n",
       "  'title': 'Identification of RT-PCR-Negative Asymptomatic COVID-19 Patients via Serological Testing',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '6',\n",
       "  'abstract': 'Asymptomatic individuals with coronavirus disease (COVID-19) have been identified via nucleic acid testing for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2); however, the epidemiologic characteristics and viral shedding pattern of asymptomatic patients remain largely unknown. In this study, serological testing was applied when identifying nine asymptomatic cases of COVID-19 who showed persistent negative RT-PCR test results for SARS-CoV-2 nucleic acid and no symptoms of COVID-19. Two asymptomatic cases were presumed to be index patients who had cleared the virus when their close contacts developed symptoms of COVID-19. Three of the asymptomatic cases were local individuals who spontaneously recovered before their presumed index patients developed symptoms of COVID-19. This report presents the epidemiologic and clinical characteristics of asymptomatic individuals with SARS-CoV-2 infection that were undetected on RT-PCR tests in previous epidemiologic investigations probably due to the transient viral shedding duration.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Jinru Wu 1, Xinyi Liu 1, Dan Zhou 2, Guangqian Qiu 2, Miao Dai 2, Qingting Yang 2, Zhonghui Pan 2, Ning Zhou 3, Pa Wu 1'],\n",
       "  'related_topics': ['Asymptomatic', 'Viral shedding', 'Coronavirus'],\n",
       "  'references': ['3002539152',\n",
       "   '3006961006',\n",
       "   '3008696669',\n",
       "   '3008818676',\n",
       "   '3015571324',\n",
       "   '3008874180',\n",
       "   '3010781325',\n",
       "   '3035011439',\n",
       "   '3015792206',\n",
       "   '3012188173']},\n",
       " {'id': '3037552531',\n",
       "  'title': 'Analysis of clinical features and early warning signs in patients with severe COVID-19: A retrospective cohort study.',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '4',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) was first identified in Wuhan, China, in December 2019. Although previous studies have described the clinical aspects of COVID-19, few studies have focused on the early detection of severe COVID-19. Therefore, this study aimed to identify the predictors of severe COVID-19 and to compare clinical features between patients with severe COVID-19 and those with less severe COVID-19. Patients admitted to designated hospital in the Henan Province of China who were either discharged or died prior to February 15, 2020 were enrolled retrospectively. Additionally, patients who underwent at least one of the following treatments were assigned to the severe group: continuous renal replacement therapy, high-flow oxygen absorption, noninvasive and invasive mechanical ventilation, or extracorporeal membrane oxygenation. The remaining patients were assigned to the non-severe group. Demographic information, initial symptoms, and first visit examination results were collected from the electronic medical records and compared between the groups. Multivariate logistic regression analysis was performed to determine the predictors of severe COVID-19. A receiver operating characteristic curve was used to identify a threshold for each predictor. Altogether,104 patients were enrolled in our study with 30 and 74 patients in the severe and non-severe groups, respectively. Multivariate logistic analysis indicated that patients aged ≥63 years (odds ratio = 41.0; 95% CI: 2.8, 592.4), with an absolute lymphocyte value of ≤1.02×109/L (odds ratio = 6.1; 95% CI = 1.5, 25.2) and a C-reactive protein level of ≥65.08mg/L (odds ratio = 8.9; 95% CI = 1.0, 74.2) were at a higher risk of severe illness. Thus, our results could be helpful in the early detection of patients at risk for severe illness, enabling the implementation of effective interventions and likely lowering the morbidity of COVID-19 patients.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Xinkui Liu 1, Xinpei Yue 1, Furong Liu 1, Le Wei 1, Yuntian Chu 2, Honghong Bao 1, Yichao Dong 1, Wenjie Cheng 1, Linpeng Yang 1'],\n",
       "  'related_topics': ['Odds ratio',\n",
       "   'Retrospective cohort study',\n",
       "   'Predictive value of tests',\n",
       "   'Multivariate analysis',\n",
       "   'Renal replacement therapy',\n",
       "   'Logistic regression',\n",
       "   'Extracorporeal membrane oxygenation',\n",
       "   'Medical record',\n",
       "   'Internal medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3009885589',\n",
       "   '3004280078',\n",
       "   '3008818676',\n",
       "   '3007814559',\n",
       "   '3009976289',\n",
       "   '2131262274',\n",
       "   '3009859788']},\n",
       " {'id': '3037851904',\n",
       "  'title': 'Could urinary ACE2 protein level help identify individuals susceptible to SARS-CoV-2 infection and complication?',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '1',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Xiaotian Ni 1, 2, Changqing Sun 1, Yaping Tian 3, Yanjie Huang 4, Tongqing Gong 5, Lan Song 2, Xing Yang 5, Kai Li 2, Nairen Zheng 2, Jianping Wang 5, Hongxing Wu 5, Ruoxian Zhang 2, Yi Wang 2, Guangshun Wang 1, Jun Qin 1, 2'],\n",
       "  'related_topics': ['Complication',\n",
       "   'Virology',\n",
       "   'Urinary system',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Protein level',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3008827533',\n",
       "   '3004280078',\n",
       "   '3008818676',\n",
       "   '2601317354',\n",
       "   '2946740876']},\n",
       " {'id': '2102634410',\n",
       "  'title': 'Fleischner Society: Glossary of Terms for Thoracic Imaging',\n",
       "  'reference_count': '134',\n",
       "  'citation_count': '3,000',\n",
       "  'abstract': 'Members of the Fleischner Society compiled a glossary of terms for thoracic imaging that replaces previous glossaries published in 1984 and 1996 for thoracic radiography and computed tomography (CT), respectively. The need to update the previous versions came from the recognition that new words have emerged, others have become obsolete, and the meaning of some terms has changed. Brief descriptions of some diseases are included, and pictorial examples (chest radiographs and CT scans) are provided for the majority of terms.',\n",
       "  'date': 2008,\n",
       "  'authors': ['David M Hansell',\n",
       "   'Alexander A Bankier',\n",
       "   'Heber MacMahon',\n",
       "   'Theresa C McLoud',\n",
       "   'Nestor L Müller',\n",
       "   'Jacques Remy'],\n",
       "  'related_topics': ['Glossary',\n",
       "   'Radiography',\n",
       "   'Thorax',\n",
       "   'Medical physics',\n",
       "   'Medicine',\n",
       "   'Computed tomography',\n",
       "   'Cystic lung disease',\n",
       "   'Thoracic Radiography',\n",
       "   'Thoracic imaging',\n",
       "   'X ray computed'],\n",
       "  'references': ['2416914730',\n",
       "   '2626588662',\n",
       "   '2017898137',\n",
       "   '1924766221',\n",
       "   '2121350896',\n",
       "   '2041775285',\n",
       "   '2107051779',\n",
       "   '2157678780',\n",
       "   '2114857071',\n",
       "   '2155323443']},\n",
       " {'id': '2800783955',\n",
       "  'title': 'Radiographic and CT Features of Viral Pneumonia.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '428',\n",
       "  'abstract': 'Viruses are the most common causes of respiratory infection. The imaging findings of viral pneumonia are diverse and overlap with those of other nonviral infectious and inflammatory conditions. However, identification of the underlying viral pathogens may not always be easy. There are a number of indicators for identifying viral pathogens on the basis of imaging patterns, which are associated with the pathogenesis of viral infections. Viruses in the same viral family share a similar pathogenesis of pneumonia, and the imaging patterns have distinguishable characteristics. Although not all cases manifest with typical patterns, most typical imaging patterns of viral pneumonia can be classified according to viral families. Although a definite diagnosis cannot be achieved on the basis of imaging features alone, recognition of viral pneumonia patterns may aid in differentiating viral pathogens, thus reducing the use of antibiotics. Recently, new viruses associated with recent outbreaks including human metapneumovirus, severe acute respiratory syndrome coronavirus, and Middle East respiratory syndrome coronavirus have been discovered. The imaging findings of these emerging pathogens have been described in a few recent studies. This review focuses on the radiographic and computed tomographic patterns of viral pneumonia caused by different pathogens, including new pathogens. Clinical characteristics that could affect imaging, such as patient age and immune status, seasonal variation and community outbreaks, and pathogenesis, are also discussed. The first goal of this review is to indicate that there are imaging features that should raise the possibility of viral infections. Second, to help radiologists differentiate viral infections, viruses in the same viridae that have similar pathogenesis and can have similar imaging characteristics are shown. By considering both the clinical and radiologic characteristics, radiologists can suggest the diagnosis of viral pneumonia. ©RSNA, 2018.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Hyun Jung Koo',\n",
       "   'Soyeoun Lim',\n",
       "   'Jooae Choe',\n",
       "   'Sang Ho Choi',\n",
       "   'Heungsup Sung',\n",
       "   'Kyung Hyun Do'],\n",
       "  'related_topics': ['Viral pneumonia',\n",
       "   'Pneumonia (non-human)',\n",
       "   'Respiratory infection',\n",
       "   'Human metapneumovirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Immunology',\n",
       "   'Pathogenesis',\n",
       "   'Outbreak',\n",
       "   'Antibiotics',\n",
       "   'Medicine'],\n",
       "  'references': ['3004906315',\n",
       "   '3006643024',\n",
       "   '3004668429',\n",
       "   '3010278110',\n",
       "   '3008207212',\n",
       "   '3010061930',\n",
       "   '3009925957',\n",
       "   '3010902474',\n",
       "   '3007670341',\n",
       "   '3012817089']},\n",
       " {'id': '2112136274',\n",
       "  'title': 'Middle East Respiratory Syndrome Coronavirus (MERS-CoV) Infection: Chest CT Findings',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '266',\n",
       "  'abstract': 'OBJECTIVE. The purpose of this study was to describe the chest CT findings in seven patients with Middle East respiratory syndrome coronavirus (MERS-CoV) infection. CONCLUSION. The most common CT finding in hospitalized patients with MERS-CoV infection is that of bilateral predominantly subpleural and basilar airspace changes, with more extensive ground-glass opacities than consolidation. The subpleural and peribronchovascular predilection of the abnormalities is suggestive of an organizing pneumonia pattern.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Amr M. Ajlan',\n",
       "   'Rayan A. Ahyad',\n",
       "   'Lamia Ghazi Jamjoom',\n",
       "   'Ahmed Alharthy',\n",
       "   'Tariq A. Madani'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Coronavirus',\n",
       "   'Internal medicine',\n",
       "   'Pathology',\n",
       "   'Medicine',\n",
       "   'Chest ct',\n",
       "   'Ct findings',\n",
       "   'Hospitalized patients',\n",
       "   'Infection chest',\n",
       "   'Organizing pneumonia'],\n",
       "  'references': ['2166867592',\n",
       "   '2107053896',\n",
       "   '2006434809',\n",
       "   '2149661971',\n",
       "   '1703839189',\n",
       "   '2045002682',\n",
       "   '1852588318',\n",
       "   '2109520345',\n",
       "   '2119837294',\n",
       "   '2049975503']},\n",
       " {'id': '2056155046',\n",
       "  'title': 'Severe Acute Respiratory Syndrome: Temporal Lung Changes at Thin-Section CT in 30 Patients',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '279',\n",
       "  'abstract': 'PURPOSE: To evaluate lung abnormalities on serial thin-section computed tomographic (CT) scans in patients with severe acute respiratory syndrome (SARS) during acute and convalescent periods. MATERIALS AND METHODS: Serial thin-section CT scans in 30 patients (17 men, aged 42.5 years ± 12.2 [SD]) with SARS were reviewed by two radiologists together for predominant patterns of lung abnormalities: ground-glass opacities, ground-glass opacities with superimposed linear opacities, consolidation, reticular pattern, and mixed pattern (consolidation, ground-glass opacities, and reticular pattern). Scans were classified according to duration in weeks after symptom onset. Longitudinal changes of specific abnormalities were documented in 17 patients with serial scans obtained during 3 weeks. Each lung was divided into three zones; each zone was evaluated for percentage of lung involvement. Summation of scores from all six lung zones provided overall CT score (maximal CT score, 24). RESULTS: Median CT scores increase...',\n",
       "  'date': 2004,\n",
       "  'authors': ['Gaik C. Ooi',\n",
       "   'Pek L. Khong',\n",
       "   'Nestor L. Müller',\n",
       "   'Wai C. Yiu',\n",
       "   'Lin J. Zhou',\n",
       "   'James C. M. Ho',\n",
       "   'Bing Lam',\n",
       "   'Savvas Nicolaou',\n",
       "   'Kenneth W. T. Tsang'],\n",
       "  'related_topics': ['Zones of the lung',\n",
       "   'Respiratory disease',\n",
       "   'Lung',\n",
       "   'Severe acute respiratory syndrome',\n",
       "   'Pulmonary fibrosis',\n",
       "   'Radiology',\n",
       "   'Respiratory system',\n",
       "   'Tomography',\n",
       "   'Surgery',\n",
       "   'Spontaneous remission',\n",
       "   'Medicine'],\n",
       "  'references': ['2025170735',\n",
       "   '2131262274',\n",
       "   '2125251240',\n",
       "   '1971054351',\n",
       "   '1976741900',\n",
       "   '2151996610',\n",
       "   '2119467724',\n",
       "   '2099918622',\n",
       "   '2098293966',\n",
       "   '2124413369']},\n",
       " {'id': '2279340859',\n",
       "  'title': 'Middle East Respiratory Syndrome-Coronavirus Infection: A Case Report of Serial Computed Tomographic Findings in a Young Male Patient.',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '26',\n",
       "  'abstract': 'Radiologic findings of Middle East respiratory syndrome (MERS), a novel coronavirus infection, have been rarely reported. We report a 30-year-old male presented with fever, abdominal pain, and diarrhea, who was diagnosed with MERS. A chest computed tomographic scan revealed rapidly developed multifocal nodular consolidations with ground-glass opacity halo and mixed consolidation, mainly in the dependent and peripheral areas. After treatment, follow-up imaging showed that these abnormalities markedly decreased but fibrotic changes developed.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Won Jin Choi', 'Ki Nam Lee', 'Eun Ju Kang', 'Hyuck Lee'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Middle East respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Abdominal pain',\n",
       "   'Diarrhea',\n",
       "   'Radiology',\n",
       "   'Radiography',\n",
       "   'Pathology',\n",
       "   'Medicine',\n",
       "   'Computed tomographic',\n",
       "   'Young male'],\n",
       "  'references': ['2166867592',\n",
       "   '2006434809',\n",
       "   '1703839189',\n",
       "   '2112136274',\n",
       "   '2119837294',\n",
       "   '2103118479',\n",
       "   '2111742711']},\n",
       " {'id': '2080286891',\n",
       "  'title': 'Severe acute respiratory syndrome: radiographic appearances and pattern of progression in 138 patients',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '392',\n",
       "  'abstract': 'PURPOSE: To retrospectively evaluate the radiographic appearances and pattern of progression of severe acute respiratory syndrome (SARS). MATERIALS AND METHODS: Chest radiographs obtained at clinical presentation and during treatment in 138 patients with confirmed SARS (66 men, 72 women; mean age, 39 years; age range, 20–83 years) were assessed. Radiographic appearances of pulmonary parenchymal abnormality, distribution, and extent of involvement on initial chest radiographs were documented. Recognizable patterns of radiographic progression were determined by comparing the overall mean percentage of lung involvement for each patient on serial radiographs. RESULTS: Initial chest radiographs were abnormal in 108 of 138 (78.3%) patients and showed air-space opacity. Lower lung zone (70 of 108, 64.8%) and right lung (82 of 108, 75.9%) were more commonly involved. In most patients, peripheral lung involvement was more common (81 of 108, 75.0%). Unifocal involvement (59 of 108, 54.6%) was more common than multi...',\n",
       "  'date': 2003,\n",
       "  'authors': ['K. T. Wong 1, Gregory E. Antonio',\n",
       "   'David S. C. Hui',\n",
       "   'Nelson Lee',\n",
       "   'Edmund H. Y. Yuen',\n",
       "   'Alan Wu',\n",
       "   'C. B. Leung',\n",
       "   'Timothy Rainer',\n",
       "   'Peter Cameron 2, Sydney S. C. Chung',\n",
       "   'Joseph J. Y. Sung',\n",
       "   'Anil T. Ahuja'],\n",
       "  'related_topics': ['Respiratory disease',\n",
       "   'Lung',\n",
       "   'Pneumonia',\n",
       "   'Radiology',\n",
       "   'Retrospective cohort study',\n",
       "   'Radiography',\n",
       "   'Surgery',\n",
       "   'Abnormality',\n",
       "   'Respiratory system',\n",
       "   'Medicine',\n",
       "   'Mean age'],\n",
       "  'references': ['2131262274',\n",
       "   '2151996610',\n",
       "   '2119467724',\n",
       "   '2129716802',\n",
       "   '2321891473']},\n",
       " {'id': '2162899218',\n",
       "  'title': 'Radiologic pattern of disease in patients with severe acute respiratory syndrome: the Toronto experience.',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '90',\n",
       "  'abstract': 'Severe acute respiratory syndrome (SARS) is a transmissible febrile respiratory illness caused by a recently discovered coronavirus. Various patterns of disease progression may be observed that have different implications for the prognosis in those affected by SARS. The appearance of the lungs on chest radiographs of patients with this condition may be normal or may include focal airspace opacity or multifocal or diffuse opacities. Thoracic computed tomography (CT) is more sensitive in depicting SARS than is conventional chest radiography, and CT images obtained in patients with normal chest radiographs may show extensive disease and airspace consolidation. However, because the radiologic appearance of SARS is not distinct from that of other diseases that cause lower respiratory tract infection, early identification of SARS will depend in part on the prompt recognition of clusters of cases of febrile respiratory tract illness. To aid in the differential diagnosis and management of SARS, radiologists must ...',\n",
       "  'date': 2004,\n",
       "  'authors': ['Narinder S. Paul 1, Heidi Roberts 2, Jagdish Butany 2, Tae Bong Chung 2, Wayne Gold 1, 2, Sangeeta Mehta 1, Eli Konen 2, Anuradha Rao 2, Yves Provost 2, Harry H. Hong 3, Leon Zelovitsky 2, Gordon L. Weisbrod 2'],\n",
       "  'related_topics': ['Lower respiratory tract infection',\n",
       "   'Differential diagnosis',\n",
       "   'Coronavirus'],\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '2131262274',\n",
       "   '2100820722',\n",
       "   '2163627712',\n",
       "   '2080286891',\n",
       "   '2124413369',\n",
       "   '2155100049',\n",
       "   '2123101845',\n",
       "   '2049695691']},\n",
       " {'id': '3004802901',\n",
       "  'title': 'CT Manifestations of Two Cases of 2019 Novel Coronavirus (2019-nCoV) Pneumonia.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '192',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yicheng Fang',\n",
       "   'Huangqi Zhang',\n",
       "   'Yunyu Xu',\n",
       "   'Jicheng Xie',\n",
       "   'Peipei Pang',\n",
       "   'Wenbin Ji'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Medicine',\n",
       "   'Pathology',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease progression',\n",
       "   'Lung pathology',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed',\n",
       "   'X ray computed'],\n",
       "  'references': ['3006643024',\n",
       "   '3006882119',\n",
       "   '3012211282',\n",
       "   '3008627141',\n",
       "   '3010659930',\n",
       "   '3025334942',\n",
       "   '3007355693',\n",
       "   '3034593359',\n",
       "   '3008928918',\n",
       "   '3013468450']},\n",
       " {'id': '2092969802',\n",
       "  'title': 'A decade after SARS: strategies for controlling emerging coronaviruses',\n",
       "  'reference_count': '153',\n",
       "  'citation_count': '595',\n",
       "  'abstract': 'Two novel coronaviruses have emerged in humans in the twenty-first century: severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV), both of which cause acute respiratory distress syndrome (ARDS) and are associated with high mortality rates. There are no clinically approved vaccines or antiviral drugs available for either of these infections; thus, the development of effective therapeutic and preventive strategies that can be readily applied to new emergent strains is a research priority. In this Review, we describe the emergence and identification of novel human coronaviruses over the past 10 years, discuss their key biological features, including tropism and receptor use, and summarize approaches for developing broadly effective vaccines.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Rachel L. Graham', 'Eric F. Donaldson', 'Ralph S. Baric'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'ARDS',\n",
       "   'Tissue tropism',\n",
       "   'Viral Epidemiology',\n",
       "   'Tropism',\n",
       "   'Intensive care medicine',\n",
       "   'Immunology',\n",
       "   'Biology',\n",
       "   'Acute respiratory distress',\n",
       "   'High mortality',\n",
       "   'Severe acute respiratory syndrome coronavirus'],\n",
       "  'references': ['2166867592',\n",
       "   '2132260239',\n",
       "   '2104548316',\n",
       "   '2107053896',\n",
       "   '2025170735',\n",
       "   '2006434809',\n",
       "   '1993577573',\n",
       "   '2119111857',\n",
       "   '1966238900',\n",
       "   '2160011624']},\n",
       " {'id': '3005272159',\n",
       "  'title': 'Chest CT Findings in 2019 Novel Coronavirus (2019-nCoV) Infections from Wuhan, China: Key Points for the Radiologist.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '492',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Jeffrey P Kanne'],\n",
       "  'related_topics': ['Radiology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Chest ct',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease progression',\n",
       "   'Lung pathology',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Tomography x ray computed'],\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '2903899730',\n",
       "   '2306794997',\n",
       "   '2103118479',\n",
       "   '2111742711',\n",
       "   '2056155046',\n",
       "   '3135910874',\n",
       "   '3024264813',\n",
       "   '3009749892']},\n",
       " {'id': '3001465255',\n",
       "  'title': 'A novel coronavirus outbreak of global health concern.',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '4,279',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Chen Wang 1, Peter W Horby 2, Frederick G Hayden 3, George F Gao 4'],\n",
       "  'related_topics': ['Coronavirus', 'Betacoronavirus', 'Outbreak'],\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '2006434809',\n",
       "   '2141877163',\n",
       "   '2162112824',\n",
       "   '2156614913']},\n",
       " {'id': '3001456238',\n",
       "  'title': 'Emerging coronaviruses: Genome structure, replication, and pathogenesis.',\n",
       "  'reference_count': '70',\n",
       "  'citation_count': '1,973',\n",
       "  'abstract': 'The recent emergence of a novel coronavirus (2019-nCoV), which is causing an outbreak of unusual viral pneumonia in patients in Wuhan, a central city in China, is another warning of the risk of CoVs posed to public health. In this minireview, we provide a brief introduction of the general features of CoVs and describe diseases caused by different CoVs in humans and animals. This review will help understand the biology and potential risk of CoVs that exist in richness in wildlife such as bats.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yu Chen 1, Qianyun Liu 1, Deyin Guo 2'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Virus classification',\n",
       "   'Outbreak',\n",
       "   'Viral pneumonia',\n",
       "   'Viral replication',\n",
       "   'Genetics',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Genome structure',\n",
       "   'Potential risk',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['2903899730',\n",
       "   '2470646526',\n",
       "   '2306794997',\n",
       "   '1993577573',\n",
       "   '311927316',\n",
       "   '2046153984',\n",
       "   '2105637133',\n",
       "   '2794573360',\n",
       "   '2148822770',\n",
       "   '2017248106']},\n",
       " {'id': '3004668429',\n",
       "  'title': 'Emerging 2019 Novel Coronavirus (2019-nCoV) Pneumonia.',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '917',\n",
       "  'abstract': 'BackgroundThe chest CT findings of patients with 2019 Novel Coronavirus (2019-nCoV) pneumonia have not previously been described in detail.PurposeTo investigate the clinical, laboratory, and imaging findings of emerging 2019-nCoV pneumonia in humans.Materials and MethodsFifty-one patients (25 men and 26 women; age range 16-76 years) with laboratory-confirmed 2019-nCoV infection by using real-time reverse transcription polymerase chain reaction underwent thin-section CT. The imaging findings, clinical data, and laboratory data were evaluated.ResultsFifty of 51 patients (98%) had a history of contact with individuals from the endemic center in Wuhan, China. Fever (49 of 51, 96%) and cough (24 of 51, 47%) were the most common symptoms. Most patients had a normal white blood cell count (37 of 51, 73%), neutrophil count (44 of 51, 86%), and either normal (17 of 51, 35%) or reduced (33 of 51, 65%) lymphocyte count. CT images showed pure ground-glass opacity (GGO) in 39 of 51 (77%) patients and GGO with reticular and/or interlobular septal thickening in 38 of 51 (75%) patients. GGO with consolidation was present in 30 of 51 (59%) patients, and pure consolidation was present in 28 of 51 (55%) patients. Forty-four of 51 (86%) patients had bilateral lung involvement, while 41 of 51 (80%) involved the posterior part of the lungs and 44 of 51 (86%) were peripheral. There were more consolidated lung lesions in patients 5 days or more from disease onset to CT scan versus 4 days or fewer (431 of 712 lesions vs 129 of 612 lesions; P < .001). Patients older than 50 years had more consolidated lung lesions than did those aged 50 years or younger (212 of 470 vs 198 of 854; P < .001). Follow-up CT in 13 patients showed improvement in seven (54%) patients and progression in four (31%) patients.ConclusionPatients with fever and/or cough and with conspicuous ground-glass opacity lesions in the peripheral and posterior lungs on CT images, combined with normal or decreased white blood cells and a history of epidemic exposure, are highly suspected of having 2019 Novel Coronavirus (2019-nCoV) pneumonia.© RSNA, 2020.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Fengxiang Song',\n",
       "   'Nannan Shi',\n",
       "   'Fei Shan',\n",
       "   'Zhiyong Zhang',\n",
       "   'Jie Shen',\n",
       "   'Hongzhou Lu',\n",
       "   'Yun Ling',\n",
       "   'Yebin Jiang',\n",
       "   'Yuxin Shi'],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'Absolute neutrophil count',\n",
       "   'Lung',\n",
       "   'Gastroenterology',\n",
       "   'Retrospective cohort study',\n",
       "   'Lymphocyte',\n",
       "   'Young adult',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Medicine',\n",
       "   'Internal medicine',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3002108456',\n",
       "   '2999409984',\n",
       "   '1993577573',\n",
       "   '3003901880',\n",
       "   '2800783955',\n",
       "   '2128312233',\n",
       "   '2015323375',\n",
       "   '2789752210',\n",
       "   '1922522683',\n",
       "   '1997020575']},\n",
       " {'id': '3012211282',\n",
       "  'title': 'Coronavirus Disease 2019 (COVID-19): A Systematic Review of Imaging Findings in 919 Patients.',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '991',\n",
       "  'abstract': 'OBJECTIVE. Available information on CT features of the 2019 novel coronavirus disease (COVID-19) is scattered in different publications, and a cohesive literature review has yet to be compiled. MAT...',\n",
       "  'date': 2020,\n",
       "  'authors': ['Sana Salehi',\n",
       "   'Aidin Abedi',\n",
       "   'Sudheer Balakrishnan',\n",
       "   'Ali Gholamrezanezhad'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pneumonia',\n",
       "   'Outbreak',\n",
       "   'Disease',\n",
       "   'Internal medicine',\n",
       "   'Medicine',\n",
       "   'Computed tomography',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3005679569',\n",
       "   '3004906315',\n",
       "   '3006354146',\n",
       "   '3003901880',\n",
       "   '3005272159',\n",
       "   '3006485704',\n",
       "   '3005656138']},\n",
       " {'id': '3008627141',\n",
       "  'title': 'Coronavirus Disease 2019 (COVID-19): A Perspective from China',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '1,249',\n",
       "  'abstract': 'In December 2019, an outbreak of severe acute respiratory syndrome coronavirus 2 infection occurred in Wuhan, Hubei Province, China, and spread across China and beyond. On February 12, 2020, the World Health Organization officially named the disease caused by the novel coronavirus as coronavirus disease 2019 (COVID-19). Because most patients infected with COVID-19 had pneumonia and characteristic CT imaging patterns, radiologic examinations have become vital in early diagnosis and the assessment of disease course. To date, CT findings have been recommended as major evidence for clinical diagnosis of COVID-19 in Hubei, China. This review focuses on the etiology, epidemiology, and clinical symptoms of COVID-19 while highlighting the role of chest CT in prevention and disease control.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Zi Yue Zu 1, Meng Di Jiang 2, Peng Peng Xu',\n",
       "   'Wen Chen',\n",
       "   'Qian Qian Ni',\n",
       "   'Guang Ming Lu',\n",
       "   'Long Jiang Zhang'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Pneumonia',\n",
       "   'Disease',\n",
       "   'Epidemiology',\n",
       "   'Pandemic',\n",
       "   'Etiology',\n",
       "   'Viral Epidemiology',\n",
       "   'Pediatrics',\n",
       "   'Medicine'],\n",
       "  'references': ['3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '3004318991',\n",
       "   '3004239190',\n",
       "   '3005679569',\n",
       "   '3003951199',\n",
       "   '3004906315',\n",
       "   '3008985036']},\n",
       " {'id': '3025334942',\n",
       "  'title': 'Clinical Features, Diagnosis, and Treatment of COVID-19 in Hospitalized Patients: A Systematic Review of Case Reports and Case Series',\n",
       "  'reference_count': '106',\n",
       "  'citation_count': '29',\n",
       "  'abstract': 'Introduction: The 2019 novel coronavirus (COVID-19) has been declared a public health emergency worldwide. The objective of this systematic review was to characterize the clinical, diagnostic, and treatment characteristics of hospitalized patients presenting with COVID-19. Methods: We conducted a structured search using PubMed/Medline, Embase, and Web of Science to collect both case reports and case series on COVID-19 published up to April 24, 2020. There were no restrictions regarding publication language. Results: Eighty articles were included analyzing a total of 417 patients with a mean age of 48 years. The most common presenting symptom in patients who tested positive for COVID-19 was fever, reported in up to 62% of patients from 82% of the analyzed studies. Other symptoms including rhinorrhea, dizziness, and chills were less frequently reported. Additionally, in studies that reported C-reactive protein (CRP) measurements, a large majority of patients displayed an elevated CRP (60%). Progression to acute respiratory distress syndrome (ARDS) was the most common complication of patients testing positive for COVID-19 (21%). CT images displayed ground-glass opacification (GGO) patterns (80%) as well as bilateral lung involvement (69%). The most commonly used antiviral treatment modalities included, lopinavir (HIV protease inhibitor), arbidiol hydrochloride (influenza fusion inhibitor), and oseltamivir (neuraminidase inhibitor). Conclusions: Development of ARDS may play a role in estimating disease progression and mortality risk. Early detection of elevations in serum CRP, combined with a clinical COVID-19 symptom presentation may be used as a surrogate marker for the presence and severity of the disease. There is a paucity of data surrounding the efficacy of treatments. There is currently not a well-established gold standard therapy for the treatment of diagnosed COVID-19. Further prospective investigations are necessary.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Azin Tahvildari 1, Mahta Arbabi 1, Yeganeh Farsi 1, Parnian Jamshidi 1, Saba Hasanzadeh 1, Tess Moore Calcagno 1, Mohammad Javad Nasiri 2, Mehdi Mirsaeidi 1'],\n",
       "  'related_topics': ['Chills',\n",
       "   'Surrogate endpoint',\n",
       "   'ARDS',\n",
       "   'Oseltamivir',\n",
       "   'rhinorrhea',\n",
       "   'Complication',\n",
       "   'Disease',\n",
       "   'Lopinavir',\n",
       "   'Internal medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '2007872832',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '2156098321']},\n",
       " {'id': '3009992310',\n",
       "  'title': 'Molecular immune pathogenesis and diagnosis of COVID-19.',\n",
       "  'reference_count': '73',\n",
       "  'citation_count': '1,036',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is a kind of viral pneumonia which is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The emergence of SARS-CoV-2 has been marked as the third introduction of a highly pathogenic coronavirus into the human population after the severe acute respiratory syndrome coronavirus (SARS-CoV) and the Middle East respiratory syndrome coronavirus (MERS-CoV) in the twenty-first century. In this minireview, we provide a brief introduction of the general features of SARS-CoV-2 and discuss current knowledge of molecular immune pathogenesis, diagnosis and treatment of COVID-19 on the base of the present understanding of SARS-CoV and MERS-CoV infections, which may be helpful in offering novel insights and potential therapeutic targets for combating the SARS-CoV-2 infection.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Xiaowei Li 1, 2, Manman Geng 1, 2, Yizhao Peng 1, 2, Liesu Meng 1, 2, Shemin Lu 1, 2'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Middle East respiratory syndrome coronavirus',\n",
       "   'Viral pneumonia'],\n",
       "  'references': ['3001118548',\n",
       "   '3004280078',\n",
       "   '3001195213',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '3003573988',\n",
       "   '3003217347',\n",
       "   '3007940623',\n",
       "   '3005212621',\n",
       "   '3004906315']},\n",
       " {'id': '3008801544',\n",
       "  'title': 'Clinical and computed tomographic imaging features of novel coronavirus pneumonia caused by SARS-CoV-2.',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '414',\n",
       "  'abstract': 'Summary Purpose To investigate the clinical and imaging characteristics of computed tomography (CT) in novel coronavirus pneumonia (NCP) caused by SARS-CoV-2. Materials and methods A retrospective analysis was performed on the imaging findings of patients confirmed with COVID-19 pneumonia who had chest CT scanning and treatment after disease onset. The clinical and imaging data were analyzed. Results Fifty patients were enrolled, including mild type in nine, common in 28, severe in 10 and critically severe in the rest three. Mild patients (29 years) were significantly (P °C), 49 (98%) patients had normal or slightly reduced leukocyte count, 14 (28%) had decreased counts of lymphocytes, and 26 (52%) patients had increased C-reactive protein. Nine mild patients were negative in CT imaging. For all the other types of NCP, the lesion was in the right upper lobe in 30 cases, right middle lobe in 22, right lower lobe in 39, left upper lobe in 33 and left lower lobe in 36. The lesion was primarily located in the peripheral area under the pleura with possible extension towards the pulmonary hilum. Symmetrical lesions were seen in 26 cases and asymmetrical in 15. The density of lesion was mostly uneven with ground glass opacity as the primary presentation accompanied by partial consolidation and fibrosis. Conclusion CT imaging presentations of NCP are mostly patchy ground glass opacities in the peripheral areas under the pleura with partial consolidation which will be absorbed with formation of fibrotic stripes if improved. CT scanning provides important bases for early diagnosis and treatment of NCP.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yu-Huan Xu 1, Jing-Hui Dong 1, Wei-Min An 1, Xiao-Yan Lv 1, Xiao-Ping Yin 2, Jian-Zeng Zhang 1, Li Dong 3, Xi Ma 2, Hong-Jie Zhang 2, Bu-Lang Gao 1'],\n",
       "  'related_topics': ['Ground-glass opacity',\n",
       "   'Lesion',\n",
       "   'Radiology',\n",
       "   'Fibrosis',\n",
       "   'Peripheral',\n",
       "   'Retrospective cohort study',\n",
       "   'Medicine',\n",
       "   'Computed tomography',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3005079553',\n",
       "   '3004906315',\n",
       "   '3006110666',\n",
       "   '3003901880',\n",
       "   '3006485704',\n",
       "   '3006472059',\n",
       "   '3004896587',\n",
       "   '3004511262',\n",
       "   '3004517278']},\n",
       " {'id': '3008928918',\n",
       "  'title': 'First case of Coronavirus Disease 2019 (COVID-19) pneumonia in Taiwan.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '155',\n",
       "  'abstract': 'An outbreak of respiratory illness proved to be infected by a 2019 novel coronavirus, officially named Coronavirus Disease 2019 (COVID-19), was notified first in Wuhan, China, and has spread rapidly in China and to other parts of the world. Herein, we reported the first confirmed case of novel coronavirus pneumonia (NCP) imported from China in Taiwan. This case report revealed a natural course of NCP with self-recovery, which may be a good example in comparison with medical treatments.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Shao-Chung Cheng 1, Yuan-Chia Chang 1, Yu-Long Fan Chiang 1, Yu-Chan Chien 1, Mingte Cheng 1, Chin-Hua Yang 2, Chia-Husn Huang 1, Yuan-Nian Hsu 1'],\n",
       "  'related_topics': ['Pneumonia', 'Betacoronavirus', 'Outbreak'],\n",
       "  'references': ['3001118548',\n",
       "   '3005079553',\n",
       "   '3003465021',\n",
       "   '3004906315',\n",
       "   '3003901880',\n",
       "   '3004668429',\n",
       "   '2791599184',\n",
       "   '3004511262',\n",
       "   '3004802901',\n",
       "   '2156614913']},\n",
       " {'id': '3014051579',\n",
       "  'title': 'Coronavirus disease 2019 (COVID-19): current status and future perspectives.',\n",
       "  'reference_count': '117',\n",
       "  'citation_count': '414',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) originated in the city of Wuhan, Hubei Province, Central China, and has spread quickly to 72 countries to date. COVID-19 is caused by a novel coronavirus, named severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) [previously provisionally known as 2019 novel coronavirus (2019-nCoV)]. At present, the newly identified SARS-CoV-2 has caused a large number of deaths with tens of thousands of confirmed cases worldwide, posing a serious threat to public health. However, there are no clinically approved vaccines or specific therapeutic drugs available for COVID-19. Intensive research on the newly emerged SARS-CoV-2 is urgently needed to elucidate the pathogenic mechanisms and epidemiological characteristics and to identify potential drug targets, which will contribute to the development of effective prevention and treatment strategies. Hence, this review will focus on recent progress regarding the structure of SARS-CoV-2 and the characteristics of COVID-19, such as the aetiology, pathogenesis and epidemiological characteristics.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Heng Li 1, Shang-Ming Liu 1, Xiao-Hua Yu 2, Shi-Lin Tang 1, Chao-Ke Tang 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Pandemic',\n",
       "   'Betacoronavirus',\n",
       "   'Epidemiology',\n",
       "   'Public health',\n",
       "   'Etiology',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '3001195213',\n",
       "   '3004318991',\n",
       "   '3003465021']},\n",
       " {'id': '3013468450',\n",
       "  'title': 'Novel Coronavirus Infection (COVID-19) in Humans: A Scoping Review and Meta-Analysis',\n",
       "  'reference_count': '84',\n",
       "  'citation_count': '340',\n",
       "  'abstract': 'A growing body of literature on the 2019 novel coronavirus (SARS-CoV-2) is becoming available, but a synthesis of available data has not been conducted. We performed a scoping review of currently available clinical, epidemiological, laboratory, and chest imaging data related to the SARS-CoV-2 infection. We searched MEDLINE, Cochrane CENTRAL, EMBASE, Scopus and LILACS from 01 January 2019 to 24 February 2020. Study selection, data extraction and risk of bias assessment were performed by two independent reviewers. Qualitative synthesis and meta-analysis were conducted using the clinical and laboratory data, and random-effects models were applied to estimate pooled results. A total of 61 studies were included (59,254 patients). The most common disease-related symptoms were fever (82%, 95% confidence interval (CI) 56%–99%; n = 4410), cough (61%, 95% CI 39%–81%; n = 3985), muscle aches and/or fatigue (36%, 95% CI 18%–55%; n = 3778), dyspnea (26%, 95% CI 12%–41%; n = 3700), headache in 12% (95% CI 4%–23%, n = 3598 patients), sore throat in 10% (95% CI 5%–17%, n = 1387) and gastrointestinal symptoms in 9% (95% CI 3%–17%, n = 1744). Laboratory findings were described in a lower number of patients and revealed lymphopenia (0.93 × 109/L, 95% CI 0.83–1.03 × 109/L, n = 464) and abnormal C-reactive protein (33.72 mg/dL, 95% CI 21.54–45.91 mg/dL; n = 1637). Radiological findings varied, but mostly described ground-glass opacities and consolidation. Data on treatment options were limited. All-cause mortality was 0.3% (95% CI 0.0%–1.0%; n = 53,631). Epidemiological studies showed that mortality was higher in males and elderly patients. The majority of reported clinical symptoms and laboratory findings related to SARS-CoV-2 infection are non-specific. Clinical suspicion, accompanied by a relevant epidemiological history, should be followed by early imaging and virological assay.',\n",
       "  'date': 2020,\n",
       "  'authors': [\"Israel Júnior Borges do Nascimento 1, Nensi Cacic 2, Hebatullah Mohamed Abdulazeem 3, Thilo Caspar von Groote 4, Umesh Jayarajah 5, Ishanka Weerasekara 6, 7, Meisam Abdar Esfahani 8, Vinicius Tassoni Civile 9, Ana Marusic 2, Ana Jeroncic 2, Nelson Carvas Junior 10, Tina Poklepovic Pericic 2, Irena Zakarija-Grkovic 2, Silvana Mangeon Meirelles Guimarães 1, Nicola Luigi Bragazzi 11, Maria Bjorklund 12, Ahmad Sofi-Mahmudi 8, Mohammad Altujjar 13, Maoyi Tian 14, Diana Maria Cespedes Arcani 15, Dónal P O'Mathúna 16, 17, Milena Soriano Marcolino 1\"],\n",
       "  'related_topics': ['Sore throat', 'Epidemiology', 'Confidence interval'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3004318991',\n",
       "   '3003465021',\n",
       "   '3008090866',\n",
       "   '3004239190']},\n",
       " {'id': '2786098272',\n",
       "  'title': 'Serological Evidence of Bat SARS-Related Coronavirus Infection in Humans, China',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '171',\n",
       "  'abstract': 'In our previous works, we have reported genetically diverse SARS-related coronaviruses (SARSr-CoV) in a single bat cave, Yunnan province, China, and suggested that some SARSr-CoVs may have high potential to infect humans without the necessity for an intermediate host. In this report, we developed a specific ELISA based on the nucleocapsid protein of a SARSr-CoV strain and detected its antibody in humans who are highly exposed to bat populations. From 218 human serum samples, 6 were positive against the nucleocapsid protein by ELISA and further confirmed by Western blot. For the first time, we demonstrated the SARSr-CoV had spillover to humans, although did not cause clinical diseases.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Ning Wang 1, Shi-Yue Li 2, Xing-Lou Yang 1, Hui-Min Huang 2, Yu-Ji Zhang 1, Hua Guo 1, Chu-Ming Luo 1, Maureen Miller 3, Guangjian Zhu 3, Aleksei A. Chmura 3, Emily Hagan 3, Ji-Hua Zhou 4, Yun-Zhi Zhang 5, Lin-Fa Wang 6, Peter Daszak 3, Zheng-Li Shi 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Serology',\n",
       "   'Intermediate host',\n",
       "   'Antibody',\n",
       "   'Virology',\n",
       "   'Western blot',\n",
       "   'Medical microbiology',\n",
       "   'Strain (biology)',\n",
       "   'Biology',\n",
       "   'Severe acute respiratory syndrome-related coronavirus'],\n",
       "  'references': ['2104548316',\n",
       "   '2025170735',\n",
       "   '1993577573',\n",
       "   '2103503670',\n",
       "   '2298153446',\n",
       "   '2134061616',\n",
       "   '2126707939',\n",
       "   '2285897784',\n",
       "   '2200668708',\n",
       "   '2151461700']},\n",
       " {'id': '2021442163',\n",
       "  'title': 'Organ distribution of severe acute respiratory syndrome (SARS) associated coronavirus (SARS-CoV) in SARS patients: implications for pathogenesis and virus transmission pathways.',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '842',\n",
       "  'abstract': 'We previously identified the major pathological changes in the respiratory and immune systems of patients who died of severe acute respiratory syndrome (SARS) but gained little information on the organ distribution of SARS-associated coronavirus (SARS-CoV). In the present study, we used a murine monoclonal antibody specific for SARS-CoV nucleoprotein, and probes specific for a SARS-CoV RNA polymerase gene fragment, for immunohistochemistry and in situ hybridization, respectively, to detect SARS-CoV systematically in tissues from patients who died of SARS. SARS-CoV was found in lung, trachea/bronchus, stomach, small intestine, distal convoluted renal tubule, sweat gland, parathyroid, pituitary, pancreas, adrenal gland, liver and cerebrum, but was not detected in oesophagus, spleen, lymph node, bone marrow, heart, aorta, cerebellum, thyroid, testis, ovary, uterus or muscle. These results suggest that, in addition to the respiratory system, the gastrointestinal tract and other organs with detectable SARS-CoV may also be targets of SARS-CoV infection. The pathological changes in these organs may be caused directly by the cytopathic effect mediated by local replication of the SARS-CoV; or indirectly as a result of systemic responses to respiratory failure or the harmful immune response induced by viral infection. In addition to viral spread through a respiratory route, SARS-CoV in the intestinal tract, kidney and sweat glands may be excreted via faeces, urine and sweat, thereby leading to virus transmission. This study provides important information for understanding the pathogenesis of SARS-CoV infection and sheds light on possible virus transmission pathways. This data will be useful for designing new strategies for prevention and treatment of SARS.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Yanqing Ding 1, Li He 1, Qingling Zhang 1, Zhongxi Huang 1, Xiaoyan Che 2, Jinlin Hou 3, Huijun Wang 1, Hong Shen 1, Liwen Qiu 3, Zhuguo Li 1, Jian Geng 1, Junjie Cai 1, Huixia Han 1, Xin Li 1, Wei Kang 1, Desheng Weng 1, Ping Liang 1, Shibo Jiang 4'],\n",
       "  'related_topics': ['Severe acute respiratory syndrome',\n",
       "   'Coronavirus',\n",
       "   'Respiratory disease',\n",
       "   'Lung',\n",
       "   'Pathogenesis',\n",
       "   'Sweat gland',\n",
       "   'Coronaviridae',\n",
       "   'Respiratory system',\n",
       "   'Pathology',\n",
       "   'Immunology',\n",
       "   'Biology'],\n",
       "  'references': ['2132260239',\n",
       "   '2104548316',\n",
       "   '2025170735',\n",
       "   '2116586125',\n",
       "   '1966238900',\n",
       "   '2169198329',\n",
       "   '2134061616',\n",
       "   '2168446943',\n",
       "   '1757215199',\n",
       "   '2158118659']},\n",
       " {'id': '3025232310',\n",
       "  'title': 'Humoral Immune Responses in COVID-19 Patients: A Window on the State of the Art.',\n",
       "  'reference_count': '75',\n",
       "  'citation_count': '68',\n",
       "  'abstract': 'The novel SARS-CoV-2 is a recently emerging virus causing a human pandemic. A great variety of symptoms associated with COVID-19 disease, ranging from mild to severe symptoms, eventually leading to death. Specific SARS-CoV-2 RT-PCR is the standard method to screen symptomatic people; however, asymptomatic subjects and subjects with undetectable viral load escape from the screening, contributing to viral spread. Currently, the lock down imposed by many governments is an important measure to contain the spread, as there is no specific antiviral therapy or a vaccine and the main treatments are supportive. Therefore, there is urgent need to characterize the virus and the viral-mediated responses, in order to develop specific diagnostic and therapeutic tools to prevent viral transmission and efficiently cure COVID-19 patients. Here, we review the current studies on two viral mediated-responses, specifically the cytokine storm occurring in a subset of patients and the antibody response triggered by the infection. Further studies are needed to explore both the dynamics and the mechanisms of the humoral immune response in COVID-19 patients, in order to guide future vaccine design and antibody-based therapies for the management of the disease.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Gabriel Siracusano', 'Claudia Pastori', 'Lucia Lopalco'],\n",
       "  'related_topics': ['Viral load',\n",
       "   'Disease',\n",
       "   'Cytokine storm',\n",
       "   'Virus',\n",
       "   'Immune system',\n",
       "   'Antibody',\n",
       "   'Pandemic',\n",
       "   'Asymptomatic',\n",
       "   'Immunology',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3004280078',\n",
       "   '3001465255',\n",
       "   '3003217347',\n",
       "   '3007643904',\n",
       "   '3005679569',\n",
       "   '3012756997',\n",
       "   '3009314935',\n",
       "   '3013985547']},\n",
       " {'id': '3028321619',\n",
       "  'title': 'SARS-CoV-2 Infection and the Newborn',\n",
       "  'reference_count': '83',\n",
       "  'citation_count': '16',\n",
       "  'abstract': 'Severe Acute Respiratory Syndrome Coronavirus Type 2 (SARS-CoV-2) affects people at all ages and it may be encountered in pregnant women and newborns also. The information about its clinical features, laboratory findings and prognosis in children and newborns is scarce. All the reported cases in pregnant women were in the 2nd or 3rd trimester and only 1% of them developed severe disease. Miscarriages are rare. Materno-fetal transmission of the disease is controversial. Definitive diagnosis can be made by a history of contact with a proven case, fever, pneumonia and gastrointestinal disorder and a Polymerase chain reaction (PCR) test of nasopharyngeal swabs. Lymphopenia as well as liver and renal dysfunctions may be seen. Suspected or proven cases of newborns with symptoms should be quarantined in the neonatal intensive care unit for at least 14 days with standart and droplet isolation precautions. Asymptomatic infants may be quaratined at home. Transport of the neonates should be performed in a dedicated transport incubator and ambulance with isolation precautions. There is no specific treatment for the disease, but hemodynamic stabilization of the infant, respiratory management and other daily care are essential. Drugs against cytokine storm syndrome such as corticosteroids or tocilizumab are under investigation. Routine antibiotics are not recommended. No deaths have been reported so far in the neonatal population. Families and healthcare staff should receive pyschological support. Since the infection is quite new and knowledge is constantly accumulating, following developments and continuous updates are crucial.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Fahri Ovalı'],\n",
       "  'related_topics': ['Neonatal intensive care unit',\n",
       "   'Gastrointestinal disorder',\n",
       "   'Population',\n",
       "   'Pneumonia',\n",
       "   'Asymptomatic',\n",
       "   'Disease',\n",
       "   'Pregnancy',\n",
       "   'Transmission (medicine)',\n",
       "   'Pediatrics',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3008028633',\n",
       "   '3002539152',\n",
       "   '3004318991',\n",
       "   '3007940623',\n",
       "   '3010233963',\n",
       "   '3010604545',\n",
       "   '3005212621',\n",
       "   '3011610993',\n",
       "   '3005679569']},\n",
       " {'id': '3027541845',\n",
       "  'title': 'Psycho-Neuroendocrine-Immune Interactions in COVID-19: Potential Impacts on Mental Health.',\n",
       "  'reference_count': '153',\n",
       "  'citation_count': '25',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2). The impacts of the disease may be beyond the respiratory system, also affecting mental health. Several factors may be involved in the association between COVID-19 and psychiatric outcomes, such as fear inherent in the pandemic, adverse effects of treatments, as well as financial stress, and social isolation. Herein we discuss the growing evidence suggesting that the relationship between SARS-CoV-2 and host may also trigger changes in brain and behavior. Based on the similarity of SARS-CoV-2 with other coronaviruses, it is conceivable that changes in endocrine and immune response in the periphery or in the central nervous system may be involved in the association between SARS-CoV-2 infection and impaired mental health. This is likely to be further enhanced, since millions of people worldwide are isolated in quarantine to minimize the transmission of SARS-CoV-2 and social isolation can also lead to neuroendocrine-immune changes. Accordingly, we highlight here the hypothesis that neuroendocrine-immune interactions may be involved in negative impacts of SARS-CoV-2 infection and social isolation on psychiatric issues.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ícaro Raony 1, Camila Saggioro de Figueiredo 1, Pablo Pandolfo 1, Elizabeth Giestal-de-Araujo 1, 2, Priscilla Oliveira-Silva Bomfim 1, 2, 3, Wilson Savino 2, 3'],\n",
       "  'related_topics': ['Disease', 'Social isolation', 'Mental health'],\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3006659024',\n",
       "   '2903899730',\n",
       "   '3012421327',\n",
       "   '3009506062',\n",
       "   '3006645647',\n",
       "   '3004348779',\n",
       "   '3015197879']},\n",
       " {'id': '3003637715',\n",
       "  'title': 'Molecular Diagnosis of a Novel Coronavirus (2019-nCoV) Causing an Outbreak of Pneumonia.',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '703',\n",
       "  'abstract': 'BACKGROUND: A novel coronavirus of zoonotic origin (2019-nCoV) has recently been identified in patients with acute respiratory disease. This virus is genetically similar to SARS coronavirus and bat SARS-like coronaviruses. The outbreak was initially detected in Wuhan, a major city of China, but has subsequently been detected in other provinces of China. Travel-associated cases have also been reported in a few other countries. Outbreaks in health care workers indicate human-to-human transmission. Molecular tests for rapid detection of this virus are urgently needed for early identification of infected patients. METHODS: We developed two 1-step quantitative real-time reverse-transcription PCR assays to detect two different regions (ORF1b and N) of the viral genome. The primer and probe sets were designed to react with this novel coronavirus and its closely related viruses, such as SARS coronavirus. These assays were evaluated using a panel of positive and negative controls. In addition, respiratory specimens from two 2019-nCoV-infected patients were tested. RESULTS: Using RNA extracted from cells infected by SARS coronavirus as a positive control, these assays were shown to have a dynamic range of at least seven orders of magnitude (2x10-4-2000 TCID50/reaction). Using DNA plasmids as positive standards, the detection limits of these assays were found to be below 10 copies per reaction. All negative control samples were negative in the assays. Samples from two 2019-nCoV-infected patients were positive in the tests. CONCLUSIONS: The established assays can achieve a rapid detection of 2019n-CoV in human samples, thereby allowing early identification of patients.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Daniel K W Chu 1, Yang Pan 2, 3, Samuel M S Cheng 1, Kenrie P Y Hui 1, Pavithra Krishnan 1, Yingzhi Liu 1, Daisy Y M Ng 1, Carrie K C Wan 1, Peng Yang 2, 3, Quanyi Wang 3, Malik Peiris 1, Leo L M Poon 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Virus',\n",
       "   'Outbreak',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Plasmid',\n",
       "   'Pneumonia',\n",
       "   'Viral Epidemiology',\n",
       "   'Biology'],\n",
       "  'references': ['3001195213',\n",
       "   '2903899730',\n",
       "   '2799524357',\n",
       "   '2470646526',\n",
       "   '2775086803',\n",
       "   '2134061616',\n",
       "   '2789368753',\n",
       "   '1998201250',\n",
       "   '1971383779',\n",
       "   '2954438954']},\n",
       " {'id': '3009885589',\n",
       "  'title': 'Clinical course and risk factors for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective cohort study.',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '21,762',\n",
       "  'abstract': 'Summary Background Since December, 2019, Wuhan, China, has experienced an outbreak of coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Epidemiological and clinical characteristics of patients with COVID-19 have been reported but risk factors for mortality and a detailed clinical course of illness, including viral shedding, have not been well described. Methods In this retrospective, multicentre cohort study, we included all adult inpatients (≥18 years old) with laboratory-confirmed COVID-19 from Jinyintan Hospital and Wuhan Pulmonary Hospital (Wuhan, China) who had been discharged or had died by Jan 31, 2020. Demographic, clinical, treatment, and laboratory data, including serial samples for viral RNA detection, were extracted from electronic medical records and compared between survivors and non-survivors. We used univariable and multivariable logistic regression methods to explore the risk factors associated with in-hospital death. Findings 191 patients (135 from Jinyintan Hospital and 56 from Wuhan Pulmonary Hospital) were included in this study, of whom 137 were discharged and 54 died in hospital. 91 (48%) patients had a comorbidity, with hypertension being the most common (58 [30%] patients), followed by diabetes (36 [19%] patients) and coronary heart disease (15 [8%] patients). Multivariable regression showed increasing odds of in-hospital death associated with older age (odds ratio 1·10, 95% CI 1·03–1·17, per year increase; p=0·0043), higher Sequential Organ Failure Assessment (SOFA) score (5·65, 2·61–12·23; p Interpretation The potential risk factors of older age, high SOFA score, and d-dimer greater than 1 μg/mL could help clinicians to identify patients with poor prognosis at an early stage. Prolonged viral shedding provides the rationale for a strategy of isolation of infected patients and optimal antiviral interventions in the future. Funding Chinese Academy of Medical Sciences Innovation Fund for Medical Sciences; National Science Grant for Distinguished Young Scholars; National Key Research and Development Program of China; The Beijing Science and Technology Project; and Major Projects of National Science and Technology on New Drug Creation and Development.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Fei Zhou 1, Ting Yu 2, Ronghui Du 3, Guohui Fan 4, Ying Liu 2, Zhibo Liu 1, Jie Xiang 5, Yeming Wang 6, Bin Song 2, Xiaoying Gu 1, 4, Lulu Guan 3, Yuan Wei 2, Hui Li 1, Xudong Wu 7, Jiuyang Xu 8, Shengjin Tu 2, Yi Zhang 1, Hua Chen 2, Bin Cao'],\n",
       "  'related_topics': ['Cohort study',\n",
       "   'Retrospective cohort study',\n",
       "   'Odds ratio',\n",
       "   'SOFA score',\n",
       "   'Epidemiology',\n",
       "   'Comorbidity',\n",
       "   'Young adult',\n",
       "   'Risk assessment',\n",
       "   'Internal medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003668884',\n",
       "   '3008090866',\n",
       "   '3006961006',\n",
       "   '2280404143',\n",
       "   '3007940623',\n",
       "   '1803784511',\n",
       "   '2026274122']},\n",
       " {'id': '3006961006',\n",
       "  'title': 'SARS-CoV-2 Viral Load in Upper Respiratory Specimens of Infected Patients.',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '3,839',\n",
       "  'abstract': 'SARS-CoV-2 Viral Load in Upper Respiratory Specimens The authors report results of an analysis of nasal and throat swabs from 17 patients in Zhuhai, China, who had received a diagnosis of Covid-19....',\n",
       "  'date': 2020,\n",
       "  'authors': ['Lirong Zou 1, Feng Ruan 1, Mingxing Huang 2, Lijun Liang 1, Huitao Huang 1, Zhongsi Hong 2, Jianxiang Yu 1, Min Kang 1, Yingchao Song 1, Jinyu Xia 2, Qianfang Guo 1, Tie Song 1, Jianfeng He 1, Hui Ling Yen 3, Malik Peiris 3, Jie Wu 1'],\n",
       "  'related_topics': ['Viral load', 'Throat', 'Respiratory system'],\n",
       "  'references': ['3004239190',\n",
       "   '2129542667',\n",
       "   '3024919756',\n",
       "   '2147166346',\n",
       "   '3034411794',\n",
       "   '3034408674',\n",
       "   '3018339046',\n",
       "   '2133748753']},\n",
       " {'id': '3130405932',\n",
       "  'title': 'Surgical site infections: guidance for elective surgery during the SARS-CoV-2 pandemic - international recommendations and clinical experience.',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Summary Background The COVID-19 pandemic not only had an impact on public life and healthcare facilities in general, but also affected established surgical workflows for elective procedures. The strategy to protect patients and healthcare workers from infection by SARS-CoV-2 in surgical departments has needed step-by-step development. Based on the evaluation of international recommendations and guidelines, as well as personal experiences in a clinical “hot spot” and in a 450-bed surgical clinic, an adapted surgical site infection (SSI) prevention checklist was needed to develop concise instructions, which described roles and responsibilities of health care professionals that could be used for wider guidance in pandemic conditions. Method Publications of COVID-19-related recommendations and guidelines, produced by health authorities and organizations, such as WHO, US-CDC, ECDC, the American College of Surgery and the Robert Koch Institute, were retrieved, assessed and referenced up to January 31st, 2020. Additionally, clinical personal experiences in Germany were evaluated and considered. Results Part 1 of this guidance summarizes the experience of a tertiary care, surgical centre which utilised redundant hospital buildings for immediate spatial separation in a “hot spot” COVID-19 area. Part 2 outlines the successful screening and isolation strategy in a surgical clinic in a region of Germany with outbreaks in surrounding medical centres. Part 3 provides the synopsis of personal experiences and international recommendations suggested for implementation during the COVID-19 pandemic. Conclusion Understanding of COVID-19, and SARS-CoV-2-related epidemiology, is constantly and rapidly changing, requiring continuous adaptation and re-evaluation of recommendations. Established national and local guidelines for continuation of surgical services and prevention of SSI require ongoing scrutiny and focused implementation. This manuscript presents a core facility checklist to support medical institutions to continue their clinical and surgical work during the COVID-19 pandemic.',\n",
       "  'date': 2021,\n",
       "  'authors': ['O Assadian 1, M Golling 2, C M Krüger 3, D Leaper 4, N T Mutters 5, B Roth 6, A Kramer 7'],\n",
       "  'related_topics': ['Health care',\n",
       "   'Checklist',\n",
       "   'Isolation (health care)',\n",
       "   'Elective surgery',\n",
       "   'Pandemic',\n",
       "   'Triage',\n",
       "   'Medical emergency',\n",
       "   'Personal experience',\n",
       "   'MEDLINE',\n",
       "   'Medicine'],\n",
       "  'references': ['3032742287',\n",
       "   '3030259932',\n",
       "   '2610936380',\n",
       "   '3034408674',\n",
       "   '3022440138',\n",
       "   '2743706615',\n",
       "   '3015734309',\n",
       "   '2101828698',\n",
       "   '3024451700',\n",
       "   '3025500386']},\n",
       " {'id': '3144171767',\n",
       "  'title': 'A comparative study on convective heat transfer in indoor applications',\n",
       "  'reference_count': '63',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Abstract Analytical solutions may not always be applicable in the calculation of the turbulent flow convective heat transfer rate, unlike the radiative and the conductive ones. Therefore, experimental correlations on convective heat transfer coefficient have been developed in enclosures. Convective heat transfer in a cavity is classified as natural, forced, and mixed convection on the basis of the driving forces (buoyant or mechanical). In recent years, there has been an increasing interest in the mixed convection, particularly in cooled ceiling – displacement ventilation indoor applications. It seems this interest is tending to increase while contamination of viruses and energy saving are growing as health and environmental concerns worldwide. Hence, the reasons behind the interest in mixed convection applications have been investigated along the paper. This comparative study seeks to explain the progress of convective heat transfer at indoor applications by reviewing mostly experimental correlation studies in time. The mixed convection has not been widely studied experimentally in indoor applications in comparison to natural and forced convection. Therefore, this study is devoted to indicate this gap in the literature on this issue and it includes all convection types with a wide and up-to-date review, descriptions, explanations, and comparisons, as well. Moreover, almost all empirical correlations on the topic are given in tables in detail. It can be concluded that general correlations for mixed convection applications is needed. Correlations related to radiant floor cooling applications are nearly non-existent. Additionally, more experimental studies are required for various split air conditioner cases. These gaps in the literature are unveiled and comparison of applications with various convection types have been made as a first comparative study in the literature.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Muhammet Camci 1, Yakup Karakoyun 2, Ozgen Acikgoz 1, Ahmet Selim Dalkilic 1'],\n",
       "  'related_topics': ['Combined forced and natural convection',\n",
       "   'Convective heat transfer',\n",
       "   'Forced convection',\n",
       "   'Convection',\n",
       "   'Heat transfer coefficient',\n",
       "   'Displacement ventilation',\n",
       "   'Air conditioning',\n",
       "   'Radiative transfer',\n",
       "   'Mechanics',\n",
       "   'Environmental science'],\n",
       "  'references': ['2980797340',\n",
       "   '3034408674',\n",
       "   '2026259163',\n",
       "   '2047181367',\n",
       "   '2161461039',\n",
       "   '2067863919',\n",
       "   '1994960483',\n",
       "   '2049828661',\n",
       "   '1979977611',\n",
       "   '64902706']},\n",
       " {'id': '3013893137',\n",
       "  'title': 'Virological assessment of hospitalized patients with COVID-2019.',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '3,947',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is an acute infection of the respiratory tract that emerged in late 20191,2. Initial outbreaks in China involved 13.8% of cases with severe courses, and 6.1% of cases with critical courses3. This severe presentation may result from the virus using a virus receptor that is expressed predominantly in the lung2,4; the same receptor tropism is thought to have determined the pathogenicity-but also aided in the control-of severe acute respiratory syndrome (SARS) in 20035. However, there are reports of cases of COVID-19 in which the patient shows mild upper respiratory tract symptoms, which suggests the potential for pre- or oligosymptomatic transmission6-8. There is an urgent need for information on virus replication, immunity and infectivity in specific sites of the body. Here we report a detailed virological analysis of nine cases of COVID-19 that provides proof of active virus replication in tissues of the upper respiratory tract. Pharyngeal virus shedding was very high during the first week of symptoms, with a peak at 7.11 × 108 RNA copies per throat swab on day 4. Infectious virus was readily isolated from samples derived from the throat or lung, but not from stool samples-in spite of high concentrations of virus RNA. Blood and urine samples never yielded virus. Active replication in the throat was confirmed by the presence of viral replicative RNA intermediates in the throat samples. We consistently detected sequence-distinct virus populations in throat and lung samples from one patient, proving independent replication. The shedding of viral RNA from sputum outlasted the end of symptoms. Seroconversion occurred after 7 days in 50% of patients (and by day 14 in all patients), but was not followed by a rapid decline in viral load. COVID-19 can present as a mild illness of the upper respiratory tract. The confirmation of active virus replication in the upper respiratory tract has implications for the containment of COVID-19.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Roman Wölfel 1, Victor M. Corman 2, Wolfgang Guggemos 3, Michael Seilmaier 3, Sabine Zange 1, Marcel A. Müller 2, Daniela Niemeyer 2, Terry C. Jones 2, 4, Patrick Vollmar 1, Camilla Rothe 5, Michael Hoelscher 5, Tobias Bleicker 2, Sebastian Brünink 2, Julia Schneider 2, Rosina Ehmann 1, Katrin Zwirglmaier 1, Christian Drosten 2, Clemens Wendtner 3'],\n",
       "  'related_topics': ['Virus receptor', 'Viral shedding', 'Viral load'],\n",
       "  'references': ['3001897055',\n",
       "   '3002108456',\n",
       "   '3001195213',\n",
       "   '3003465021',\n",
       "   '3006961006',\n",
       "   '3004239190',\n",
       "   '3009912996',\n",
       "   '3009906937',\n",
       "   '3010338568',\n",
       "   '2132260239']},\n",
       " {'id': '3004824173',\n",
       "  'title': 'A rapid advice guideline for the diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infected pneumonia (standard version)',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '1,445',\n",
       "  'abstract': 'In December 2019, a new type viral pneumonia cases occurred in Wuhan, Hubei Province; and then named “2019 novel coronavirus (2019-nCoV)” by the World Health Organization (WHO) on 12 January 2020. For it is a never been experienced respiratory disease before and with infection ability widely and quickly, it attracted the world’s attention but without treatment and control manual. For the request from frontline clinicians and public health professionals of 2019-nCoV infected pneumonia management, an evidence-based guideline urgently needs to be developed. Therefore, we drafted this guideline according to the rapid advice guidelines methodology and general rules of WHO guideline development; we also added the first-hand management data of Zhongnan Hospital of Wuhan University. This guideline includes the guideline methodology, epidemiological characteristics, disease screening and population prevention, diagnosis, treatment and control (including traditional Chinese Medicine), nosocomial infection prevention and control, and disease nursing of the 2019-nCoV. Moreover, we also provide a whole process of a successful treatment case of the severe 2019-nCoV infected pneumonia and experience and lessons of hospital rescue for 2019-nCoV infections. This rapid advice guideline is suitable for the first frontline doctors and nurses, managers of hospitals and healthcare sections, community residents, public health persons, relevant researchers, and all person who are interested in the 2019-nCoV.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ying-Hui Jin 1, Lin Cai 1, Zhen-Shun Cheng 1, Hong Cheng 1, Tong Deng 1, 2, Yi-Pin Fan 3, Cheng Fang 1, Di Huang 1, Lu-Qi Huang 3, Qiao Huang 1, Yong Han 1, Bo Hu 1, Fen Hu 1, Bing-Hui Li 1, 2, Yi-Rong Li 1, Ke Liang 1, Li-Kai Lin 1, Li-Sha Luo 1, Jing Ma 1, Lin-Lu Ma 1, Zhi-Yong Peng 1, Yun-Bao Pan 1, Zhen-Yu Pan 1, Xue-Qun Ren 2, Hui-Min Sun 1, Ying Wang 1, Yun-Yun Wang 1, Hong Weng 1, Chao-Jie Wei 1, Dong-Fang Wu 1, Jian Xia 1, Yong Xiong 1, Hai-Bo Xu 1, Xiao-Mei Yao 4, Yu-Feng Yuan 1, Tai-Sheng Ye 1, Xiao-Chun Zhang 1, Ying-Wen Zhang 1, Yin-Gao Zhang 1, Hua-Min Zhang 3, Yan Zhao 1, Ming-Juan Zhao 1, Hao Zi 1, 2, Xian-Tao Zeng 1, Yong-Yan Wang 3, Xing-Huan Wang 1'],\n",
       "  'related_topics': ['Guideline', 'Mass screening', 'Health care'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002539152',\n",
       "   '3004280078',\n",
       "   '2165010366',\n",
       "   '1803784511',\n",
       "   '3002715510',\n",
       "   '2034462612',\n",
       "   '2150120685',\n",
       "   '1945961678']},\n",
       " {'id': '3003464757',\n",
       "  'title': 'Genomic characterization of the 2019 novel human-pathogenic coronavirus isolated from a patient with atypical pneumonia after visiting Wuhan.',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '1,878',\n",
       "  'abstract': \"A mysterious outbreak of atypical pneumonia in late 2019 was traced to a seafood wholesale market in Wuhan of China. Within a few weeks, a novel coronavirus tentatively named as 2019 novel coronavirus (2019-nCoV) was announced by the World Health Organization. We performed bioinformatics analysis on a virus genome from a patient with 2019-nCoV infection and compared it with other related coronavirus genomes. Overall, the genome of 2019-nCoV has 89% nucleotide identity with bat SARS-like-CoVZXC21 and 82% with that of human SARS-CoV. The phylogenetic trees of their orf1a/b, Spike, Envelope, Membrane and Nucleoprotein also clustered closely with those of the bat, civet and human SARS coronaviruses. However, the external subdomain of Spike's receptor binding domain of 2019-nCoV shares only 40% amino acid identity with other SARS-related coronaviruses. Remarkably, its orf3b encodes a completely novel short protein. Furthermore, its new orf8 likely encodes a secreted protein with an alpha-helix, following with a beta-sheet(s) containing six strands. Learning from the roles of civet in SARS and camel in MERS, hunting for the animal source of 2019-nCoV and its more ancestral virus would be important for understanding the origin and evolution of this novel lineage B betacoronavirus. These findings provide the basis for starting further studies on the pathogenesis, and optimizing the design of diagnostic, antiviral and vaccination strategies for this emerging infection.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Jasper Fuk Woo Chan 1, Kin Hang Kok 1, 2, Zheng Zhu 2, Hin Chu 1, 2, Kelvin Kai Wang To 3, Shuofeng Yuan 1, 2, Kwok Yung Yuen 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Betacoronavirus',\n",
       "   'Civet',\n",
       "   'Genome',\n",
       "   'Virus',\n",
       "   'Atypical pneumonia',\n",
       "   'Sequence analysis',\n",
       "   'Phylogenetics',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '2799524357',\n",
       "   '2097706568',\n",
       "   '2025170735',\n",
       "   '2030966943',\n",
       "   '2115555188',\n",
       "   '2170933940',\n",
       "   '2162496804',\n",
       "   '2255897570']},\n",
       " {'id': '3009834387',\n",
       "  'title': 'Evidence for gastrointestinal infection of SARS-CoV-2',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '1,697',\n",
       "  'abstract': 'No abstract available Keywords: ACE2; Gastrointestinal Infection; Oral-Fecal Transmission; SARS-CoV-2.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Fei Xiao',\n",
       "   'Meiwen Tang',\n",
       "   'Xiaobin Zheng',\n",
       "   'Ye Liu',\n",
       "   'Xiaofeng Li',\n",
       "   'Hong Shan'],\n",
       "  'related_topics': ['Coronavirus', 'Betacoronavirus', 'Pneumonia'],\n",
       "  'references': ['3001118548',\n",
       "   '3003668884',\n",
       "   '3004280078',\n",
       "   '3003465021',\n",
       "   '3010441732',\n",
       "   '3005272159',\n",
       "   '3031532178',\n",
       "   '2131988685',\n",
       "   '3034277126',\n",
       "   '1974901207']},\n",
       " {'id': '3011863580',\n",
       "  'title': 'Prolonged presence of SARS-CoV-2 viral RNA in faecal samples.',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '912',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yongjian Wu 1, Cheng Guo 2, Lantian Tang 1, Zhongsi Hong 1, Jianhui Zhou 1, Xin Dong 1, Huan Yin 1, Qiang Xiao 1, Yanping Tang 1, Xiujuan Qu 1, Liangjian Kuang 1, Xiaomin Fang 1, Nischay Mishra 2, Jiahai Lu 1, Hong Shan 1, Guanmin Jiang 1, Xi Huang 1'],\n",
       "  'related_topics': ['Viral shedding', 'Viral Epidemiology', 'RNA'],\n",
       "  'references': ['3008696669', '3002533507', '3006846061', '3008352032']},\n",
       " {'id': '3010096538',\n",
       "  'title': 'Features, Evaluation, and Treatment of Coronavirus (COVID-19)',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '850',\n",
       "  'abstract': 'According to the World Health Organization (WHO), viral diseases continue to emerge and represent a serious issue to public health. In the last twenty years, several viral epidemics such as the severe acute respiratory syndrome coronavirus (SARS-CoV) from 2002 to 2003, and H1N1 influenza in 2009, have been recorded. Most recently, the Middle East respiratory syndrome coronavirus (MERS-CoV) was first identified in Saudi Arabia in 2012.In a timeline that reaches the present day, an epidemic of cases with unexplained low respiratory infections detected in Wuhan, the largest metropolitan area in China\\'s Hubei province, was first reported to the WHO Country Office in China, on December 31, 2019. Published literature can trace the beginning of symptomatic individuals back to the beginning of December 2019. As they were unable to identify the causative agent, these first cases (n=29) were classified as \"pneumonia of unknown etiology.\" The Chinese Center for Disease Control and Prevention (CDC) and local CDCs organized an intensive outbreak investigation program. The etiology of this illness was attributed to a novel virus belonging to the coronavirus (CoV) family.On February 11, 2020, the WHO Director-General, Dr. Tedros Adhanom Ghebreyesus, announced that the disease caused by this new CoV was a \"COVID-19,\" which is the acronym of \"coronavirus disease 2019\". In the past twenty years, two additional CoVs epidemics have occurred. SARS-CoV provoked a large-scale epidemic beginning in China and involving two dozen countries with approximately 8000 cases and 800 deaths (fatality rate of 9,6%), and the MERS-CoV that began in Saudi Arabia and has approximately 2,500 cases and 800 deaths (fatality rate of 35%) and still causes as sporadic cases.This new virus is very contagious and has quickly spread globally. In a meeting on January 30, 2020, per the International Health Regulations (IHR, 2005), the outbreak was declared by the WHO a Public Health Emergency of International Concern (PHEIC) as it had spread to 18 countries with four countries reporting human-to-human transmission. An additional landmark occurred on February 26, 2020, as the first case of the disease, not imported from China, was recorded in the United States (US). Initially, the new virus was called 2019-nCoV. Subsequently, the task of experts of the International Committee on Taxonomy of Viruses (ICTV) termed it the SARS-CoV-2 virus as it is very similar to the one that caused the SARS outbreak (SARS-CoVs). The CoVs have become the major pathogens of emerging respiratory disease outbreaks. They are a large family of single-stranded RNA viruses (+ssRNA) that can be isolated in different animal species. For reasons yet to be explained, these viruses can cross species barriers and can cause, in humans, illness ranging from the common cold to more severe diseases such as MERS and SARS. Interestingly, these latter viruses have probably originated from bats and then moving into other mammalian hosts — the Himalayan palm civet for SARS-CoV, and the dromedary camel for MERS-CoV — before jumping to humans. The dynamics of SARS-Cov-2 are currently unknown, but there is speculation that it also has an animal origin.The potential for these viruses to grow to become a pandemic worldwide represents a serious public health risk. Concerning COVID-19, the WHO raised the threat to the CoV epidemic to the \"very high\" level, on February 28, 2020. On March 11, as the number of COVID-19 cases outside China has increased 13 times and the number of countries involved has tripled with more than 118,000 cases in 114 countries and over 4,000 deaths, WHO declared the COVID-19 a pandemic.World governments are at work to establish countermeasures to stem the devastating effects and it has been estimated that strict shutdowns may have saved 3 million lives across 11 European countries. Health organizations coordinate information flows and issues directives and guidelines to best mitigate the impact of the threat. At the same time, scientists around the world work tirelessly, and information about the transmission mechanisms, the clinical spectrum of disease, new diagnostics, and prevention and therapeutic strategies are rapidly developing. Many uncertainties remain with regard to both the virus-host interaction and the evolution of the pandemic, with specific reference to the times when it will reach its peak.At the moment, the therapeutic strategies to deal with the infection are only supportive, and prevention aimed at reducing transmission in the community is our best weapon. Aggressive isolation measures in China have led to a progressive reduction of cases. From China, the disease spread to Europe. In Italy, in geographic regions of the north, initially, and subsequently throughout the peninsula, political and health authorities have made incredible efforts to contain a shock wave that has severely tested the health system. Afterward, the COVID-19 quickly crossed the ocean and as of June 20, 2020, about 2,282,000 cases (with 121,000 deaths) have been recorded in the US, whereas Brazil with more than 1,000,000 cases and about 50,000 deaths is the most affected state in South America and the second in the world after the US. Although over time the lethality rate (total number of deaths for a given disease in relation to the total number of patients) of COVID-19 has been significantly lower than that of the SARS and MERS epidemics, the transmission of the SARS-CoV-2 virus is much larger than that of the previous viruses, with a much higher total number of deaths. It has been estimated that about one in five individuals worldwide could be at increased risk of severe COVID-19 disease if they become infected, due to underlying health conditions.In the midst of the crisis, the authors have chosen to use the \"Statpearls\" platform because, within the PubMed scenario, it represents a unique tool that may allow them to make updates in real-time. The aim, therefore, is to collect information and scientific evidence and to provide an overview of the topic that will be continuously updated.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Marco Cascella 1, Michael Rajnik 2, Arturo Cuomo 3, Scott C. Dulebohn',\n",
       "   'Raffaela Di Napoli 4'],\n",
       "  'related_topics': ['Middle East respiratory syndrome coronavirus',\n",
       "   'Outbreak',\n",
       "   'Case fatality rate',\n",
       "   'Pandemic',\n",
       "   'Public health',\n",
       "   'International Health Regulations',\n",
       "   'Disease',\n",
       "   'Novel virus',\n",
       "   'Environmental health',\n",
       "   'Geography'],\n",
       "  'references': ['3001118548',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3008028633',\n",
       "   '3009912996',\n",
       "   '2280404143',\n",
       "   '3010233963',\n",
       "   '3010930696',\n",
       "   '3012379316']},\n",
       " {'id': '3006846061',\n",
       "  'title': 'Enteric involvement of coronaviruses: is faecal-oral transmission of SARS-CoV-2 possible?',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '594',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Charleen Yeo', 'Sanghvi Kaushal', 'Danson Yeo'],\n",
       "  'related_topics': ['Viral shedding',\n",
       "   'Pneumonia',\n",
       "   'Transmission (medicine)',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral transmission'],\n",
       "  'references': ['3002108456',\n",
       "   '3005079553',\n",
       "   '3003465021',\n",
       "   '3004348779',\n",
       "   '2006434809',\n",
       "   '3003464757',\n",
       "   '2769543984',\n",
       "   '2144410942',\n",
       "   '1984335993',\n",
       "   '2064850047']},\n",
       " {'id': '3008443627',\n",
       "  'title': 'An interactive web-based dashboard to track COVID-19 in real time.',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '4,544',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ensheng Dong', 'Hongru Du', 'Lauren Gardner'],\n",
       "  'related_topics': ['Dashboard (business)',\n",
       "   'Web application',\n",
       "   'Track (disk drive)',\n",
       "   'Human–computer interaction',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Web browser'],\n",
       "  'references': ['2406220407']},\n",
       " {'id': '3013967887',\n",
       "  'title': 'Estimates of the severity of coronavirus disease 2019: a model-based analysis.',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '2,545',\n",
       "  'abstract': 'Background In the face of rapidly changing data, a range of case fatality ratio estimates for coronavirus disease 2019 (COVID-19) have been produced that differ substantially in magnitude. We aimed to provide robust estimates, accounting for censoring and ascertainment biases. Methods We collected individual-case data for patients who died from COVID-19 in Hubei, mainland China (reported by national and provincial health commissions to Feb 8, 2020), and for cases outside of mainland China (from government or ministry of health websites and media reports for 37 countries, as well as Hong Kong and Macau, until Feb 25, 2020). These individual-case data were used to estimate the time between onset of symptoms and outcome (death or discharge from hospital). We next obtained age-stratified estimates of the case fatality ratio by relating the aggregate distribution of cases to the observed cumulative deaths in China, assuming a constant attack rate by age and adjusting for demography and age-based and location-based under-ascertainment. We also estimated the case fatality ratio from individual line-list data on 1334 cases identified outside of mainland China. Using data on the prevalence of PCR-confirmed cases in international residents repatriated from China, we obtained age-stratified estimates of the infection fatality ratio. Furthermore, data on age-stratified severity in a subset of 3665 cases from China were used to estimate the proportion of infected individuals who are likely to require hospitalisation. Findings Using data on 24 deaths that occurred in mainland China and 165 recoveries outside of China, we estimated the mean duration from onset of symptoms to death to be 17·8 days (95% credible interval [CrI] 16·9-19·2) and to hospital discharge to be 24·7 days (22·9-28·1). In all laboratory confirmed and clinically diagnosed cases from mainland China (n=70 117), we estimated a crude case fatality ratio (adjusted for censoring) of 3·67% (95% CrI 3·56-3·80). However, after further adjusting for demography and under-ascertainment, we obtained a best estimate of the case fatality ratio in China of 1·38% (1·23-1·53), with substantially higher ratios in older age groups (0·32% [0·27-0·38] in those aged Interpretation These early estimates give an indication of the fatality ratio across the spectrum of COVID-19 disease and show a strong age gradient in risk of death. Funding UK Medical Research Council.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Robert Verity 1, Lucy C Okell 1, Ilaria Dorigatti 1, Peter Winskill 1, Charles Whittaker 1, Natsuko Imai 1, Gina Cuomo-Dannenburg 1, Hayley Thompson 1, Patrick G T Walker 1, Han Fu 1, Amy Dighe 1, Jamie T Griffin 2, Marc Baguelin 1, Sangeeta Bhatia 1, Adhiratha Boonyasiri 1, Anne Cori 1, Zulma Cucunubá 1, Rich FitzJohn 1, Katy Gaythorpe 1, Will Green 1, Arran Hamlet 1, Wes Hinsley 1, Daniel Laydon 1, Gemma Nedjati-Gilani 1, Steven Riley 1, Sabine van Elsland 1, Erik Volz 1, Haowei Wang 1, Yuanrong Wang 1, Xiaoyue Xi 1, Christl A Donnelly 1, 3, Azra C Ghani 1, Neil M Ferguson 1'],\n",
       "  'related_topics': ['Case fatality rate',\n",
       "   'Mainland China',\n",
       "   'Incidence (epidemiology)'],\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3002108456',\n",
       "   '3003668884',\n",
       "   '3009885589',\n",
       "   '3002539152',\n",
       "   '3008818676',\n",
       "   '3007189521',\n",
       "   '3020184843',\n",
       "   '3001971765']},\n",
       " {'id': '3015571324',\n",
       "  'title': 'Temporal dynamics in viral shedding and transmissibility of COVID-19.',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '3,484',\n",
       "  'abstract': \"We report temporal patterns of viral shedding in 94 patients with laboratory-confirmed COVID-19 and modeled COVID-19 infectiousness profiles from a separate sample of 77 infector-infectee transmission pairs. We observed the highest viral load in throat swabs at the time of symptom onset, and inferred that infectiousness peaked on or before symptom onset. We estimated that 44% (95% confidence interval, 25-69%) of secondary cases were infected during the index cases' presymptomatic stage, in settings with substantial household clustering, active case finding and quarantine outside the home. Disease control measures should be adjusted to account for probable substantial presymptomatic transmission.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Xi He 1, Eric H Y Lau 2, Peng Wu 2, Xilong Deng 1, Jian Wang 1, Xinxin Hao 2, Yiu Chung Lau 2, Jessica Y Wong 2, Yujuan Guan 1, Xinghua Tan 1, Xiaoneng Mo 1, Yanqing Chen 1, Baolin Liao 1, Weilie Chen 1, Fengyu Hu 1, Qing Zhang 1, Mingqiu Zhong 1, Yanrong Wu 1, Lingzhai Zhao 1, Fuchun Zhang 1, Benjamin J Cowling 2, Fang Li 1, Gabriel M Leung 2'],\n",
       "  'related_topics': ['Viral load',\n",
       "   'Viral shedding',\n",
       "   'Serial interval',\n",
       "   'Viral Epidemiology',\n",
       "   'Confidence interval',\n",
       "   'Transmission (mechanics)',\n",
       "   'Transmissibility (vibration)',\n",
       "   'Throat',\n",
       "   'Immunology',\n",
       "   'Biology'],\n",
       "  'references': ['3003668884',\n",
       "   '3009885589',\n",
       "   '3006961006',\n",
       "   '3003573988',\n",
       "   '3008696669',\n",
       "   '3013893137',\n",
       "   '3012756997',\n",
       "   '3008294222',\n",
       "   '2129542667',\n",
       "   '3009983851']},\n",
       " {'id': '3006642361',\n",
       "  'title': 'The reproductive number of COVID-19 is higher compared to SARS coronavirus.',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '2,443',\n",
       "  'abstract': 'Teaser: Our review found the average R0 for 2019-nCoV to be 3.28, which exceeds WHO estimates of 1.4 to 2.5.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Ying Liu 1, Albert A Gayle 2, Annelies Wilder-Smith 2, 3, Joacim Rocklöv 2'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease transmission',\n",
       "   'Severe acute respiratory syndrome coronavirus',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'Viral transmission'],\n",
       "  'references': ['3003668884',\n",
       "   '3003573988',\n",
       "   '3004397688',\n",
       "   '3002764620',\n",
       "   '3004026249',\n",
       "   '3002533591',\n",
       "   '3002747665',\n",
       "   '3001343166',\n",
       "   '3001392146',\n",
       "   '3023259384']},\n",
       " {'id': '3013594674',\n",
       "  'title': 'The effect of human mobility and control measures on the COVID-19 epidemic in China.',\n",
       "  'reference_count': '38',\n",
       "  'citation_count': '1,343',\n",
       "  'abstract': 'The ongoing coronavirus disease 2019 (COVID-19) outbreak expanded rapidly throughout China. Major behavioral, clinical, and state interventions were undertaken to mitigate the epidemic and prevent the persistence of the virus in human populations in China and worldwide. It remains unclear how these unprecedented interventions, including travel restrictions, affected COVID-19 spread in China. We used real-time mobility data from Wuhan and detailed case data including travel history to elucidate the role of case importation in transmission in cities across China and to ascertain the impact of control measures. Early on, the spatial distribution of COVID-19 cases in China was explained well by human mobility data. After the implementation of control measures, this correlation dropped and growth rates became negative in most locations, although shifts in the demographics of reported cases were still indicative of local chains of transmission outside of Wuhan. This study shows that the drastic control measures implemented in China substantially mitigated the spread of COVID-19.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Moritz U.G. Kraemer 1, 2, Chia Hung Yang 3, Bernardo Gutierrez 1, Chieh Hsi Wu 4, Brennan Klein 3, David M. Pigott 5, Louis du Plessis 1, Nuno R. Faria 1, Ruoran Li 2, William P. Hanage 2, John S. Brownstein 2, Maylis Layan 6, Alessandro Vespignani 3, Huaiyu Tian 7, Christopher Dye 1, Oliver G. Pybus 1, Samuel V. Scarpino 3'],\n",
       "  'related_topics': ['China', 'Transmission (mechanics)', 'Outbreak'],\n",
       "  'references': ['3001897055',\n",
       "   '3003668884',\n",
       "   '3008028633',\n",
       "   '1951724000',\n",
       "   '3003573988',\n",
       "   '3008818676',\n",
       "   '2097360283',\n",
       "   '3012284084',\n",
       "   '2122825543',\n",
       "   '3004912618']},\n",
       " {'id': '3012789146',\n",
       "  'title': 'The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study.',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '1,361',\n",
       "  'abstract': 'BACKGROUND: In December, 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a novel coronavirus, emerged in Wuhan, China. Since then, the city of Wuhan has taken unprecedented measures in response to the outbreak, including extended school and workplace closures. We aimed to estimate the effects of physical distancing measures on the progression of the COVID-19 epidemic, hoping to provide some insights for the rest of the world. METHODS: To examine how changes in population mixing have affected outbreak progression in Wuhan, we used synthetic location-specific contact patterns in Wuhan and adapted these in the presence of school closures, extended workplace closures, and a reduction in mixing in the general community. Using these matrices and the latest estimates of the epidemiological parameters of the Wuhan outbreak, we simulated the ongoing trajectory of an outbreak in Wuhan using an age-structured susceptible-exposed-infected-removed (SEIR) model for several physical distancing measures. We fitted the latest estimates of epidemic parameters from a transmission model to data on local and internationally exported cases from Wuhan in an age-structured epidemic framework and investigated the age distribution of cases. We also simulated lifting of the control measures by allowing people to return to work in a phased-in way and looked at the effects of returning to work at different stages of the underlying outbreak (at the beginning of March or April). FINDINGS: Our projections show that physical distancing measures were most effective if the staggered return to work was at the beginning of April; this reduced the median number of infections by more than 92% (IQR 66-97) and 24% (13-90) in mid-2020 and end-2020, respectively. There are benefits to sustaining these measures until April in terms of delaying and reducing the height of the peak, median epidemic size at end-2020, and affording health-care systems more time to expand and respond. However, the modelled effects of physical distancing measures vary by the duration of infectiousness and the role school children have in the epidemic. INTERPRETATION: Restrictions on activities in Wuhan, if maintained until April, would probably help to delay the epidemic peak. Our projections suggest that premature and sudden lifting of interventions could lead to an earlier secondary peak, which could be flattened by relaxing the interventions gradually. However, there are limitations to our analysis, including large uncertainties around estimates of R0 and the duration of infectiousness. FUNDING: Bill & Melinda Gates Foundation, National Institute for Health Research, Wellcome Trust, and Health Data Research UK.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Kiesha Prem',\n",
       "   'Yang Liu',\n",
       "   'Timothy W Russell',\n",
       "   'Adam J Kucharski',\n",
       "   'Rosalind M Eggo',\n",
       "   'Nicholas Davies',\n",
       "   'Mark Jit',\n",
       "   'Petra Klepac'],\n",
       "  'related_topics': ['Psychological intervention', 'Outbreak', 'Demography'],\n",
       "  'references': ['3001897055',\n",
       "   '3003668884',\n",
       "   '3002539152',\n",
       "   '3003573988',\n",
       "   '3006659024',\n",
       "   '3009577418',\n",
       "   '3009468976',\n",
       "   '3004912618',\n",
       "   '3004026249',\n",
       "   '3020184843']},\n",
       " {'id': '3001195213',\n",
       "  'title': 'Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR.',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '4,091',\n",
       "  'abstract': 'Background The ongoing outbreak of the recently emerged novel coronavirus (2019-nCoV) poses a challenge for public health laboratories as virus isolates are unavailable while there is growing evidence that the outbreak is more widespread than initially thought, and international spread through travellers does already occur. Aim We aimed to develop and deploy robust diagnostic methodology for use in public health laboratory settings without having virus material available. Methods Here we present a validated diagnostic workflow for 2019-nCoV, its design relying on close genetic relatedness of 2019-nCoV with SARS coronavirus, making use of synthetic nucleic acid technology. Results The workflow reliably detects 2019-nCoV, and further discriminates 2019-nCoV from SARS-CoV. Through coordination between academic and public laboratories, we confirmed assay exclusivity based on 297 original clinical specimens containing a full spectrum of human respiratory viruses. Control material is made available through European Virus Archive – Global (EVAg), a European Union infrastructure project. Conclusion The present study demonstrates the enormous response capacity achieved through coordination of academic and public laboratories in national and European research networks.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Victor M. Corman 1, Olfert Landt 2, Marco Kaiser 3, Richard Molenkamp 4, Adam Meijer 5, Daniel K.W. Chu 6, Tobias Bleicker 1, Sebastian Brünink 1, Julia Schneider 1, Marie Luisa Schmidt 1, Daphne G.J.C. Mulders 4, Bart L. Haagmans 4, Bas Van Der Veer 5, Sharon Van Den Brink 5, Lisa Wijsman 5, Gabriel Goderski 5, Jean Louis Romette 7, Joanna Ellis 8, Maria Zambon 8, Malik Peiris 6, Herman Goossens 9, Chantal Reusken 5, Marion P.G. Koopmans 4, Christian Drosten 1'],\n",
       "  'related_topics': ['European union', 'Coronavirus', 'Global health'],\n",
       "  'references': ['2903899730',\n",
       "   '2132260239',\n",
       "   '1703839189',\n",
       "   '1852588318',\n",
       "   '2793008036',\n",
       "   '2167080692',\n",
       "   '2894950287',\n",
       "   '2884280018',\n",
       "   '2031705962',\n",
       "   '2101063972']},\n",
       " {'id': '2105275554',\n",
       "  'title': 'Loop-mediated isothermal amplification of DNA',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '8,349',\n",
       "  'abstract': 'We have developed a novel method, termed loop-mediated isothermal amplification (LAMP), that amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions. This method employs a DNA polymerase and a set of four specially designed primers that recognize a total of six distinct sequences on the target DNA. An inner primer containing sequences of the sense and antisense strands of the target DNA initiates LAMP. The following strand displacement DNA synthesis primed by an outer primer releases a single-stranded DNA. This serves as template for DNA synthesis primed by the second inner and outer primers that hybridize to the other end of the target, which produces a stem–loop DNA structure. In subsequent LAMP cycling one inner primer hybridizes to the loop on the product and initiates displacement DNA synthesis, yielding the original stem–loop DNA and a new stem–loop DNA with a stem twice as long. The cycling reaction continues with accumulation of 109 copies of target in less than an hour. The final products are stem–loop DNAs with several inverted repeats of the target and cauliflower-like structures with multiple loops formed by annealing between alternately inverted repeats of the target in the same strand. Because LAMP recognizes the target by six distinct sequences initially and by four distinct sequences afterwards, it is expected to amplify the target sequence with high selectivity.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Tsugunori Notomi',\n",
       "   'Hiroto Okayama',\n",
       "   'Harumi Masubuchi',\n",
       "   'Toshihiro Yonekawa',\n",
       "   'Keiko Watanabe',\n",
       "   'Nobuyuki Amino',\n",
       "   'Tetsu Hase'],\n",
       "  'related_topics': ['DNA clamp',\n",
       "   'Primer (molecular biology)',\n",
       "   'Base pair',\n",
       "   'Primase',\n",
       "   'In vitro recombination',\n",
       "   'DNA polymerase',\n",
       "   'Multiple displacement amplification',\n",
       "   'Nucleic acid thermodynamics',\n",
       "   'Molecular biology',\n",
       "   'Biology'],\n",
       "  'references': ['2032118018',\n",
       "   '2050717506',\n",
       "   '2035792726',\n",
       "   '2062756489',\n",
       "   '1990689151',\n",
       "   '2142539585',\n",
       "   '2083121396',\n",
       "   '2082277951',\n",
       "   '1970137322',\n",
       "   '2056636525']},\n",
       " {'id': '3011969828',\n",
       "  'title': '2019 Novel Coronavirus Disease (COVID-19): Paving the Road for Rapid Detection and Point-of-Care Diagnostics.',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '194',\n",
       "  'abstract': 'We believe a point-of-care (PoC) device for the rapid detection of the 2019 novel Coronavirus (SARS-CoV-2) is crucial and urgently needed. With this perspective, we give suggestions regarding a potential candidate for the rapid detection of the coronavirus disease 2019 (COVID-19), as well as factors for the preparedness and response to the outbreak of the COVID-19.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Trieu Nguyen', 'Dang Duong Bang', 'Anders Wolff'],\n",
       "  'related_topics': ['Preparedness',\n",
       "   'Outbreak',\n",
       "   'Point-of-care testing',\n",
       "   'Disease',\n",
       "   'Intensive care medicine',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Potential candidate',\n",
       "   'Rapid detection',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3004280078',\n",
       "   '3001195213',\n",
       "   '3003465021',\n",
       "   '3004239190',\n",
       "   '3003573988',\n",
       "   '3001465255',\n",
       "   '3003951199',\n",
       "   '3003637715',\n",
       "   '3000771439']},\n",
       " {'id': '2770752141',\n",
       "  'title': 'Loop-mediated isothermal amplification (LAMP): a versatile technique for detection of micro-organisms.',\n",
       "  'reference_count': '168',\n",
       "  'citation_count': '181',\n",
       "  'abstract': 'Summary Loop-mediated isothermal amplification (LAMP) amplifies DNA with high specificity, efficiency and rapidity under isothermal conditions by using a DNA polymerase with high displacement strand activity and a set of specifically designed primers to amplify targeted DNA strands. Following its first discovery by Notomi et al. (2000 Nucleic Acids Res 28: E63), LAMP was further developed over the years which involved the combination of this technique with other molecular approaches, such as reverse transcription and multiplex amplification for the detection of infectious diseases caused by micro-organisms in humans, livestock and plants. In this review, available types of LAMP techniques will be discussed together with their applications in detection of various micro-organisms. Up to date, there are varieties of LAMP detection methods available including colorimetric and fluorescent detection, real-time monitoring using turbidity metre and detection using lateral flow device which will also be highlighted in this review. Apart from that, commercialization of LAMP technique had also been reported such as lyophilized form of LAMP reagents kit and LAMP primer sets for detection of pathogenic micro-organisms. On top of that, advantages and limitations of this molecular detection method are also described together with its future potential as a diagnostic method for infectious disease.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Y.-P. Wong 1, S. Othman 1, Y.-L. Lau 2, S. Radu 1, H.-Y. Chee 1'],\n",
       "  'related_topics': ['Loop-mediated isothermal amplification',\n",
       "   'Multiplex',\n",
       "   'Polymerase',\n",
       "   'Primer (molecular biology)',\n",
       "   'DNA',\n",
       "   'Nucleic acid',\n",
       "   'Computational biology',\n",
       "   'Materials science',\n",
       "   'Diagnostic methods'],\n",
       "  'references': ['2105275554',\n",
       "   '1979974453',\n",
       "   '1975801865',\n",
       "   '1562219715',\n",
       "   '89741002',\n",
       "   '2063533401',\n",
       "   '1997925861',\n",
       "   '2050515639',\n",
       "   '2141633648',\n",
       "   '2035792726']},\n",
       " {'id': '2263084061',\n",
       "  'title': 'Loop-Mediated Isothermal Amplification Assay for Identification of Five Human Plasmodium Species in Malaysia',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '38',\n",
       "  'abstract': 'The lack of rapid, affordable, and accurate diagnostic tests represents the primary hurdle affecting malaria surveillance in resource- and expertise-limited areas. Loop-mediated isothermal amplification (LAMP) is a sensitive, rapid, and cheap diagnostic method. Five species-specific LAMP assays were developed based on 18S rRNA gene. Sensitivity and specificity of LAMP results were calculated as compared with microscopic examination and nested polymerase chain reaction. LAMP reactions were highly sensitive with the detection limit of one copy for Plasmodium vivax, Plasmodium falciparum, and Plasmodium malariae and 10 copies for Plasmodium knowlesi and Plasmodium ovale. LAMP positively detected all human malaria species in all positive samples (N = 134; sensitivity = 100%) within 35 minutes. All negative samples were not amplified by LAMP (N = 67; specificity = 100%). LAMP successfully detected two samples with very low parasitemia. LAMP may offer a rapid, simple, and reliable test for the diagnosis of malaria in areas where malaria is prevalent.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Yee Ling Lau',\n",
       "   'Meng Yee Lai',\n",
       "   'Mun Yik Fong',\n",
       "   'Jenarun Jelip',\n",
       "   'Rohela Mahmud'],\n",
       "  'related_topics': ['Plasmodium malariae',\n",
       "   'Plasmodium ovale',\n",
       "   'Plasmodium vivax',\n",
       "   'Plasmodium knowlesi',\n",
       "   'Plasmodium falciparum',\n",
       "   'Nucleic acid amplification technique',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Nested polymerase chain reaction',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2105275554',\n",
       "   '2788073857',\n",
       "   '2133724824',\n",
       "   '2066385972',\n",
       "   '2149136689',\n",
       "   '2102131955',\n",
       "   '2043762150',\n",
       "   '1987713777',\n",
       "   '2110753623',\n",
       "   '2144345536']},\n",
       " {'id': '2175815746',\n",
       "  'title': 'Development of reverse-transcription loop-mediated isothermal amplification assay for rapid detection and differentiation of dengue virus serotypes 1-4.',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '26',\n",
       "  'abstract': 'Dengue virus (DENV), the most widely prevalent arbovirus, continues to be a threat to human health in the tropics and subtropics. Early and rapid detection of DENV infection during the acute phase of illness is crucial for proper clinical patient management and preventing the spread of infection. The aim of the current study was to develop a specific, sensitive, and robust reverse transcriptase loop-mediated isothermal amplification (RT-LAMP) assay for detection and differentiation of DENV1-4 serotypes. The method detection primers, which were designed to target the different DENV serotypes, were identified by inspection of multiple sequence alignments of the non-structural protein (NS) 2A of DENV1, NS4B of DENV2, NS4A of DENV3 and the 3′ untranslated region of the NS protein of DENV4. No cross-reactions of the four serotypes were observed during the tests. The detection limits of the DENV1-4-specific RT-LAMP assays were approximately 10-copy templates per reaction. The RT-LAMP assays were ten-fold more sensitive than RT-PCR or real-time PCR. The diagnostic rate was 100 % for clinical strains of DENV, and 98.9 % of the DENV-infected patients whose samples were tested were detected by RT-LAMP. Importantly, no false-positives were detected with the new equipment and methodology that was used to avoid aerosol contamination of the samples. The RT-LAMP method used in our study is specific, sensitive, and suitable for further investigation as a useful alternative to the current methods used for clinical diagnosis of DENV1-4, especially in hospitals and laboratories that lack sophisticated diagnostic systems.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Sheng-feng Hu 1, Miao Li 1, Lan-lan Zhong 1, Shi-miao Lu 1, Ze-xia Liu 1, Jie-ying Pu 1, Jin-sheng Wen 2, Xi Huang 1, 2'],\n",
       "  'related_topics': ['Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Dengue virus',\n",
       "   'Nucleic acid amplification technique',\n",
       "   'Dengue fever',\n",
       "   'Arbovirus',\n",
       "   'Reverse transcriptase',\n",
       "   'Parasitology',\n",
       "   'Virology',\n",
       "   'Microbiology',\n",
       "   'Biology'],\n",
       "  'references': ['2105275554',\n",
       "   '2042479028',\n",
       "   '2066385972',\n",
       "   '2128110156',\n",
       "   '2149136689',\n",
       "   '2160892791',\n",
       "   '2101258647',\n",
       "   '1994850469',\n",
       "   '2124449928',\n",
       "   '2141627564']},\n",
       " {'id': '1991420168',\n",
       "  'title': 'Utility of IgM ELISA, TaqMan real-time PCR, reverse transcription PCR, and RT-LAMP assay for the diagnosis of Chikungunya fever.',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '66',\n",
       "  'abstract': 'Chikungunya fever a re-emerging infection with expanding geographical boundaries, can mimic symptoms of other infections like dengue, malaria which makes the definitive diagnosis of the infection important. The present study compares the utility of four laboratory diagnostic methods viz. IgM capture ELISA, an in house reverse transcription PCR for the diagnosis of Chikungunya fever, TaqMan real-time PCR, and a one step reverse transcription-loop mediated isothermal amplification assay (RT-LAMP). Out of the 70 serum samples tested, 29 (41%) were positive for Chikungunya IgM antibody by ELISA and 50 (71%) samples were positive by one of the three molecular assays. CHIKV specific nucleic acid was detected in 33/70 (47%) by reverse transcription PCR, 46/70 (66%) by TaqMan real-time PCR, and 43/70 (62%) by RT-LAMP assay. A majority of the samples (62/70; 89%) were positive by at least one of the four assays used in the study. The molecular assays were more sensitive for diagnosis in the early stages of illness (2–5 days post onset) when antibodies were not detectable. In the later stages of illness, the IgM ELISA is a more sensitive diagnostic test. In conclusion we recommend that the IgM ELISA be used as an initial screening test followed one of the molecular assays in samples that are collected in the early phase of illness and negative for CHIKV IgM antibodies. Such as approach would enable rapid confirmation of the diagnosis and implementation of public health measures especially during outbreaks. J. Med. Virol. 84:1771–1778, 2012. © 2012 Wiley Periodicals, Inc.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Vijayalakshmi Reddy 1, Vasanthapuram Ravi 1, Anita Desai 1, Manmohan Parida 2, Ann M. Powers 3, Barbara W. Johnson 3'],\n",
       "  'related_topics': ['TaqMan',\n",
       "   'Reverse transcription polymerase chain reaction',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Chikungunya',\n",
       "   'Dengue fever',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Antibody',\n",
       "   'Virology',\n",
       "   'Malaria',\n",
       "   'Biology'],\n",
       "  'references': ['2141987735',\n",
       "   '2120801593',\n",
       "   '1977296748',\n",
       "   '2129358311',\n",
       "   '1969557290',\n",
       "   '2067506266',\n",
       "   '2052129607',\n",
       "   '2108924397',\n",
       "   '1966636515',\n",
       "   '2051304065']},\n",
       " {'id': '2073600962',\n",
       "  'title': 'Evaluation of a Direct Reverse Transcription Loop-Mediated Isothermal Amplification Method without RNA Extraction for the Detection of Human Enterovirus 71 Subgenotype C4 in Nasopharyngeal Swab Specimens',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '31',\n",
       "  'abstract': 'Human enterovirus 71 (EV71) is the major causative agent of hand, foot, and mouth disease (HFMD) worldwide and has been associated with neurological complications which resulted in fatalities during recent outbreak in Asia pacific region. A direct reverse transcription loop-mediated isothermal amplification (direct RT-LAMP) assay using heat-treated samples without RNA extraction was developed and evaluated for the detection of EV71 subgenotype C4 in nasopharyngeal swab specimens. The analytical sensitivity and specificity of the direct RT-LAMP assay were examined. The detection limit of the direct RT-LAMP assays was 1.6 of a 50% tissue culture infective dose (TCID50) per reaction and no cross-reaction was observed with control viruses including Cosackievirus A (CVA) viruses (CVA2,4,5,7,9,10,14,16, and 24), Coxsackievirus B (CVB) viruses (CVB1,2,3,4, and 5) or ECHO viruses (ECHO3,6,11, and 19). The direct RT-LAMP assay was evaluated and compared to both RT-LAMP and quantitative real-time PCR (qRT-PCR) in detecting EV71 infection with 145 nasopharyngeal swab specimens. The clinical performance demonstrated the sensitivity and specificity of direct RT-LAMP was reported to be 90.3% and 100% respectively, compared to RT-LAMP, and 86.83% and 100% respectively, compared to qRT-PCR. These data demonstrated that the direct RT-LAMP assay can potentially be developed for the point of care screening of EV71 infection in China.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Kai Nie 1, Shun-xiang Qi 2, Yong Zhang 1, Le Luo 1, Yun Xie 2, Meng-jie Yang 1, Yi Zhang 1, Jin Li 1, Hongwei Shen 1, Qi Li 2, Xue-jun Ma 1'],\n",
       "  'related_topics': ['Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Nucleic acid amplification technique',\n",
       "   'Coxsackievirus',\n",
       "   'Reverse transcriptase',\n",
       "   'RNA extraction',\n",
       "   'Polymerase chain reaction',\n",
       "   'RNA',\n",
       "   'Virology',\n",
       "   'Biology'],\n",
       "  'references': ['2105275554',\n",
       "   '2135086664',\n",
       "   '2160892791',\n",
       "   '2057680658',\n",
       "   '2140363623',\n",
       "   '2065860560',\n",
       "   '1975134712',\n",
       "   '1970186243',\n",
       "   '2051085144',\n",
       "   '1984421786']},\n",
       " {'id': '2084576921',\n",
       "  'title': 'Visual detection of turkey coronavirus RNA in tissues and feces by reverse-transcription loop-mediated isothermal amplification (RT-LAMP) with hydroxynaphthol blue dye',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '64',\n",
       "  'abstract': 'Abstract A sensitive reverse-transcription loop-mediated isothermal amplification (RT-LAMP) assay was developed for the rapid visual detection of turkey coronavirus (TCoV) infection. The reaction is performed in one step in a single tube at 65 °C for 45 min, with hydroxynaphthol blue (HNB) dye added prior to amplification. The detection limit of the RT-LAMP assay was approximately 10 2 EID 50/50 μl TCoV genome, and no cross-reaction with other avian viruses was observed. The assay was evaluated further in tissue suspensions prepared from the ileum and ileum–caecal junctions of infected turkey embryos; 100% of these samples were positive in the RT-LAMP assay. All individual feces samples collected in the field were considered positive by both conventional RT-PCR and RT-LAMP. In conclusion, RT-LAMP with HNB dye was shown to be a sensitive, simple assay for the rapid diagnosis of TCoV infection, either directly from feces or in association with virus isolation methods.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Tereza C. Cardoso',\n",
       "   'Heitor F. Ferrari',\n",
       "   'Livia C. Bregano',\n",
       "   'Camila Silva-Frade',\n",
       "   'Ana Carolina G. Rosa',\n",
       "   'Alexandre Lima de Andrade'],\n",
       "  'related_topics': ['Hydroxynaphthol blue',\n",
       "   'Reverse Transcription Loop-mediated Isothermal Amplification',\n",
       "   'Loop-mediated isothermal amplification',\n",
       "   'Turkey coronavirus',\n",
       "   'Feces',\n",
       "   'RNA',\n",
       "   'Molecular biology',\n",
       "   'Virology',\n",
       "   'Biology',\n",
       "   'Single tube',\n",
       "   'Visual detection'],\n",
       "  'references': ['2105275554',\n",
       "   '1544561280',\n",
       "   '2028571354',\n",
       "   '1997738379',\n",
       "   '2057637320',\n",
       "   '2155485281',\n",
       "   '1997343737',\n",
       "   '2009652162',\n",
       "   '1565181081',\n",
       "   '2038706592']},\n",
       " {'id': '3008090866',\n",
       "  'title': 'Clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia in Wuhan, China: a single-centered, retrospective, observational study.',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '7,506',\n",
       "  'abstract': 'Summary Background An ongoing outbreak of pneumonia associated with the severe acute respiratory coronavirus 2 (SARS-CoV-2) started in December, 2019, in Wuhan, China. Information about critically ill patients with SARS-CoV-2 infection is scarce. We aimed to describe the clinical course and outcomes of critically ill patients with SARS-CoV-2 pneumonia. Methods In this single-centered, retrospective, observational study, we enrolled 52 critically ill adult patients with SARS-CoV-2 pneumonia who were admitted to the intensive care unit (ICU) of Wuhan Jin Yin-tan hospital (Wuhan, China) between late December, 2019, and Jan 26, 2020. Demographic data, symptoms, laboratory values, comorbidities, treatments, and clinical outcomes were all collected. Data were compared between survivors and non-survivors. The primary outcome was 28-day mortality, as of Feb 9, 2020. Secondary outcomes included incidence of SARS-CoV-2-related acute respiratory distress syndrome (ARDS) and the proportion of patients requiring mechanical ventilation. Findings Of 710 patients with SARS-CoV-2 pneumonia, 52 critically ill adult patients were included. The mean age of the 52 patients was 59·7 (SD 13·3) years, 35 (67%) were men, 21 (40%) had chronic illness, 51 (98%) had fever. 32 (61·5%) patients had died at 28 days, and the median duration from admission to the intensive care unit (ICU) to death was 7 (IQR 3–11) days for non-survivors. Compared with survivors, non-survivors were older (64·6 years [11·2] vs 51·9 years [12·9]), more likely to develop ARDS (26 [81%] patients vs 9 [45%] patients), and more likely to receive mechanical ventilation (30 [94%] patients vs 7 [35%] patients), either invasively or non-invasively. Most patients had organ function damage, including 35 (67%) with ARDS, 15 (29%) with acute kidney injury, 12 (23%) with cardiac injury, 15 (29%) with liver dysfunction, and one (2%) with pneumothorax. 37 (71%) patients required mechanical ventilation. Hospital-acquired infection occurred in seven (13·5%) patients. Interpretation The mortality of critically ill patients with SARS-CoV-2 pneumonia is considerable. The survival time of the non-survivors is likely to be within 1–2 weeks after ICU admission. Older patients (>65 years) with comorbidities and ARDS are at increased risk of death. The severity of SARS-CoV-2 pneumonia poses great strain on critical care resources in hospitals, especially if they are not adequately staffed or resourced. Funding None.',\n",
       "  'date': 2020,\n",
       "  'authors': [\"Xiaobo Yang 1, Yuan Yu 1, Jiqian Xu 1, Huaqing Shu 1, Jia'an Xia 2, Hong Liu 1, 2, Yongran Wu 1, Lu Zhang 3, Zhui Yu 4, Minghao Fang 1, Ting Yu 2, Yaxin Wang 1, Shangwen Pan 1, Xiaojing Zou 1, Shiying Yuan 1, You Shang 1, 2\"],\n",
       "  'related_topics': ['Pneumonia',\n",
       "   'ARDS',\n",
       "   'Intensive care unit',\n",
       "   'Mechanical ventilation',\n",
       "   'Retrospective cohort study',\n",
       "   'Incidence (epidemiology)',\n",
       "   'Acute kidney injury',\n",
       "   'Pneumothorax',\n",
       "   'Emergency medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['3001118548',\n",
       "   '3001897055',\n",
       "   '3002108456',\n",
       "   '3005079553',\n",
       "   '3003465021',\n",
       "   '3004239190',\n",
       "   '2470646526',\n",
       "   '2026274122',\n",
       "   '3005403371',\n",
       "   '2286228001']},\n",
       " {'id': '3007940623',\n",
       "  'title': 'Pathological findings of COVID-19 associated with acute respiratory distress syndrome.',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '5,565',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Zhe Xu 1, Lei Shi 1, Yijin Wang 2, Jiyuan Zhang 1, Lei Huang 1, Chao Zhang 1, Shuhong Liu 2, Peng Zhao 1, Hongxia Liu 1, Li Zhu 2, Yanhong Tai 2, Changqing Bai 3, Tingting Gao 2, Jinwen Song 1, Peng Xia 1, Jinghui Dong 4, Jingmin Zhao 2, Fu Sheng Wang 1'],\n",
       "  'related_topics': ['Cytokine storm',\n",
       "   'Medicine',\n",
       "   'Pathological',\n",
       "   'Internal medicine',\n",
       "   'MEDLINE',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Acute respiratory distress',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '3003217347',\n",
       "   '2256430766',\n",
       "   '2158118659']},\n",
       " {'id': '3011242477',\n",
       "  'title': 'COVID-19 and Italy: what next?',\n",
       "  'reference_count': '2',\n",
       "  'citation_count': '2,324',\n",
       "  'abstract': \"Summary The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already taken on pandemic proportions, affecting over 100 countries in a matter of weeks. A global response to prepare health systems worldwide is imperative. Although containment measures in China have reduced new cases by more than 90%, this reduction is not the case elsewhere, and Italy has been particularly affected. There is now grave concern regarding the Italian national health system's capacity to effectively respond to the needs of patients who are infected and require intensive care for SARS-CoV-2 pneumonia. The percentage of patients in intensive care reported daily in Italy between March 1 and March 11, 2020, has consistently been between 9% and 11% of patients who are actively infected. The number of patients infected since Feb 21 in Italy closely follows an exponential trend. If this trend continues for 1 more week, there will be 30\\u2008000 infected patients. Intensive care units will then be at maximum capacity; up to 4000 hospital beds will be needed by mid-April, 2020. Our analysis might help political leaders and health authorities to allocate enough resources, including personnel, beds, and intensive care facilities, to manage the situation in the next few days and weeks. If the Italian outbreak follows a similar trend as in Hubei province, China, the number of newly infected patients could start to decrease within 3–4 days, departing from the exponential trend. However, this cannot currently be predicted because of differences between social distancing measures and the capacity to quickly build dedicated facilities in China.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Andrea Remuzzi 1, Giuseppe Remuzzi 2'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Global health',\n",
       "   'Pandemic',\n",
       "   'China',\n",
       "   'Environmental health',\n",
       "   'Outbreak',\n",
       "   'Social distance',\n",
       "   'Medicine',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)'],\n",
       "  'references': ['3003668884', '3007613835']},\n",
       " {'id': '3010449299',\n",
       "  'title': 'Air, Surface Environmental, and Personal Protective Equipment Contamination by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) From a Symptomatic Patient.',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '1,741',\n",
       "  'abstract': 'This study documents results of SARS-CoV-2 polymerase chain reaction (PCR) testing of environmental surfaces and personal protective equipment surrounding 3 COVID-19 patients in isolation rooms in a Singapore hospital.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Sean Wei Xiang Ong 1, Yian Kim Tan 2, Po Ying Chia 1, Tau Hong Lee 1, Oon Tek Ng 1, Michelle Su Yen Wong 2, Kalisvar Marimuthu 1'],\n",
       "  'related_topics': ['Isolation (health care)',\n",
       "   'Coronavirus',\n",
       "   'Personal protective equipment',\n",
       "   'Equipment Contamination',\n",
       "   'Pneumonia',\n",
       "   'Viral shedding',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'Emergency medicine',\n",
       "   'Medicine',\n",
       "   'Contamination'],\n",
       "  'references': ['3005079553',\n",
       "   '3001195213',\n",
       "   '3010338568',\n",
       "   '1815575713',\n",
       "   '2250074178']},\n",
       " {'id': '3018334611',\n",
       "  'title': 'Aerodynamic analysis of SARS-CoV-2 in two Wuhan hospitals.',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '1,118',\n",
       "  'abstract': 'The ongoing outbreak of coronavirus disease 2019 (COVID-19) has spread rapidly on a global scale. Although it is clear that severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is transmitted through human respiratory droplets and direct contact, the potential for aerosol transmission is poorly understood1-3. Here we investigated the aerodynamic nature of SARS-CoV-2 by measuring viral RNA in aerosols in different areas of two Wuhan hospitals during the outbreak of COVID-19 in February and March 2020. The concentration of SARS-CoV-2 RNA in aerosols that was detected in isolation wards and ventilated patient rooms was very low, but it was higher in the toilet areas used by the patients. Levels of airborne SARS-CoV-2 RNA in the most public areas was undetectable, except in two areas that were prone to crowding; this increase was possibly due to individuals infected with SARS-CoV-2 in the crowd. We found that some medical staff areas initially had high concentrations of viral RNA with aerosol size distributions that showed peaks in the submicrometre and/or supermicrometre regions; however, these levels were reduced to undetectable levels after implementation of rigorous sanitization procedures. Although we have not established the infectivity of the virus detected in these hospital areas, we propose that SARS-CoV-2 may have the potential to be transmitted through aerosols. Our results indicate that room ventilation, open space, sanitization of protective apparel, and proper use and disinfection of toilet areas can effectively limit the concentration of SARS-CoV-2 RNA in aerosols. Future work should explore the infectivity of aerosolized virus.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Yuan Liu 1, Zhi Ning 2, Yu Chen 1, Ming Guo 1, Yingle Liu 1, Nirmal Kumar Gali 2, Li Sun 2, Yusen Duan 3, Jing Cai 4, Dane Westerdahl 2, Xinjin Liu 1, Ke Xu 1, Kin fai Ho 5, Haidong Kan 4, Qingyan Fu 3, Ke Lan 1'],\n",
       "  'related_topics': ['Coronavirus',\n",
       "   'Outbreak',\n",
       "   'Infectivity',\n",
       "   'Aerosol',\n",
       "   'Virus',\n",
       "   'Isolation (health care)',\n",
       "   'Viral Epidemiology',\n",
       "   'Betacoronavirus',\n",
       "   'Virology',\n",
       "   'Medicine'],\n",
       "  'references': ['3004280078',\n",
       "   '3012099172',\n",
       "   '3010604545',\n",
       "   '3009906937',\n",
       "   '3010449299',\n",
       "   '3005510968',\n",
       "   '3027866910',\n",
       "   '3018724240',\n",
       "   '2215636165',\n",
       "   '2147350479']},\n",
       " {'id': '3015704123',\n",
       "  'title': 'Aerosol and Surface Distribution of Severe Acute Respiratory Syndrome Coronavirus 2 in Hospital Wards, Wuhan, China, 2020.',\n",
       "  'reference_count': '7',\n",
       "  'citation_count': '692',\n",
       "  'abstract': 'To determine distribution of severe acute respiratory syndrome coronavirus 2 in hospital wards in Wuhan, China, we tested air and surface samples. Contamination was greater in intensive care units than general wards. Virus was widely distributed on floors, computer mice, trash cans, and sickbed handrails and was detected in air ≈4 m from patients.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Zhen Dong Guo 1, Zhong Yi Wang 1, Shou Feng Zhang 1, Xiao Li 1, Lin Li 1, Chao Li 2, Yan Cui 3, Rui Bin Fu 3, Yun Zhu Dong 1, Xiang Yang Chi 1, Meng Yao Zhang 1, Kun Liu 3, Cheng Cao 1, Bin Liu 1, Ke Zhang 1, Yu Wei Gao 2, Bing Lu 3, Wei Chen 4, 5'],\n",
       "  'related_topics': ['Intensive care', 'Emergency medicine', 'Medicine'],\n",
       "  'references': ['3011242477',\n",
       "   '3010223921',\n",
       "   '3010449299',\n",
       "   '3010149441',\n",
       "   '3008429297',\n",
       "   '3010633777',\n",
       "   '2614711400']},\n",
       " {'id': '3030968929',\n",
       "  'title': 'Detection of air and surface contamination by SARS-CoV-2 in hospital rooms of infected patients.',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '282',\n",
       "  'abstract': 'Understanding the particle size distribution in the air and patterns of environmental contamination of SARS-CoV-2 is essential for infection prevention policies. Here we screen surface and air samples from hospital rooms of COVID-19 patients for SARS-CoV-2 RNA. Environmental sampling is conducted in three airborne infection isolation rooms (AIIRs) in the ICU and 27 AIIRs in the general ward. 245 surface samples are collected. 56.7% of rooms have at least one environmental surface contaminated. High touch surface contamination is shown in ten (66.7%) out of 15 patients in the first week of illness, and three (20%) beyond the first week of illness (p = 0.01, χ2 test). Air sampling is performed in three of the 27 AIIRs in the general ward, and detects SARS-CoV-2 PCR-positive particles of sizes >4 µm and 1-4 µm in two rooms, despite these rooms having 12 air changes per hour. This warrants further study of the airborne transmission potential of SARS-CoV-2.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Po Ying Chia 1, 2, Kristen Kelli Coleman 3, Yian Kim Tan 4, Sean Wei Xiang Ong 2, Marcus Gum 4, Sok Kiang Lau 4, Xiao Fang Lim 4, Ai Sim Lim 4, Stephanie Sutjipto 2, Pei Hua Lee 2, Barnaby Edward Young 1, 2, Donald K Milton 5, Gregory C Gray 3, 6, Stephan Schuster 1, Timothy Barkham 2, 3, Partha Pratim De 1, 2, Shawn Vasoo 1, 2, Monica Chan 2, Brenda Sze Peng Ang 7, Boon Huan Tan 4, Yee-Sin Leo 7, Oon-Tek Ng 1, 2, Michelle Su Yen Wong 4, Kalisvar Marimuthu 2, 3'],\n",
       "  'related_topics': ['Airborne transmission',\n",
       "   'Air changes per hour',\n",
       "   'Isolation (health care)'],\n",
       "  'references': ['3002539152',\n",
       "   '3001195213',\n",
       "   '3006961006',\n",
       "   '3012099172',\n",
       "   '3008696669',\n",
       "   '3013893137',\n",
       "   '3010604545',\n",
       "   '3010338568',\n",
       "   '2132260239',\n",
       "   '3010449299']},\n",
       " {'id': '2158121945',\n",
       "  'title': 'Guidelines for environmental infection control in health-care facilities. Recommendations of CDC and the Healthcare Infection Control Practices Advisory Committee (HICPAC).',\n",
       "  'reference_count': '500',\n",
       "  'citation_count': '1,360',\n",
       "  'abstract': 'The health-care facility environment is rarely implicated in disease transmission, except among patients who are immunocompromised. Nonetheless, inadvertent exposures to environmental pathogens (e.g., Aspergillus spp. and Legionella spp.) or airborne pathogens (e.g., Mycobacterium tuberculosis and varicella-zoster virus) can result in adverse patient outcomes and cause illness among health-care workers. Environmental infection-control strategies and engineering controls can effectively prevent these infections. The incidence of health-care--associated infections and pseudo-outbreaks can be minimized by 1) appropriate use of cleaners and disinfectants; 2) appropriate maintenance of medical equipment (e.g., automated endoscope reprocessors or hydrotherapy equipment); 3) adherence to water-quality standards for hemodialysis, and to ventilation standards for specialized care environments (e.g., airborne infection isolation rooms, protective environments, or operating rooms); and 4) prompt management of water intrusion into the facility. Routine environmental sampling is not usually advised, except for water quality determinations in hemodialysis settings and other situations where sampling is directed by epidemiologic principles, and results can be applied directly to infection-control decisions. This report reviews previous guidelines and strategies for preventing environment-associated infections in health-care facilities and offers recommendations. These include 1) evidence-based recommendations supported by studies; 2) requirements of federal agencies (e.g., Food and Drug Administration, U.S. Environmental Protection Agency, U.S. Department of Labor, Occupational Safety and Health Administration, and U.S. Department of Justice); 3) guidelines and standards from building and equipment professional organizations (e.g., American Institute of Architects, Association for the Advancement of Medical Instrumentation, and American Society of Heating, Refrigeration, and Air-Conditioning Engineers); 4) recommendations derived from scientific theory or rationale; and 5) experienced opinions based upon infection-control and engineering practices. The report also suggests a series of performance measurements as a means to evaluate infection-control efforts.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Lynne Sehulster'],\n",
       "  'related_topics': ['Isolation (health care)',\n",
       "   'Infection control',\n",
       "   'Health care',\n",
       "   'Occupational safety and health',\n",
       "   'Medical equipment',\n",
       "   'Justice (ethics)',\n",
       "   'Professional association',\n",
       "   'Medical emergency',\n",
       "   'Guideline',\n",
       "   'Environmental health',\n",
       "   'Medicine'],\n",
       "  'references': ['1856219842',\n",
       "   '2798795107',\n",
       "   '1833207062',\n",
       "   '2093487930',\n",
       "   '2905912541',\n",
       "   '1536339477',\n",
       "   '2465608195',\n",
       "   '2153911335',\n",
       "   '2315217144',\n",
       "   '1589603082']},\n",
       " {'id': '3015636815',\n",
       "  'title': 'Rapid Detection of COVID-19 Causative Virus (SARS-CoV-2) in Human Nasopharyngeal Swab Specimens Using Field-Effect Transistor-Based Biosensor.',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '409',\n",
       "  'abstract': 'Coronavirus disease 2019 (COVID-19) is a newly emerging human infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2, previously called 2019-nCoV). Based on the rapid increase in the rate of human infection, the World Health Organization (WHO) has classified the COVID-19 outbreak as a pandemic. Because no specific drugs or vaccines for COVID-19 are yet available, early diagnosis and management are crucial for containing the outbreak. Here, we report a field-effect transistor (FET)-based biosensing device for detecting SARS-CoV-2 in clinical samples. The sensor was produced by coating graphene sheets of the FET with a specific antibody against SARS-CoV-2 spike protein. The performance of the sensor was determined using antigen protein, cultured virus, and nasopharyngeal swab specimens from COVID-19 patients. Our FET device could detect the SARS-CoV-2 spike protein at concentrations of 1 fg/mL in phosphate-buffered saline and 100 fg/mL clinical transport medium. In addition, the FET sensor successfully detected SARS-CoV-2 in culture medium (limit of detection [LOD]: 1.6 × 101 pfu/mL) and clinical samples (LOD: 2.42 × 102 copies/mL). Thus, we have successfully fabricated a promising FET biosensor for SARS-CoV-2; our device is a highly sensitive immunological diagnostic method for COVID-19 that requires no sample pretreatment or labeling.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Giwan Seo 1, Geonhee Lee 2, Mi Jeong Kim 1, Seung Hwa Baek 3, Minsuk Choi 3, Keun Bon Ku 3, Chang Seop Lee 4, Sangmi Jun 3, Daeui Park 3, Hong Gi Kim 3, Seong Jun Kim 3, Jeong O. Lee 2, Bum Tae Kim 3, Edmond Changkyun Park 1, Seung Il Kim 1'],\n",
       "  'related_topics': ['Virus', 'Antigen', 'Detection limit'],\n",
       "  'references': ['3003668884',\n",
       "   '3004280078',\n",
       "   '2014935324',\n",
       "   '3004318991',\n",
       "   '3006961006',\n",
       "   '3003217347',\n",
       "   '3008696669',\n",
       "   '3007643904',\n",
       "   '3009906937',\n",
       "   '2470646526']},\n",
       " {'id': '3018724240',\n",
       "  'title': 'SARS-CoV-2 can be detected in urine, blood, anal swabs, and oropharyngeal swabs specimens.',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '213',\n",
       "  'abstract': \"Purpose The purpose of this study was to detect severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) ribonucleic acid (RNA) in urine and blood specimens, and anal and oropharyngeal swabs from patients with confirmed SARS-CoV-2 infection, and correlated positive results with clinical findings. Methods Patients with confirmed SARS-CoV-2 infections were included in this study. Patients' demographic and clinical data were recorded. Quantitative real-time polymerase chain reaction was used to detect SARS-CoV-2 RNA in urine and blood specimens, and anal and oropharyngeal swabs. The study is registered at ClinicalTrials.gov (No. NCT04279782, 19 February, 2020). Results SARS-CoV-2 RNA was present in all four specimen types, though not all specimen types were positive simultaneously. The presence of viral RNA was not necessarily predictive of clinical symptoms, for example, the presence of viral RNA in the urine did not necessarily predict urinary tract symptoms. Conclusions SARS-CoV-2 can infect multiple systems, including the urinary tract. Testing different specimen types may be useful for monitoring disease changes and progression, and for establishing a prognosis.\",\n",
       "  'date': 2020,\n",
       "  'authors': ['Liang Peng 1, Jing Liu 1, Wenxiong Xu 1, Qiumin Luo 1, Dabiao Chen 1, Ziying Lei 1, Zhanlian Huang 1, Xuejun Li 1, Keji Deng 2, Bingliang Lin 1, Zhiliang Gao 1'],\n",
       "  'related_topics': ['Urinary system',\n",
       "   'Urine',\n",
       "   'Real-time polymerase chain reaction',\n",
       "   'RNA',\n",
       "   'Polymerase chain reaction',\n",
       "   'Gastroenterology',\n",
       "   'Virology',\n",
       "   'Medicine',\n",
       "   'Internal medicine',\n",
       "   'Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)',\n",
       "   'URINE BLOOD',\n",
       "   'Viral rna'],\n",
       "  'references': ['3001118548',\n",
       "   '3002539152',\n",
       "   '3003465021',\n",
       "   '3010604545',\n",
       "   '3011060952']},\n",
       " {'id': '3008028633',\n",
       "  'title': 'Characteristics of and Important Lessons From the Coronavirus Disease 2019 (COVID-19) Outbreak in China: Summary of a Report of 72 314 Cases From the Chinese Center for Disease Control and Prevention',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '11,988',\n",
       "  'abstract': '',\n",
       "  'date': 2020,\n",
       "  'authors': ['Zunyou Wu', 'Jennifer M. McGoogan'],\n",
       "  'related_topics': ['Outbreak',\n",
       "   'Pandemic',\n",
       "   'China',\n",
       "   'Medicine',\n",
       "   'Viral Epidemiology',\n",
       "   'Betacoronavirus',\n",
       "   'Family medicine',\n",
       "   '2019-20 coronavirus outbreak',\n",
       "   'Coronavirus disease 2019 (COVID-19)',\n",
       "   'Disease control'],\n",
       "  'references': ['3006533361',\n",
       "   '3024919756',\n",
       "   '3082757469',\n",
       "   '3006044875',\n",
       "   '3005409672',\n",
       "   '3004434564',\n",
       "   '3033263492',\n",
       "   '3031410613',\n",
       "   '3026023364',\n",
       "   '3034094473']},\n",
       " {'id': '3010930696',\n",
       "  'title': 'Hydroxychloroquine and azithromycin as a treatment of COVID-19: results of an open-label non-randomized clinical trial.',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '4,437',\n",
       "  'abstract': 'Background Chloroquine and hydroxychloroquine have been found to be efficient on SARS-CoV-2, and reported to be efficient in Chinese COV-19 patients. We evaluate the role of hydroxychloroquine on respiratory viral loads. Patients and methods French Confirmed COVID-19 patients were included in a single arm protocol from early March to March 16th, to receive 600mg of hydroxychloroquine daily and their viral load in nasopharyngeal swabs was tested daily in a hospital setting. Depending on their clinical presentation, azithromycin was added to the treatment. Untreated patients from another center and cases refusing the protocol were included as negative controls. Presence and absence of virus at Day6-post inclusion was considered the end point. Results Six patients were asymptomatic, 22 had upper respiratory tract infection symptoms and eight had lower respiratory tract infection symptoms. Twenty cases were treated in this study and showed a significant reduction of the viral carriage at D6-post inclusion compared to controls, and much lower average carrying duration than reported of untreated patients in the literature. Azithromycin added to hydroxychloroquine was significantly more efficient for virus elimination. Conclusion Despite its small sample size our survey shows that hydroxychloroquine treatment is significantly associated with viral load reduction/disappearance in COVID-19 patients and its effect is reinforced by azithromycin.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Philippe Gautret 1, Jean Christophe Lagier 1, Philippe Parola 1, Van Thuan Hoang 1, Line Meddeb 2, Morgane Mailhe 2, Barbara Doudier 2, Johan Courjon 3, Valérie Giordanengo 4, Vera Esteves Vieira 2, Hervé Tissot Dupont 1, Stéphane Honoré 1, Philippe Colson 1, Eric Chabrière 1, Bernard La Scola 1, Jean Marc Rolain 1, Philippe Brouqui 1, Didier Raoult 1'],\n",
       "  'related_topics': ['Hydroxychloroquine',\n",
       "   'Viral load',\n",
       "   'Lower respiratory tract infection',\n",
       "   'Upper respiratory tract infection',\n",
       "   'Azithromycin',\n",
       "   'Asymptomatic',\n",
       "   'Randomized controlled trial',\n",
       "   'Clinical trial',\n",
       "   'Internal medicine',\n",
       "   'Medicine'],\n",
       "  'references': ['3009885589',\n",
       "   '3008028633',\n",
       "   '3005212621',\n",
       "   '3006645647',\n",
       "   '3009577418',\n",
       "   '3008763357',\n",
       "   '3010277308',\n",
       "   '2302013022',\n",
       "   '3024400578',\n",
       "   '3011032288']},\n",
       " {'id': '3014294089',\n",
       "  'title': 'Baseline Characteristics and Outcomes of 1591 Patients Infected With SARS-CoV-2 Admitted to ICUs of the Lombardy Region, Italy.',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '3,125',\n",
       "  'abstract': 'Importance In December 2019, a novel coronavirus (severe acute respiratory syndrome coronavirus 2 [SARS-CoV-2]) emerged in China and has spread globally, creating a pandemic. Information about the clinical characteristics of infected patients who require intensive care is limited. Objective To characterize patients with coronavirus disease 2019 (COVID-19) requiring treatment in an intensive care unit (ICU) in the Lombardy region of Italy. Design, Setting, and Participants Retrospective case series of 1591 consecutive patients with laboratory-confirmed COVID-19 referred for ICU admission to the coordinator center (Fondazione IRCCS Ca’ Granda Ospedale Maggiore Policlinico, Milan, Italy) of the COVID-19 Lombardy ICU Network and treated at one of the ICUs of the 72 hospitals in this network between February 20 and March 18, 2020. Date of final follow-up was March 25, 2020. Exposures SARS-CoV-2 infection confirmed by real-time reverse transcriptase–polymerase chain reaction (RT-PCR) assay of nasal and pharyngeal swabs. Main Outcomes and Measures Demographic and clinical data were collected, including data on clinical management, respiratory failure, and patient mortality. Data were recorded by the coordinator center on an electronic worksheet during telephone calls by the staff of the COVID-19 Lombardy ICU Network. Results Of the 1591 patients included in the study, the median (IQR) age was 63 (56-70) years and 1304 (82%) were male. Of the 1043 patients with available data, 709 (68%) had at least 1 comorbidity and 509 (49%) had hypertension. Among 1300 patients with available respiratory support data, 1287 (99% [95% CI, 98%-99%]) needed respiratory support, including 1150 (88% [95% CI, 87%-90%]) who received mechanical ventilation and 137 (11% [95% CI, 9%-12%]) who received noninvasive ventilation. The median positive end-expiratory pressure (PEEP) was 14 (IQR, 12-16) cm H2O, and Fio2was greater than 50% in 89% of patients. The median Pao2/Fio2was 160 (IQR, 114-220). The median PEEP level was not different between younger patients (n\\u2009=\\u2009503 aged ≤63 years) and older patients (n\\u2009=\\u2009514 aged ≥64 years) (14 [IQR, 12-15] vs 14 [IQR, 12-16] cm H2O, respectively; median difference, 0 [95% CI, 0-0];P\\u2009=\\u2009.94). Median Fio2was lower in younger patients: 60% (IQR, 50%-80%) vs 70% (IQR, 50%-80%) (median difference, −10% [95% CI, −14% to 6%];P\\u2009=\\u2009.006), and median Pao2/Fio2was higher in younger patients: 163.5 (IQR, 120-230) vs 156 (IQR, 110-205) (median difference, 7 [95% CI, −8 to 22];P\\u2009=\\u2009.02). Patients with hypertension (n\\u2009=\\u2009509) were older than those without hypertension (n\\u2009=\\u2009526) (median [IQR] age, 66 years [60-72] vs 62 years [54-68];P\\u2009 Conclusions and Relevance In this case series of critically ill patients with laboratory-confirmed COVID-19 admitted to ICUs in Lombardy, Italy, the majority were older men, a large proportion required mechanical ventilation and high levels of PEEP, and ICU mortality was 26%.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Giacomo Grasselli 1, 2, Alberto Zangrillo 3, Alberto Zanella 1, 2, Massimo Antonelli 4, 5, Luca Cabrini 6, Antonio Castelli 1, Danilo Cereda 7, Antonio Coluccello 8, Giuseppe Foti 9, Roberto Fumagalli 9, Giorgio Iotti 10, Nicola Latronico 11, Luca Lorini 12, Stefano Merler 13, Giuseppe Natalini 14, Alessandra Piatti 6, Marco Vito Ranieri 15, Anna Mara Scandroglio 16, Enrico Storti 17, Maurizio Cecconi 18, Antonio Pesenti 1, 2'],\n",
       "  'related_topics': ['Intensive care',\n",
       "   'Intensive care unit',\n",
       "   'Respiratory failure'],\n",
       "  'references': ['3001118548',\n",
       "   '3008827533',\n",
       "   '3005079553',\n",
       "   '3009885589',\n",
       "   '3008090866',\n",
       "   '3011508296',\n",
       "   '3011559677',\n",
       "   '3014538785',\n",
       "   '2158525457']},\n",
       " {'id': '2493916176',\n",
       "  'title': 'Enriching Word Vectors with Subword Information',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '5,936',\n",
       "  'abstract': 'Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models to learn such representations  ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Piotr Bojanowski',\n",
       "   'Edouard Grave',\n",
       "   'Armand Joulin',\n",
       "   'Tomas Mikolov'],\n",
       "  'related_topics': ['Word lists by frequency',\n",
       "   'Word embedding',\n",
       "   'Word (computer architecture)',\n",
       "   'Character (mathematics)',\n",
       "   'Similarity (psychology)',\n",
       "   'Natural language processing',\n",
       "   'Analogy',\n",
       "   'Speech recognition',\n",
       "   'Representation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Word representation'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2117130368',\n",
       "   '2962784628',\n",
       "   '1810943226',\n",
       "   '2963012544',\n",
       "   '2251012068',\n",
       "   '2147152072',\n",
       "   '1662133657',\n",
       "   '1938755728']},\n",
       " {'id': '2962711740',\n",
       "  'title': 'How Powerful are Graph Neural Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,774',\n",
       "  'abstract': 'Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Keyulu Xu 1, Weihua Hu 2, Jure Leskovec 2, Stefanie Jegelka 1'],\n",
       "  'related_topics': ['Graph (abstract data type)',\n",
       "   'Graph isomorphism',\n",
       "   'Feature learning',\n",
       "   'Representation (mathematics)',\n",
       "   'Discriminative model',\n",
       "   'Node (networking)',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Class (biology)',\n",
       "   'Scheme (programming language)'],\n",
       "  'references': ['2907492528',\n",
       "   '2905224888',\n",
       "   '2918342466',\n",
       "   '2916106175',\n",
       "   '3100078588',\n",
       "   '2963465695',\n",
       "   '2996604169',\n",
       "   '3007332492']},\n",
       " {'id': '2907492528',\n",
       "  'title': 'A Comprehensive Survey on Graph Neural Networks',\n",
       "  'reference_count': '189',\n",
       "  'citation_count': '1,586',\n",
       "  'abstract': 'Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Zonghan Wu 1, Shirui Pan 2, Fengwen Chen 1, Guodong Long 1, Chengqi Zhang 1, Philip S. Yu 3'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Feature extraction',\n",
       "   'Natural language understanding',\n",
       "   'Machine learning',\n",
       "   'Contextual image classification',\n",
       "   'Video processing',\n",
       "   'Computer science',\n",
       "   'Kernel (linear algebra)',\n",
       "   'Task analysis',\n",
       "   'Interdependence',\n",
       "   'Euclidean space',\n",
       "   'Artificial intelligence',\n",
       "   'Graph'],\n",
       "  'references': ['2194775991',\n",
       "   '639708223',\n",
       "   '2963403868',\n",
       "   '2099471712',\n",
       "   '2130942839',\n",
       "   '2963037989',\n",
       "   '2157331557',\n",
       "   '2310919327',\n",
       "   '2064675550',\n",
       "   '2160815625']},\n",
       " {'id': '3100848837',\n",
       "  'title': 'Graph Convolutional Neural Networks for Web-Scale Recommender Systems',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '918',\n",
       "  'abstract': 'Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Rex Ying 1, Ruining He 2, Kaifeng Chen 3, Pong Eksombatchai 2, William L. Hamilton 1, Jure Leskovec 1'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Convolutional neural network',\n",
       "   'Recommender system',\n",
       "   'Scalability',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Random walk',\n",
       "   'Artificial intelligence',\n",
       "   'Graph'],\n",
       "  'references': ['2962835968',\n",
       "   '2153579005',\n",
       "   '2271840356',\n",
       "   '2962756421',\n",
       "   '2964015378',\n",
       "   '3104097132',\n",
       "   '2964321699',\n",
       "   '2624431344',\n",
       "   '2296073425',\n",
       "   '2950898568']},\n",
       " {'id': '2963224980',\n",
       "  'title': 'A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications',\n",
       "  'reference_count': '146',\n",
       "  'citation_count': '871',\n",
       "  'abstract': 'Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Hongyun Cai 1, Vincent W. Zheng 1, Kevin Chen-Chuan Chang 2'],\n",
       "  'related_topics': ['Graph embedding',\n",
       "   'Graph (abstract data type)',\n",
       "   'Graph property',\n",
       "   'Graph theory',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Computation',\n",
       "   'Task analysis',\n",
       "   'Graph'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '1880262756',\n",
       "   '2064675550',\n",
       "   '2163922914',\n",
       "   '2962756421',\n",
       "   '2964015378',\n",
       "   '2053186076',\n",
       "   '3104097132',\n",
       "   '2127795553']},\n",
       " {'id': '2962883549',\n",
       "  'title': 'Deep Learning in Mobile and Wireless Networking: A Survey',\n",
       "  'reference_count': '500',\n",
       "  'citation_count': '666',\n",
       "  'abstract': 'The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Chaoyun Zhang 1, Paul Patras 1, Hamed Haddadi 2'],\n",
       "  'related_topics': ['Mobile device',\n",
       "   'Big data',\n",
       "   'Wireless network',\n",
       "   'Analytics',\n",
       "   'Agile management',\n",
       "   'User experience design',\n",
       "   'Wireless',\n",
       "   'Deep learning',\n",
       "   'Multimedia',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2964121744',\n",
       "   '2097117768',\n",
       "   '2919115771',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2099471712',\n",
       "   '2145339207',\n",
       "   '2130942839']},\n",
       " {'id': '2963184176',\n",
       "  'title': 'Image Generation from Scene Graphs',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '346',\n",
       "  'abstract': \"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.\",\n",
       "  'date': 2018,\n",
       "  'authors': ['Justin Johnson', 'Agrim Gupta', 'Li Fei-Fei'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Visualization',\n",
       "   'Natural language',\n",
       "   'Segmentation',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Graph',\n",
       "   'Image generation'],\n",
       "  'references': ['2964121744',\n",
       "   '2097117768',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2153579005',\n",
       "   '1861492603',\n",
       "   '2963073614',\n",
       "   '1959608418',\n",
       "   '2963684088',\n",
       "   '2963373786']},\n",
       " {'id': '3100278010',\n",
       "  'title': 'Neural Graph Collaborative Filtering',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '311',\n",
       "  'abstract': \"Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect. In this work, we propose to integrate the user-item interactions - more specifically the bipartite graph structure - into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec [39] and Collaborative Memory Network [5]. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at https://github.com/xiangwang1223/neural_graph_collaborative_filtering.\",\n",
       "  'date': 2019,\n",
       "  'authors': ['Xiang Wang 1, Xiangnan He 2, Meng Wang 3, Fuli Feng 1, Tat-Seng Chua 1'],\n",
       "  'related_topics': ['Collaborative filtering',\n",
       "   'Recommender system',\n",
       "   'Graph (abstract data type)',\n",
       "   'Deep learning',\n",
       "   'Bipartite graph',\n",
       "   'Embedding',\n",
       "   'Theoretical computer science',\n",
       "   'Matrix decomposition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Graph'],\n",
       "  'references': ['2964121744',\n",
       "   '1533861849',\n",
       "   '2964015378',\n",
       "   '2054141820',\n",
       "   '1994389483',\n",
       "   '2964321699',\n",
       "   '2157881433',\n",
       "   '3098649723',\n",
       "   '2963323306',\n",
       "   '2253995343']},\n",
       " {'id': '2905224888',\n",
       "  'title': 'Graph Neural Networks: A Review of Methods and Applications',\n",
       "  'reference_count': '257',\n",
       "  'citation_count': '1,098',\n",
       "  'abstract': 'Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Jie Zhou',\n",
       "   'Ganqu Cui',\n",
       "   'Zhengyan Zhang',\n",
       "   'Cheng Yang',\n",
       "   'Zhiyuan Liu',\n",
       "   'Lifeng Wang',\n",
       "   'Changcheng Li',\n",
       "   'Maosong Sun'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Connectionism',\n",
       "   'Scene graph',\n",
       "   'Network architecture',\n",
       "   'Message passing',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Fixed point',\n",
       "   'Categorization',\n",
       "   'Graph',\n",
       "   'Graph neural networks'],\n",
       "  'references': ['2194775991',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '2919115771',\n",
       "   '2117539524',\n",
       "   '2964308564',\n",
       "   '1614298861',\n",
       "   '2963446712',\n",
       "   '2157331557',\n",
       "   '2310919327']},\n",
       " {'id': '2918342466',\n",
       "  'title': 'Fast Graph Representation Learning with PyTorch Geometric',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '919',\n",
       "  'abstract': 'We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Matthias Fey', 'Jan Eric Lenssen'],\n",
       "  'related_topics': ['Graph (abstract data type)',\n",
       "   'Data structure',\n",
       "   'CUDA',\n",
       "   'Statistical relational learning',\n",
       "   'Deep learning',\n",
       "   'Point cloud',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Manifold',\n",
       "   'Artificial intelligence',\n",
       "   'Graph'],\n",
       "  'references': ['2899771611',\n",
       "   '2964015378',\n",
       "   '2963121255',\n",
       "   '2963858333',\n",
       "   '1920022804',\n",
       "   '2962767366',\n",
       "   '2964321699',\n",
       "   '2606780347',\n",
       "   '2979750740',\n",
       "   '2962711740']},\n",
       " {'id': '2796426482',\n",
       "  'title': 'FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation',\n",
       "  'reference_count': '58',\n",
       "  'citation_count': '384',\n",
       "  'abstract': 'Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet',\n",
       "  'date': 2018,\n",
       "  'authors': ['Yaoqing Yang 1, Chen Feng 2, Yiru Shen 3, Dong Tian 4'],\n",
       "  'related_topics': ['Point cloud',\n",
       "   'Encoder',\n",
       "   'Unsupervised learning',\n",
       "   'Artificial neural network',\n",
       "   'Grid',\n",
       "   'Supervised learning',\n",
       "   'Graph (abstract data type)',\n",
       "   'Autoencoder',\n",
       "   'Discriminative model',\n",
       "   'Iterative reconstruction',\n",
       "   'Algorithm',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Surface reconstruction'],\n",
       "  'references': ['2187089797',\n",
       "   '2964015378',\n",
       "   '2560609797',\n",
       "   '2963121255',\n",
       "   '1920022804',\n",
       "   '2964321699',\n",
       "   '2211722331',\n",
       "   '2964311892',\n",
       "   '2558748708',\n",
       "   '2546066744']},\n",
       " {'id': '2120419212',\n",
       "  'title': 'A discriminatively trained, multiscale, deformable part model',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '3,079',\n",
       "  'abstract': 'This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.',\n",
       "  'date': 2008,\n",
       "  'authors': ['P. Felzenszwalb 1, D. McAllester 2, D. Ramanan 3'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Support vector machine',\n",
       "   'Discriminative model',\n",
       "   'Pascal (programming language)',\n",
       "   'Object detection',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Histogram',\n",
       "   'Computer science',\n",
       "   'Grammar',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2161969291',\n",
       "   '2154422044',\n",
       "   '1576520375',\n",
       "   '2030536784',\n",
       "   '2112020727',\n",
       "   '2186094539',\n",
       "   '2101534792',\n",
       "   '1518641734',\n",
       "   '1970255615',\n",
       "   '2166770390']},\n",
       " {'id': '2152826865',\n",
       "  'title': 'Active appearance models',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '10,287',\n",
       "  'abstract': 'We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.',\n",
       "  'date': 2001,\n",
       "  'authors': ['T.F. Cootes 1, G.J. Edwards 2, C.J. Taylor 1'],\n",
       "  'related_topics': ['Active appearance model',\n",
       "   'Active shape model',\n",
       "   'Point distribution model',\n",
       "   'Statistical model',\n",
       "   'Matching (statistics)',\n",
       "   'Iterative method',\n",
       "   'Image segmentation',\n",
       "   'Blossom algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2152826865',\n",
       "   '2038952578',\n",
       "   '2408227189',\n",
       "   '2095757522',\n",
       "   '2103876808',\n",
       "   '2159173611',\n",
       "   '2129150631',\n",
       "   '2293264518',\n",
       "   '2133001582']},\n",
       " {'id': '2145072179',\n",
       "  'title': 'PCA-SIFT: a more distinctive representation for local image descriptors',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '4,922',\n",
       "  'abstract': \"Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.\",\n",
       "  'date': 2004,\n",
       "  'authors': ['Yan Ke', 'R. Sukthankar'],\n",
       "  'related_topics': ['Principal curvature-based region detector',\n",
       "   'Feature detection (computer vision)',\n",
       "   'GLOH',\n",
       "   'Feature (computer vision)',\n",
       "   'Image gradient',\n",
       "   'Feature extraction',\n",
       "   'Scale-invariant feature transform',\n",
       "   'Image processing',\n",
       "   'Image retrieval',\n",
       "   'Image registration',\n",
       "   'Histogram',\n",
       "   'Principal component analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Normalization (statistics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '1902027874',\n",
       "   '2154422044',\n",
       "   '2148694408',\n",
       "   '2119747362',\n",
       "   '2111308925',\n",
       "   '2098693229',\n",
       "   '1541642243']},\n",
       " {'id': '1576520375',\n",
       "  'title': 'Making large scale SVM learning practical',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '15,657',\n",
       "  'abstract': 'Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Thorsten Joachims'],\n",
       "  'related_topics': ['Ranking SVM',\n",
       "   'Support vector machine',\n",
       "   'Quadratic programming',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Quadratic equation',\n",
       "   'Constraint (information theory)',\n",
       "   'Scale (chemistry)'],\n",
       "  'references': ['2153635508',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '1880262756',\n",
       "   '2166706824',\n",
       "   '1964357740',\n",
       "   '2108646579',\n",
       "   '2120419212',\n",
       "   '2172000360']},\n",
       " {'id': '2115763357',\n",
       "  'title': 'A general framework for object detection',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '2,123',\n",
       "  'abstract': 'This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.',\n",
       "  'date': 1998,\n",
       "  'authors': ['C.P. Papageorgiou', 'M. Oren', 'T. Poggio'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Object detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Face detection',\n",
       "   'Representation (systemics)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Wavelet',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2132984323',\n",
       "   '2087347434',\n",
       "   '2124351082',\n",
       "   '2125848778',\n",
       "   '2104671481',\n",
       "   '2159173611',\n",
       "   '2137346077',\n",
       "   '2056695679',\n",
       "   '1676612073',\n",
       "   '2030989822']},\n",
       " {'id': '2161381512',\n",
       "  'title': 'Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '2,979',\n",
       "  'abstract': 'Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large- scale visual recognition challenge (ILSVRC2012). The suc- cess of CNNs is attributed to their ability to learn rich mid- level image representations as opposed to hand-designed low-level features used in other image classification meth- ods. Learning CNNs, however, amounts to estimating mil- lions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be effi- ciently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred rep- resentation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Maxime Oquab 1, 2, Leon Bottou 2, Ivan Laptev 1, Josef Sivic 1'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Contextual image classification',\n",
       "   'Pascal (programming language)',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2168356304',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2310919327',\n",
       "   '2031489346']},\n",
       " {'id': '2166851633',\n",
       "  'title': 'Stochastic variational inference',\n",
       "  'reference_count': '97',\n",
       "  'citation_count': '1,981',\n",
       "  'abstract': 'We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Matthew D. Hoffman 1, David M. Blei 2, Chong Wang 3, John Paisley 4'],\n",
       "  'related_topics': ['Frequentist inference',\n",
       "   'Variational message passing',\n",
       "   'Fiducial inference',\n",
       "   'Predictive inference',\n",
       "   'Inference',\n",
       "   'Statistical inference',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Bayesian inference',\n",
       "   'Theoretical computer science',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1880262756',\n",
       "   '1503398984',\n",
       "   '2125838338',\n",
       "   '1506806321',\n",
       "   '1511986666',\n",
       "   '1981457167',\n",
       "   '2159080219',\n",
       "   '2001082470',\n",
       "   '2174706414',\n",
       "   '2158266063']},\n",
       " {'id': '2097268041',\n",
       "  'title': 'Deep AutoRegressive Networks',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '225',\n",
       "  'abstract': 'We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Karol Gregor',\n",
       "   'Ivo Danihelka',\n",
       "   'Andriy Mnih',\n",
       "   'Charles Blundell',\n",
       "   'Daan Wierstra'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Approximate inference',\n",
       "   'Feedforward neural network',\n",
       "   'Autoregressive model',\n",
       "   'MNIST database',\n",
       "   'Estimation theory',\n",
       "   'Upper and lower bounds',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2310919327',\n",
       "   '3120740533',\n",
       "   '2025768430',\n",
       "   '3140968660',\n",
       "   '1810943226',\n",
       "   '2952509347',\n",
       "   '189596042',\n",
       "   '2096192494',\n",
       "   '2108677974',\n",
       "   '2134842679']},\n",
       " {'id': '2963173382',\n",
       "  'title': 'Black Box Variational Inference',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '796',\n",
       "  'abstract': 'Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires signicant model-specic analysis. These eorts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a \\\\black box\" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid dicult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We nd that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Rajesh Ranganath', 'Sean Gerrish', 'David M. Blei'],\n",
       "  'related_topics': ['Inference',\n",
       "   'Black box',\n",
       "   'Stochastic optimization',\n",
       "   'Monte Carlo method',\n",
       "   'Sampling (statistics)',\n",
       "   'Latent variable',\n",
       "   'Algorithm',\n",
       "   'Variance (accounting)',\n",
       "   'Distribution (mathematics)',\n",
       "   'Computer science'],\n",
       "  'references': ['1663973292',\n",
       "   '1959608418',\n",
       "   '2146502635',\n",
       "   '2166851633',\n",
       "   '2120340025',\n",
       "   '1516111018',\n",
       "   '2951493172',\n",
       "   '2128925311',\n",
       "   '2119196781',\n",
       "   '3104819538']},\n",
       " {'id': '2951493172',\n",
       "  'title': 'Variational Bayesian Inference with Stochastic Search',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '307',\n",
       "  'abstract': 'Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.',\n",
       "  'date': 2012,\n",
       "  'authors': ['John Paisley 1, David Blei 2, Michael Jordan 1'],\n",
       "  'related_topics': ['Bayesian inference',\n",
       "   'Stochastic optimization',\n",
       "   'Marginal likelihood',\n",
       "   'Upper and lower bounds',\n",
       "   'Posterior probability',\n",
       "   'Control variates',\n",
       "   'Inference',\n",
       "   'Bayesian probability',\n",
       "   'Applied mathematics',\n",
       "   'Logistic regression',\n",
       "   'Computer science',\n",
       "   'Joint likelihood'],\n",
       "  'references': ['1880262756',\n",
       "   '2158266063',\n",
       "   '2108677974',\n",
       "   '2115979064',\n",
       "   '2127498532',\n",
       "   '2187741934',\n",
       "   '2117111086',\n",
       "   '1496451467',\n",
       "   '2142508340',\n",
       "   '2162995719']},\n",
       " {'id': '2171490498',\n",
       "  'title': 'Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '306',\n",
       "  'abstract': 'Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Koray Kavukcuoglu', \"Marc'Aurelio Ranzato\", 'Yann LeCun'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   '3D single-object recognition',\n",
       "   'Neural coding',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Basis function',\n",
       "   'Representation (systemics)',\n",
       "   'Pattern recognition',\n",
       "   'Algorithm',\n",
       "   'Image (mathematics)',\n",
       "   'Inference',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2310919327',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2063978378',\n",
       "   '2078204800',\n",
       "   '2166049352',\n",
       "   '2151693816',\n",
       "   '2113606819',\n",
       "   '2154332973',\n",
       "   '2139427956']},\n",
       " {'id': '2119196781',\n",
       "  'title': 'Variational Bayesian Inference with Stochastic Search',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '404',\n",
       "  'abstract': 'Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.',\n",
       "  'date': 2012,\n",
       "  'authors': ['David M. Blei 1, Michael I. Jordan 2, John W. Paisley 2'],\n",
       "  'related_topics': ['Bayesian linear regression',\n",
       "   'Bayesian inference',\n",
       "   'Stochastic optimization',\n",
       "   'Marginal likelihood',\n",
       "   'Upper and lower bounds',\n",
       "   'Posterior probability',\n",
       "   'Control variates',\n",
       "   'Inference',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['1880262756',\n",
       "   '2158266063',\n",
       "   '2108677974',\n",
       "   '1516111018',\n",
       "   '2165599843',\n",
       "   '2115979064',\n",
       "   '2127498532',\n",
       "   '2187741934',\n",
       "   '2117111086',\n",
       "   '1496451467']},\n",
       " {'id': '3104819538',\n",
       "  'title': 'Fixed-form variational posterior approximation through stochastic linear regression',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '186',\n",
       "  'abstract': 'textabstractWe propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribu- tion. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approxi- mation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several exam- ples illustrate the speed and accuracy of our approximation method in practice.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Tim Salimans', 'David A. Knowles'],\n",
       "  'related_topics': ['Exponential family',\n",
       "   'Posterior probability',\n",
       "   'Stochastic approximation',\n",
       "   'Divergence (statistics)',\n",
       "   'Approximate inference',\n",
       "   'Distribution (mathematics)',\n",
       "   'Bayesian probability',\n",
       "   'Linear regression',\n",
       "   'Applied mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['1663973292',\n",
       "   '114517082',\n",
       "   '2166851633',\n",
       "   '2120340025',\n",
       "   '1992208280',\n",
       "   '1516111018',\n",
       "   '1545319692',\n",
       "   '2165599843',\n",
       "   '3125096521',\n",
       "   '1515272691']},\n",
       " {'id': '4919037',\n",
       "  'title': 'Regularization of Neural Networks using DropConnect',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '2,335',\n",
       "  'abstract': 'We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Li Wan',\n",
       "   'Matthew Zeiler',\n",
       "   'Sixin Zhang',\n",
       "   'Yann Le Cun',\n",
       "   'Rob Fergus'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Pattern recognition',\n",
       "   'Regularization (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3118608800',\n",
       "   '2310919327',\n",
       "   '1904365287',\n",
       "   '1665214252',\n",
       "   '2131241448',\n",
       "   '2335728318',\n",
       "   '2141125852',\n",
       "   '2134557905',\n",
       "   '2963574257',\n",
       "   '188867022']},\n",
       " {'id': '2206858481',\n",
       "  'title': 'Visualizing and Understanding Convolutional Neural Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '484',\n",
       "  'abstract': 'Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\\\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Matthew D Zeiler', 'Rob Fergus'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Softmax function',\n",
       "   'Classifier (linguistics)',\n",
       "   'Benchmark (computing)',\n",
       "   'Feature (machine learning)',\n",
       "   'Network model',\n",
       "   'Visualization',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Function (engineering)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '639708223',\n",
       "   '1677182931',\n",
       "   '2109255472',\n",
       "   '2964153729',\n",
       "   '2016053056']},\n",
       " {'id': '2120480077',\n",
       "  'title': 'Building high-level features using large scale unsupervised learning',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '2,558',\n",
       "  'abstract': 'We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Quoc V. Le'],\n",
       "  'related_topics': ['Autoencoder',\n",
       "   'Unsupervised learning',\n",
       "   'Object detection',\n",
       "   'Facial recognition system',\n",
       "   'Detector',\n",
       "   'Pattern recognition',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2108598243',\n",
       "   '2136922672',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '2100495367',\n",
       "   '2168231600',\n",
       "   '2546302380',\n",
       "   '2110798204',\n",
       "   '1782590233',\n",
       "   '2130325614']},\n",
       " {'id': '2150165932',\n",
       "  'title': 'How to Explain Individual Classification Decisions',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '671',\n",
       "  'abstract': 'After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.',\n",
       "  'date': 2010,\n",
       "  'authors': ['David Baehrens 1, Timon Schroeter 1, Stefan Harmeling 2, Motoaki Kawanabe 3, Katja Hansen 1, Klaus-Robert Müller 1'],\n",
       "  'related_topics': ['Decision tree',\n",
       "   'Classifier (UML)',\n",
       "   'Black box',\n",
       "   'Kernel method',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Classification methods'],\n",
       "  'references': ['2156909104',\n",
       "   '1746819321',\n",
       "   '1554663460',\n",
       "   '1480376833',\n",
       "   '2119479037',\n",
       "   '3023786531',\n",
       "   '740415',\n",
       "   '2108995755',\n",
       "   '1618905105',\n",
       "   '1564947197']},\n",
       " {'id': '2159269332',\n",
       "  'title': 'A universal image quality index',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '5,977',\n",
       "  'abstract': 'We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality_index/demo.html.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Zhou Wang 1, A.C. Bovik 2'],\n",
       "  'related_topics': ['Distortion',\n",
       "   'Image quality',\n",
       "   'Image processing',\n",
       "   'Human visual system model',\n",
       "   'Mean squared error',\n",
       "   'Signal-to-noise ratio',\n",
       "   'Metric (mathematics)',\n",
       "   'Luminance',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153777140', '2912116903', '1543242897']},\n",
       " {'id': '2142276208',\n",
       "  'title': 'A new, fast, and efficient image codec based on set partitioning in hierarchical trees',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '8,658',\n",
       "  'abstract': 'Embedded zerotree wavelet (EZW) coding, introduced by Shapiro (see IEEE Trans. Signal Processing, vol.41, no.12, p.3445, 1993), is a very effective and computationally simple technique for image compression. We offer an alternative explanation of the principles of its operation, so that the reasons for its excellent performance can be better understood. These principles are partial ordering by magnitude with a set partitioning sorting algorithm, ordered bit plane transmission, and exploitation of self-similarity across different scales of an image wavelet transform. Moreover, we present a new and different implementation based on set partitioning in hierarchical trees (SPIHT), which provides even better performance than our previously reported extension of EZW that surpassed the performance of the original EZW. The image coding results, calculated from actual file sizes and images reconstructed by the decoding algorithm, are either comparable to or surpass previous results obtained through much more sophisticated and computationally complex methods. In addition, the new coding and decoding procedures are extremely fast, and they can be made even faster, with only small loss in performance, by omitting entropy coding of the bit stream by the arithmetic code.',\n",
       "  'date': 1996,\n",
       "  'authors': ['A. Said 1, W.A. Pearlman 2'],\n",
       "  'related_topics': ['Set partitioning in hierarchical trees',\n",
       "   'Data compression',\n",
       "   'Entropy encoding',\n",
       "   'Image compression',\n",
       "   'Transform coding',\n",
       "   'Wavelet transform',\n",
       "   'Bit plane',\n",
       "   'Image processing',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2053691921',\n",
       "   '2148593155',\n",
       "   '2103504761',\n",
       "   '2129652681',\n",
       "   '1931641413',\n",
       "   '2166087152',\n",
       "   '2058719583',\n",
       "   '1592970628',\n",
       "   '2147399030',\n",
       "   '2117465325']},\n",
       " {'id': '2118217749',\n",
       "  'title': 'JPEG2000 : image compression fundamentals, standards, and practice',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,977',\n",
       "  'abstract': 'This is nothing less than a totally essential reference for engineers and researchers in any field of work that involves the use of compressed imagery. Beginning with a thorough and up-to-date overview of the fundamentals of image compression, the authors move on to provide a complete description of the JPEG2000 standard. They then devote space to the implementation and exploitation of that standard. The final section describes other key image compression systems. This work has specific applications for those involved in the development of software and hardware solutions for multimedia, internet, and medical imaging applications.',\n",
       "  'date': 2001,\n",
       "  'authors': ['David S. Taubman', 'Michael W. Marcellin'],\n",
       "  'related_topics': ['Image compression',\n",
       "   'The Internet',\n",
       "   'Software',\n",
       "   'JPEG 2000',\n",
       "   'JPIP',\n",
       "   'Multimedia',\n",
       "   'Field (computer science)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Work (electrical)'],\n",
       "  'references': ['2133665775',\n",
       "   '2119667497',\n",
       "   '2101675075',\n",
       "   '2129768577',\n",
       "   '1976709621',\n",
       "   '2161907179',\n",
       "   '2046119925',\n",
       "   '2158787690']},\n",
       " {'id': '2053691921',\n",
       "  'title': 'Embedded image coding using zerotrees of wavelet coefficients',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '8,503',\n",
       "  'abstract': 'The embedded zerotree wavelet algorithm (EZW) is a simple, yet remarkably effective, image compression algorithm, having the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. The embedded code represents a sequence of binary decisions that distinguish an image from the \"null\" image. Using an embedded coding algorithm, an encoder can terminate the encoding at any point thereby allowing a target rate or target distortion metric to be met exactly. Also, given a bit stream, the decoder can cease decoding at any point in the bit stream and still produce exactly the same image that would have been encoded at the bit rate corresponding to the truncated bit stream. In addition to producing a fully embedded bit stream, the EZW consistently produces compression results that are competitive with virtually all known compression algorithms on standard test images. Yet this performance is achieved with a technique that requires absolutely no training, no pre-stored tables or codebooks, and requires no prior knowledge of the image source. The EZW algorithm is based on four key concepts: (1) a discrete wavelet transform or hierarchical subband decomposition, (2) prediction of the absence of significant information across scales by exploiting the self-similarity inherent in images, (3) entropy-coded successive-approximation quantization, and (4) universal lossless data compression which is achieved via adaptive arithmetic coding. >',\n",
       "  'date': 1993,\n",
       "  'authors': ['J.M. Shapiro'],\n",
       "  'related_topics': ['Data compression',\n",
       "   'Set partitioning in hierarchical trees',\n",
       "   'Lossless compression',\n",
       "   'Arithmetic coding',\n",
       "   'Signal compression',\n",
       "   'Wavelet transform',\n",
       "   'Binary image',\n",
       "   'Discrete wavelet transform',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2132984323',\n",
       "   '2098914003',\n",
       "   '2140196014',\n",
       "   '1996021349',\n",
       "   '2156447271',\n",
       "   '1970352604',\n",
       "   '2103504761',\n",
       "   '2129652681',\n",
       "   '2166982406',\n",
       "   '2186435531']},\n",
       " {'id': '2153777140',\n",
       "  'title': 'Image quality measures and their performance',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '1,991',\n",
       "  'abstract': \"A number of quality measures are evaluated for gray scale image compression. They are all bivariate, exploiting the differences between corresponding pixels in the original and degraded images. It is shown that although some numerical measures correlate well with the observers' response for a given compression technique, they are not reliable for an evaluation across different techniques. A graphical measure called Hosaka plots, however, can be used to appropriately specify not only the amount, but also the type of degradation in reconstructed images.\",\n",
       "  'date': 1995,\n",
       "  'authors': ['A.M. Eskicioglu', 'P.S. Fisher'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Image compression',\n",
       "   'Data compression',\n",
       "   'Image resolution',\n",
       "   'Grayscale',\n",
       "   'Pixel',\n",
       "   'Transform coding',\n",
       "   'Histogram',\n",
       "   'Iterative reconstruction',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3021180913', '1487065163', '2170745401']},\n",
       " {'id': '2912116903',\n",
       "  'title': 'Image dissimilarity',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '143',\n",
       "  'abstract': '',\n",
       "  'date': 1998,\n",
       "  'authors': ['Jean-Bernard Martens', 'Lydia Meesters'],\n",
       "  'related_topics': ['Image (mathematics)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2103504761',\n",
       "   '1991605728',\n",
       "   '2103232506',\n",
       "   '2134774992',\n",
       "   '42232744',\n",
       "   '1990873664',\n",
       "   '2108657140',\n",
       "   '1551978325',\n",
       "   '2056930330',\n",
       "   '2118491738']},\n",
       " {'id': '2107790757',\n",
       "  'title': 'Shiftable multiscale transforms',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '1,966',\n",
       "  'abstract': 'One of the major drawbacks of orthogonal wavelet transforms is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal and, in two dimensions, rotations of the input signal. The authors formalize these problems by defining a type of translation invariance called shiftability. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be applied in the context of other domains, particularly orientation and scale. Jointly shiftable transforms that are simultaneously shiftable in more than one domain are explored. Two examples of jointly shiftable transforms are designed and implemented: a 1-D transform that is jointly shiftable in position and scale, and a 2-D transform that is jointly shiftable in position and orientation. The usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement is demonstrated. >',\n",
       "  'date': 1992,\n",
       "  'authors': ['E.P. Simoncelli 1, W.T. Freeman 1, E.H. Adelson 1, D.J. Heeger 2'],\n",
       "  'related_topics': ['Wavelet',\n",
       "   'Orthogonal wavelet',\n",
       "   'Wavelet transform',\n",
       "   'Stereophotography',\n",
       "   'Nyquist–Shannon sampling theorem',\n",
       "   'Image processing',\n",
       "   'Orthogonal transformation',\n",
       "   'Signal processing',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2170120409',\n",
       "   '2132984323',\n",
       "   '2098914003',\n",
       "   '1996021349',\n",
       "   '2103504761',\n",
       "   '1991605728',\n",
       "   '2118877769',\n",
       "   '2109863423',\n",
       "   '2166982406',\n",
       "   '1627054999']},\n",
       " {'id': '2158564760',\n",
       "  'title': 'Why is image quality assessment so difficult',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '925',\n",
       "  'abstract': 'Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in recent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the literature. Furthermore, we propose a new philosophy in designing image quality metrics: The main function of the human eyes is to extract structural information from the viewing field, and the human visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approximation of perceived image distortion. Based on the new philosophy, we implemented a simple but effective image quality indexing algorithm, which is very promising as shown by our current results.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Zhou Wang', 'Alan C. Bovik', 'Ligang Lu'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Subjective video quality',\n",
       "   'Image processing',\n",
       "   'Human visual system model',\n",
       "   'Distortion',\n",
       "   'Search engine indexing',\n",
       "   'Field (computer science)',\n",
       "   'Machine learning',\n",
       "   'Function (engineering)',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2159269332',\n",
       "   '2153777140',\n",
       "   '2912116903',\n",
       "   '2145792107',\n",
       "   '1543242897',\n",
       "   '2015937190',\n",
       "   '1527289589',\n",
       "   '42232744',\n",
       "   '2029826041',\n",
       "   '1990873664']},\n",
       " {'id': '2124731682',\n",
       "  'title': 'Image compression via joint statistical characterization in the wavelet domain',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '785',\n",
       "  'abstract': 'We develop a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. Pairs of wavelet coefficients, corresponding to basis functions at adjacent spatial locations, orientations, and scales, are found to be non-Gaussian in both their marginal and joint statistical properties. Specifically, their marginals are heavy-tailed, and although they are typically decorrelated, their magnitudes are highly correlated. We propose a Markov model that explains these dependencies using a linear predictor for magnitude coupled with both multiplicative and additive uncertainties, and show that it accounts for the statistics of a wide variety of images including photographic images, graphical images, and medical images. In order to directly demonstrate the power of the model, we construct an image coder called EPWIC (embedded predictive wavelet image coder), in which subband coefficients are encoded one bitplane at a time using a nonadaptive arithmetic encoder that utilizes conditional probabilities calculated from the model. Bitplanes are ordered using a greedy algorithm that considers the MSE reduction per encoded bit. The decoder uses the statistical model to predict coefficient values based on the bits it has received. Despite the simplicity of the model, the rate-distortion performance of the coder is roughly comparable to the best image coders in the literature.',\n",
       "  'date': 1999,\n",
       "  'authors': ['R.W. Buccigrossi 1, E.P. Simoncelli 2'],\n",
       "  'related_topics': ['Wavelet transform', 'Wavelet', 'Statistical model'],\n",
       "  'references': []},\n",
       " {'id': '2115838129',\n",
       "  'title': 'Linear transform for simultaneous diagonalization of covariance and perceptual metric matrix in image coding',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '28',\n",
       "  'abstract': 'Two types ofredundancies are contained in images: statistical redundancy and psychovisual redundancy. Image representation techniques for image coding should remove both redundancies in order to obtain good results. In order to establish an appropriate representation, the standard approach to transform coding only considers the statistical redundancy, whereas the psychovisual factors are introduced after the selection ofthe representation as a simple scalar weighting in the transform domain. In this work, we take into account the psychovisual factors in the de8nition of the representation together with the statistical factors, by means of the perceptual metric and the covariance matrix, respectively. In general the ellipsoids described by these matrices are not aligned. Therefore, the optimal basis for image representation should simultaneously diagonalize both matrices. This approach to the basis selection problem has several advantages in the particular application ofimage coding. As the transform domain is Euclidean (by de8nition), the quantizer design is highly simpli8ed and at the same time, the use ofscalar quantizers is truly justi8ed. The proposed representation is compared to covariance-based representations such as the DCT and the KLT or PCA using standard JPEG-like and Max-Lloyd quantizers. ? 2003 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Irene Epifanio 1, Jaime Gutierrez 2, Jesus Malo 2'],\n",
       "  'related_topics': ['Transform coding',\n",
       "   'Covariance matrix',\n",
       "   'Covariance',\n",
       "   'Image compression',\n",
       "   'Discrete cosine transform',\n",
       "   'Matrix (mathematics)',\n",
       "   'Redundancy (information theory)',\n",
       "   'Weighting',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1548802052',\n",
       "   '2798909945',\n",
       "   '2140196014',\n",
       "   '1634005169',\n",
       "   '2145889472',\n",
       "   '3017143921',\n",
       "   '2137234026',\n",
       "   '2134383396',\n",
       "   '98769269',\n",
       "   '1500256440']},\n",
       " {'id': '2481240925',\n",
       "  'title': 'Deep Visual-Semantic Alignments for Generating Image Descriptions',\n",
       "  'reference_count': '65',\n",
       "  'citation_count': '4,269',\n",
       "  'abstract': 'We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks (RNN) over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions outperform retrieval baselines on both full images and on a new dataset of region-level annotations. Finally, we conduct large-scale analysis of our RNN language model on the Visual Genome dataset of 4.1 million captions and highlight the differences between image and region-level caption statistics.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Andrej Karpathy', 'Li Fei-Fei'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Convolutional neural network',\n",
       "   'Recurrent neural network',\n",
       "   'Image segmentation',\n",
       "   'Natural language',\n",
       "   'Visualization',\n",
       "   'Context (language use)',\n",
       "   'Natural language processing',\n",
       "   'Pattern recognition',\n",
       "   'Sentence',\n",
       "   'Computer science',\n",
       "   'Embedding',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '2153579005',\n",
       "   '2250539671',\n",
       "   '2108598243',\n",
       "   '2310919327',\n",
       "   '2031489346']},\n",
       " {'id': '1947481528',\n",
       "  'title': 'Long-term recurrent convolutional networks for visual recognition and description',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '5,117',\n",
       "  'abstract': 'Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Jeff Donahue 1, Lisa Anne Hendricks 1, Sergio Guadarrama 1, Marcus Rohrbach 1, Subhashini Venugopalan 2, Trevor Darrell 1, Kate Saenko 3'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Deep learning',\n",
       "   'Visual learning',\n",
       "   'Backpropagation',\n",
       "   'Natural language',\n",
       "   'Pattern recognition',\n",
       "   'Term (time)',\n",
       "   'Benchmark (computing)',\n",
       "   'State (computer science)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2117539524',\n",
       "   '2108598243',\n",
       "   '2130942839',\n",
       "   '2155893237',\n",
       "   '1849277567',\n",
       "   '1861492603',\n",
       "   '2064675550']},\n",
       " {'id': '3003301247',\n",
       "  'title': 'A Style-Based Generator Architecture for Generative Adversarial Networks.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,077',\n",
       "  'abstract': 'We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Tero Karras', 'Samuli Laine', 'Timo Aila'],\n",
       "  'related_topics': ['Deep learning',\n",
       "   'Interpolation',\n",
       "   'Generator (mathematics)'],\n",
       "  'references': ['3109317361',\n",
       "   '3102761173',\n",
       "   '3118365541',\n",
       "   '3085152052',\n",
       "   '3040894825',\n",
       "   '3046884811',\n",
       "   '3132458488',\n",
       "   '3156252492',\n",
       "   '3162546695']},\n",
       " {'id': '2962974533',\n",
       "  'title': 'Semantic Image Synthesis With Spatially-Adaptive Normalization',\n",
       "  'reference_count': '35',\n",
       "  'citation_count': '727',\n",
       "  'abstract': 'We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Taesung Park 1, Ming-Yu Liu 2, Ting-Chun Wang 2, Jun-Yan Zhu 3'],\n",
       "  'related_topics': ['Normalization (statistics)',\n",
       "   'Normalization (image processing)',\n",
       "   'Affine transformation',\n",
       "   'Deep learning',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Image synthesis',\n",
       "   'Spatially adaptive'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2964121744',\n",
       "   '1836465849',\n",
       "   '2099471712',\n",
       "   '1959608418',\n",
       "   '2340897893',\n",
       "   '3003301247',\n",
       "   '2405756170',\n",
       "   '2963981733']},\n",
       " {'id': '2804078698',\n",
       "  'title': 'Self-Attention Generative Adversarial Networks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,558',\n",
       "  'abstract': 'In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Han Zhang 1, Ian J. Goodfellow 2, Dimitris N. Metaxas 3, Augustus Odena 2'],\n",
       "  'related_topics': ['Feature (computer vision)',\n",
       "   'Boosting (machine learning)',\n",
       "   'Generator (mathematics)',\n",
       "   'Normalization (image processing)',\n",
       "   'Visualization',\n",
       "   'Pattern recognition',\n",
       "   'Dependency (UML)',\n",
       "   'Image (mathematics)',\n",
       "   'Discriminator',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3003301247',\n",
       "   '2962974533',\n",
       "   '3035574324',\n",
       "   '2883583109',\n",
       "   '3034431451',\n",
       "   '2989855043',\n",
       "   '2990452356']},\n",
       " {'id': '3035574324',\n",
       "  'title': 'Analyzing and Improving the Image Quality of StyleGAN',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '604',\n",
       "  'abstract': 'The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Tero Karras 1, Samuli Laine 1, Miika Aittala 1, Janne Hellsten 1, Jaakko Lehtinen 2, Timo Aila 1'],\n",
       "  'related_topics': ['Image quality',\n",
       "   'Normalization (image processing)',\n",
       "   'Image resolution',\n",
       "   'Normalization (statistics)',\n",
       "   'Unsupervised learning',\n",
       "   'Data mining',\n",
       "   'Data visualization',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '1901129140',\n",
       "   '2117539524',\n",
       "   '1677182931',\n",
       "   '1533861849',\n",
       "   '2962879692',\n",
       "   '3003301247',\n",
       "   '2962760235',\n",
       "   '648143168']},\n",
       " {'id': '2982763192',\n",
       "  'title': 'Free-Form Image Inpainting With Gated Convolution',\n",
       "  'reference_count': '51',\n",
       "  'citation_count': '450',\n",
       "  'abstract': 'We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \\\\url{https://github.com/JiahuiYu/generative_inpainting}.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Jiahui Yu 1, Zhe Lin 2, Jimei Yang 2, Xiaohui Shen 3, Xin Lu 2, Thomas Huang 1'],\n",
       "  'related_topics': ['Inpainting',\n",
       "   'Convolution',\n",
       "   'Channel (digital image)',\n",
       "   'Pixel',\n",
       "   'Computer vision',\n",
       "   'Image (mathematics)',\n",
       "   'Code (cryptography)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Simple (abstract algebra)'],\n",
       "  'references': ['1901129140',\n",
       "   '2117539524',\n",
       "   '2963073614',\n",
       "   '2331128040',\n",
       "   '2963420686',\n",
       "   '2963420272',\n",
       "   '2519091744',\n",
       "   '2607333215',\n",
       "   '2962760235',\n",
       "   '2963800363']},\n",
       " {'id': '2890139949',\n",
       "  'title': 'Generative adversarial network in medical imaging: A review.',\n",
       "  'reference_count': '272',\n",
       "  'citation_count': '410',\n",
       "  'abstract': 'Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Xin Yi 1, Ekta Walia 1, 2, Paul S. Babyn 1'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Adversarial system',\n",
       "   'Deep learning',\n",
       "   'Test data generation',\n",
       "   'Consistency (database systems)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Medical imaging',\n",
       "   'Segmentation',\n",
       "   'Scheme (programming language)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '1836465849',\n",
       "   '1901129140',\n",
       "   '2099471712',\n",
       "   '2133665775',\n",
       "   '2963073614',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2963373786']},\n",
       " {'id': '2963841322',\n",
       "  'title': 'Video-to-Video Synthesis',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '435',\n",
       "  'abstract': 'We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without modeling temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generators and discriminators, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our method to future video prediction, outperforming several competing systems. Code, models, and more results are available at our website: https://github.com/NVIDIA/vid2vid. (Please use Adobe Reader to see the embedded videos in the paper.)',\n",
       "  'date': 2018,\n",
       "  'authors': ['Ting-Chun Wang 1, Ming-Yu Liu 1, Jun-Yan Zhu 2, Guilin Liu 3, Andrew Tao 1, Jan Kautz 1, Bryan Catanzaro 1'],\n",
       "  'related_topics': ['Segmentation',\n",
       "   'Computer vision',\n",
       "   'Code (cryptography)',\n",
       "   'Set (abstract data type)',\n",
       "   'Computer science',\n",
       "   'Image (mathematics)',\n",
       "   'Sequence',\n",
       "   'Function (mathematics)',\n",
       "   'Translation (geometry)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2962974533',\n",
       "   '2942074357',\n",
       "   '2964074081',\n",
       "   '2913399670',\n",
       "   '2908541468',\n",
       "   '3100398946',\n",
       "   '2970415880',\n",
       "   '2970315999',\n",
       "   '3048510980']},\n",
       " {'id': '2985068832',\n",
       "  'title': 'Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '184',\n",
       "  'abstract': 'We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHD dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Rameen Abdal', 'Yipeng Qin', 'Peter Wonka'],\n",
       "  'related_topics': ['Embedding',\n",
       "   'Image editing',\n",
       "   'Set (abstract data type)',\n",
       "   'Expression (mathematics)',\n",
       "   'Face (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Image (mathematics)',\n",
       "   'Image resolution',\n",
       "   'Artificial intelligence',\n",
       "   'Artificial neural network',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2099471712',\n",
       "   '2963073614',\n",
       "   '1959608418',\n",
       "   '2962793481',\n",
       "   '2963684088',\n",
       "   '2331128040',\n",
       "   '2962879692',\n",
       "   '2739748921',\n",
       "   '3003301247']},\n",
       " {'id': '2963840672',\n",
       "  'title': 'Multi-Scale Context Aggregation by Dilated Convolutions',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,848',\n",
       "  'abstract': 'Abstract: State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.',\n",
       "  'date': 2016,\n",
       "  'authors': ['Fisher Yu 1, Vladlen Koltun 2'],\n",
       "  'related_topics': ['Contextual image classification',\n",
       "   'Context (language use)',\n",
       "   'Segmentation',\n",
       "   'Pattern recognition',\n",
       "   'Aggregate (data warehouse)',\n",
       "   'Computer science',\n",
       "   'Scale (map)',\n",
       "   'Resolution (logic)',\n",
       "   'Adaptation (computer science)',\n",
       "   'Artificial intelligence',\n",
       "   'Contextual information'],\n",
       "  'references': ['2412782625',\n",
       "   '2963881378',\n",
       "   '2340897893',\n",
       "   '2560023338',\n",
       "   '2395611524',\n",
       "   '2326925005',\n",
       "   '2601564443']},\n",
       " {'id': '2131975293',\n",
       "  'title': 'Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '5,230',\n",
       "  'abstract': 'We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Matei Zaharia',\n",
       "   'Mosharaf Chowdhury',\n",
       "   'Tathagata Das',\n",
       "   'Ankur Dave',\n",
       "   'Justin Ma',\n",
       "   'Murphy McCauley',\n",
       "   'Michael J. Franklin',\n",
       "   'Scott Shenker',\n",
       "   'Ion Stoica'],\n",
       "  'related_topics': ['Distributed memory',\n",
       "   'Shared memory',\n",
       "   'Computer cluster',\n",
       "   'Fault tolerance',\n",
       "   'Programming paradigm',\n",
       "   'Spark (mathematics)',\n",
       "   'Abstraction (linguistics)',\n",
       "   'Distributed computing',\n",
       "   'State (computer science)',\n",
       "   'Computer science'],\n",
       "  'references': ['2173213060',\n",
       "   '1554944419',\n",
       "   '3013264884',\n",
       "   '2170616854',\n",
       "   '2100830825',\n",
       "   '2098935637',\n",
       "   '2096125134',\n",
       "   '2163961697',\n",
       "   '2060204338',\n",
       "   '2109722477']},\n",
       " {'id': '2125389028',\n",
       "  'title': 'Conditional Generative Adversarial Nets',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '7,210',\n",
       "  'abstract': 'Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Mehdi Mirza', 'Simon Osindero'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Generative Design',\n",
       "   'MNIST database',\n",
       "   'Generative grammar',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Generator (mathematics)',\n",
       "   'Class (computer programming)',\n",
       "   'Computer science',\n",
       "   'Image translation',\n",
       "   'Image (mathematics)'],\n",
       "  'references': ['2097117768',\n",
       "   '2099471712',\n",
       "   '1614298861',\n",
       "   '1904365287',\n",
       "   '2546302380',\n",
       "   '2294059674',\n",
       "   '2123024445',\n",
       "   '2951446714',\n",
       "   '154472438',\n",
       "   '1496559305']},\n",
       " {'id': '2962897886',\n",
       "  'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '3,211',\n",
       "  'abstract': 'We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation - rules for gradient backpropagation through stochastic variables - and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Danilo Jimenez Rezende', 'Shakir Mohamed', 'Daan Wierstra'],\n",
       "  'related_topics': ['Approximate inference',\n",
       "   'Bayesian inference',\n",
       "   'Backpropagation',\n",
       "   'Inference',\n",
       "   'Missing data',\n",
       "   'Posterior probability',\n",
       "   'Algorithm',\n",
       "   'Upper and lower bounds',\n",
       "   'Computer science'],\n",
       "  'references': ['1959608418',\n",
       "   '2145094598',\n",
       "   '2335728318',\n",
       "   '2166851633',\n",
       "   '2951446714',\n",
       "   '2044758663',\n",
       "   '2108677974',\n",
       "   '2097268041',\n",
       "   '2963173382',\n",
       "   '2167433878']},\n",
       " {'id': '2136504847',\n",
       "  'title': 'Semi-Supervised Learning Literature Survey',\n",
       "  'reference_count': '157',\n",
       "  'citation_count': '4,491',\n",
       "  'abstract': '',\n",
       "  'date': 2004,\n",
       "  'authors': ['Xiaojin Zhu'],\n",
       "  'related_topics': ['Literature survey',\n",
       "   'Semi-supervised learning',\n",
       "   'Co-training',\n",
       "   'Technical report',\n",
       "   'Computer science',\n",
       "   'Natural language processing',\n",
       "   'Artificial intelligence',\n",
       "   'Unlabelled data'],\n",
       "  'references': ['1880262756',\n",
       "   '2148603752',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '2001141328',\n",
       "   '2125838338',\n",
       "   '2165874743',\n",
       "   '2097308346',\n",
       "   '2114524997',\n",
       "   '1479807131']},\n",
       " {'id': '1676820704',\n",
       "  'title': 'Solving multiclass learning problems via error-correcting output codes',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '3,580',\n",
       "  'abstract': 'Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Thomas G. Dietterich 1, Ghulum Bakiri 2'],\n",
       "  'related_topics': ['Multiclass classification',\n",
       "   'Multi-task learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Overfitting',\n",
       "   'Pruning (decision trees)',\n",
       "   'Generalization',\n",
       "   'Backpropagation',\n",
       "   'Concept learning',\n",
       "   'Theoretical computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '1594031697',\n",
       "   '2147800946',\n",
       "   '2173629880',\n",
       "   '2019363670',\n",
       "   '2093717447',\n",
       "   '3036751298',\n",
       "   '2176028050',\n",
       "   '1667614912',\n",
       "   '1980501707']},\n",
       " {'id': '2407712691',\n",
       "  'title': 'Deep Learning via Semi-Supervised Embedding',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '1,027',\n",
       "  'abstract': 'We show how nonlinear embedding algorithms popular for use with \"shallow\" semi-supervised learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This trick provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Jason Weston',\n",
       "   'Frédéric Ratle',\n",
       "   'Hossein Mobahi',\n",
       "   'Ronan Collobert'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Kernel method',\n",
       "   'Embedding',\n",
       "   'Deep learning',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear embedding'],\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2001141328',\n",
       "   '2097308346',\n",
       "   '1479807131',\n",
       "   '2139427956',\n",
       "   '2914746235',\n",
       "   '2159291644',\n",
       "   '2145038566',\n",
       "   '2148029428']},\n",
       " {'id': '2158049734',\n",
       "  'title': 'Semi-Supervised Learning for Natural Language',\n",
       "  'reference_count': '74',\n",
       "  'citation_count': '408',\n",
       "  'abstract': '',\n",
       "  'date': 2004,\n",
       "  'authors': ['Percy Liang'],\n",
       "  'related_topics': ['Informatics engineering',\n",
       "   'Semi-supervised learning',\n",
       "   'Applied science',\n",
       "   'Natural language',\n",
       "   'Software engineering',\n",
       "   'Computer science',\n",
       "   'Mechanical engineering'],\n",
       "  'references': ['2147880316',\n",
       "   '2139212933',\n",
       "   '2121947440',\n",
       "   '2048679005',\n",
       "   '2008652694',\n",
       "   '2097089247',\n",
       "   '2138745909',\n",
       "   '2156515921',\n",
       "   '2107008379',\n",
       "   '2144578941']},\n",
       " {'id': '2122457239',\n",
       "  'title': 'Semi-Supervised Learning in Gigantic Image Collections',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '319',\n",
       "  'abstract': 'With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. \"Clean labels\" can be manually obtained on a small fraction, \"noisy labels\" may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images. Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images gathered from the Internet.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Rob Fergus 1, Yair Weiss 2, Antonio Torralba 3'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Laplacian matrix',\n",
       "   'Pattern recognition'],\n",
       "  'references': ['3118608800',\n",
       "   '2110764733',\n",
       "   '2145607950',\n",
       "   '1566135517',\n",
       "   '2293597654',\n",
       "   '1560724230',\n",
       "   '1479807131',\n",
       "   '2111993661',\n",
       "   '2154455818',\n",
       "   '2104290444']},\n",
       " {'id': '2963207607',\n",
       "  'title': 'Explaining and Harnessing Adversarial Examples',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '7,585',\n",
       "  'abstract': \"Abstract: Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.\",\n",
       "  'date': 2014,\n",
       "  'authors': ['Ian J. Goodfellow', 'Jonathon Shlens', 'Christian Szegedy'],\n",
       "  'related_topics': ['Adversarial machine learning',\n",
       "   'Overfitting',\n",
       "   'MNIST database',\n",
       "   'Test set',\n",
       "   'Artificial neural network',\n",
       "   'Adversarial system',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Backdoor',\n",
       "   'Nonlinear system',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2963857521',\n",
       "   '2964253222',\n",
       "   '2604763608',\n",
       "   '3102564565',\n",
       "   '2180612164',\n",
       "   '2243397390',\n",
       "   '2963143631',\n",
       "   '2964082701']},\n",
       " {'id': '2963382180',\n",
       "  'title': 'Striving for Simplicity: The All Convolutional Net',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,234',\n",
       "  'abstract': 'Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the \"deconvolution approach\" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Jost Tobias Springenberg',\n",
       "   'Alexey Dosovitskiy',\n",
       "   'Thomas Brox',\n",
       "   'Martin A. Riedmiller'],\n",
       "  'related_topics': ['Convolutional neural network',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pipeline (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Convolution',\n",
       "   'Deconvolution',\n",
       "   'Layer (object-oriented design)',\n",
       "   'Computer science',\n",
       "   'Range (mathematics)',\n",
       "   'State (computer science)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1026270304',\n",
       "   '3102564565',\n",
       "   '2963399829',\n",
       "   '2963685250',\n",
       "   '2963564844',\n",
       "   '2964159205']},\n",
       " {'id': '1479807131',\n",
       "  'title': 'Semi-Supervised Learning',\n",
       "  'reference_count': '374',\n",
       "  'citation_count': '5,483',\n",
       "  'abstract': 'In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series',\n",
       "  'date': 2010,\n",
       "  'authors': ['Olivier Chapelle', 'Bernhard Schlkopf', 'Alexander Zien'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'Transduction (machine learning)',\n",
       "   'Co-training',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Generative grammar',\n",
       "   'Computation',\n",
       "   'Artificial intelligence',\n",
       "   'Manifold regularization'],\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2158714788',\n",
       "   '2055043387',\n",
       "   '1480376833',\n",
       "   '2119821739',\n",
       "   '2053186076',\n",
       "   '2121947440',\n",
       "   '3124955340',\n",
       "   '2912934387']},\n",
       " {'id': '2112076978',\n",
       "  'title': 'Experiments with a new boosting algorithm',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '10,889',\n",
       "  'abstract': 'In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman\\'s \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Yoav Freund', 'Robert E. Schapire'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'Gradient boosting',\n",
       "   'AdaBoost',\n",
       "   'LPBoost',\n",
       "   'LogitBoost',\n",
       "   'Ensembles of classifiers',\n",
       "   'Stability (learning theory)',\n",
       "   'Cascading classifiers',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Boosting methods for object categorization',\n",
       "   'Generalization error'],\n",
       "  'references': ['3124955340',\n",
       "   '2912934387',\n",
       "   '2125055259',\n",
       "   '1504694836',\n",
       "   '1670263352',\n",
       "   '1966280301',\n",
       "   '2093717447',\n",
       "   '2132166479',\n",
       "   '2070534370',\n",
       "   '2137291015']},\n",
       " {'id': '1975846642',\n",
       "  'title': 'Boosting the margin: a new explanation for the effectiveness of voting methods',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '3,542',\n",
       "  'abstract': \"One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.\",\n",
       "  'date': 1998,\n",
       "  'authors': ['Robert E. Schapire 1, Yoav Freund 1, Peter Bartlett 2, Wee Sun Lee 3'],\n",
       "  'related_topics': ['Margin classifier',\n",
       "   'BrownBoost',\n",
       "   'LPBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'Support vector machine',\n",
       "   'Voting',\n",
       "   'Classification rule',\n",
       "   'LogitBoost',\n",
       "   'Algorithm',\n",
       "   'Statistics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2156909104',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '2112076978',\n",
       "   '1594031697',\n",
       "   '2087347434',\n",
       "   '1605688901',\n",
       "   '2032210760',\n",
       "   '2982720039']},\n",
       " {'id': '2152761983',\n",
       "  'title': 'An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '2,325',\n",
       "  'abstract': 'Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only “hard” areas but also outliers and noise.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Eric Bauer 1, Ron Kohavi 2'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'AdaBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'LPBoost',\n",
       "   'Ensembles of classifiers',\n",
       "   'Mean squared error',\n",
       "   'Statistical classification',\n",
       "   'Decision tree',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1995945562',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '2084812512',\n",
       "   '2125055259',\n",
       "   '2112076978',\n",
       "   '1975846642',\n",
       "   '1680392829',\n",
       "   '2140785063',\n",
       "   '3017143921']},\n",
       " {'id': '2113242816',\n",
       "  'title': 'The random subspace method for constructing decision forests',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '6,236',\n",
       "  'abstract': \"Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.\",\n",
       "  'date': 1998,\n",
       "  'authors': ['Tin Kam Ho'],\n",
       "  'related_topics': ['Random forest',\n",
       "   'Random subspace method',\n",
       "   'Decision tree',\n",
       "   'Overfitting',\n",
       "   'Ensembles of classifiers',\n",
       "   'Binary tree',\n",
       "   'Support vector machine',\n",
       "   'Feature vector',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2912934387',\n",
       "   '2125055259',\n",
       "   '2112076978',\n",
       "   '1594031697',\n",
       "   '2149706766',\n",
       "   '2101522199',\n",
       "   '1966280301',\n",
       "   '2102734279',\n",
       "   '1930624869']},\n",
       " {'id': '1605688901',\n",
       "  'title': 'An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '3,140',\n",
       "  'abstract': 'Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Thomas G. Dietterich'],\n",
       "  'related_topics': ['Gradient boosting',\n",
       "   'BrownBoost',\n",
       "   'Ensembles of classifiers',\n",
       "   'Boosting (machine learning)',\n",
       "   'Random forest',\n",
       "   'LPBoost',\n",
       "   'Ensemble learning',\n",
       "   'Decision tree',\n",
       "   'Machine learning',\n",
       "   'Data mining',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2912934387',\n",
       "   '2112076978',\n",
       "   '2152761983',\n",
       "   '2982720039',\n",
       "   '1966280301',\n",
       "   '2167277498',\n",
       "   '2073738917',\n",
       "   '2976840617',\n",
       "   '1562197959',\n",
       "   '1850527962']},\n",
       " {'id': '2120240539',\n",
       "  'title': 'Shape quantization and recognition with randomized trees',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '1,371',\n",
       "  'abstract': 'We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred L AT E X symbols. Stateof-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on L AT E X symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Yali Amit 1, Donald Geman 2'],\n",
       "  'related_topics': ['Feature vector', 'Tree-depth', 'Decision tree'],\n",
       "  'references': ['2099111195',\n",
       "   '2912934387',\n",
       "   '1594031697',\n",
       "   '2149706766',\n",
       "   '2101522199',\n",
       "   '1676820704',\n",
       "   '2165758113',\n",
       "   '2154579312',\n",
       "   '2076118331',\n",
       "   '2168228682']},\n",
       " {'id': '2099968818',\n",
       "  'title': 'Boosting in the limit: maximizing the margin of learned ensembles',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '365',\n",
       "  'abstract': 'The \"minimum margin\" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \"LPboosting\" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Adam J. Grove', 'Dale Schuurmans'],\n",
       "  'related_topics': ['BrownBoost',\n",
       "   'LPBoost',\n",
       "   'Boosting (machine learning)',\n",
       "   'AdaBoost',\n",
       "   'Linear programming',\n",
       "   'Machine learning',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Classifier (UML)',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization error',\n",
       "   'Training set'],\n",
       "  'references': ['2119821739',\n",
       "   '3124955340',\n",
       "   '2125055259',\n",
       "   '2112076978',\n",
       "   '2152761983',\n",
       "   '1504694836',\n",
       "   '2982720039',\n",
       "   '1966280301',\n",
       "   '2266946488',\n",
       "   '1553313034']},\n",
       " {'id': '2067885219',\n",
       "  'title': 'Arcing classifier (with discussion and a rejoinder by the author)',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,939',\n",
       "  'abstract': 'Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym “arcing”) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'related_topics': ['Test set',\n",
       "   'Boosting (machine learning)',\n",
       "   'Weighted voting',\n",
       "   'BrownBoost',\n",
       "   'Resampling',\n",
       "   'LPBoost',\n",
       "   'Ensemble learning',\n",
       "   'Artificial neural network',\n",
       "   'Pattern recognition',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2911964244',\n",
       "   '2053463056',\n",
       "   '2032210760',\n",
       "   '1966701961',\n",
       "   '3104887532',\n",
       "   '2075647286',\n",
       "   '2167917621',\n",
       "   '1540007258',\n",
       "   '2155806188',\n",
       "   '2168020168']},\n",
       " {'id': '1580948147',\n",
       "  'title': 'Randomizing Outputs to Increase Prediction Accuracy',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '301',\n",
       "  'abstract': 'Bagging and boosting reduce error by changing both the inputs and outputs to form perturbed training sets, growing predictors on these perturbed training sets and combining them. An interesting question is whether it is possible to get comparable performance by perturbing the outputs alone. Two methods of randomizing outputs are experimented with. One is called output smearing and the other output flipping. Both are shown to consistently do better than bagging.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Leo Breiman'],\n",
       "  'related_topics': ['Boosting (machine learning)',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3124955340',\n",
       "   '2912934387',\n",
       "   '2112076978',\n",
       "   '1594031697',\n",
       "   '1605688901',\n",
       "   '2102201073',\n",
       "   '2067885219',\n",
       "   '2076118331',\n",
       "   '2172195373',\n",
       "   '2073738917']},\n",
       " {'id': '2145889472',\n",
       "  'title': 'Emergence of simple-cell receptive field properties by learning a sparse code for natural images',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '6,557',\n",
       "  'abstract': 'The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Bruno A. Olshausen 1, 2, David J. Field 2'],\n",
       "  'related_topics': ['Efficient coding hypothesis',\n",
       "   'Simple cell',\n",
       "   'Sparse image',\n",
       "   'Unsupervised learning',\n",
       "   'Receptive field',\n",
       "   'Visual cortex',\n",
       "   'Wavelet transform',\n",
       "   'Representation (mathematics)',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2108384452',\n",
       "   '1993845689',\n",
       "   '2180838288',\n",
       "   '2167034998',\n",
       "   '2120838001',\n",
       "   '2122925692',\n",
       "   '1914401667',\n",
       "   '2106884367',\n",
       "   '2911607583',\n",
       "   '2117731089']},\n",
       " {'id': '2122922389',\n",
       "  'title': 'Self-taught learning: transfer learning from unlabeled data',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '1,812',\n",
       "  'abstract': 'We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.',\n",
       "  'date': 2007,\n",
       "  'authors': ['Rajat Raina',\n",
       "   'Alexis Battle',\n",
       "   'Honglak Lee',\n",
       "   'Benjamin Packer',\n",
       "   'Andrew Y. Ng'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Unsupervised learning',\n",
       "   'Multi-task learning',\n",
       "   'Active learning (machine learning)',\n",
       "   'Instance-based learning',\n",
       "   'Online machine learning',\n",
       "   'Stability (learning theory)',\n",
       "   'Inductive transfer',\n",
       "   'Transfer of learning',\n",
       "   'Fisher kernel',\n",
       "   'Support vector machine',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Generalization error'],\n",
       "  'references': ['1880262756',\n",
       "   '2100495367',\n",
       "   '2162915993',\n",
       "   '2135046866',\n",
       "   '2053186076',\n",
       "   '2001141328',\n",
       "   '2063978378',\n",
       "   '2166049352',\n",
       "   '2147152072',\n",
       "   '2113606819']},\n",
       " {'id': '2139427956',\n",
       "  'title': 'Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '1,313',\n",
       "  'abstract': 'We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.',\n",
       "  'date': 2007,\n",
       "  'authors': ['M.A. Ranzato', 'Fu Jie Huang', 'Y.-L. Boureau', 'Yann LeCun'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Supervised learning',\n",
       "   'MNIST database',\n",
       "   'Feature extraction',\n",
       "   'Caltech 101',\n",
       "   'Convolutional Deep Belief Networks',\n",
       "   'Invariant (mathematics)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Object detection',\n",
       "   'Sigmoid function',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2136922672',\n",
       "   '2310919327',\n",
       "   '2162915993',\n",
       "   '2110798204',\n",
       "   '2166049352',\n",
       "   '1624854622',\n",
       "   '2172174689',\n",
       "   '2168002178',\n",
       "   '2105464873']},\n",
       " {'id': '2118020653',\n",
       "  'title': 'Machine learning in automated text categorization',\n",
       "  'reference_count': '165',\n",
       "  'citation_count': '10,897',\n",
       "  'abstract': 'The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Fabrizio Sebastiani'],\n",
       "  'related_topics': ['Multi-task learning',\n",
       "   'Categorization',\n",
       "   'Document classification'],\n",
       "  'references': ['1574901103',\n",
       "   '2149684865',\n",
       "   '2147152072',\n",
       "   '2435251607',\n",
       "   '2097089247',\n",
       "   '2053463056',\n",
       "   '2114535528',\n",
       "   '2005422315',\n",
       "   '2140785063',\n",
       "   '1978394996']},\n",
       " {'id': '2164019165',\n",
       "  'title': 'Improving Word Representations via Global Context and Multiple Word Prototypes',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '1,370',\n",
       "  'abstract': 'Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Eric Huang',\n",
       "   'Richard Socher',\n",
       "   'Christopher Manning',\n",
       "   'Andrew Ng'],\n",
       "  'related_topics': ['Word lists by frequency',\n",
       "   'Word (computer architecture)',\n",
       "   'Language model',\n",
       "   'Polysemy',\n",
       "   'Context (language use)',\n",
       "   'Semantics',\n",
       "   'Natural language processing',\n",
       "   'Homonym',\n",
       "   'Computer science',\n",
       "   'Representation (mathematics)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1532325895',\n",
       "   '2117130368',\n",
       "   '2132339004',\n",
       "   '2118020653',\n",
       "   '2158139315',\n",
       "   '1423339008',\n",
       "   '71795751',\n",
       "   '2081580037',\n",
       "   '1970381522',\n",
       "   '2131462252']},\n",
       " {'id': '1544827683',\n",
       "  'title': 'Teaching machines to read and comprehend',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '2,016',\n",
       "  'abstract': 'Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Karl Moritz Hermann 1, Tomáš Kočiský 2, Edward Grefenstette 1, Lasse Espeholt 1, Will Kay 1, Mustafa Suleyman 1, Phil Blunsom 2'],\n",
       "  'related_topics': ['Reading comprehension',\n",
       "   'Natural language',\n",
       "   'Class (computer programming)',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Test (assessment)',\n",
       "   'Scale (chemistry)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964308564',\n",
       "   '2130942839',\n",
       "   '2064675550',\n",
       "   '2158899491',\n",
       "   '2120615054',\n",
       "   '1793121960',\n",
       "   '2147527908',\n",
       "   '2962741254',\n",
       "   '2144499799',\n",
       "   '2125436846']},\n",
       " {'id': '2125436846',\n",
       "  'title': 'MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '537',\n",
       "  'abstract': 'We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text. 1 Reading Comprehension A major goal for NLP is for machines to be able to understand text as well as people. Several research disciplines are focused on this problem: for example, information extraction, relation extraction, semantic role labeling, and recognizing textual entailment. Yet these techniques are necessarily evaluated individually, rather than by how much they advance us towards the end goal. On the other hand, the goal of semantic parsing is the machine comprehension of text (MCT), yet its evaluation requires adherence to a specific knowledge representation, and it is currently unclear what the best representation is, for open-domain text. We believe that it is useful to directly tackle the top-level task of MCT. For this, we need a way to measure progress. One common method for evaluating someone’s understanding of text is by giving them a multiple-choice reading comprehension test. This has the advantage that it is objectively gradable (vs. essays) yet may test a range of abilities such as causal or counterfactual reasoning, inference among relations, or just basic understanding of the world in which the passage is set. Therefore, we propose a multiple-choice reading comprehension task as a way to evaluate progress on MCT. We have built a reading comprehension dataset containing 500 fictional stories, with 4 multiple choice questions per story. It was built using methods which can easily scale to at least 5000 stories, since the stories were created, and the curation was done, using crowd sourcing almost entirely, at a total of $4.00 per story. We plan to periodically update the dataset to ensure that methods are not overfitting to the existing data. The dataset is open-domain, yet restricted to concepts and words that a 7 year old is expected to understand. This task is still beyond the capability of today’s computers and algorithms.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Matthew Richardson', 'Christopher J.C. Burges', 'Erin Renshaw'],\n",
       "  'related_topics': ['Reading comprehension',\n",
       "   'Comprehension',\n",
       "   'Relationship extraction',\n",
       "   'Textual entailment',\n",
       "   'Semantic role labeling',\n",
       "   'Information extraction',\n",
       "   'Multiple choice',\n",
       "   'Causal reasoning',\n",
       "   'Parsing',\n",
       "   'Grammar',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3122078363',\n",
       "   '2525127255',\n",
       "   '2126631960',\n",
       "   '2167090521',\n",
       "   '3126123353',\n",
       "   '1979532929',\n",
       "   '2989499211',\n",
       "   '2096979215',\n",
       "   '2097550833',\n",
       "   '2142898321']},\n",
       " {'id': '2964267515',\n",
       "  'title': \"The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations\",\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '445',\n",
       "  'abstract': \"Abstract: We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.\",\n",
       "  'date': 2015,\n",
       "  'authors': ['Felix Hill', 'Antoine Bordes', 'Sumit Chopra', 'Jason Weston'],\n",
       "  'related_topics': ['Language model',\n",
       "   'Reading (process)',\n",
       "   'Encoding (memory)',\n",
       "   'Goldilocks principle',\n",
       "   'Explicit memory',\n",
       "   'Generality',\n",
       "   'Natural language processing',\n",
       "   'Function (engineering)',\n",
       "   'Standard language',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2963748441',\n",
       "   '2551396370',\n",
       "   '2963339397',\n",
       "   '2962809918',\n",
       "   '2740747242',\n",
       "   '2964223283',\n",
       "   '2962985038']},\n",
       " {'id': '2962809918',\n",
       "  'title': 'A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '450',\n",
       "  'abstract': 'Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1',\n",
       "  'date': 2015,\n",
       "  'authors': ['Danqi Chen 1, Jason Bolton 2, Christopher D. Manning 2'],\n",
       "  'related_topics': ['Task (project management)',\n",
       "   'Reading comprehension',\n",
       "   'Natural language processing'],\n",
       "  'references': ['2250539671',\n",
       "   '1902237438',\n",
       "   '1544827683',\n",
       "   '1793121960',\n",
       "   '2250861254',\n",
       "   '2125436846',\n",
       "   '2964267515',\n",
       "   '2584341106',\n",
       "   '2964091467',\n",
       "   '2962790689']},\n",
       " {'id': '2171278097',\n",
       "  'title': 'Building Watson: An Overview of the DeepQA Project',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '1,770',\n",
       "  'abstract': 'IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.',\n",
       "  'date': 2010,\n",
       "  'authors': ['David A. Ferrucci 1, Eric W. Brown 1, Jennifer Chu-Carroll 1, James Fan 1, David Gondek 1, Aditya Kalyanpur 1, Adam Lally 1, J. William Murdock 1, Eric Nyberg 2, John M. Prager 1, Nico Schlaefer 2, Christopher A. Welty 1'],\n",
       "  'related_topics': ['Watson',\n",
       "   'IBM',\n",
       "   'Champion',\n",
       "   'Architecture',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Field (computer science)',\n",
       "   'Operations research',\n",
       "   'Extensible architecture'],\n",
       "  'references': ['2081580037',\n",
       "   '2047221353',\n",
       "   '2096797897',\n",
       "   '2150884987',\n",
       "   '2087064593',\n",
       "   '2988119488',\n",
       "   '2107658650',\n",
       "   '2158823144',\n",
       "   '2122537498',\n",
       "   '2080278171']},\n",
       " {'id': '2962790689',\n",
       "  'title': 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '661',\n",
       "  'abstract': 'Abstract: One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Jason Weston',\n",
       "   'Antoine Bordes',\n",
       "   'Sumit Chopra',\n",
       "   'Alexander M. Rush',\n",
       "   'Bart van Merriënboer',\n",
       "   'Armand Joulin',\n",
       "   'Tomas Mikolov'],\n",
       "  'related_topics': ['Question answering',\n",
       "   'Set (psychology)',\n",
       "   'AI-complete',\n",
       "   'Natural language',\n",
       "   'Chaining',\n",
       "   'Reading comprehension',\n",
       "   'Human–computer interaction',\n",
       "   'Computer science',\n",
       "   'Simple (philosophy)',\n",
       "   'Measure (data warehouse)'],\n",
       "  'references': ['1933349210',\n",
       "   '2962809918',\n",
       "   '2561715562',\n",
       "   '2964091467',\n",
       "   '2768661419',\n",
       "   '2964223283',\n",
       "   '2963448850']},\n",
       " {'id': '2251818205',\n",
       "  'title': 'WikiQA: A Challenge Dataset for Open-Domain Question Answering',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '564',\n",
       "  'abstract': 'We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Yi Yang 1, Wen-tau Yih 2, Christopher Meek 2'],\n",
       "  'related_topics': ['Sentence',\n",
       "   'Question answering',\n",
       "   'Selection (linguistics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Information retrieval',\n",
       "   'Matching (statistics)',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Component (UML)',\n",
       "   'Process (engineering)'],\n",
       "  'references': ['2153579005',\n",
       "   '2131744502',\n",
       "   '2070246124',\n",
       "   '2118091490',\n",
       "   '1591825359',\n",
       "   '1514986335',\n",
       "   '2125313055',\n",
       "   '2989499211',\n",
       "   '2120735855',\n",
       "   '2251921768']},\n",
       " {'id': '2251349042',\n",
       "  'title': 'Learning to Automatically Solve Algebra Word Problems',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '230',\n",
       "  'abstract': 'We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.',\n",
       "  'date': 2014,\n",
       "  'authors': ['Nate Kushman 1, Yoav Artzi 2, Luke Zettlemoyer 2, Regina Barzilay 1'],\n",
       "  'related_topics': ['Word problem (mathematics education)',\n",
       "   'System of linear equations',\n",
       "   'Task (project management)'],\n",
       "  'references': ['2252136820',\n",
       "   '1508977358',\n",
       "   '2163561827',\n",
       "   '2123661878',\n",
       "   '1496189301',\n",
       "   '2189089430',\n",
       "   '2251673953',\n",
       "   '2118781169',\n",
       "   '1923162067',\n",
       "   '1559723967']},\n",
       " {'id': '2169415915',\n",
       "  'title': 'Constructing free-energy approximations and generalized belief propagation algorithms',\n",
       "  'reference_count': '56',\n",
       "  'citation_count': '1,756',\n",
       "  'abstract': 'Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a \"valid\" or \"maxent-normal\" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the \"Bethe method\", the \"junction graph method\", the \"cluster variation method\", and the \"region graph method\". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.',\n",
       "  'date': 2005,\n",
       "  'authors': ['J.S. Yedidia 1, W.T. Freeman 2, Y. Weiss 3'],\n",
       "  'related_topics': ['Belief propagation',\n",
       "   'Approximation algorithm',\n",
       "   'Factor graph',\n",
       "   'Graph (abstract data type)',\n",
       "   'Graph theory',\n",
       "   'Stationary point',\n",
       "   'Fixed point',\n",
       "   'Coding theory',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2099111195',\n",
       "   '2125838338',\n",
       "   '2798766386',\n",
       "   '2137813581',\n",
       "   '2159080219',\n",
       "   '2121606987',\n",
       "   '2987657883',\n",
       "   '1516111018',\n",
       "   '1530042113',\n",
       "   '1746680969']},\n",
       " {'id': '2158164339',\n",
       "  'title': 'Modeling Human Motion Using Binary Latent Variables',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '887',\n",
       "  'abstract': 'We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued \"visible\" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Graham W. Taylor', 'Geoffrey E. Hinton', 'Sam T. Roweis'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Latent class model',\n",
       "   'Generative model',\n",
       "   'Motion capture',\n",
       "   'Motion (physics)',\n",
       "   'Inference',\n",
       "   'Set (abstract data type)',\n",
       "   'Algorithm',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2136922672',\n",
       "   '2310919327',\n",
       "   '2116064496',\n",
       "   '2124914669',\n",
       "   '2293741035',\n",
       "   '2114153178',\n",
       "   '2147010501',\n",
       "   '2248685949',\n",
       "   '1991942383',\n",
       "   '2123236823']},\n",
       " {'id': '66838807',\n",
       "  'title': 'On Contrastive Divergence Learning.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '803',\n",
       "  'abstract': '',\n",
       "  'date': 2004,\n",
       "  'authors': ['Miguel Á. Carreira-Perpiñán', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Computer science',\n",
       "   'Linguistics',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Contrastive divergence'],\n",
       "  'references': ['2116064496',\n",
       "   '2130416410',\n",
       "   '2157444450',\n",
       "   '1651266332',\n",
       "   '2095844239',\n",
       "   '1802356529',\n",
       "   '1568229137',\n",
       "   '1813659000',\n",
       "   '2482531687',\n",
       "   '2130313186']},\n",
       " {'id': '2064630666',\n",
       "  'title': 'Representational power of restricted boltzmann machines and deep belief networks',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '788',\n",
       "  'abstract': 'Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Nicolas Le Roux', 'Yoshua Bengio'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'Deep belief network',\n",
       "   'Boltzmann machine',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Statistical model',\n",
       "   'Inference',\n",
       "   'Artificial intelligence',\n",
       "   'Block (data storage)',\n",
       "   'Mathematics'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2110798204',\n",
       "   '2137983211',\n",
       "   '3146803896',\n",
       "   '2172174689',\n",
       "   '2613634265',\n",
       "   '2124914669',\n",
       "   '205159212']},\n",
       " {'id': '1513873506',\n",
       "  'title': 'Annealed importance sampling',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '1,306',\n",
       "  'abstract': 'Simulated annealing—moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions—has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Radford M. Neal'],\n",
       "  'related_topics': ['Slice sampling',\n",
       "   'Importance sampling',\n",
       "   'Markov chain Monte Carlo',\n",
       "   'Markov chain',\n",
       "   'Umbrella sampling',\n",
       "   'Autocorrelation',\n",
       "   'Sequence',\n",
       "   'Thermodynamic integration',\n",
       "   'Statistical physics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2581275558',\n",
       "   '2130416410',\n",
       "   '1567512734',\n",
       "   '2149801992',\n",
       "   '2615953416',\n",
       "   '2057565703',\n",
       "   '2013164703',\n",
       "   '1565709818',\n",
       "   '2138309709',\n",
       "   '2033057584']},\n",
       " {'id': '2135094946',\n",
       "  'title': 'A new class of upper bounds on the log partition function',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '584',\n",
       "  'abstract': 'We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants.',\n",
       "  'date': 2005,\n",
       "  'authors': ['M.J. Wainwright 1, T.S. Jaakkola 2, A.S. Willsky 2'],\n",
       "  'related_topics': ['Partition function (quantum field theory)',\n",
       "   'Upper and lower bounds',\n",
       "   'Fixed point',\n",
       "   'Local optimum',\n",
       "   'Variational method',\n",
       "   'Belief propagation',\n",
       "   'Factor graph',\n",
       "   'Markov random field',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2798766386',\n",
       "   '2137813581',\n",
       "   '2004308928',\n",
       "   '1516111018',\n",
       "   '2169415915',\n",
       "   '1530042113',\n",
       "   '2914659449',\n",
       "   '2019599312',\n",
       "   '1515272691',\n",
       "   '1513861746']},\n",
       " {'id': '2102381086',\n",
       "  'title': 'Introduction to WordNet: An On-line Lexical Database',\n",
       "  'reference_count': '81',\n",
       "  'citation_count': '6,807',\n",
       "  'abstract': 'Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.',\n",
       "  'date': 1990,\n",
       "  'authors': ['George A. Miller 1, Richard Beckwith 1, Christiane Fellbaum 1, Derek Gross 2, Katherine J. Miller 1'],\n",
       "  'related_topics': ['Lexical database',\n",
       "   'WordNet',\n",
       "   'eXtended WordNet',\n",
       "   'EuroWordNet',\n",
       "   'Train of thought',\n",
       "   'Natural language processing',\n",
       "   'Simple (philosophy)',\n",
       "   'Computer science',\n",
       "   'Interrupt',\n",
       "   'Word (computer architecture)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1933657216',\n",
       "   '2103318667',\n",
       "   '2090626368',\n",
       "   '1483126227',\n",
       "   '2017668967',\n",
       "   '2123987305',\n",
       "   '2040300040',\n",
       "   '2013596317',\n",
       "   '2052262800',\n",
       "   '2059799772']},\n",
       " {'id': '2103318667',\n",
       "  'title': 'Contextual correlates of semantic similarity',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '2,030',\n",
       "  'abstract': 'Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.',\n",
       "  'date': 1990,\n",
       "  'authors': ['George A. Miller 1, Walter G. Charles 2'],\n",
       "  'related_topics': ['Semantic similarity',\n",
       "   'Similarity (psychology)',\n",
       "   'Contextual Associations',\n",
       "   'Semantics',\n",
       "   'Sentence',\n",
       "   'Noun',\n",
       "   'Syntax',\n",
       "   'Pragmatics',\n",
       "   'Natural language processing',\n",
       "   'Linguistics',\n",
       "   'Psychology',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1483126227',\n",
       "   '1971220772',\n",
       "   '2017580301',\n",
       "   '2114826854',\n",
       "   '13823885',\n",
       "   '2109334311',\n",
       "   '2064332540',\n",
       "   '1536719366',\n",
       "   '1634667895',\n",
       "   '2020159140']},\n",
       " {'id': '2065157922',\n",
       "  'title': 'A semantic concordance',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '795',\n",
       "  'abstract': 'A semantic concordance is a textual corpus and a lexicon so combined that every substantive word in the text is linked to its appropriate sense in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semantically, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facilitate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances am proposed.',\n",
       "  'date': 1993,\n",
       "  'authors': ['George A. Miller',\n",
       "   'Claudia Leacock',\n",
       "   'Randee Tengi',\n",
       "   'Ross T. Bunker'],\n",
       "  'related_topics': ['Explicit semantic analysis',\n",
       "   'Semantic similarity',\n",
       "   'Semantic compression',\n",
       "   'WordNet',\n",
       "   'Semantic property',\n",
       "   'Lexicon',\n",
       "   'Brown Corpus',\n",
       "   'Semantic HTML',\n",
       "   'Natural language processing',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2102381086',\n",
       "   '2081687495',\n",
       "   '1483126227',\n",
       "   '2017668967',\n",
       "   '2007780422',\n",
       "   '2012908435']},\n",
       " {'id': '1483126227',\n",
       "  'title': 'FREQUENCY ANALYSIS OF ENGLISH USAGE: LEXICON AND GRAMMAR',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,918',\n",
       "  'abstract': '',\n",
       "  'date': 1982,\n",
       "  'authors': ['W. Nelson Francis', 'Henry Kučera', 'Andrew W. Mackie'],\n",
       "  'related_topics': ['Lexicon',\n",
       "   'Grammar',\n",
       "   'Brown Corpus',\n",
       "   'Regular and irregular verbs',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Frequency analysis',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2081580037',\n",
       "   '1632114991',\n",
       "   '2102381086',\n",
       "   '2136930489',\n",
       "   '2117805756',\n",
       "   '2117400858',\n",
       "   '1659833910',\n",
       "   '2141766660',\n",
       "   '1997161938']},\n",
       " {'id': '2017668967',\n",
       "  'title': 'Semantic networks of English.',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '499',\n",
       "  'abstract': 'Principles of lexical semantics developed in the course of building an on-line lexical database are discussed. The approach is relational rather than componential. The fundamental semantic relation is synonymy, which is required in order to define the lexicalized concepts that words can be used to express. Other semantic relations between these concepts are then described. No single set of semantic relations or organizational structure is adequate for the entire lexicon: nouns, adjectives, and verbs each have their own semantic relations and their own organization determined by the role they must play in the construction of linguistic messages.',\n",
       "  'date': 1991,\n",
       "  'authors': ['George A. Miller', 'Christiane Fellbaum'],\n",
       "  'related_topics': ['Semantic computing',\n",
       "   'Semantic network',\n",
       "   'Semantic similarity',\n",
       "   'Semantic property',\n",
       "   'Componential analysis',\n",
       "   'Lexical database',\n",
       "   'Lexical semantics',\n",
       "   'Semantic compression',\n",
       "   'Linguistics',\n",
       "   'Psychology'],\n",
       "  'references': ['2103318667',\n",
       "   '1503404806',\n",
       "   '2123987305',\n",
       "   '2040300040',\n",
       "   '2013596317',\n",
       "   '2052262800',\n",
       "   '2059799772',\n",
       "   '1576632330',\n",
       "   '2025589690',\n",
       "   '2069736034']},\n",
       " {'id': '1518768680',\n",
       "  'title': 'Towards building contextual representations of word senses using statistical models',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '100',\n",
       "  'abstract': 'A b s t r a c t Automatic corpus-based sense resolution, or sense dlsambiguation, techniques tend to focus either on very local context or on topical context. Both components axe needed for word sense resolution. A contextual representation of a word sense consists of topical context and local context. Our goal is to construct contextual representations by automatically extracting topical and local information from textual corpora. We review an experiment evaluating three statistical classifiers that automatically extract topical context. An experiment designed to examine human subject performance with similar input is described. Finally, we investigate a method for automatically extracting local context from a corpus. Preliminary results show improved perfor-',\n",
       "  'date': 1996,\n",
       "  'authors': ['Claudia Leacock', 'Geoffrey Towell', 'Ellen M. Voorhees'],\n",
       "  'related_topics': ['Context (language use)',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Statistical model',\n",
       "   'Construct (python library)',\n",
       "   'Representation (systemics)',\n",
       "   'Resolution (logic)',\n",
       "   'Focus (linguistics)',\n",
       "   'Computer science',\n",
       "   'Subject (grammar)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2154642048',\n",
       "   '2103318667',\n",
       "   '1977182536',\n",
       "   '2165612380',\n",
       "   '2047620598',\n",
       "   '1999114220',\n",
       "   '2066444522',\n",
       "   '2148426685',\n",
       "   '2090543924']},\n",
       " {'id': '13823885',\n",
       "  'title': 'The categorization of sentential contexts',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '9',\n",
       "  'abstract': 'A new experimental method, involving the sorting of linguistic contexts, is shown to be effective in discriminating the contexts of polysemous words as well as the contexts of synonyms of those words. These results are interpreted as support for the claim that the method of sorting linguistic contexts is a valid technique for studying the contextual information available to support inferences about word meanings.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Walter G. Charles'],\n",
       "  'related_topics': ['Context (archaeology)',\n",
       "   'Categorization',\n",
       "   'Psycholinguistics',\n",
       "   'Sorting',\n",
       "   'Lexico',\n",
       "   'Linguistics',\n",
       "   'Natural language processing',\n",
       "   'Word (computer architecture)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Contextual information',\n",
       "   'Semantic relation'],\n",
       "  'references': ['2017580301', '1995875735', '2045585593', '2316811974']},\n",
       " {'id': '1997063559',\n",
       "  'title': 'Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images*',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '26,658',\n",
       "  'abstract': 'We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, non-linear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low-energy states (‘annealing’), or what is the same thing, the most probable states under the Gib...',\n",
       "  'date': 1992,\n",
       "  'authors': ['Stuart Geman 1, Donald Geman 2'],\n",
       "  'related_topics': ['Gibbs sampling',\n",
       "   'Categorical distribution',\n",
       "   'Boltzmann distribution',\n",
       "   'Markov random field',\n",
       "   'Posterior probability',\n",
       "   'Physical system',\n",
       "   'Statistical mechanics',\n",
       "   'Multiplicative function',\n",
       "   'Statistical physics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2581275558',\n",
       "   '2150060382',\n",
       "   '1622620102',\n",
       "   '2154061444',\n",
       "   '2114220616',\n",
       "   '1979622972',\n",
       "   '2065301447',\n",
       "   '2107792892',\n",
       "   '2056760934',\n",
       "   '1567885833']},\n",
       " {'id': '2049633694',\n",
       "  'title': 'Maximum likelihood from incomplete data via the EM algorithm',\n",
       "  'reference_count': '74',\n",
       "  'citation_count': '66,114',\n",
       "  'abstract': '',\n",
       "  'date': 1977,\n",
       "  'authors': ['Arthur P. Dempster', 'Nan M. Laird', 'Donald B. Rubin'],\n",
       "  'related_topics': ['Maximum likelihood sequence estimation',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'MM algorithm',\n",
       "   'Mixture model',\n",
       "   'Baum–Welch algorithm',\n",
       "   'Forward–backward algorithm',\n",
       "   'Cluster-weighted modeling',\n",
       "   'Observed information',\n",
       "   'Mathematics',\n",
       "   'Statistics',\n",
       "   'Pattern recognition'],\n",
       "  'references': ['2100358124',\n",
       "   '2327022120',\n",
       "   '2074673068',\n",
       "   '2403035479',\n",
       "   '1982585616',\n",
       "   '1575431606',\n",
       "   '2086699924',\n",
       "   '2144578442',\n",
       "   '2000084758',\n",
       "   '2121493622']},\n",
       " {'id': '1964724001',\n",
       "  'title': 'Exploratory Projection Pursuit',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '1,158',\n",
       "  'abstract': 'Abstract A new projection pursuit algorithm for exploring multivariate data is presented that has both statistical and computational advantages over previous methods. A number of practical issues concerning its application are addressed. A connection to multivariate density estimation is established, and its properties are investigated through simulation studies and application to real data. The goal of exploratory projection pursuit is to use the data to find low- (one-, two-, or three-) dimensional projections that provide the most revealing views of the full-dimensional data. With these views the human gift for pattern recognition can be applied to help discover effects that may not have been anticipated in advance. Since linear effects are directly captured by the covariance structure of the variable pairs (which are straightforward to estimate) the emphasis here is on the discovery of nonlinear effects such as clustering or other general nonlinear associations among the variables. Although arbitrary ...',\n",
       "  'date': 1987,\n",
       "  'authors': ['Jerome H. Friedman'],\n",
       "  'related_topics': ['Projection pursuit',\n",
       "   'Exploratory data analysis',\n",
       "   'Cluster analysis',\n",
       "   'Covariance',\n",
       "   'Density estimation',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Data mapping',\n",
       "   'Statistical graphics',\n",
       "   'Data mining',\n",
       "   'Mathematics'],\n",
       "  'references': ['2954064014',\n",
       "   '2800289289',\n",
       "   '2155199877',\n",
       "   '2029469881',\n",
       "   '2082612735',\n",
       "   '1573763320',\n",
       "   '1968104963',\n",
       "   '2161831609',\n",
       "   '2052740976',\n",
       "   '1983993791']},\n",
       " {'id': '2121407732',\n",
       "  'title': 'Finite Mixture Distributions',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,043',\n",
       "  'abstract': \"1 General introduction.- 1.1 Introduction.- 1.2 Some applications of finite mixture distributions.- 1.3 Definition.- 1.4 Estimation methods.- 1.4.1 Maximum likelihood.- 1.4.2 Bayesian estimation.- 1.4.3 Inversion and error minimization.- 1.4.4 Other methods.- 1.4.5 Estimating the number of components.- 1.5 Summary.- 2 Mixtures of normal distributions.- 2.1 Introduction.- 2.2 Some descriptive properties of mixtures of normal distributions.- 2.3 Estimating the parameters in normal mixture distributions.- 2.3.1 Method of moments estimation.- 2.3.2 Maximum likelihood estimation.- 2.3.3 Maximum likelihood estimates for grouped data.- 2.3.4 Obtaining initial parameter values for the maximum likelihood estimation algorithms.- 2.3.5 Graphical estimation techniques.- 2.3.6 Other estimation methods.- 2.4 Summary.- 3 Mixtures of exponential and other continuous distributions.- 3.1 Exponential mixtures.- 3.2 Estimating exponential mixture parameters.- 3.2.1 The method of moments and generalizations.- 3.2.2 Maximum likelihood.- 3.3 Properties of exponential mixtures.- 3.4 Other continuous distributions.- 3.4.1 Non-central chi-squared distribution.- 3.4.2 Non-central F distribution.- 3.4.3 Beta distributions.- 3.4.4 Doubly non-central t distribution.- 3.4.5 Planck's distribution.- 3.4.6 Logistic.- 3.4.7 Laplace.- 3.4.8 Weibull.- 3.4.9 Gamma.- 3.5 Mixtures of different component types.- 3.6 Summary.- 4 Mixtures of discrete distributions.- 4.1 Introduction.- 4.2 Mixtures of binomial distributions.- 4.2.1 Moment estimators for binomial mixtures.- 4.2.2 Maximum likelihood estimators for mixtures of binomial distributions.- 4.2.3 Other estimation methods for mixtures of binomial distributions.- 4.3 Mixtures of Poisson distributions.- 4.3.1 Moment estimators for mixtures of Poisson distributions.- 4.3.2 Maximum likelihood estimators for a Poisson mixture.- 4.4 Mixtures of Poisson and binomial distributions.- 4.5 Mixtures of other discrete distributions.- 4.6 Summary.- 5 Miscellaneous topics.- 5.1 Introduction.- 5.2 Determining the number of components in a mixture.- 5.2.1 Informal diagnostic tools for the detection of mixtures.- 5.2.2 Testing hypotheses on the number of components in a mixture.- 5.3 Probability density function estimation.- 5.4 Miscellaneous problems.- 5.5 Summary.- References.\",\n",
       "  'date': 1981,\n",
       "  'authors': ['Brian Everitt', 'D. J. Hand'],\n",
       "  'related_topics': ['Estimating equations',\n",
       "   'Normal distribution',\n",
       "   'Poisson distribution'],\n",
       "  'references': ['1966385142',\n",
       "   '2098710805',\n",
       "   '2289748525',\n",
       "   '2098405376',\n",
       "   '2044579390',\n",
       "   '3123226880',\n",
       "   '2096335861',\n",
       "   '2165225968',\n",
       "   '2171814052',\n",
       "   '2156142001']},\n",
       " {'id': '2725061391',\n",
       "  'title': 'A mean field theory learning algorithm for neural networks',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '687',\n",
       "  'abstract': '',\n",
       "  'date': 1986,\n",
       "  'authors': ['Carsten Peterson', 'James R. Anderson'],\n",
       "  'related_topics': ['Types of artificial neural networks',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Competitive learning',\n",
       "   'Unsupervised learning',\n",
       "   'Rprop',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048', '2293063825', '1507849272']},\n",
       " {'id': '2315016682',\n",
       "  'title': 'Feature extraction using an unsupervised neural network',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '120',\n",
       "  'abstract': 'A novel unsupervised neural network for dimensionality reduction which seeks directions emphasizing distinguishing features in the data is presented. A statistical framework for the parameter estimation problem associated with this neural network is given and its connection to exploratory projection pursuit methods is established. The network is shown to minimize a loss function (projection index) over a set of parameters, yielding an optimal decision rule under some norm. A specific projection index that favors directions possessing multimodality is presented. This leads to a similar form to the synaptic modification equations governing learning in Bienenstock, Cooper, and Munro (BCM) neurons (1982). The importance of a dimensionality reduction principal based, solely on distinguishing features, is demonstrated using a linguistically motivated phoneme recognition experiment, and compared with feature extraction using principal components and back-propagation network.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Nathan Intrator'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Time delay neural network',\n",
       "   'Projection pursuit',\n",
       "   'Feature extraction',\n",
       "   'Deep learning',\n",
       "   'Artificial neural network',\n",
       "   'Projection (set theory)',\n",
       "   'Principal component analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1811843574', '2100544508', '1785063204', '2096230369']},\n",
       " {'id': '3035524453',\n",
       "  'title': 'Momentum Contrast for Unsupervised Visual Representation Learning',\n",
       "  'reference_count': '58',\n",
       "  'citation_count': '1,071',\n",
       "  'abstract': 'We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Kaiming He',\n",
       "   'Haoqi Fan',\n",
       "   'Yuxin Wu',\n",
       "   'Saining Xie',\n",
       "   'Ross Girshick'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Feature learning',\n",
       "   'Visualization'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2963341956',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '1903029394',\n",
       "   '2099471712',\n",
       "   '2108598243',\n",
       "   '1536680647']},\n",
       " {'id': '343636949',\n",
       "  'title': 'Unsupervised Visual Representation Learning by Context Prediction',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '1,279',\n",
       "  'abstract': 'This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [19] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Carl Doersch 1, Abhinav Gupta 1, Alexei A. Efros 2'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Spatial contextual awareness',\n",
       "   'Artificial neural network',\n",
       "   'Visualization',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Pascal (programming language)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Training set'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '1903029394',\n",
       "   '2153579005',\n",
       "   '2108598243',\n",
       "   '2155893237',\n",
       "   '2136922672',\n",
       "   '2168356304']},\n",
       " {'id': '219040644',\n",
       "  'title': 'Unsupervised Learning of Visual Representations Using Videos',\n",
       "  'reference_count': '55',\n",
       "  'citation_count': '861',\n",
       "  'abstract': 'Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Xiaolong Wang', 'Abhinav Gupta'],\n",
       "  'related_topics': ['Unsupervised learning',\n",
       "   'Convolutional neural network',\n",
       "   'Feature vector'],\n",
       "  'references': ['2618530766',\n",
       "   '2151103935',\n",
       "   '2102605133',\n",
       "   '2161969291',\n",
       "   '2155893237',\n",
       "   '2100495367',\n",
       "   '2031489346',\n",
       "   '1677409904',\n",
       "   '2163922914',\n",
       "   '2154889144']},\n",
       " {'id': '2808631503',\n",
       "  'title': 'VoxCeleb2: Deep Speaker Recognition.',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '667',\n",
       "  'abstract': 'The objective of this paper is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Joon Son Chung', 'Arsha Nagrani', 'Andrew Zisserman'],\n",
       "  'related_topics': ['Speaker recognition',\n",
       "   'Convolutional neural network',\n",
       "   'Margin (machine learning)',\n",
       "   'Speech recognition',\n",
       "   'Benchmark (computing)',\n",
       "   'Pipeline (software)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)'],\n",
       "  'references': ['2194775991',\n",
       "   '3106250896',\n",
       "   '2096733369',\n",
       "   '2145287260',\n",
       "   '2325939864',\n",
       "   '2963173190',\n",
       "   '2150769028',\n",
       "   '1963882359',\n",
       "   '2963026686',\n",
       "   '2963839617']},\n",
       " {'id': '602397586',\n",
       "  'title': 'Flowing ConvNets for Human Pose Estimation in Videos',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '451',\n",
       "  'abstract': \"The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps, (ii) spatial fusion layers that learn an implicit spatial model, (iii) optical flow is used to align heatmap predictions from neighbouring frames, and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also [5, 35] in the high precision region).\",\n",
       "  'date': 2015,\n",
       "  'authors': ['Tomas Pfister 1, James Charles 2, Andrew Zisserman 3'],\n",
       "  'related_topics': ['Pose',\n",
       "   'Optical flow',\n",
       "   'Graphical model',\n",
       "   'Margin (machine learning)',\n",
       "   'Benchmark (computing)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2618530766',\n",
       "   '2102605133',\n",
       "   '1849277567',\n",
       "   '2963542991',\n",
       "   '2155541015',\n",
       "   '2156303437',\n",
       "   '2145287260',\n",
       "   '2016053056',\n",
       "   '2062118960',\n",
       "   '2113325037']},\n",
       " {'id': '2619697695',\n",
       "  'title': 'Look, Listen and Learn',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '399',\n",
       "  'abstract': 'We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself – the correspondence between the visual and the audio streams, and we introduce a novel “Audio-Visual Correspondence” learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Relja Arandjelovic 1, Andrew Zisserman 2'],\n",
       "  'related_topics': ['Task (project management)',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Set (psychology)',\n",
       "   'Visualization',\n",
       "   'Feature extraction',\n",
       "   'Active listening',\n",
       "   'Semantics',\n",
       "   'Speech recognition',\n",
       "   'Computer science'],\n",
       "  'references': ['2618530766',\n",
       "   '2962835968',\n",
       "   '2964121744',\n",
       "   '1836465849',\n",
       "   '2117539524',\n",
       "   '2183341477',\n",
       "   '2963420272',\n",
       "   '2326925005',\n",
       "   '2123024445',\n",
       "   '343636949']},\n",
       " {'id': '2962865004',\n",
       "  'title': 'The Sound of Pixels',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '250',\n",
       "  'abstract': 'We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Hang Zhao 1, Chuang Gan 1, 2, Andrew Rouditchenko 1, Carl Vondrick 1, 3, Josh H. McDermott 1, Antonio Torralba 1'],\n",
       "  'related_topics': ['Pixel',\n",
       "   'Source separation',\n",
       "   'Computer vision',\n",
       "   'Synchronization (computer science)',\n",
       "   'Set (abstract data type)',\n",
       "   'Sound (medical instrument)',\n",
       "   'Computer science',\n",
       "   'Volume (computing)',\n",
       "   'Image (mathematics)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '1901129140',\n",
       "   '2295107390',\n",
       "   '2963420272',\n",
       "   '2184188583',\n",
       "   '343636949',\n",
       "   '2221409856',\n",
       "   '2520707650',\n",
       "   '219040644',\n",
       "   '2737258237']},\n",
       " {'id': '3101577715',\n",
       "  'title': 'The VIA Annotation Software for Images, Audio and Video',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '270',\n",
       "  'abstract': 'In this paper, we introduce a simple and standalone manual annotation tool for images, audio and video: the VGG Image Annotator (VIA). This is a light weight, standalone and offline software package that does not require any installation or setup and runs solely in a web browser. The VIA software allows human annotators to define and describe spatial regions in images or video frames, and temporal segments in audio or video. These manual annotations can be exported to plain text data formats such as JSON and CSV and therefore are amenable to further processing by other software tools. VIA also supports collaborative annotation of a large dataset by a group of human annotators. The BSD open source license of this software allows it to be used in any academic project or commercial application.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Abhishek Dutta', 'Andrew Zisserman'],\n",
       "  'related_topics': ['Software',\n",
       "   'Automatic image annotation',\n",
       "   'JSON',\n",
       "   'Plain text',\n",
       "   'Annotation',\n",
       "   'Information retrieval',\n",
       "   'Computer science',\n",
       "   'SIMPLE (military communications protocol)'],\n",
       "  'references': ['639708223',\n",
       "   '2808631503',\n",
       "   '2810605094',\n",
       "   '2961568334',\n",
       "   '2976600002']},\n",
       " {'id': '3161838454',\n",
       "  'title': 'Segmenter: Transformer for Semantic Segmentation.',\n",
       "  'reference_count': '51',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution based approaches, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on the challenging ADE20K dataset and performs on-par on Pascal Context and Cityscapes.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Robin A. M. Strudel',\n",
       "   'Ricardo Garcia',\n",
       "   'Ivan Laptev',\n",
       "   'Cordelia Schmid'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Contextual image classification',\n",
       "   'Segmentation',\n",
       "   'Context (language use)',\n",
       "   'Pattern recognition',\n",
       "   'Convolution',\n",
       "   'Leverage (statistics)',\n",
       "   'Image (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2963341956',\n",
       "   '2963403868',\n",
       "   '1836465849',\n",
       "   '1901129140',\n",
       "   '1903029394',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '2412782625',\n",
       "   '2963881378',\n",
       "   '2340897893']},\n",
       " {'id': '3119997354',\n",
       "  'title': 'Transformers in Vision: A Survey',\n",
       "  'reference_count': '190',\n",
       "  'citation_count': '50',\n",
       "  'abstract': 'Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Salman Khan',\n",
       "   'Muzammal Naseer',\n",
       "   'Munawar Hayat',\n",
       "   'Syed Waqas Zamir',\n",
       "   'Fahad Shahbaz Khan',\n",
       "   'Mubarak Shah'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Contextual image classification',\n",
       "   'Visual reasoning',\n",
       "   'Activity recognition',\n",
       "   'Parallel processing (psychology)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '639708223',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '2919115771',\n",
       "   '2099471712',\n",
       "   '2108598243',\n",
       "   '1536680647',\n",
       "   '3106250896',\n",
       "   '2963037989']},\n",
       " {'id': '3159056052',\n",
       "  'title': 'Few-Shot Classification with Feature Map Reconstruction Networks.',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'In this paper we reformulate few-shot classification as a reconstruction problem in latent space. The ability of the network to reconstruct a query feature map from support features of a given class predicts membership of the query in that class. We introduce a novel mechanism for few-shot classification by regressing directly from support features to query features in closed form, without introducing any new modules or large-scale learnable parameters. The resulting Feature Map Reconstruction Networks are both more performant and computationally efficient than previous approaches. We demonstrate consistent and substantial accuracy gains on four fine-grained benchmarks with varying neural architectures. Our model is also competitive on the non-fine-grained mini-ImageNet and tiered-ImageNet benchmarks with minimal bells and whistles.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Davis Wertheimer', 'Luming Tang', 'Bharath Hariharan'],\n",
       "  'related_topics': ['Feature (computer vision)',\n",
       "   'Pattern recognition',\n",
       "   'Class (biology)',\n",
       "   'Computer science',\n",
       "   'Space (commercial competition)',\n",
       "   'Shot (filmmaking)',\n",
       "   'Artificial intelligence',\n",
       "   'Reconstruction problem'],\n",
       "  'references': ['2194775991',\n",
       "   '2963341924',\n",
       "   '2601450892',\n",
       "   '2753160622',\n",
       "   '2035379092',\n",
       "   '1797268635',\n",
       "   '1990937109',\n",
       "   '2964105864',\n",
       "   '2963741406',\n",
       "   '2595142274']},\n",
       " {'id': '3139613640',\n",
       "  'title': 'Few-shot Weakly-Supervised Object Detection via Directional Statistics.',\n",
       "  'reference_count': '41',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Detecting novel objects from few examples has become an emerging topic in computer vision recently. However, these methods need fully annotated training images to learn new object categories which limits their applicability in real world scenarios such as field robotics. In this work, we propose a probabilistic multiple instance learning approach for few-shot Common Object Localization (COL) and few-shot Weakly Supervised Object Detection (WSOD). In these tasks, only image-level labels, which are much cheaper to acquire, are available. We find that operating on features extracted from the last layer of a pre-trained Faster-RCNN is more effective compared to previous episodic learning based few-shot COL methods. Our model simultaneously learns the distribution of the novel objects and localizes them via expectation-maximization steps. As a probabilistic model, we employ von Mises-Fisher (vMF) distribution which captures the semantic information better than Gaussian distribution when applied to the pre-trained embedding space. When the novel objects are localized, we utilize them to learn a linear appearance model to detect novel classes in new images. Our extensive experiments show that the proposed method, despite being simple, outperforms strong baselines in few-shot COL and WSOD, as well as large-scale WSOD tasks.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Amirreza Shaban',\n",
       "   'Amir Rahimi',\n",
       "   'Thalaiyasingam Ajanthan',\n",
       "   'Byron Boots',\n",
       "   'Richard I. Hartley'],\n",
       "  'related_topics': ['Object detection',\n",
       "   'Active appearance model',\n",
       "   'Object (computer science)',\n",
       "   'Probabilistic logic',\n",
       "   'Field (computer science)',\n",
       "   'Statistical model',\n",
       "   'Directional statistics',\n",
       "   'Pattern recognition',\n",
       "   'Embedding',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2962835968',\n",
       "   '639708223',\n",
       "   '2108598243',\n",
       "   '1861492603',\n",
       "   '2963341924',\n",
       "   '2601450892',\n",
       "   '2108745803',\n",
       "   '2798836702',\n",
       "   '2964069537']},\n",
       " {'id': '3143315506',\n",
       "  'title': 'Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot Classification Benchmark.',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Meta and transfer learning are two successful families of approaches to few-shot learning. Despite highly related goals, state-of-the-art advances in each family are measured largely in isolation of each other. As a result of diverging evaluation norms, a direct or thorough comparison of different approaches is challenging. To bridge this gap, we perform a cross-family study of the best transfer and meta learners on both a large-scale meta-learning benchmark (Meta-Dataset, MD), and a transfer learning benchmark (Visual Task Adaptation Benchmark, VTAB). We find that, on average, large-scale transfer methods (Big Transfer, BiT) outperform competing approaches on MD, even when trained only on ImageNet. In contrast, meta-learning approaches struggle to compete on VTAB when trained and validated on MD. However, BiT is not without limitations, and pushing for scale does not improve performance on highly out-of-distribution MD tasks. In performing this study, we reveal a number of discrepancies in evaluation norms and study some of these in light of the performance gap. We hope that this work facilitates sharing of insights from each community, and accelerates progress on few-shot learning.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Vincent Dumoulin',\n",
       "   'Neil Houlsby',\n",
       "   'Utku Evci',\n",
       "   'Xiaohua Zhai',\n",
       "   'Ross Goroshin',\n",
       "   'Sylvain Gelly',\n",
       "   'Hugo Larochelle'],\n",
       "  'related_topics': ['Meta learning (computer science)',\n",
       "   'Transfer of learning',\n",
       "   'Benchmark (computing)'],\n",
       "  'references': ['2097117768',\n",
       "   '2963403868',\n",
       "   '2108598243',\n",
       "   '1861492603',\n",
       "   '3118608800',\n",
       "   '2165698076',\n",
       "   '2963341924',\n",
       "   '2952122856',\n",
       "   '2601450892',\n",
       "   '3034978746']},\n",
       " {'id': '3135385999',\n",
       "  'title': 'Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning.',\n",
       "  'reference_count': '76',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the objective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predominantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the significance of powerful feature representations with a simple embedding network that can outperform existing sophisticated FSL algorithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been employed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Simultaneous optimization for both of these contrasting objectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transformations. These complementary sets of features help generalize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive experimentation shows that even without knowledge distillation our proposed method can outperform current state-of-the-art FSL methods on five popular benchmark datasets.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Mamshad Nayeem Rizve',\n",
       "   'Salman Khan',\n",
       "   'Fahad Shahbaz Khan',\n",
       "   'Mubarak Shah'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Metric (mathematics)',\n",
       "   'Transformation geometry',\n",
       "   'Transformation (function)',\n",
       "   'Benchmark (computing)',\n",
       "   'Invariant (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Invariant (physics)',\n",
       "   'Embedding',\n",
       "   'Set (abstract data type)',\n",
       "   'Computer science',\n",
       "   'Equivariant map',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2095705004',\n",
       "   '2108598243',\n",
       "   '2183341477',\n",
       "   '2806070179',\n",
       "   '2135046866',\n",
       "   '2963703618',\n",
       "   '1821462560',\n",
       "   '2963524571',\n",
       "   '2062118960']},\n",
       " {'id': '3136670918',\n",
       "  'title': 'Universal Representation Learning from Multiple Domains for Few-shot Classification.',\n",
       "  'reference_count': '53',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'In this paper, we look at the problem of few-shot classification that aims to learn a classifier for previously unseen classes and domains from few labeled samples. Recent methods use adaptation networks for aligning their features to new domains or select the relevant features from multiple domain-specific feature extractors. In this work, we propose to learn a single set of universal deep representations by distilling knowledge of multiple separately trained networks after co-aligning their features with the help of adapters and centered kernel alignment. We show that the universal representations can be further refined for previously unseen domains by an efficient adaptation step in a similar spirit to distance learning methods. We rigorously evaluate our model in the recent Meta-Dataset benchmark and demonstrate that it significantly outperforms the previous methods while being more efficient. Our code will be available at this https URL',\n",
       "  'date': 2021,\n",
       "  'authors': ['Wei-Hong Li', 'Xialei Liu', 'Hakan Bilen'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Feature (machine learning)',\n",
       "   'Classifier (UML)',\n",
       "   'Machine learning',\n",
       "   'Set (abstract data type)',\n",
       "   'Benchmark (computing)',\n",
       "   'Computer science',\n",
       "   'Code (cryptography)',\n",
       "   'Adaptation (computer science)',\n",
       "   'Distance education',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2117539524',\n",
       "   '1861492603',\n",
       "   '3118608800',\n",
       "   '2310919327',\n",
       "   '1821462560',\n",
       "   '2970971581',\n",
       "   '2963341924',\n",
       "   '2601450892',\n",
       "   '2964118293']},\n",
       " {'id': '3139264293',\n",
       "  'title': 'Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning.',\n",
       "  'reference_count': '141',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naive average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-time Crop & Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Mandela Patrick',\n",
       "   'Yuki Markus Asano',\n",
       "   'Bernie Huang',\n",
       "   'Ishan Misra',\n",
       "   'Florian Metze',\n",
       "   'João F. Henriques',\n",
       "   'Andrea Vedaldi'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Feature vector',\n",
       "   'Feature (computer vision)',\n",
       "   'Pooling',\n",
       "   'Machine learning',\n",
       "   'Transformer (machine learning model)',\n",
       "   'Computer science',\n",
       "   'Modal',\n",
       "   'Space (commercial competition)',\n",
       "   'Scale (descriptive set theory)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2964121744',\n",
       "   '639708223',\n",
       "   '2963341956',\n",
       "   '2963403868',\n",
       "   '2108598243',\n",
       "   '2161969291',\n",
       "   '2156303437',\n",
       "   '1522734439']},\n",
       " {'id': '3126009523',\n",
       "  'title': 'Temporal-Relational CrossTransformers for Few-Shot Action Recognition',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '1',\n",
       "  'abstract': 'We propose a novel approach to few-shot action recognition, finding temporally-corresponding frame tuples between the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes using the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video representations are formed from ordered tuples of varying numbers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared. Our proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Toby Perrett',\n",
       "   'Alessandro Masullo',\n",
       "   'Tilo Burghardt',\n",
       "   'Majid Mirmehdi',\n",
       "   'Dima Damen'],\n",
       "  'related_topics': ['Tuple',\n",
       "   'Set (abstract data type)',\n",
       "   'Margin (machine learning)',\n",
       "   'Frame (networking)',\n",
       "   'Matching (graph theory)',\n",
       "   'Class (set theory)',\n",
       "   'Pattern recognition',\n",
       "   'Construct (python library)',\n",
       "   'Computer science',\n",
       "   'Shot (filmmaking)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2963403868',\n",
       "   '2108598243',\n",
       "   '2963524571',\n",
       "   '2963341924',\n",
       "   '2601450892',\n",
       "   '2507009361',\n",
       "   '2126579184',\n",
       "   '2990503944',\n",
       "   '2770804203']},\n",
       " {'id': '3122301478',\n",
       "  'title': 'Revisiting Contrastive Learning for Few-Shot Classification.',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '1',\n",
       "  'abstract': 'Instance discrimination based contrastive learning has emerged as a leading approach for self-supervised learning of visual representations. Yet, its generalization to novel tasks remains elusive when compared to representations learned with supervision, especially in the few-shot setting. We demonstrate how one can incorporate supervision in the instance discrimination based contrastive self-supervised learning framework to learn representations that generalize better to novel tasks. We call our approach CIDS (Contrastive Instance Discrimination with Supervision). CIDS performs favorably compared to existing algorithms on popular few-shot benchmarks like Mini-ImageNet or Tiered-ImageNet. We also propose a novel model selection algorithm that can be used in conjunction with a universal embedding trained using CIDS to outperform state-of-the-art algorithms on the challenging Meta-Dataset benchmark.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Orchid Majumder',\n",
       "   'Avinash Ravichandran',\n",
       "   'Subhransu Maji',\n",
       "   'Marzia Polito',\n",
       "   'Rahul Bhotika',\n",
       "   'Stefano Soatto'],\n",
       "  'related_topics': ['Benchmark (computing)',\n",
       "   'Generalization',\n",
       "   'Model selection',\n",
       "   'Machine learning',\n",
       "   'Embedding',\n",
       "   'Conjunction (grammar)',\n",
       "   'Computer science',\n",
       "   'Shot (filmmaking)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2117539524',\n",
       "   '1677182931',\n",
       "   '2096733369',\n",
       "   '2187089797',\n",
       "   '2963341924',\n",
       "   '2601450892',\n",
       "   '3091905774',\n",
       "   '3034978746',\n",
       "   '3035524453']},\n",
       " {'id': '3131944163',\n",
       "  'title': 'Data-Efficient Reinforcement Learning with Self-Predictive Representations',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '18',\n",
       "  'abstract': \"While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, SPR, trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We've made the code associated with this work available at https://anonymous.4open.science/r/b4b93ec6-6e5d-4f43-9b53-54bdf73bea95/.\",\n",
       "  'date': 2021,\n",
       "  'authors': ['Max Schwarzer 1, Ankesh Anand 1, Rishab Goel 2, R Devon Hjelm 3, Aaron Courville 1, Philip Bachman 3'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Feature learning',\n",
       "   'Maximization'],\n",
       "  'references': ['1836465849',\n",
       "   '2095705004',\n",
       "   '2145339207',\n",
       "   '2970971581',\n",
       "   '2952509347',\n",
       "   '3035524453',\n",
       "   '343636949',\n",
       "   '3136604105',\n",
       "   '3098903812',\n",
       "   '2982316857']},\n",
       " {'id': '3102762742',\n",
       "  'title': 'MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '10',\n",
       "  'abstract': 'This paper introduces MDP homomorphic networks for deep reinforcement learning. MDP homomorphic networks are neural networks that are equivariant under symmetries in the joint state-action space of an MDP. Current approaches to deep reinforcement learning do not usually exploit knowledge about such structure. By building this prior knowledge into policy and value networks using an equivariance constraint, we can reduce the size of the solution space. We specifically focus on group-structured symmetries (invertible transformations). Additionally, we introduce an easy method for constructing equivariant network layers numerically, so the system designer need not solve the constraints by hand, as is typically done. We construct MDP homomorphic MLPs and CNNs that are equivariant under either a group of reflections or rotations. We show that such networks converge faster than unstructured baselines on CartPole, a grid world and Pong.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Elise van der Pol 1, Daniel E. Worrall 1, Herke van Hoof 1, Frans A. Oliehoek 2, Max Welling 1'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Artificial neural network',\n",
       "   'Equivariant map',\n",
       "   'Homomorphic encryption',\n",
       "   'Theoretical computer science',\n",
       "   'Grid',\n",
       "   'Homogeneous space',\n",
       "   'Group (mathematics)',\n",
       "   'Structure (category theory)',\n",
       "   'Computer science'],\n",
       "  'references': ['3156762592',\n",
       "   '3163735823',\n",
       "   '3161443521',\n",
       "   '3135649981',\n",
       "   '3133724915']},\n",
       " {'id': '3137443434',\n",
       "  'title': 'How to Train Your HERON',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '0',\n",
       "  'abstract': \"In this letter we apply Deep Reinforcement Learning (Deep RL) and Domain Randomization to solve a navigation task in a natural environment relying solely on a 2D laser scanner. We train a model-based RL agent in simulation to follow lake and river shores and apply it on a real Unmanned Surface Vehicle in a zero-shot setup. We demonstrate that even though the agent has not been trained in the real world, it can fulfill its task successfully and adapt to changes in the robot's environment and dynamics. Finally, we show that the RL agent is more robust, faster, and more accurate than a state-aware Model-Predictive-Controller. Code, simulation environments, pre-trained models, and datasets are available at https://github.com/AntoineRichard/Heron-RL-ICRA.git .\",\n",
       "  'date': 2021,\n",
       "  'authors': ['Antoine Richard 1, Stephanie Aravecchia 2, Thomas Schillaci 1, Matthieu Geist 3, Cedric Pradalier 2'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Robot',\n",
       "   'Task (computing)',\n",
       "   'Vehicle dynamics',\n",
       "   'Task analysis',\n",
       "   'Real-time computing',\n",
       "   'Domain (software engineering)',\n",
       "   'Code (cryptography)',\n",
       "   'Computer science',\n",
       "   'Laser scanning'],\n",
       "  'references': ['2145339207',\n",
       "   '2402144811',\n",
       "   '2964043796',\n",
       "   '2605102758',\n",
       "   '1977655452',\n",
       "   '2012587148',\n",
       "   '2990747716',\n",
       "   '2911087563',\n",
       "   '3101442004',\n",
       "   '2900152462']},\n",
       " {'id': '3132674603',\n",
       "  'title': 'Reinforcement Learning with Prototypical Representations',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Denis Yarats',\n",
       "   'Rob Fergus',\n",
       "   'Alessandro Lazaric',\n",
       "   'Lerrel Pinto'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Feature learning',\n",
       "   'Automatic summarization',\n",
       "   'Set (psychology)',\n",
       "   'Representation (mathematics)',\n",
       "   'Human–computer interaction',\n",
       "   'Task (project management)',\n",
       "   'Computer science',\n",
       "   'Control (linguistics)',\n",
       "   'Sample (statistics)'],\n",
       "  'references': ['2964121744',\n",
       "   '2257979135',\n",
       "   '2963864421',\n",
       "   '2025768430',\n",
       "   '3034978746',\n",
       "   '3035524453',\n",
       "   '343636949',\n",
       "   '2751973545',\n",
       "   '2158131535',\n",
       "   '2321533354']},\n",
       " {'id': '3134085768',\n",
       "  'title': 'Low-Precision Reinforcement Learning.',\n",
       "  'reference_count': '56',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Low-precision training has become a popular approach to reduce computation time, memory footprint, and energy consumption in supervised learning. In contrast, this promising approach has not enjoyed similarly widespread adoption within the reinforcement learning (RL) community, in part because RL agents can be notoriously hard to train -- even in full precision. In this paper we consider continuous control with the state-of-the-art SAC agent and demonstrate that a naive adaptation of low-precision methods from supervised learning fails. We propose a set of six modifications, all straightforward to implement, that leaves the underlying agent unchanged but improves its numerical stability dramatically. The resulting modified SAC agent has lower memory and compute requirements while matching full-precision rewards, thus demonstrating the feasibility of low-precision RL.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Johan Bjorck',\n",
       "   'Xiangyu Chen',\n",
       "   'Christopher De Sa',\n",
       "   'Carla P. Gomes',\n",
       "   'Kilian Q. Weinberger'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Supervised learning',\n",
       "   'Machine learning',\n",
       "   'Energy consumption',\n",
       "   'Set (psychology)',\n",
       "   'Computer science',\n",
       "   'Matching (statistics)',\n",
       "   'Adaptation (computer science)',\n",
       "   'Numerical stability',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964121744',\n",
       "   '2257979135',\n",
       "   '2964043796',\n",
       "   '2970971581',\n",
       "   '2121863487',\n",
       "   '2963114950',\n",
       "   '2963374099',\n",
       "   '2962902376',\n",
       "   '2963985863',\n",
       "   '2963120839']},\n",
       " {'id': '3094246835',\n",
       "  'title': 'What About Taking Policy as Input of Value Function: Policy-extended Value Function Approximator',\n",
       "  'reference_count': '64',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'The value function lies in the heart of Reinforcement Learning (RL), which defines the long-term evaluation of a policy in a given state. In this paper, we propose Policy-extended Value Function Approximator (PeVFA) which extends the conventional value to be not only a function of state but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies in contrast to a conventional one with limited capacity for only one policy, inducing the new characteristic of \\\\emph{value generalization among policies}. From both the theoretical and empirical lens, we study value generalization along the policy improvement path (called local generalization), from which we derive a new form of Generalized Policy Iteration with PeVFA to improve the conventional learning process. Besides, we propose a framework to learn the representation of an RL policy, studying several different approaches to learn an effective policy representation from policy network parameters and state-action pairs through contrastive learning and action prediction. In our experiments, Proximal Policy Optimization (PPO) with PeVFA significantly outperforms its vanilla counterpart in MuJoCo continuous control tasks, demonstrating the effectiveness of value generalization offered by PeVFA and policy representation learning.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Hongyao Tang 1, Zhaopeng Meng',\n",
       "   'Jianye Hao 1, Chen Chen 2, Daniel Graves 3, Dong Li 3, Wulong Liu 3, Yaodong Yang 3'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Feature learning',\n",
       "   'Generalization'],\n",
       "  'references': ['2145339207',\n",
       "   '2257979135',\n",
       "   '2187089797',\n",
       "   '2963864421',\n",
       "   '2964043796',\n",
       "   '2736601468',\n",
       "   '2949608212',\n",
       "   '3034978746',\n",
       "   '2810075754',\n",
       "   '2165150801']},\n",
       " {'id': '3107857059',\n",
       "  'title': 'Intervention Design for Effective Sim2Real Transfer.',\n",
       "  'reference_count': '32',\n",
       "  'citation_count': '1',\n",
       "  'abstract': 'The goal of this work is to address the recent success of domain randomization and data augmentation for the sim2real setting. We explain this success through the lens of causal inference, positioning domain randomization and data augmentation as interventions on the environment which encourage invariance to irrelevant features. Such interventions include visual perturbations that have no effect on reward and dynamics. This encourages the learning algorithm to be robust to these types of variations and learn to attend to the true causal mechanisms for solving the task. This connection leads to two key findings: (1) perturbations to the environment do not have to be realistic, but merely show variation along dimensions that also vary in the real world, and (2) use of an explicit invariance-inducing objective improves generalization in sim2sim and sim2real transfer settings over just data augmentation or domain randomization alone. We demonstrate the capability of our method by performing zero-shot transfer of a robot arm reach task on a 7DoF Jaco arm learning from pixel observations.',\n",
       "  'date': 2020,\n",
       "  'authors': ['Melissa Mozifian', 'Amy Zhang', 'Joelle Pineau', 'David Meger'],\n",
       "  'related_topics': ['Causal inference',\n",
       "   'Task (project management)',\n",
       "   'Generalization',\n",
       "   'Domain (software engineering)',\n",
       "   'Machine learning',\n",
       "   'Variation (game tree)',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Randomization',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2143891888',\n",
       "   '3034978746',\n",
       "   '1585160083',\n",
       "   '2605102758',\n",
       "   '2900152462',\n",
       "   '2962935454',\n",
       "   '2534269850',\n",
       "   '2790376986',\n",
       "   '2953494151',\n",
       "   '2963680188']},\n",
       " {'id': '3134032827',\n",
       "  'title': 'Robust Deep Reinforcement Learning via Multi-View Information Bottleneck.',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Deep reinforcement learning (DRL) agents are often sensitive to visual changes that were unseen in their training environments. To address this problem, we introduce a robust representation learning approach for RL. We introduce an auxiliary objective based on the multi-view information bottleneck (MIB) principle which encourages learning representations that are both predictive of the future and less sensitive to task-irrelevant distractions. This enables us to train high-performance policies that are robust to visual distractions and can generalize to unseen environments. We demonstrate that our approach can achieve SOTA performance on challenging visual control tasks, even when the background is replaced with natural videos. In addition, we show that our approach outperforms well-established baselines on generalization to unseen environments using the large-scale Procgen benchmark.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Jiameng Fan', 'Wenchao Li'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Feature learning',\n",
       "   'Information bottleneck method',\n",
       "   'Benchmark (computing)',\n",
       "   'Generalization',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Natural (music)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2187089797',\n",
       "   '2736601468',\n",
       "   '3034978746',\n",
       "   '2887997457',\n",
       "   '2842511635',\n",
       "   '2561238782',\n",
       "   '2619947201',\n",
       "   '2979454998',\n",
       "   '2803832867',\n",
       "   '2963430173']},\n",
       " {'id': '3133595589',\n",
       "  'title': 'Improving Computational Efficiency in Visual Reinforcement Learning via Stored Embeddings',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Recent advances in off-policy deep reinforcement learning (RL) have led to impressive success in complex tasks from visual observations. Experience replay improves sample-efficiency by reusing experiences from the past, and convolutional neural networks (CNNs) process high-dimensional inputs effectively. However, such techniques demand high memory and computational bandwidth. In this paper, we present Stored Embeddings for Efficient Reinforcement Learning (SEER), a simple modification of existing off-policy RL methods, to address these computational and memory requirements. To reduce the computational overhead of gradient updates in CNNs, we freeze the lower layers of CNN encoders early in training due to early convergence of their parameters. Additionally, we reduce memory requirements by storing the low-dimensional latent vectors for experience replay instead of high-dimensional images, enabling an adaptive increase in the replay buffer capacity, a useful technique in constrained-memory settings. In our experiments, we show that SEER does not degrade the performance of RL agents while significantly saving computation and memory across a diverse set of DeepMind Control environments and Atari games. Finally, we show that SEER is useful for computation-efficient transfer learning in RL because lower layers of CNNs extract generalizable features, which can be used for different tasks and domains.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Lili Chen', 'Kimin Lee', 'Aravind Srinivas', 'Pieter Abbeel'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Convolutional neural network',\n",
       "   'Transfer of learning',\n",
       "   'Machine learning',\n",
       "   'Process (computing)',\n",
       "   'Encoder',\n",
       "   'Computer science',\n",
       "   'Set (abstract data type)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': []},\n",
       " {'id': '3131871335',\n",
       "  'title': 'State Entropy Maximization with Random Encoders for Efficient Exploration',\n",
       "  'reference_count': '51',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at this https URL.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Younggyo Seo',\n",
       "   'Lili Chen',\n",
       "   'Jinwoo Shin',\n",
       "   'Honglak Lee',\n",
       "   'Pieter Abbeel',\n",
       "   'Kimin Lee'],\n",
       "  'related_topics': ['Entropy (information theory)',\n",
       "   'Entropy (energy dispersal)',\n",
       "   'Encoder',\n",
       "   'Reinforcement learning',\n",
       "   'Entropy (classical thermodynamics)',\n",
       "   'Entropy (statistical thermodynamics)',\n",
       "   'Maximization',\n",
       "   'Source code',\n",
       "   'Entropy (arrow of time)',\n",
       "   'Estimator',\n",
       "   'Representation (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Entropy (order and disorder)',\n",
       "   'State entropy'],\n",
       "  'references': []},\n",
       " {'id': '2963703618',\n",
       "  'title': 'Dynamic Routing Between Capsules',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,887',\n",
       "  'abstract': 'A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Sara Sabour', 'Nicholas Frosst', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['MNIST database', 'Algorithm', 'Scalar (mathematics)'],\n",
       "  'references': ['2902302021',\n",
       "   '2997428643',\n",
       "   '2805516822',\n",
       "   '2954226438',\n",
       "   '2963847595',\n",
       "   '2972584841',\n",
       "   '2900954917',\n",
       "   '2764024122']},\n",
       " {'id': '1821462560',\n",
       "  'title': 'Distilling the Knowledge in a Neural Network',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '12,360',\n",
       "  'abstract': 'A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Geoffrey E. Hinton', 'Oriol Vinyals', 'Jeffrey Dean'],\n",
       "  'related_topics': ['Ensemble learning',\n",
       "   'MNIST database',\n",
       "   'Artificial neural network',\n",
       "   'Acoustic model',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Type (model theory)',\n",
       "   'Mixture of experts'],\n",
       "  'references': ['2618530766',\n",
       "   '2095705004',\n",
       "   '1904365287',\n",
       "   '2160815625',\n",
       "   '2168231600',\n",
       "   '1534477342',\n",
       "   '2294370754',\n",
       "   '2150884987',\n",
       "   '2402040300']},\n",
       " {'id': '2963069010',\n",
       "  'title': 'Grammar as a foreign language',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '877',\n",
       "  'abstract': 'Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Oriol Vinyals',\n",
       "   'Lukasz Kaiser',\n",
       "   'Terry Koo',\n",
       "   'Slav Petrov',\n",
       "   'Ilya Sutskever',\n",
       "   'Geoffrey Hinton'],\n",
       "  'related_topics': ['Parsing', 'Syntax', 'Grammar'],\n",
       "  'references': ['2964308564',\n",
       "   '1614298861',\n",
       "   '2130942839',\n",
       "   '2064675550',\n",
       "   '1895577753',\n",
       "   '1753482797',\n",
       "   '1810943226',\n",
       "   '1423339008',\n",
       "   '2100664567',\n",
       "   '1632114991']},\n",
       " {'id': '3129170303',\n",
       "  'title': 'Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '3',\n",
       "  'abstract': 'Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Rishabh Agarwal 1, Marlos C. Machado 1, Pablo Samuel Castro 1, Marc G Bellemare 2'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Feature learning',\n",
       "   'Similarity (psychology)',\n",
       "   'Generalization',\n",
       "   'Metric (mathematics)',\n",
       "   'Machine learning',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Bisimulation',\n",
       "   'Computer science',\n",
       "   'Spurious relationship',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2604763608',\n",
       "   '385466589',\n",
       "   '3034978746',\n",
       "   '2786672974',\n",
       "   '2119567691',\n",
       "   '2962902376',\n",
       "   '2605102758',\n",
       "   '2138621090',\n",
       "   '2842511635',\n",
       "   '2097381042']},\n",
       " {'id': '3139419546',\n",
       "  'title': 'A Metric Space Perspective on Self-Supervised Policy Adaptation',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'One of the most challenging aspects of real-world reinforcement learning (RL) is the multitude of unpredictable and ever-changing distractions that could divert an agent from what was tasked to do in its training environment. While an agent could learn from reward signals to ignore them, the complexity of the real-world can make rewards hard to acquire, or, at best, extremely sparse. A recent class of self-supervised methods have shown promise that reward-free adaptation under challenging distractions is possible. However, previous work focused on a short one-episode adaptation setting. In this letter, we consider a long-term adaptation setup that is more akin to the specifics of the real-world and propose a metric space perspective on self-supervised adaptation. We empirically describe the processes that take place in the embedding space during this adaptation process, reveal some of its undesirable effects on performance and show how they can be eliminated. Moreover, we theoretically study how actor-based and actor-free agents can further generalise to the target environment by manipulating the Lipschitz constant of the actor and critic functions.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Cristian Bodnar 1, Karol Hausman 2, Gabriel Dulac-Arnold 2, Rico Jonschkowski 2'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Adaptation (computer science)',\n",
       "   'Process (engineering)',\n",
       "   'Metric space',\n",
       "   'Space (commercial competition)',\n",
       "   'Task analysis',\n",
       "   'Class (computer programming)',\n",
       "   'Perspective (graphical)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2962879692',\n",
       "   '2593768305',\n",
       "   '2605102758',\n",
       "   '2168359464',\n",
       "   '3101442004',\n",
       "   '2781585732',\n",
       "   '2963906246',\n",
       "   '2963521487',\n",
       "   '3125947392',\n",
       "   '2977481643']},\n",
       " {'id': '3162945626',\n",
       "  'title': 'Offline Reinforcement Learning with Pseudometric Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Offline Reinforcement Learning methods seek to learn a policy from logged transitions of an environment, without any interaction. In the presence of function approximation, and under the assumption of limited coverage of the state-action space of the environment, it is necessary to enforce the policy to visit state-action pairs close to the support of logged transitions. In this work, we propose an iterative procedure to learn a pseudometric (closely related to bisimulation metrics) from logged transitions, and use it to define this notion of closeness. We show its convergence and extend it to the function approximation setting. We then use this pseudometric to define a new lookup based bonus in an actor-critic algorithm: PLOff. This bonus encourages the actor to stay close, in terms of the defined pseudometric, to the support of logged transitions. Finally, we evaluate the method on hand manipulation and locomotion tasks.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Robert Dadashi 1, Shideh Rezaeifar 2, Nino Vieillard 1, Léonard Hussenot 1, Olivier Pietquin 1, Matthieu Geist 3'],\n",
       "  'related_topics': ['Pseudometric space',\n",
       "   'Reinforcement learning',\n",
       "   'Closeness',\n",
       "   'Function approximation',\n",
       "   'Bisimulation',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Convergence (routing)',\n",
       "   'Space (commercial competition)',\n",
       "   'Hand manipulation'],\n",
       "  'references': ['2594103415',\n",
       "   '2158969944',\n",
       "   '2154023516',\n",
       "   '3096759198',\n",
       "   '1982948368',\n",
       "   '2105486945',\n",
       "   '3162945626',\n",
       "   '2945309379',\n",
       "   '3120444854',\n",
       "   '3007817409']},\n",
       " {'id': '3159394092',\n",
       "  'title': 'CLAR: Contrastive Learning of Auditory Representations.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Learning rich visual representations using contrastive self-supervised learning has been extremely successful. However, it is still a major question whether we could use a similar approach to learn superior auditory representations. In this paper, we expand on prior work (SimCLR) to learn better auditory representations. We (1) introduce various data augmentations suitable for auditory data and evaluate their impact on predictive performance, (2) show that training with time-frequency audio features substantially improves the quality of the learned representations compared to raw signals, and (3) demonstrate that training with both supervised and contrastive losses simultaneously improves the learned representations compared to self-supervised pre-training followed by supervised fine-tuning. We illustrate that by combining all these methods and with substantially less labeled data, our framework (CLAR) achieves significant improvement on prediction performance compared to supervised approach. Moreover, compared to self-supervised approach, our framework converges faster with significantly better representations.',\n",
       "  'date': 2019,\n",
       "  'authors': ['Haider Al-Tahan', 'Yalda Mohsenzadeh'],\n",
       "  'related_topics': ['Natural language processing',\n",
       "   'Quality (business)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Labeled data'],\n",
       "  'references': ['3034978746',\n",
       "   '3103215803',\n",
       "   '3122924117',\n",
       "   '3130223764',\n",
       "   '3092916550',\n",
       "   '3162133897',\n",
       "   '3043462782',\n",
       "   '3128238149',\n",
       "   '3098222900',\n",
       "   '3108916577']},\n",
       " {'id': '3135138557',\n",
       "  'title': 'Deep Graph Structure Learning for Robust Representations: A Survey.',\n",
       "  'reference_count': '59',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'Graph Neural Networks (GNNs) are widely used for analyzing graph-structured data. Most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure for learning informative embeddings. However, the pervasiveness of noise in graphs necessitates learning robust representations for real-world problems. To improve the robustness of GNN models, many studies have been proposed around the central concept of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding representations. Towards this end, in the presented survey, we broadly review recent progress of GSL methods for learning robust representations. Specifically, we first formulate a general paradigm of GSL, and then review state-of-the-art methods classified by how they model graph structures, followed by applications that incorporate the idea of GSL in other graph tasks. Finally, we point out some issues in current studies and discuss future directions.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Yanqiao Zhu',\n",
       "   'Weizhi Xu',\n",
       "   'Jinghao Zhang',\n",
       "   'Qiang Liu',\n",
       "   'Shu Wu',\n",
       "   'Liang Wang'],\n",
       "  'related_topics': ['Perfect graph',\n",
       "   'Robustness (computer science)',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science',\n",
       "   'Noise (video)',\n",
       "   'Point (typography)',\n",
       "   'Current (mathematics)',\n",
       "   'Quality (business)',\n",
       "   'Graph',\n",
       "   'Structure learning'],\n",
       "  'references': ['1959608418',\n",
       "   '2964153729',\n",
       "   '2963858333',\n",
       "   '2547875792',\n",
       "   '2225156818',\n",
       "   '2979750740',\n",
       "   '2038276547',\n",
       "   '2548228487',\n",
       "   '2811124557',\n",
       "   '2963695795']},\n",
       " {'id': '3134210100',\n",
       "  'title': 'Graph Self-Supervised Learning: A Survey.',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '1',\n",
       "  'abstract': 'Deep learning on graphs has attracted significant interest recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from other domains like computer vision/natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We divide these into four categories according to the design of their pretext tasks. We further discuss the remaining challenges and potential future directions in this research field.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Yixin Liu',\n",
       "   'Shirui Pan',\n",
       "   'Ming Jin',\n",
       "   'Chuan Zhou',\n",
       "   'Feng Xia',\n",
       "   'Philip S. Yu'],\n",
       "  'related_topics': ['Supervised learning',\n",
       "   'Deep learning',\n",
       "   'Field (computer science)',\n",
       "   'Machine learning',\n",
       "   'Robustness (computer science)',\n",
       "   'Computer science',\n",
       "   'Generalization (learning)',\n",
       "   'Artificial intelligence',\n",
       "   'Graph',\n",
       "   'Self supervised learning'],\n",
       "  'references': ['2153579005',\n",
       "   '1614298861',\n",
       "   '2962756421',\n",
       "   '2964015378',\n",
       "   '3104097132',\n",
       "   '2963858333',\n",
       "   '2130354913',\n",
       "   '2624431344',\n",
       "   '3034978746',\n",
       "   '3043547428']},\n",
       " {'id': '3132869322',\n",
       "  'title': 'Towards Robust Graph Contrastive Learning.',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '1',\n",
       "  'abstract': 'We study the problem of adversarially robust self-supervised learning on graphs. In the contrastive learning framework, we introduce a new method that increases the adversarial robustness of the learned representations through i) adversarial transformations and ii) transformations that not only remove but also insert edges. We evaluate the learned representations in a preliminary set of experiments, obtaining promising results. We believe this work takes an important step towards incorporating robustness as a viable auxiliary task in graph contrastive learning.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Nikola Jovanovic',\n",
       "   'Zhao Meng',\n",
       "   'Lukas Faber',\n",
       "   'Roger Wattenhofer'],\n",
       "  'related_topics': ['Robustness (computer science)',\n",
       "   'Theoretical computer science',\n",
       "   'Set (psychology)',\n",
       "   'Computer science',\n",
       "   'Task (project management)',\n",
       "   'Graph'],\n",
       "  'references': ['2963207607',\n",
       "   '2964153729',\n",
       "   '2964015378',\n",
       "   '2964253222',\n",
       "   '2970971581',\n",
       "   '2243397390',\n",
       "   '3034978746',\n",
       "   '2027731328',\n",
       "   '2963312446',\n",
       "   '2887997457']},\n",
       " {'id': '3154313998',\n",
       "  'title': 'When Contrastive Learning Meets Active Learning: A Novel Graph Active Learning Paradigm with Self-Supervision.',\n",
       "  'reference_count': '34',\n",
       "  'citation_count': '0',\n",
       "  'abstract': 'This paper studies active learning (AL) on graphs, whose purpose is to discover the most informative nodes to maximize the performance of graph neural networks (GNNs). Previously, most graph AL methods focus on learning node representations from a carefully selected labeled dataset with large amount of unlabeled data neglected. Motivated by the success of contrastive learning (CL), we propose a novel paradigm that seamlessly integrates graph AL with CL. While being able to leverage the power of abundant unlabeled data in a self-supervised manner, nodes selected by AL further provide semantic information that can better guide representation learning. Besides, previous work measures the informativeness of nodes without considering the neighborhood propagation scheme of GNNs, so that noisy nodes may be selected. We argue that due to the smoothing nature of GNNs, the central nodes from homophilous subgraphs should benefit the model training most. To this end, we present a minimax selection scheme that explicitly harnesses neighborhood information and discover homophilous subgraphs to facilitate active selection. Comprehensive, confounding-free experiments on five public datasets demonstrate the superiority of our method over state-of-the-arts.',\n",
       "  'date': 2021,\n",
       "  'authors': ['Yanqiao Zhu', 'Weizhi Xu', 'Qiang Liu', 'Shu Wu'],\n",
       "  'related_topics': ['Active learning (machine learning)',\n",
       "   'Feature learning',\n",
       "   'Node (computer science)',\n",
       "   'Machine learning',\n",
       "   'Minimax',\n",
       "   'Smoothing',\n",
       "   'Computer science',\n",
       "   'Leverage (statistics)',\n",
       "   'Focus (linguistics)',\n",
       "   'Selection (linguistics)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2964121744',\n",
       "   '2964015378',\n",
       "   '2970971581',\n",
       "   '2963858333',\n",
       "   '1479807131',\n",
       "   '2903158431',\n",
       "   '3034978746',\n",
       "   '3035524453',\n",
       "   '2883725317',\n",
       "   '2887997457']},\n",
       " {'id': '1991848143',\n",
       "  'title': 'Self-Organization and Associative Memory',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '12,946',\n",
       "  'abstract': '1. Various Aspects of Memory.- 1.1 On the Purpose and Nature of Biological Memory.- 1.1.1 Some Fundamental Concepts.- 1.1.2 The Classical Laws of Association.- 1.1.3 On Different Levels of Modelling.- 1.2 Questions Concerning the Fundamental Mechanisms of Memory.- 1.2.1 Where Do the Signals Relating to Memory Act Upon?.- 1.2.2 What Kind of Encoding is Used for Neural Signals?.- 1.2.3 What are the Variable Memory Elements?.- 1.2.4 How are Neural Signals Addressed in Memory?.- 1.3 Elementary Operations Implemented by Associative Memory.- 1.3.1 Associative Recall.- 1.3.2 Production of Sequences from the Associative Memory.- 1.3.3 On the Meaning of Background and Context.- 1.4 More Abstract Aspects of Memory.- 1.4.1 The Problem of Infinite-State Memory.- 1.4.2 Invariant Representations.- 1.4.3 Symbolic Representations.- 1.4.4 Virtual Images.- 1.4.5 The Logic of Stored Knowledge.- 2. Pattern Mathematics.- 2.1 Mathematical Notations and Methods.- 2.1.1 Vector Space Concepts.- 2.1.2 Matrix Notations.- 2.1.3 Further Properties of Matrices.- 2.1.4 Matrix Equations.- 2.1.5 Projection Operators.- 2.1.6 On Matrix Differential Calculus.- 2.2 Distance Measures for Patterns.- 2.2.1 Measures of Similarity and Distance in Vector Spaces.- 2.2.2 Measures of Similarity and Distance Between Symbol Strings.- 2.2.3 More Accurate Distance Measures for Text.- 3. Classical Learning Systems.- 3.1 The Adaptive Linear Element (Adaline).- 3.1.1 Description of Adaptation by the Stochastic Approximation.- 3.2 The Perceptron.- 3.3 The Learning Matrix.- 3.4 Physical Realization of Adaptive Weights.- 3.4.1 Perceptron and Adaline.- 3.4.2 Classical Conditioning.- 3.4.3 Conjunction Learning Switches.- 3.4.4 Digital Representation of Adaptive Circuits.- 3.4.5 Biological Components.- 4. A New Approach to Adaptive Filters.- 4.1 Survey of Some Necessary Functions.- 4.2 On the \"Transfer Function\" of the Neuron.- 4.3 Models for Basic Adaptive Units.- 4.3.1 On the Linearization of the Basic Unit.- 4.3.2 Various Cases of Adaptation Laws.- 4.3.3 Two Limit Theorems.- 4.3.4 The Novelty Detector.- 4.4 Adaptive Feedback Networks.- 4.4.1 The Autocorrelation Matrix Memory.- 4.4.2 The Novelty Filter.- 5. Self-Organizing Feature Maps.- 5.1 On the Feature Maps of the Brain.- 5.2 Formation of Localized Responses by Lateral Feedback.- 5.3 Computational Simplification of the Process.- 5.3.1 Definition of the Topology-Preserving Mapping.- 5.3.2 A Simple Two-Dimensional Self-Organizing System.- 5.4 Demonstrations of Simple Topology-Preserving Mappings.- 5.4.1 Images of Various Distributions of Input Vectors.- 5.4.2 \"The Magic TV\".- 5.4.3 Mapping by a Feeler Mechanism.- 5.5 Tonotopic Map.- 5.6 Formation of Hierarchical Representations.- 5.6.1 Taxonomy Example.- 5.6.2 Phoneme Map.- 5.7 Mathematical Treatment of Self-Organization.- 5.7.1 Ordering of Weights.- 5.7.2 Convergence Phase.- 5.8 Automatic Selection of Feature Dimensions.- 6. Optimal Associative Mappings.- 6.1 Transfer Function of an Associative Network.- 6.2 Autoassociative Recall as an Orthogonal Projection.- 6.2.1 Orthogonal Projections.- 6.2.2 Error-Correcting Properties of Projections.- 6.3 The Novelty Filter.- 6.3.1 Two Examples of Novelty Filter.- 6.3.2 Novelty Filter as an Autoassociative Memory.- 6.4 Autoassociative Encoding.- 6.4.1 An Example of Autoassociative Encoding.- 6.5 Optimal Associative Mappings.- 6.5.1 The Optimal Linear Associative Mapping.- 6.5.2 Optimal Nonlinear Associative Mappings.- 6.6 Relationship Between Associative Mapping, Linear Regression, and Linear Estimation.- 6.6.1 Relationship of the Associative Mapping to Linear Regression.- 6.6.2 Relationship of the Regression Solution to the Linear Estimator.- 6.7 Recursive Computation of the Optimal Associative Mapping.- 6.7.1 Linear Corrective Algorithms.- 6.7.2 Best Exact Solution (Gradient Projection).- 6.7.3 Best Approximate Solution (Regression).- 6.7.4 Recursive Solution in the General Case.- 6.8 Special Cases.- 6.8.1 The Correlation Matrix Memory.- 6.8.2 Relationship Between Conditional Averages and Optimal Estimator.- 7. Pattern Recognition.- 7.1 Discriminant Functions.- 7.2 Statistical Formulation of Pattern Classification.- 7.3 Comparison Methods.- 7.4 The Subspace Methods of Classification.- 7.4.1 The Basic Subspace Method.- 7.4.2 The Learning Subspace Method (LSM).- 7.5 Learning Vector Quantization.- 7.6 Feature Extraction.- 7.7 Clustering.- 7.7.1 Simple Clustering (Optimization Approach).- 7.7.2 Hierarchical Clustering (Taxonomy Approach).- 7.8 Structural Pattern Recognition Methods.- 8. More About Biological Memory.- 8.1 Physiological Foundations of Memory.- 8.1.1 On the Mechanisms of Memory in Biological Systems.- 8.1.2 Structural Features of Some Neural Networks.- 8.1.3 Functional Features of Neurons.- 8.1.4 Modelling of the Synaptic Plasticity.- 8.1.5 Can the Memory Capacity Ensue from Synaptic Changes?.- 8.2 The Unified Cortical Memory Model.- 8.2.1 The Laminar Network Organization.- 8.2.2 On the Roles of Interneurons.- 8.2.3 Representation of Knowledge Over Memory Fields.- 8.2.4 Self-Controlled Operation of Memory.- 8.3 Collateral Reading.- 8.3.1 Physiological Results Relevant to Modelling.- 8.3.2 Related Modelling.- 9. Notes on Neural Computing.- 9.1 First Theoretical Views of Neural Networks.- 9.2 Motives for the Neural Computing Research.- 9.3 What Could the Purpose of the Neural Networks be?.- 9.4 Definitions of Artificial \"Neural Computing\" and General Notes on Neural Modelling.- 9.5 Are the Biological Neural Functions Localized or Distributed?.- 9.6 Is Nonlinearity Essential to Neural Computing?.- 9.7 Characteristic Differences Between Neural and Digital Computers.- 9.7.1 The Degree of Parallelism of the Neural Networks is Still Higher than that of any \"Massively Parallel\" Digital Computer.- 9.7.2 Why the Neural Signals Cannot be Approximated by Boolean Variables.- 9.7.3 The Neural Circuits do not Implement Finite Automata.- 9.7.4 Undue Views of the Logic Equivalence of the Brain and Computers on a High Level.- 9.8 \"Connectionist Models\".- 9.9 How can the Neural Computers be Programmed?.- 10. Optical Associative Memories.- 10.1 Nonholographic Methods.- 10.2 General Aspects of Holographic Memories.- 10.3 A Simple Principle of Holographic Associative Memory.- 10.4 Addressing in Holographic Memories.- 10.5 Recent Advances of Optical Associative Memories.- Bibliography on Pattern Recognition.- References.',\n",
       "  'date': 1983,\n",
       "  'authors': ['Teuvo Kohonen'],\n",
       "  'related_topics': ['Holographic associative memory',\n",
       "   'Content-addressable memory',\n",
       "   'Autoassociative memory',\n",
       "   'Artificial neural network',\n",
       "   'Memory model',\n",
       "   'Perceptron',\n",
       "   'Learning vector quantization',\n",
       "   'Associative property',\n",
       "   'Algorithm',\n",
       "   'Computer science'],\n",
       "  'references': ['2076063813',\n",
       "   '2053186076',\n",
       "   '1992419399',\n",
       "   '2141125852',\n",
       "   '2121601095',\n",
       "   '2161160262',\n",
       "   '2186428165',\n",
       "   '2153791616',\n",
       "   '2115689562']},\n",
       " {'id': '2107636931',\n",
       "  'title': 'GTM: the generative topographic mapping',\n",
       "  'reference_count': '38',\n",
       "  'citation_count': '1,762',\n",
       "  'abstract': 'Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping, for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Christopher M. Bishop',\n",
       "   'Markus Svensén',\n",
       "   'Christopher K. I. Williams'],\n",
       "  'related_topics': ['Generative topographic map',\n",
       "   'Latent variable model',\n",
       "   'Probabilistic latent semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Expectation–maximization algorithm',\n",
       "   'Self-organizing map',\n",
       "   'Toy problem',\n",
       "   'Probability density function',\n",
       "   'Pattern recognition',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['1554663460',\n",
       "   '1679913846',\n",
       "   '2049633694',\n",
       "   '2044758663',\n",
       "   '2125027820',\n",
       "   '65738273',\n",
       "   '2146610201',\n",
       "   '2166698530',\n",
       "   '2137969290',\n",
       "   '2051719061']},\n",
       " {'id': '2122538988',\n",
       "  'title': 'Nonlinear principal component analysis using autoassociative neural networks',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '2,616',\n",
       "  'abstract': 'Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal “bottleneck” layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.',\n",
       "  'date': 1991,\n",
       "  'authors': ['Mark A. Kramer'],\n",
       "  'related_topics': ['Dimensionality reduction',\n",
       "   'Principal component analysis',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial neural network',\n",
       "   'Feature vector',\n",
       "   'Curse of dimensionality',\n",
       "   'Nonlinear system',\n",
       "   'Exploratory data analysis',\n",
       "   'Pattern recognition',\n",
       "   'Statistics',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2154642048',\n",
       "   '1965324089',\n",
       "   '2103496339',\n",
       "   '3017143921',\n",
       "   '2158863190',\n",
       "   '1507849272',\n",
       "   '2131329059',\n",
       "   '3121126077',\n",
       "   '2078626246',\n",
       "   '145476170']},\n",
       " {'id': '2047870719',\n",
       "  'title': 'Topology representing networks',\n",
       "  'reference_count': '48',\n",
       "  'citation_count': '1,138',\n",
       "  'abstract': 'Abstract A Hebbian adaptation rule with winner-take-all like competition is introduced. It is shown that this competitive Hebbian rule forms so-called Delaunay triangulations, which play an important role in computational geometry for efficiently solving proximity problems. Given a set of neural units i, i = 1,…, N, the synaptic weights of which can be interpreted as pointers wi, i = 1,…, N in RD, the competitive Hebbian rule leads to a connectivity structure between the units i that corresponds to the Delaunay triangulation of the set of pointers wi. Such competitive Hebbian rule develops connections (Cij > 0) between neural units i, j with neighboring receptive fields (Voronoi polygons) Vi, Vj, whereas between all other units i, j no connections evolve (Cij = 0). Combined with a procedure that distributes the pointers wi over a given feature manifold M, for example, a submanifold M ⊂ RD, the competitive Hebbian rule provides a novel approach to the problem of constructing topology preserving feature maps and representing intricately structured manifolds. The competitive Hebbian rule connects only neural units, the receptive fields (Voronoi polygons) Vi, Vj of which are adjacent on the given manifold M. This leads to a connectivity structure that defines a perfectly topology preserving map and forms a discrete, path preserving representation of M, also in cases where M has an intricate topology. This makes this novel approach particularly useful in all applications where neighborhood relations have to be exploited or the shape and topology of submanifolds have to be take into account.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Thomas Martinetz 1, 2, Klaus Schulten 1'],\n",
       "  'related_topics': ['Hebbian theory',\n",
       "   'Delaunay triangulation',\n",
       "   'Proximity problems',\n",
       "   'Manifold',\n",
       "   'Computational geometry',\n",
       "   'Path (graph theory)',\n",
       "   'Submanifold',\n",
       "   'Topology (chemistry)',\n",
       "   'Topology',\n",
       "   'Mathematics'],\n",
       "  'references': ['2046079134',\n",
       "   '1991848143',\n",
       "   '3017143921',\n",
       "   '65738273',\n",
       "   '22297218',\n",
       "   '2913399920',\n",
       "   '2005314985',\n",
       "   '2166322089',\n",
       "   '2002182716',\n",
       "   '2098929365']},\n",
       " {'id': '2070320140',\n",
       "  'title': 'Image representations for visual learning.',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '403',\n",
       "  'abstract': 'Computer vision researchers are developing new approaches to object recognition and detection that are based almost directly on images and avoid the use of intermediate three-dimensional models. Many of these techniques depend on a representation of images that induce a linear vector space structure and in principle requires dense feature correspondence. This image representation allows the use of learning techniques for the analysis of images (for computer vision) as well as for the synthesis of images (for computer graphics).',\n",
       "  'date': 1996,\n",
       "  'authors': ['David Beymer', 'Tomaso Poggio'],\n",
       "  'related_topics': ['Image-based modeling and rendering',\n",
       "   'Image processing',\n",
       "   'Scale-space axioms',\n",
       "   'Feature (computer vision)',\n",
       "   'Computer graphics',\n",
       "   'Visual learning',\n",
       "   'Image analysis',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2123977795',\n",
       "   '2095757522',\n",
       "   '2138835141',\n",
       "   '2135463994',\n",
       "   '2142912032',\n",
       "   '94523489',\n",
       "   '1981025032',\n",
       "   '2053197265',\n",
       "   '1963565426']},\n",
       " {'id': '1513400187',\n",
       "  'title': 'Data Structures and Network Algorithms',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '3,107',\n",
       "  'abstract': 'Foundations Disjoint Sets Heaps Search Trees Linking and Cutting Trees Minimum Spanning Trees Shortest Paths Network Flows Matchings.',\n",
       "  'date': 1982,\n",
       "  'authors': ['Robert Endre Tarjan'],\n",
       "  'related_topics': ['Weight-balanced tree',\n",
       "   'Spanning tree',\n",
       "   'Disjoint sets',\n",
       "   'Skew heap',\n",
       "   'Fibonacci heap',\n",
       "   'Expected linear time MST algorithm',\n",
       "   'Data structure',\n",
       "   'Flow network',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics'],\n",
       "  'references': ['2053186076',\n",
       "   '2141870784',\n",
       "   '2063532964',\n",
       "   '1978259121',\n",
       "   '2111366547',\n",
       "   '2078962046',\n",
       "   '2109278577',\n",
       "   '2014889099',\n",
       "   '1652775531',\n",
       "   '2149237601']},\n",
       " {'id': '2019020850',\n",
       "  'title': 'Data Visualization by Multimensional Scaling: A Deterministic Annealing Approach',\n",
       "  'reference_count': '36',\n",
       "  'citation_count': '91',\n",
       "  'abstract': 'Abstract Multidimensional scaling addresses the problem how proximity data can be faithfully visualized as points in a low-dimensional Euclidean space. The quality of a data embedding is measured by a stress function which compares proximity values with Euclidean distances of the respective points. The corresponding minimization problem is non-convex and sensitive to local minima. We present a novel deterministic annealing algorithm for the frequently used objective SSTRESS and for Sammon mapping, derived in the framework of maximum entropy estimation. Experimental results demonstrate the superiority of our optimization technique compared to conventional gradient descent methods.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Hansjoerg Klock', 'Joachim M. Buhmann'],\n",
       "  'related_topics': ['Nonlinear dimensionality reduction',\n",
       "   'Sammon mapping',\n",
       "   'Stress majorization',\n",
       "   'Gradient descent',\n",
       "   'Euclidean space',\n",
       "   'Maxima and minima',\n",
       "   'Principle of maximum entropy',\n",
       "   'Scaling',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2117812871',\n",
       "   '2581275558',\n",
       "   '1997063559',\n",
       "   '2049633694',\n",
       "   '3017143921',\n",
       "   '2095757522',\n",
       "   '2159537329',\n",
       "   '2045682702',\n",
       "   '2077990749',\n",
       "   '2148394752']},\n",
       " {'id': '2099741732',\n",
       "  'title': 'Independent component analysis, a new concept?',\n",
       "  'reference_count': '49',\n",
       "  'citation_count': '11,030',\n",
       "  'abstract': 'Abstract The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.',\n",
       "  'date': 1994,\n",
       "  'authors': ['Pierre Comon'],\n",
       "  'related_topics': ['Independent component analysis',\n",
       "   'FastICA',\n",
       "   'Principal component analysis'],\n",
       "  'references': ['1996355918',\n",
       "   '2171074980',\n",
       "   '2018388266',\n",
       "   '2098301339',\n",
       "   '2114018052',\n",
       "   '1560089794',\n",
       "   '2796930440',\n",
       "   '2225937484',\n",
       "   '1995963238',\n",
       "   '2140352766']},\n",
       " {'id': '2108384452',\n",
       "  'title': 'An information-maximization approach to blind separation and blind deconvolution',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '11,140',\n",
       "  'abstract': 'We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Anthony J. Bell', 'Terrence J. Sejnowski'],\n",
       "  'related_topics': ['Blind signal separation',\n",
       "   'Blind deconvolution',\n",
       "   'Independent component analysis',\n",
       "   'Source separation',\n",
       "   'Maximization',\n",
       "   'Infomax',\n",
       "   'Deconvolution',\n",
       "   'Information transfer',\n",
       "   'Algorithm',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2124776405',\n",
       "   '2099741732',\n",
       "   '2019502123',\n",
       "   '1996355918',\n",
       "   '2038085771',\n",
       "   '2180838288',\n",
       "   '1667165204',\n",
       "   '2096789154',\n",
       "   '2006544565',\n",
       "   '2056211671']},\n",
       " {'id': '2587818897',\n",
       "  'title': 'Decadal trends in the North Atlantic oscillation: regional temperatures and precipitation',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '8,907',\n",
       "  'abstract': 'Greenland ice-core data have revealed large decadal climate variations over the North Atlantic that can be related to a major source of low-frequency variability, the North Atlantic Oscillation. Over the past decade, the Oscillation has remained in one extreme phase during the winters, contributing significantly to the recent wintertime warmth across Europe and to cold conditions in the northwest Atlantic. An evaluation of the atmospheric moisture budget reveals coherent large-scale changes since 1980 that are linked to recent dry conditions over southern Europe and the Mediterranean, whereas northern Europe and parts of Scandinavia have generally experienced wetter than normal conditions.',\n",
       "  'date': 1995,\n",
       "  'authors': ['J. W. Hurrell'],\n",
       "  'related_topics': ['North Atlantic oscillation',\n",
       "   'Atlantic Equatorial mode',\n",
       "   'Atlantic multidecadal oscillation',\n",
       "   'Mediterranean climate',\n",
       "   'Precipitation',\n",
       "   'Climatology',\n",
       "   'Oceanography',\n",
       "   'Environmental science',\n",
       "   'Atmospheric moisture'],\n",
       "  'references': ['2793609185',\n",
       "   '2129094658',\n",
       "   '2141102773',\n",
       "   '1542727819',\n",
       "   '1959198933',\n",
       "   '2803352163',\n",
       "   '2923209360',\n",
       "   '2606802079',\n",
       "   '2097171181',\n",
       "   '2990335928']},\n",
       " {'id': '2123977795',\n",
       "  'title': 'Visual learning and recognition of 3-D objects from appearance',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '3,045',\n",
       "  'abstract': \"The problem of automatically learning object models for recognition and pose estimation is addressed. In contrast to the traditional approach, the recognition problem is formulated as one of matching appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, reflectance properties, pose in the scene, and the illumination conditions. While shape and reflectance are intrinsic properties and constant for a rigid object, pose and illumination vary from scene to scene. A compact representation of object appearance is proposed that is parametrized by pose and illumination. For each object of interest, a large set of images is obtained by automatically varying pose and illumination. This image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a manifold. Given an unknown input image, the recognition system projects the image to eigenspace. The object is recognized based on the manifold it lies on. The exact position of the projection on the manifold determines the object's pose in the image. A variety of experiments are conducted using objects with complex appearance characteristics. The performance of the recognition and pose estimation algorithms is studied using over a thousand input images of sample objects. Sensitivity of recognition to the number of eigenspace dimensions and the number of learning samples is analyzed. For the objects used, appearance representation in eigenspaces with less than 20 dimensions produces accurate recognition results with an average pose estimation error of about 1.0 degree. A near real-time recognition system with 20 complex objects in the database has been developed. The paper is concluded with a discussion on various issues related to the proposed learning and recognition methodology.\",\n",
       "  'date': 1994,\n",
       "  'authors': ['Hiroshi Murase 1, Shree K. Nayar 2'],\n",
       "  'related_topics': ['3D pose estimation',\n",
       "   'Pose',\n",
       "   '3D single-object recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Learning object',\n",
       "   'Projection (set theory)',\n",
       "   'Deep-sky object',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2170120409',\n",
       "   '2098693229',\n",
       "   '2143956139',\n",
       "   '2130259898',\n",
       "   '2135346934',\n",
       "   '2053197265',\n",
       "   '1996773532',\n",
       "   '2086479969',\n",
       "   '2026311529',\n",
       "   '2105815873']},\n",
       " {'id': '1594524188',\n",
       "  'title': 'Content-addressable memories',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '473',\n",
       "  'abstract': '1 Associative Memory, Content Addressing, and Associative Recall.- 1.1 Introduction.- 1.1.1 Various Motives for the Development of Content-Addressable Memories.- 1.1.2 Definitions and Explanations of Some Basic Concepts.- 1.2 The Two Basic Implementations of Content Addressing.- 1.2.1 Software Implementation: Hash Coding.- 1.2.2 Hardware Implementation: The CAM.- 1.3 Associations.- 1.3.1 Representation and Retrieval of Associated Items.- 1.3.2 Structures of Associations.- 1.4 Associative Recall: Extensions of Concepts.- 1.4.1 The Classical Laws of Association.- 1.4.2 Similarity Measures.- 1.4.3 The Problem of Infinite Memory.- 1.4.4 Distributed Memory and optimal Associative Mappings.- 1.4.5 Sequential Recollections.- 2 Content Addressing by Software.- 2.1 Hash Coding and Formatted Data Structures.- 2.2 Hashing Functions.- 2.3 Handling of Collisions.- 2.3.1 Some Basic Concepts.- 2.3.2 Open Addressing.- 2.3.3 Chaining (Coalesced).- 2.3.4 Chaining Through a Separate overflow Area.- 2.3.5 Rehashing.- 2.3.6 Shortcut Methods for Speedup of Searching.- 2.4 Organizational Features and Formats of Hash Tables.- 2.4.1 Direct and Indirect Addressing.- 2.4.2 Basic Formats of Hash Tables.- 2.4.3 An Example of Special Hash Table Organization.- 2.5 Evaluation of Different Schemes in Hash Coding.- 2.5.1 Average Length of Search with Different Collision Handling Methods.- 2.5.2 Effect of Hashing Function on the Length of Search.- 2.5.3 Special Considerations for the Case in Which the Search Is Unsuccessful.- 2.6 Multi-Key Search.- 2.6.1 Lists and List Structures.- 2.6.2 An Example of Implementation of Multi-Key Search by Hash Index Tables.- 2.6.3 The Use of Compound Keywords in Hashing.- 2.7 Implementation of Proximity Search by Hash Coding.- 2.8 The TRIE Memory.- 2.9 Survey of Literature on Hash Coding and Related Topics.- 3 Logic Principles of Content-Addressable Memories.- 3.1 Present-Day Needs for Hardware CAMs.- 3.2 The Logic of Comparison Operations.- 3.3 The All-Parallel CAM.- 3.3.1 Circuit Logic of a CAM Bit Cell.- 3.3.2 Handling of Responses from the CAM Array.- 3.3.3 The Complete CAM Organization.- 3.3.4 Magnitude Search with the All-Parallel CAM.- 3.4 The Word-Parallel, Bit-Serial CAM.- 3.4.1 Implementation of the CAM by the Linear-Select Memory Principle.- 3.4.2 Skew Addressing.- 3.4.3 Shift Register Implementation.- 3.4.4 The Results Storage.- 3.4.5 Searching on More Complex Specifications.- 3.5 The Word-Serial, Bit-Parallel CAM.- 3.6 Byte-Serial Content-Addressable Search.- 3.6.1 Coding by the Characters.- 3.6.2 Specifications Used in Document Retrieval.- 3.6.3 A Record-Parallel, Byte-Serial CAM for Document Retrieval.- 3.7 Functional Memories.- 3.7.1 The Logic of the Bit Cell in the FM.- 3.7.2 Functional Memory 1.- 3.7.3 Functional Memory 2.- 3.7.4 Read-only Functional Memory.- 3.8 A Formalism for the Description of Micro-Operations in the CAM.- 3.9 Survey of Literature on CAMs.- 4 CAM Hardware.- 4.1 The State-of-the-Art of the Electronic CAM Devices.- 4.2 Circuits for All-Parallel CAMs.- 4.2.1 Active Electronic Circuits for CAM Bit Cells.- 4.2.2 Cryotron-Element CAMs.- 4.2.3 Josephson Junctions and SQUIDs for Memories.- 4.3 Circuits for Bit-Serial and Word-Serial CAMs.- 4.3.1 Semiconductor RAM Modules for the CAM.- 4.3.2 Magnetic Memory Implementations of the CAM.- 4.3.3 Shift Registers for Content-Addressable Memory.- 4.3.4 The Charge-Coupled Device (CCD).- 4.3.5 The Magnetic-Bubble Memory (MBM).- 4.4 Optical Content-Addressable Memories.- 4.4.1 Magneto-Optical Memories.- 4.4.2 Holographic Content-Addressable Memories.- 5 The CAM as a System Part.- 5.1 The CAM in Virtual Memory Systems.- 5.1.1 The Memory Hierarchy.- 5.1.2 The Concept of Virtual Memory and the Cache.- 5.1.3 Memory Mappings for the Cache.- 5.1.4 Replacement Algorithms.- 5.1.5 Updating of Multilevel Memories.- 5.1.6 Automatic Control of the Cache Operations.- 5.1.7 Buffering in a Multiprocessor System.- 5.1.8 Additional Literature on Memory Organizations and Their Evaluation.- 5.2 Utilization of the CAM in Dynamic Memory Allocation.- 5.2.1 Memory Map and Address Conversion.- 5.2.2 Loading of a Program Segment.- 5.3 Content-Addressable Buffer.- 5.4 Programmable Logic.- 5.4.1 RAM Implementation.- 5.4.2 CAM Implementation.- 5.4.3 FM Implementation.- 5.4.4 Other Implementations of Programmable Logic.- 5.4.5 Applications of the CAM in Various Control Operations.- 6 Content-Addressable Processors.- 6.1. Some Trends in Content-Addressable Memory Functions.- 6.2 Distributed-Logic Memories (DLMs).- 6.3 The Augmented Content-Addressable Memory (ACAM).- 6.4 The Association-Storing Processor (ASP).- 6.5 Content-Addressable Processors with High-Level Processing Elements.- 6.5.1 The Basic Array Processor Architecture.- 6.5.2 The Associative Control Switch (ACS) Architecture.- 6.5.3 An Example of Control-Addressable Array Processors: RADCAP.- 6.5.4 An Example of Content-Addressable Ensemble Processors: PEPE.- 6.6 Bit-Slice Content-Addressable Processors.- 6.6.1 The STARAN Computer.- 6.6.2 Orthogonal Computers.- 6.7 An Overview of Parallel Processors.- 6.7.1 Categorizations of Computer Architectures.- 6.7.2 Survey of Additional Literature on Content-Addressable and Parallel Processing.- 7 Review of Research Since 1979.- 7.1 Research on Hash Coding.- 7.1.1 Review Articles.- 7.1.2 Hashing Functions.- 7.1.3 Handling of Collisions.- 7.1.4 Hash Table Organization.- 7.1.5 Linear Hashing.- 7.1.6 Dynamic, Extendible, and External Hashing.- 7.1.7 Multiple-Key and Partial-Match Hashing.- 7.1.8 Hash-Coding Applications.- 7.1.9 Hash-Coding Hardware.- 7.2 CAM Hardware.- 7.2.1 CAM Cells.- 7.2.2 CAM Arrays.- 7.2.3 Dynamic Memories.- 7.2.4 CAM Systems.- 7.3 CAM Applications.- 7.4 Content-Addressable Parallel Processors.- 7.4.1 Architectures for Content-Addressable Processors.- 7.4.2 Data Base Machines.- 7.4.3 Applications of Content-Addressable Processors.- 7.5 Optical Associative Memories.- 7.5.1 General.- 7.5.2 Holographic Content-Addressable Memories.- References.',\n",
       "  'date': 1979,\n",
       "  'authors': ['Teuvo Kohonen'],\n",
       "  'related_topics': ['Memory map',\n",
       "   'Hash table',\n",
       "   'Content-addressable memory',\n",
       "   'Hash function',\n",
       "   'Distributed memory',\n",
       "   'Linear hashing',\n",
       "   'Open addressing',\n",
       "   'Virtual memory',\n",
       "   'Parallel computing',\n",
       "   'Computer science'],\n",
       "  'references': ['2293063825',\n",
       "   '2003454866',\n",
       "   '2127971792',\n",
       "   '2346685430',\n",
       "   '1536792390',\n",
       "   '2114094169',\n",
       "   '2751979537',\n",
       "   '2143248138',\n",
       "   '2115400108']},\n",
       " {'id': '2086789740',\n",
       "  'title': 'Perceptrons: An Introduction to Computational Geometry',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '3,259',\n",
       "  'abstract': \"Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons\",\n",
       "  'date': 1968,\n",
       "  'authors': ['Marvin Lee Minsky', 'Seymour Papert'],\n",
       "  'related_topics': ['Computational geometry',\n",
       "   'Perceptron',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Cognition'],\n",
       "  'references': ['2140190241',\n",
       "   '607505555',\n",
       "   '2017337590',\n",
       "   '1736726159',\n",
       "   '2293063825',\n",
       "   '2186428165',\n",
       "   '2963305465',\n",
       "   '2962790689']},\n",
       " {'id': '2970228278',\n",
       "  'title': 'The Neurophysiological Basis of Mind',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '588',\n",
       "  'abstract': '',\n",
       "  'date': 1952,\n",
       "  'authors': ['Sir John C. Eccles'],\n",
       "  'related_topics': ['Neurophysiology',\n",
       "   'Basis (linear algebra)',\n",
       "   'Cognitive science',\n",
       "   'Psychology'],\n",
       "  'references': ['2293063825',\n",
       "   '2133659563',\n",
       "   '2414319931',\n",
       "   '2020272049',\n",
       "   '2113200940',\n",
       "   '1522901071',\n",
       "   '2144205806',\n",
       "   '2011571485',\n",
       "   '3101436801']},\n",
       " {'id': '2076870593',\n",
       "  'title': 'The neuronal basis of behavior in Tritonia. I. Functional organization of the central nervous system.',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '202',\n",
       "  'abstract': 'An account is presented of the brain (paired cerebral, pleural, and pedal ganglia) of the nudibranch mollusc Tritonia diomedia. The major efferent nerve fibers are related to their nerve cell bodies and their functional roles identified as far as possible. The channels of sesory input relating to some of these neurons are given so as to provide an overall view of the organization of the brain. A standardized system of abbreviation and notation for the central ganglia, nerve trunks, and gaint somata is proposed. The system of references is intended to provide a guide to the location in the ganglia of many of the smaller neurons of which the functional attributes are known, but which cannot be consistently recognized on visual criteria alone. A system of rectangular coordinates is proposed which is applied to the natural outline of the ganglia. In addition, a system of cell notation is described which is independent of the co-ordinates used to define the position of the cell on the grid. Cell which by reason of their size, pigmentation, characteristic location and physiological attributes are consistently recognizable from animal to animal are numbered. Two principles were followed in numbering cells; (i) the series begins at unity in each ganglion; (ii) cell homologues in opposite ganglia are given the same number, but distinguished by prefixing the abbreviation for the ganglion in which they occur. It is considered that the system will facilitate the exchange of information between workers on the same species, and also benefit the comparison of neural organization of behavior in closely related forms. The brain is organized in an almost exactly bilaterally symmetrical manner. There are a few bilateral neural pathways, but the major functional routes are ipsilateral. A few motorneurons, which are uniquely identifiable anatomically, cause unique, discrete movements. Others are in small groups sharing overlapping or similar functions.',\n",
       "  'date': 1972,\n",
       "  'authors': ['A. O. D. Willows 1, D. A. Dorsett 1, G. Hoyle 2'],\n",
       "  'related_topics': ['Efferent nerve',\n",
       "   'Ganglion',\n",
       "   'Central nervous system',\n",
       "   'Neuroscience',\n",
       "   'Tritonia (gastropod)',\n",
       "   'Anatomy',\n",
       "   'Basis (universal algebra)',\n",
       "   'Biology',\n",
       "   'Cell bodies',\n",
       "   'Functional organization',\n",
       "   'Rectangular coordinates'],\n",
       "  'references': ['2167531287',\n",
       "   '1910258660',\n",
       "   '2074308372',\n",
       "   '1910429665',\n",
       "   '2167089286',\n",
       "   '2076915589',\n",
       "   '1991985107',\n",
       "   '2055408552',\n",
       "   '2042776884',\n",
       "   '2020100877']},\n",
       " {'id': '2137983211',\n",
       "  'title': 'Multilayer feedforward networks are universal approximators',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '21,034',\n",
       "  'abstract': 'Abstract This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.',\n",
       "  'date': 1989,\n",
       "  'authors': ['K. Hornik 1, M. Stinchcombe 2, H. White 2'],\n",
       "  'related_topics': ['Universal approximation theorem',\n",
       "   'Borel measure',\n",
       "   'Feed forward',\n",
       "   'Function (mathematics)',\n",
       "   'Stone–Weierstrass theorem',\n",
       "   'Algorithm',\n",
       "   'Degree (graph theory)',\n",
       "   'Class (philosophy)',\n",
       "   'Mathematics',\n",
       "   'Finite dimensional space'],\n",
       "  'references': ['2103496339',\n",
       "   '3146803896',\n",
       "   '2056099894',\n",
       "   '2416739038',\n",
       "   '2090270852',\n",
       "   '1654142532',\n",
       "   '1581292930',\n",
       "   '2097415784',\n",
       "   '135768573']},\n",
       " {'id': '2140196014',\n",
       "  'title': 'The JPEG still picture compression standard',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '7,421',\n",
       "  'abstract': \"A joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG's proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT (discrete cosine transform)-based method is specified for 'lossy' compression, and a predictive method for 'lossless' compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. The author provides an overview of the JPEG standard, and focuses in detail on the Baseline method. >\",\n",
       "  'date': 1992,\n",
       "  'authors': ['G.K. Wallace'],\n",
       "  'related_topics': ['Lossless JPEG',\n",
       "   'JPEG 2000',\n",
       "   'JPEG',\n",
       "   'JPEG File Interchange Format',\n",
       "   'Quantization (image processing)',\n",
       "   'Lossy compression',\n",
       "   'Data compression',\n",
       "   'Compression artifact',\n",
       "   'Lossless compression',\n",
       "   'Transform coding',\n",
       "   'Discrete cosine transform',\n",
       "   'Signal compression',\n",
       "   'Grayscale',\n",
       "   'Digital image',\n",
       "   'Quantization (signal processing)',\n",
       "   'Image processing',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2019972422',\n",
       "   '2026723094',\n",
       "   '1501108238',\n",
       "   '2073501560',\n",
       "   '2162168771',\n",
       "   '1572731687',\n",
       "   '1561761812',\n",
       "   '2150013946',\n",
       "   '1992371956']},\n",
       " {'id': '1634005169',\n",
       "  'title': 'Vector Quantization and Signal Compression',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '9,417',\n",
       "  'abstract': '1 Introduction.- 1.1 Signals, Coding, and Compression.- 1.2 Optimality.- 1.3 How to Use this Book.- 1.4 Related Reading.- I Basic Tools.- 2 Random Processes and Linear Systems.- 2.1 Introduction.- 2.2 Probability.- 2.3 Random Variables and Vectors.- 2.4 Random Processes.- 2.5 Expectation.- 2.6 Linear Systems.- 2.7 Stationary and Ergodic Properties.- 2.8 Useful Processes.- 2.9 Problems.- 3 Sampling.- 3.1 Introduction.- 3.2 Periodic Sampling.- 3.3 Noise in Sampling.- 3.4 Practical Sampling Schemes.- 3.5 Sampling Jitter.- 3.6 Multidimensional Sampling.- 3.7 Problems.- 4 Linear Prediction.- 4.1 Introduction.- 4.2 Elementary Estimation Theory.- 4.3 Finite-Memory Linear Prediction.- 4.4 Forward and Backward Prediction.- 4.5 The Levinson-Durbin Algorithm.- 4.6 Linear Predictor Design from Empirical Data.- 4.7 Minimum Delay Property.- 4.8 Predictability and Determinism.- 4.9 Infinite Memory Linear Prediction.- 4.10 Simulation of Random Processes.- 4.11 Problems.- II Scalar Coding.- 5 Scalar Quantization I.- 5.1 Introduction.- 5.2 Structure of a Quantizer.- 5.3 Measuring Quantizer Performance.- 5.4 The Uniform Quantizer.- 5.5 Nonuniform Quantization and Companding.- 5.6 High Resolution: General Case.- 5.7 Problems.- 6 Scalar Quantization II.- 6.1 Introduction.- 6.2 Conditions for Optimality.- 6.3 High Resolution Optimal Companding.- 6.4 Quantizer Design Algorithms.- 6.5 Implementation.- 6.6 Problems.- 7 Predictive Quantization.- 7.1 Introduction.- 7.2 Difference Quantization.- 7.3 Closed-Loop Predictive Quantization.- 7.4 Delta Modulation.- 7.5 Problems.- 8 Bit Allocation and Transform Coding.- 8.1 Introduction.- 8.2 The Problem of Bit Allocation.- 8.3 Optimal Bit Allocation Results.- 8.4 Integer Constrained Allocation Techniques.- 8.5 Transform Coding.- 8.6 Karhunen-Loeve Transform.- 8.7 Performance Gain of Transform Coding.- 8.8 Other Transforms.- 8.9 Sub-band Coding.- 8.10 Problems.- 9 Entropy Coding.- 9.1 Introduction.- 9.2 Variable-Length Scalar Noiseless Coding.- 9.3 Prefix Codes.- 9.4 Huffman Coding.- 9.5 Vector Entropy Coding.- 9.6 Arithmetic Coding.- 9.7 Universal and Adaptive Entropy Coding.- 9.8 Ziv-Lempel Coding.- 9.9 Quantization and Entropy Coding.- 9.10 Problems.- III Vector Coding.- 10 Vector Quantization I.- 10.1 Introduction.- 10.2 Structural Properties and Characterization.- 10.3 Measuring Vector Quantizer Performance.- 10.4 Nearest Neighbor Quantizers.- 10.5 Lattice Vector Quantizers.- 10.6 High Resolution Distortion Approximations.- 10.7 Problems.- 11 Vector Quantization II.- 11.1 Introduction.- 11.2 Optimality Conditions for VQ.- 11.3 Vector Quantizer Design.- 11.4 Design Examples.- 11.5 Problems.- 12 Constrained Vector Quantization.- 12.1 Introduction.- 12.2 Complexity and Storage Limitations.- 12.3 Structurally Constrained VQ.- 12.4 Tree-Structured VQ.- 12.5 Classified VQ.- 12.6 Transform VQ.- 12.7 Product Code Techniques.- 12.8 Partitioned VQ.- 12.9 Mean-Removed VQ.- 12.10 Shape-Gain VQ.- 12.11 Multistage VQ.- 12.12 Constrained Storage VQ.- 12.13 Hierarchical and Multiresolution VQ.- 12.14 Nonlinear Interpolative VQ.- 12.15 Lattice Codebook VQ.- 12.16 Fast Nearest Neighbor Encoding.- 12.17 Problems.- 13 Predictive Vector Quantization.- 13.1 Introduction.- 13.2 Predictive Vector Quantization.- 13.3 Vector Linear Prediction.- 13.4 Predictor Design from Empirical Data.- 13.5 Nonlinear Vector Prediction.- 13.6 Design Examples.- 13.7 Problems.- 14 Finite-State Vector Quantization.- 14.1 Recursive Vector Quantizers.- 14.2 Finite-State Vector Quantizers.- 14.3 Labeled-States and Labeled-Transitions.- 14.4 Encoder/Decoder Design.- 14.5 Next-State Function Design.- 14.6 Design Examples.- 14.7 Problems.- 15 Tree and Trellis Encoding.- 15.1 Delayed Decision Encoder.- 15.2 Tree and Trellis Coding.- 15.3 Decoder Design.- 15.4 Predictive Trellis Encoders.- 15.5 Other Design Techniques.- 15.6 Problems.- 16 Adaptive Vector Quantization.- 16.1 Introduction.- 16.2 Mean Adaptation.- 16.3 Gain-Adaptive Vector Quantization.- 16.4 Switched Codebook Adaptation.- 16.5 Adaptive Bit Allocation.- 16.6 Address VQ.- 16.7 Progressive Code Vector Updating.- 16.8 Adaptive Codebook Generation.- 16.9 Vector Excitation Coding.- 16.10 Problems.- 17 Variable Rate Vector Quantization.- 17.1 Variable Rate Coding.- 17.2 Variable Dimension VQ.- 17.3 Alternative Approaches to Variable Rate VQ.- 17.4 Pruned Tree-Structured VQ.- 17.5 The Generalized BFOS Algorithm.- 17.6 Pruned Tree-Structured VQ.- 17.7 Entropy Coded VQ.- 17.8 Greedy Tree Growing.- 17.9 Design Examples.- 17.10 Bit Allocation Revisited.- 17.11 Design Algorithms.- 17.12 Problems.',\n",
       "  'date': 1990,\n",
       "  'authors': ['Allen Gersho 1, Robert M. Gray 2'],\n",
       "  'related_topics': ['Vector quantization',\n",
       "   'Quantization (signal processing)',\n",
       "   'Linde–Buzo–Gray algorithm',\n",
       "   'Huffman coding',\n",
       "   'Arithmetic coding',\n",
       "   'Entropy encoding',\n",
       "   'Coding theory',\n",
       "   'Transform coding',\n",
       "   'Algorithm',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics'],\n",
       "  'references': ['2140190241',\n",
       "   '2160547390',\n",
       "   '2116467012',\n",
       "   '2141362318',\n",
       "   '2111918405',\n",
       "   '2147717514',\n",
       "   '2161160262',\n",
       "   '1976709621',\n",
       "   '2186428165',\n",
       "   '1501500081']},\n",
       " {'id': '3146803896',\n",
       "  'title': 'Multilayer feedforward networks are universal approximators',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '14,898',\n",
       "  'abstract': '',\n",
       "  'date': 1989,\n",
       "  'authors': ['HornikK.', 'StinchcombeM.', 'WhiteH.'],\n",
       "  'related_topics': ['Universal approximation theorem',\n",
       "   'Feed forward',\n",
       "   'Computer science',\n",
       "   'Control theory'],\n",
       "  'references': ['2963207607',\n",
       "   '2076063813',\n",
       "   '2605350416',\n",
       "   '2180612164',\n",
       "   '2734408173',\n",
       "   '2137983211',\n",
       "   '2593414223']},\n",
       " {'id': '1971735090',\n",
       "  'title': 'On the approximate realization of continuous mappings by neural networks',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '6,046',\n",
       "  'abstract': \"Abstract In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.\",\n",
       "  'date': 1989,\n",
       "  'authors': ['K. Funahashi'],\n",
       "  'related_topics': ['Universal approximation theorem',\n",
       "   'Sigmoid function',\n",
       "   'Artificial neural network',\n",
       "   'Realization (systems)',\n",
       "   'Backpropagation',\n",
       "   'Point (geometry)',\n",
       "   'Topology',\n",
       "   'Discrete mathematics',\n",
       "   'Unit (ring theory)',\n",
       "   'Mathematics',\n",
       "   'Hidden layer'],\n",
       "  'references': ['2154642048',\n",
       "   '2042264548',\n",
       "   '1554576613',\n",
       "   '3036751298',\n",
       "   '1613359937',\n",
       "   '3108739439',\n",
       "   '2152088994',\n",
       "   '3040500874',\n",
       "   '2105393299',\n",
       "   '2189011649']},\n",
       " {'id': '2913399920',\n",
       "  'title': 'Vector quantization',\n",
       "  'reference_count': '44',\n",
       "  'citation_count': '4,386',\n",
       "  'abstract': 'A vector quantizer is a system for mapping a sequence of continuous or discrete vectors into a digital sequence suitable for communication over or storage in a digital channel. The goal of such a system is data compression: to reduce the bit rate so as to minimize communication channel capacity or digital storage memory requirements while maintaining the necessary fidelity of the data. The mapping for each vector may or may not have memory in the sense of depending on past actions of the coder, just as in well established scalar techniques such as PCM, which has no memory, and predictive quantization, which does. Even though information theory implies that one can always obtain better performance by coding vectors instead of scalars, scalar quantizers have remained by far the most common data compression system because of their simplicity and good performance when the communication rate is sufficiently large. In addition, relatively few design techniques have existed for vector quantizers. During the past few years several design algorithms have been developed for a variety of vector quantizers and the performance of these codes has been studied for speech waveforms, speech linear predictive parameter vectors, images, and several simulated random processes. It is the purpose of this article to survey some of these design techniques and their applications.',\n",
       "  'date': 1984,\n",
       "  'authors': ['R. Gray'],\n",
       "  'related_topics': ['Vector quantization',\n",
       "   'Quantization (signal processing)',\n",
       "   'Speech coding',\n",
       "   'Linde–Buzo–Gray algorithm',\n",
       "   'Data compression',\n",
       "   'Information theory',\n",
       "   'Speech processing',\n",
       "   'Scalar (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Computer science'],\n",
       "  'references': ['2134383396',\n",
       "   '2127218421',\n",
       "   '1995875735',\n",
       "   '2583466288',\n",
       "   '2142228262',\n",
       "   '2021760654',\n",
       "   '2164240509',\n",
       "   '2150418026',\n",
       "   '2119352491',\n",
       "   '2040336387']},\n",
       " {'id': '2096710051',\n",
       "  'title': 'Detection of signals by information theoretic criteria',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '4,275',\n",
       "  'abstract': 'A new approach is presented to the problem of detecting the number of signals in a multichannel time-series, based on the application of the information theoretic criteria for model selection introduced by Akaike (AIC) and by Schwartz and Rissanen (MDL). Unlike the conventional hypothesis testing based approach, the new approach does not requite any subjective threshold settings; the number of signals is obtained merely by minimizing the AIC or the MDL criteria. Simulation results that illustrate the performance of the new method for the detection of the number of signals received by a sensor array are presented.',\n",
       "  'date': 1985,\n",
       "  'authors': ['M. Wax', 'T. Kailath'],\n",
       "  'related_topics': ['Akaike information criterion',\n",
       "   'Statistical hypothesis testing',\n",
       "   'Model selection',\n",
       "   'Information theory',\n",
       "   'Detection theory',\n",
       "   'Sensor array',\n",
       "   'Covariance matrix',\n",
       "   'Signal processing',\n",
       "   'Algorithm',\n",
       "   'Electronic engineering',\n",
       "   'Mathematics'],\n",
       "  'references': ['2168175751',\n",
       "   '2142635246',\n",
       "   '2113638573',\n",
       "   '2058815839',\n",
       "   '2054658115',\n",
       "   '2106596127',\n",
       "   '1974513581',\n",
       "   '2019833178',\n",
       "   '2165887549',\n",
       "   '1500470240']},\n",
       " {'id': '2017977879',\n",
       "  'title': 'Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '5,977',\n",
       "  'abstract': 'Abstract Locally weighted regression, or loess, is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. With local fitting we can estimate a much wider class of regression surfaces than with the usual classes of parametric functions, such as polynomials. The goal of this article is to show, through applications, how loess can be used for three purposes: data exploration, diagnostic checking of parametric models, and providing a nonparametric regression surface. Along the way, the following methodology is introduced: (a) a multivariate smoothing procedure that is an extension of univariate locally weighted regression; (b) statistical procedures that are analogous to those used in the least-squares fitting of parametric functions; (c) several graphical methods that are useful tools for understanding loess estimates and checking the a...',\n",
       "  'date': 1988,\n",
       "  'authors': ['William S. Cleveland 1, Susan J. Devlin 2'],\n",
       "  'related_topics': ['Local regression',\n",
       "   'Nonparametric regression',\n",
       "   'Regression diagnostic',\n",
       "   'Segmented regression',\n",
       "   'Polynomial regression',\n",
       "   'Regression analysis',\n",
       "   'Smoothing',\n",
       "   'Univariate',\n",
       "   'Statistics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2611591252',\n",
       "   '1506069954',\n",
       "   '2024081693',\n",
       "   '2166163519',\n",
       "   '2091886411',\n",
       "   '2801830100',\n",
       "   '2030748132',\n",
       "   '3000332379',\n",
       "   '2112081648',\n",
       "   '2025320861']},\n",
       " {'id': '2166116275',\n",
       "  'title': 'Universal approximation bounds for superpositions of a sigmoidal function',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '2,981',\n",
       "  'abstract': 'Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings. >',\n",
       "  'date': 1993,\n",
       "  'authors': ['A.R. Barron'],\n",
       "  'related_topics': ['Approximation error',\n",
       "   'Linear approximation',\n",
       "   'Function approximation',\n",
       "   'Universal approximation theorem',\n",
       "   'Approximation theory',\n",
       "   'Sigmoid function',\n",
       "   'Series expansion',\n",
       "   'Mean squared error',\n",
       "   'Applied mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2137983211',\n",
       "   '2103496339',\n",
       "   '2084544490',\n",
       "   '2095734615',\n",
       "   '1559907478',\n",
       "   '2044828368',\n",
       "   '2108959409',\n",
       "   '2095546965',\n",
       "   '2112027492',\n",
       "   '2151029520']},\n",
       " {'id': '5731987',\n",
       "  'title': 'Original Contribution: Principal components, minor components, and linear neural networks',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '1,160',\n",
       "  'abstract': 'Many neural network realizations have been recently proposed for the statistical technique of Principal Component Analysis (PCA). Explicit connections between numerical constrained adaptive algorithms and neural networks with constrained Hebbian learning rules are reviewed. The Stochastic Gradient Ascent (SGA) neural network is proposed and shown to be closely related to the Generalized Hebbian Algorithm (GHA). The SGA behaves better for extracting the less dominant eigenvectors. The SGA algorithm is further extended to the case of learning minor components. The symmetrical Subspace Network is known to give a rotated basis of the dominant eigenvector subspace, but usually not the true eigenvectors themselves. Two extensions are proposed: in the first one, each neuron has a scalar parameter which breaks the symmetry. True eigenvectors are obtained in a local and fully parallel learning rule. In the second one, the case of an arbitrary number of parallel neurons is considered, not necessarily less than the input vector dimension.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Erkki Oja'],\n",
       "  'related_topics': ['Generalized Hebbian Algorithm',\n",
       "   \"Oja's rule\",\n",
       "   'Artificial neural network',\n",
       "   'Hebbian theory',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Principal component analysis',\n",
       "   'Gradient descent',\n",
       "   'Subspace topology',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2115907784',\n",
       "   '2131329059',\n",
       "   '2122925692',\n",
       "   '2078626246',\n",
       "   '2432567885',\n",
       "   '2023963201',\n",
       "   '2017257315',\n",
       "   '1564660545',\n",
       "   '2133884101',\n",
       "   '1981479913']},\n",
       " {'id': '2142228262',\n",
       "  'title': 'Asymptotically optimal block quantization',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '1,347',\n",
       "  'abstract': 'In 1948 W. R. Bennett used a companding model for nonuniform quantization and proposed the formula D \\\\: = \\\\: \\\\frac{1}{12N^{2}} \\\\: \\\\int \\\\: p(x) [ E(x) ]^{-2} \\\\dx for the mean-square quantizing error where N is the number of levels, p (x) is the probability density of the input, and E \\\\prime (x) is the slope of the compressor curve. The formula, an approximation based on the assumption that the number of levels is large and overload distortion is negligible, is a useful tool for analytical studies of quantization. This paper gives a heuristic argument generalizing Bennett\\'s formula to block quantization where a vector of random variables is quantized. The approach is again based on the asymptotic situation where N , the number of quantized output vectors, is very large. Using the resulting heuristic formula, an optimization is performed leading to an expression for the minimum quantizing noise attainable for any block quantizer of a given block size k . The results are consistent with Zador\\'s results and specialize to known results for the one- and two-dimensional cases and for the case of infinite block length (k \\\\rightarrow \\\\infty) . The same heuristic approach also gives an alternate derivation of a bound of Elias for multidimensional quantization. Our approach leads to a rigorous method for obtaining upper bounds on the minimum distortion for block quantizers. In particular, for k = 3 we give a tight upper bound that may in fact be exact. The idea of representing a block quantizer by a block \"compressor\" mapping followed with an optimal quantizer for uniformly distributed random vectors is also explored. It is not always possible to represent an optimal quantizer with this block companding model.',\n",
       "  'date': 1979,\n",
       "  'authors': ['A. Gersho'],\n",
       "  'related_topics': ['Quantization (signal processing)',\n",
       "   'Upper and lower bounds',\n",
       "   'Heuristic argument',\n",
       "   'Block size',\n",
       "   'Random variable',\n",
       "   'Asymptotically optimal algorithm',\n",
       "   'Companding',\n",
       "   'Multidimensional systems',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['1978382377',\n",
       "   '2146519989',\n",
       "   '2004003571',\n",
       "   '2156908459',\n",
       "   '2001968606',\n",
       "   '2137263269',\n",
       "   '1589055062',\n",
       "   '2068071220',\n",
       "   '1972736931',\n",
       "   '2152316618']},\n",
       " {'id': '2063971957',\n",
       "  'title': 'Self-organizing neural network that discovers surfaces in random-dot stereograms',\n",
       "  'reference_count': '3',\n",
       "  'citation_count': '468',\n",
       "  'abstract': 'The standard form of back-propagation learning is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other. The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discovery depth in random dot stereograms of curved surfaces.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Suzanna Becker', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Perceptual learning',\n",
       "   'Artificial neural network',\n",
       "   'Modality (human–computer interaction)',\n",
       "   'Random dot stereogram',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object (computer science)',\n",
       "   'Backpropagation',\n",
       "   'Computer vision',\n",
       "   'Perception',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1498436455', '2797583072', '1944592753']},\n",
       " {'id': '2079782346',\n",
       "  'title': 'Statistical theory of learning curves under entropic loss criterion',\n",
       "  'reference_count': '47',\n",
       "  'citation_count': '215',\n",
       "  'abstract': 'The present paper elucidates a universal property of learning curves, which shows how the generalization error, training error, and the complexity of the underlying stochastic machine are related and how the behavior of a stochastic machine is improved as the number of training examples increases. The error is measured by the entropic loss. It is proved that the generalization error converges to H0, the entropy of the conditional distribution of the true machine, as H0 + m*/(2t), while the training error converges as H0-m*/(2t), where t is the number of examples and m* shows the complexity of the network. When the model is faithful, implying that the true machine is in the model, m* is reduced to m, the number of modifiable parameters. This is a universal law because it holds for any regular machine irrespective of its structure under the maximum likelihood estimator. Similar relations are obtained for the Bayes and Gibbs learning algorithms. These learning curves show the relation among the accuracy of learning, the complexity of a model, and the number of training examples.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Shun-Ichi Amari', 'Noboru Murata'],\n",
       "  'related_topics': ['Early stopping',\n",
       "   'Artificial neural network',\n",
       "   'Generalization',\n",
       "   'Entropy (information theory)',\n",
       "   'Conditional probability distribution',\n",
       "   'Statistical theory',\n",
       "   'Stochastic modelling',\n",
       "   'Learning curve',\n",
       "   'Applied mathematics',\n",
       "   'Mathematics',\n",
       "   'Statistics'],\n",
       "  'references': ['2154642048',\n",
       "   '2142635246',\n",
       "   '2165758113',\n",
       "   '2019363670',\n",
       "   '2020246210',\n",
       "   '1520168181',\n",
       "   '1968908999',\n",
       "   '2098545770',\n",
       "   '3137895569',\n",
       "   '2322002063']},\n",
       " {'id': '2089419199',\n",
       "  'title': 'Asymptotic quantization error of continuous signals and the quantization dimension',\n",
       "  'reference_count': '4',\n",
       "  'citation_count': '630',\n",
       "  'abstract': \"Extensions of the limiting qnanfizafion error formula of Bennet are proved. These are of the form D_{s,k}(N,F)=N^{-\\\\beta}B , where N is the number of output levels, D_{s,k}(N,F) is the s th moment of the metric distance between quantizer input and output, \\\\beta,B>0,k=s/\\\\beta is the signal space dimension, and F is the signal distribution. If a suitably well-behaved k -dimensional signal density f(x) exists, B=b_{s,k}[\\\\int f^{\\\\rho}(x)dx]^{1/ \\\\rho},\\\\rho=k/(s+k) , and b_{s,k} does not depend on f . For k=1,s=2 this reduces to Bennett's formula. If F is the Cantor distribution on [0,1],0 and this k equals the fractal dimension of the Cantor set [12,13] . Random quantization, optimal quantization in the presence of an output information constraint, and quantization noise in high dimensional spaces are also investigated.\",\n",
       "  'date': 1982,\n",
       "  'authors': ['P. Zador'],\n",
       "  'related_topics': ['Cantor distribution',\n",
       "   'Quantization (signal processing)',\n",
       "   'Cantor set',\n",
       "   'Fractal',\n",
       "   'Combinatorics',\n",
       "   'Discrete mathematics',\n",
       "   'Fractal dimension',\n",
       "   'Mathematics',\n",
       "   'High dimensional',\n",
       "   'Limiting',\n",
       "   'Space dimension'],\n",
       "  'references': ['2142228262', '1973387369', '1976356564', '2001968606']},\n",
       " {'id': '2162604518',\n",
       "  'title': 'Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Laplacian sources',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '132',\n",
       "  'abstract': 'Lattice vector quantizer design procedures for nonuniform sources are presented. The procedures yield lattice vector quantizers with excellent performance and retaining the structure required for fast quantization. Analytical methods for truncating and scaling lattices to be used in vector quantizations are given, and their utility is demonstrated for independent and identically distributed (i.i.d.) Gaussian and Laplacian sources. An analytical technique for piecewise linear multidimensional compandor designs is evaluated for i.i.d. Gaussian and Laplacian sources by comparing its performance to that of the other vector quantizers. >',\n",
       "  'date': 1993,\n",
       "  'authors': ['D.G. Jeong', 'J.D. Gibson'],\n",
       "  'related_topics': ['Vector Laplacian',\n",
       "   'Vector quantization',\n",
       "   'Quantization (signal processing)',\n",
       "   'Gaussian',\n",
       "   'Piecewise linear function',\n",
       "   'Piecewise',\n",
       "   'Multidimensional systems',\n",
       "   'Laplace operator',\n",
       "   'Applied mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['1634005169',\n",
       "   '2134383396',\n",
       "   '2186435531',\n",
       "   '1565930783',\n",
       "   '2142228262',\n",
       "   '2801840425',\n",
       "   '2119352491',\n",
       "   '2063678710',\n",
       "   '2089419199',\n",
       "   '2029495080']},\n",
       " {'id': '2155487652',\n",
       "  'title': 'On Edge Detection',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '1,321',\n",
       "  'abstract': 'Edge detection is the process that attempts to characterize the intensity changes in the image in terms of the physical processes that have originated them. A critical, intermediate goal of edge detection is the detection and characterization of significant intensity changes. This paper discusses this part of the edge detection problem. To characterize the types of intensity changes derivatives of different types, and possibly different scales, are needed. Thus, we consider this part of edge detection as a problem in numerical differentiation. We show that numerical differentiation of images is an ill-posed problem in the sense of Hadamard. Differentiation needs to be regularized by a regularizing filtering operation before differentiation. This shows that this part of edge detection consists of two steps, a filtering step and a differentiation step. Following this perspective, the paper discusses in detail the following theoretical aspects of edge detection. 1) The properties of different types of filters-with minimal uncertainty, with a bandpass spectrum, and with limited support-are derived. Minimal uncertainty filters optimize a tradeoff between computational efficiency and regularizing properties. 2) Relationships among several 2-D differential operators are established. In particular, we characterize the relation between the Laplacian and the second directional derivative along the gradient. Zero crossings of the Laplacian are not the only features computed in early vision. 3) Geometrical and topological properties of the zero crossings of differential operators are studied in terms of transversality and Morse theory.',\n",
       "  'date': 1986,\n",
       "  'authors': ['Vincent Torre 1, Tomaso A. Poggio 2'],\n",
       "  'related_topics': ['Edge detection',\n",
       "   'Numerical differentiation',\n",
       "   'Laplace operator',\n",
       "   'Directional derivative',\n",
       "   'Differential operator',\n",
       "   'Hadamard transform',\n",
       "   'Morse theory',\n",
       "   'Regularization (mathematics)',\n",
       "   'Algorithm',\n",
       "   'Mathematical analysis',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematics'],\n",
       "  'references': ['2740373864',\n",
       "   '2109863423',\n",
       "   '2003370853',\n",
       "   '2006500012',\n",
       "   '1995756857',\n",
       "   '2133155955',\n",
       "   '2007057443',\n",
       "   '2121203842',\n",
       "   '2038584908',\n",
       "   '2073974819']},\n",
       " {'id': '1995169133',\n",
       "  'title': 'Boltzmann machines for speech recognition',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '101',\n",
       "  'abstract': 'Boltzmann machines offer a new and exciting approach to automatic speech recognition, and provide a rigorous mathematical formalism for parallel computing arrays. In this paper we briefly summarize Boltzmann machine theory, and present results showing their ability to recognize both static and time-varying speech patterns. A machine with 2000 units was able to distinguish between the 11 steady-state vowels in English with an accuracy of 85%. The stability of the learning algorithm and methods of preprocessing and coding speech data before feeding it to the machine are also discussed. A new type of unit called a carry input unit, which involves a type of state-feedback, was developed for the processing of time-varying patterns and this was tested on a few short sentences. Use is made of the implications of recent work into associative memory, and the modelling of neural arrays to suggest a good configuration of Boltzmann machines for this sort of pattern recognition.',\n",
       "  'date': 1986,\n",
       "  'authors': ['R. W. Prager', 'T. D. Harrison', 'F. Fallside'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Content-addressable memory',\n",
       "   'sort',\n",
       "   'Speech recognition',\n",
       "   'Preprocessor',\n",
       "   'Coding (social sciences)',\n",
       "   'Boltzmann constant',\n",
       "   'Computer science',\n",
       "   'Speech patterns'],\n",
       "  'references': ['2581275558',\n",
       "   '1652505363',\n",
       "   '1997063559',\n",
       "   '2293063825',\n",
       "   '1507849272',\n",
       "   '2171850596',\n",
       "   '2112325651',\n",
       "   '2056760934',\n",
       "   '2157629899',\n",
       "   '1981025738']},\n",
       " {'id': '2591802459',\n",
       "  'title': 'G‐maximization: an unsupervised learning procedure for discovering regularities',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '68',\n",
       "  'abstract': 'Hill climbing is used to maximize an information theoretic measure of the difference betwen the actual behavior of a unit and the behavior that would be predicted by a statistician who knew the first order statistics of the inputs but believed them to be independent. This causes the unit to detect higher order correlations among its inputs. Initial simulations are presented, and seem encouraging. We describe an extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Barak A. Pearlmutter', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['Competitive learning',\n",
       "   'Unsupervised learning',\n",
       "   'Population',\n",
       "   'Hill climbing',\n",
       "   'Maximization',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Measure (mathematics)',\n",
       "   'Statistician',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Mathematics'],\n",
       "  'references': ['1993197592',\n",
       "   '1489504112',\n",
       "   '1994618660',\n",
       "   '2164152532',\n",
       "   '2123806929',\n",
       "   '1971074050',\n",
       "   '3142170516',\n",
       "   '2056415038',\n",
       "   '1978443256',\n",
       "   '2098429061']},\n",
       " {'id': '2010581677',\n",
       "  'title': 'A Theory of Adaptive Pattern Classifiers',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '710',\n",
       "  'abstract': 'This paper describes error-correction adjustment procedures for determining the weight vector of linear pattern classifiers under general pattern distribution. It is mainly aimed at clarifying theoretically the performance of adaptive pattern classifiers. In the case where the loss depends on the distance between a pattern vector and a decision boundary and where the average risk function is unimodal, it is proved that, by the procedures proposed here, the weight vector converges to the optimal one even under nonseparable pattern distributions. The speed and the accuracy of convergence are analyzed, and it is shown that there is an important tradeoff between speed and accuracy of convergence. Dynamical behaviors, when the probability distributions of patterns are changing, are also shown. The theory is generalized and made applicable to the case with general discriminant functions, including piecewise-linear discriminant functions.',\n",
       "  'date': 1967,\n",
       "  'authors': ['Shunichi Amari'],\n",
       "  'related_topics': ['Probability distribution',\n",
       "   'Decision boundary',\n",
       "   'Distribution (mathematics)',\n",
       "   'Discriminant',\n",
       "   'Weight',\n",
       "   'Convergence (routing)',\n",
       "   'Function (mathematics)',\n",
       "   'Adaptive system',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2161278885',\n",
       "   '2160133692',\n",
       "   '2065973527',\n",
       "   '2041273609',\n",
       "   '2079724156']},\n",
       " {'id': '2121947440',\n",
       "  'title': 'Normalized cuts and image segmentation',\n",
       "  'reference_count': '25',\n",
       "  'citation_count': '19,516',\n",
       "  'abstract': 'We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Jianbo Shi 1, J. Malik 2'],\n",
       "  'related_topics': ['Image segmentation',\n",
       "   'Spectral clustering',\n",
       "   'Minimum spanning tree-based segmentation',\n",
       "   'Graph partition',\n",
       "   'Graph (abstract data type)',\n",
       "   'Spectral graph theory',\n",
       "   'Graph theory',\n",
       "   'Cluster analysis',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2798909945',\n",
       "   '1578099820',\n",
       "   '1997063559',\n",
       "   '1971784203',\n",
       "   '2114487471',\n",
       "   '2913192828',\n",
       "   '2114030927',\n",
       "   '2132603077',\n",
       "   '100944330']},\n",
       " {'id': '108654854',\n",
       "  'title': 'Higher eigenvalues and isoperimetric inequalities on Riemannian manifolds and graphs',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '109',\n",
       "  'abstract': '5 Analysis on weighted graphs 23 5.1 Measures on graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.2 Discrete Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.3 Green’s formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 5.4 Integration versus Summation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.5 Eigenvalues of Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.6 Heat kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.7 Co-area formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Research supported in part by NSF Grant No. DMS 98-01446 Supported by EPSRC Fellowship B/94/AF/1782 Supported in part by NSF Grant No. DMS 95-04834',\n",
       "  'date': 1999,\n",
       "  'authors': ['Fan Chung', 'Alexander Grigor’yan', 'Shing-Tung Yau'],\n",
       "  'related_topics': ['Isoperimetric inequality',\n",
       "   'Isoperimetric dimension',\n",
       "   'Laplacian matrix',\n",
       "   'Heat kernel',\n",
       "   'Laplace operator',\n",
       "   'Ricci-flat manifold',\n",
       "   'Riemannian geometry',\n",
       "   'Exponential map (Riemannian geometry)',\n",
       "   'Combinatorics',\n",
       "   'Mathematics',\n",
       "   'Mathematical analysis'],\n",
       "  'references': ['1578099820',\n",
       "   '100944330',\n",
       "   '2131183115',\n",
       "   '181601562',\n",
       "   '1522020530',\n",
       "   '1567771969',\n",
       "   '2237678354',\n",
       "   '638069330',\n",
       "   '2051616415',\n",
       "   '2139320601']},\n",
       " {'id': '3029645440',\n",
       "  'title': 'Numerical Optimization',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '14,252',\n",
       "  'abstract': 'Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Jorge Nocedal 1, Stephen J. Wright 2'],\n",
       "  'related_topics': ['Continuous optimization',\n",
       "   'Nonlinear programming',\n",
       "   'Field (computer science)',\n",
       "   'Management science',\n",
       "   'CUTEr',\n",
       "   'Broyden–Fletcher–Goldfarb–Shanno algorithm',\n",
       "   'Trust region',\n",
       "   'Quadratic programming',\n",
       "   'Focus (computing)'],\n",
       "  'references': ['2164278908',\n",
       "   '2145096794',\n",
       "   '2134967712',\n",
       "   '2577537660',\n",
       "   '2123871098',\n",
       "   '2159211495',\n",
       "   '2142058898',\n",
       "   '2156598602',\n",
       "   '2109449402',\n",
       "   '196761320']},\n",
       " {'id': '2104095591',\n",
       "  'title': 'Snakes : Active Contour Models',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '25,285',\n",
       "  'abstract': 'A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.',\n",
       "  'date': 1987,\n",
       "  'authors': ['Michael Kass', 'Andrew P. Witkin', 'Demetri Terzopoulos'],\n",
       "  'related_topics': ['Active contour model',\n",
       "   'Active shape model',\n",
       "   'Match moving',\n",
       "   'Mumford–Shah functional',\n",
       "   'Spline (mathematics)',\n",
       "   'Level set method',\n",
       "   'Computer vision',\n",
       "   'Vector flow',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Stereo matching'],\n",
       "  'references': ['2109863423',\n",
       "   '2003370853',\n",
       "   '1995756857',\n",
       "   '1531060698',\n",
       "   '2139762693',\n",
       "   '2107198582',\n",
       "   '2582614493',\n",
       "   '2045798786',\n",
       "   '1631253743',\n",
       "   '1977699267']},\n",
       " {'id': '2150134853',\n",
       "  'title': 'Scale-space and edge detection using anisotropic diffusion',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '16,025',\n",
       "  'abstract': \"A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the 'no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image. >\",\n",
       "  'date': 1990,\n",
       "  'authors': ['P. Perona', 'J. Malik'],\n",
       "  'related_topics': ['Anisotropic diffusion',\n",
       "   'Edge-preserving smoothing',\n",
       "   'Smoothing'],\n",
       "  'references': ['2145023731',\n",
       "   '1997063559',\n",
       "   '2109863423',\n",
       "   '2114487471',\n",
       "   '2913192828',\n",
       "   '2022735534',\n",
       "   '2133155955',\n",
       "   '2002312729',\n",
       "   '1968245656',\n",
       "   '1973976434']},\n",
       " {'id': '2165874743',\n",
       "  'title': 'On Spectral Clustering: Analysis and an algorithm',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '9,863',\n",
       "  'abstract': 'Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Andrew Y. Ng 1, Michael I. Jordan 1, Yair Weiss 2'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Correlation clustering',\n",
       "   'Fuzzy clustering',\n",
       "   'Canopy clustering algorithm',\n",
       "   'Constrained clustering',\n",
       "   'CURE data clustering algorithm',\n",
       "   'Clustering high-dimensional data',\n",
       "   'Biclustering',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2140095548',\n",
       "   '1578099820',\n",
       "   '2141376824',\n",
       "   '2160167256',\n",
       "   '658559791',\n",
       "   '2130891992',\n",
       "   '2067976091',\n",
       "   '2171009857',\n",
       "   '2123320529',\n",
       "   '1981193610']},\n",
       " {'id': '2154579312',\n",
       "  'title': 'Handwritten Digit Recognition with a Back-Propagation Network',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '4,098',\n",
       "  'abstract': 'We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Yann LeCun 1, Bernhard E. Boser 2, John S. Denker 2, 3, Donnie Henderson 1, R. E. Howard 2, Wayne E. Hubbard 2, Lawrence D. Jackel 1'],\n",
       "  'related_topics': ['Word error rate',\n",
       "   'Task (project management)',\n",
       "   'Backpropagation',\n",
       "   'Speech recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Normalization (statistics)',\n",
       "   'Artificial intelligence',\n",
       "   'Digit recognition'],\n",
       "  'references': ['2154642048',\n",
       "   '2147800946',\n",
       "   '2114766824',\n",
       "   '169539560',\n",
       "   '56903235',\n",
       "   '2157475639',\n",
       "   '1965770722',\n",
       "   '2091987367',\n",
       "   '2153988646',\n",
       "   '2058841211']},\n",
       " {'id': '2122837498',\n",
       "  'title': 'Partially labeled classification with Markov random walks',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '731',\n",
       "  'abstract': 'To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classification with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classification. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classification problems.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Martin Szummer', 'Tommi Jaakkola'],\n",
       "  'related_topics': ['Random walk',\n",
       "   'Markov chain',\n",
       "   'Margin (machine learning)',\n",
       "   'Probabilistic logic',\n",
       "   'Representation (mathematics)',\n",
       "   'Regularization (mathematics)',\n",
       "   'Scale (ratio)',\n",
       "   'Set (abstract data type)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2001141328',\n",
       "   '1585385982',\n",
       "   '2017753243',\n",
       "   '2161813919',\n",
       "   '2127086485',\n",
       "   '2120720283']},\n",
       " {'id': '1511160855',\n",
       "  'title': 'Diffusion Kernels on Graphs and Other Discrete Input Spaces',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '1,077',\n",
       "  'abstract': 'The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Risi Imre Kondor', 'John D. Lafferty'],\n",
       "  'related_topics': ['Kernel (category theory)',\n",
       "   'Discretization',\n",
       "   'Heat kernel',\n",
       "   'Euclidean distance matrix',\n",
       "   'Gaussian function',\n",
       "   'Euclidean space',\n",
       "   'Diffusion equation',\n",
       "   'Matrix exponential',\n",
       "   'Algebra',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2124637492',\n",
       "   '2139212933',\n",
       "   '3023786531',\n",
       "   '2149684865',\n",
       "   '2097308346',\n",
       "   '1578099820',\n",
       "   '2009570821',\n",
       "   '2122837498',\n",
       "   '1576213419',\n",
       "   '1979711143']},\n",
       " {'id': '1585385982',\n",
       "  'title': 'Learning from Labeled and Unlabeled Data using Graph Mincuts',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,363',\n",
       "  'abstract': 'Many application domains suffer from not having enough labeled training data for learning. However, large amounts of unlabeled examples can often be gathered cheaply. As a result, there has been a great deal of work in recent years on how unlabeled data can be used to aid classification. We consider an algorithm based on finding minimum cuts in graphs, that uses pairwise relationships among the examples in order to learn from both labeled and unlabeled data.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Avrim Blum', 'Shuchi Chawla'],\n",
       "  'related_topics': ['Pairwise comparison',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Graph',\n",
       "   'Training set'],\n",
       "  'references': ['2108646579',\n",
       "   '2118978333',\n",
       "   '2114524997',\n",
       "   '1479807131',\n",
       "   '2154455818',\n",
       "   '2104290444',\n",
       "   '2139823104',\n",
       "   '2136504847',\n",
       "   '1990334093']},\n",
       " {'id': '1979711143',\n",
       "  'title': 'Large margin classification using the perceptron algorithm',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '1,612',\n",
       "  'abstract': 'We introduce and analyze a new algorithm for linear classification which combines Rosenblatt‘s perceptron algorithm with Helmbold and Warmuth‘s leave-one-out method. Like Vapnik‘s maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik‘s algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Yoav Freund', 'Robert E. Schapire'],\n",
       "  'related_topics': ['Perceptron',\n",
       "   'Linear classifier',\n",
       "   'Linear separability',\n",
       "   'Computation',\n",
       "   'Classifier (UML)',\n",
       "   'Pattern recognition',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'High dimensional'],\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2119821739',\n",
       "   '2087347434',\n",
       "   '1530699444',\n",
       "   '2069317438',\n",
       "   '1979675141',\n",
       "   '1496612019',\n",
       "   '1667072054',\n",
       "   '2011395874']},\n",
       " {'id': '2113592823',\n",
       "  'title': 'Cluster Kernels for Semi-Supervised Learning',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '643',\n",
       "  'abstract': 'We propose a framework to incorporate unlabeled data in kernel classifier, based on the idea that two points in the same cluster are more likely to have the same label. This is achieved by modifying the eigenspectrum of the kernel matrix. Experimental results assess the validity of this approach.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Olivier Chapelle', 'Jason Weston', 'Bernhard Schölkopf'],\n",
       "  'related_topics': ['Semi-supervised learning',\n",
       "   'Kernel (linear algebra)',\n",
       "   'Classifier (UML)',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2165874743',\n",
       "   '2140095548',\n",
       "   '2048679005',\n",
       "   '2158001550',\n",
       "   '2107008379',\n",
       "   '2160167256',\n",
       "   '2166473218',\n",
       "   '2122837498',\n",
       "   '2139578439',\n",
       "   '1574877594']},\n",
       " {'id': '200434350',\n",
       "  'title': 'A Random Walks View of Spectral Segmentation.',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '841',\n",
       "  'abstract': \"We present a new view of clustering and segmentation by pairwise similarities. We interpret the similarities as edge ows in a Markov random walk and study the eigenvalues and eigenvectors of the walk's transition matrix. This view shows that spectral methods for clustering and segmentation have a probabilistic foundation. We prove that the Normalized Cut method arises naturally from our framework and we provide a complete characterization of the cases when the Normalized Cut algorithm is exact. Then we discuss other spectral segmentation and clustering methods showing that several of them are essentially the same as NCut.\",\n",
       "  'date': 2000,\n",
       "  'authors': ['Marina Meila 1, Jianbo Shi 2'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Random walk',\n",
       "   'Heterogeneous random walk in one dimension',\n",
       "   'Loop-erased random walk',\n",
       "   'Segmentation',\n",
       "   'Quantum walk',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Stochastic matrix',\n",
       "   'Pattern recognition',\n",
       "   'Discrete mathematics',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2138621811',\n",
       "   '2147152072',\n",
       "   '1578099820',\n",
       "   '2160167256',\n",
       "   '2130891992',\n",
       "   '2171009857',\n",
       "   '1640070940',\n",
       "   '2065060195',\n",
       "   '2323009482']},\n",
       " {'id': '1497256448',\n",
       "  'title': 'Adaptation in natural and artificial systems',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '59,176',\n",
       "  'abstract': '',\n",
       "  'date': 1991,\n",
       "  'authors': ['John H. Holland'],\n",
       "  'related_topics': ['Artificial development',\n",
       "   'Artificial creation',\n",
       "   'Adaptation (computer science)',\n",
       "   'Evolutionary acquisition of neural topologies',\n",
       "   'Genetic fuzzy systems',\n",
       "   'Computer science',\n",
       "   'Effective fitness',\n",
       "   'Evolutionary programming',\n",
       "   'Natural (archaeology)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': []},\n",
       " {'id': '2148694408',\n",
       "  'title': 'Principal Component Analysis',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '53,967',\n",
       "  'abstract': 'Introduction * Properties of Population Principal Components * Properties of Sample Principal Components * Interpreting Principal Components: Examples * Graphical Representation of Data Using Principal Components * Choosing a Subset of Principal Components or Variables * Principal Component Analysis and Factor Analysis * Principal Components in Regression Analysis * Principal Components Used with Other Multivariate Techniques * Outlier Detection, Influential Observations and Robust Estimation * Rotation and Interpretation of Principal Components * Principal Component Analysis for Time Series and Other Non-Independent Data * Principal Component Analysis for Special Types of Data * Generalizations and Adaptations of Principal Component Analysis',\n",
       "  'date': 1986,\n",
       "  'authors': ['Ian Jolliffe'],\n",
       "  'related_topics': ['Principal component analysis',\n",
       "   'Multilinear principal component analysis',\n",
       "   'Kernel principal component analysis',\n",
       "   'Relationship square',\n",
       "   'Correspondence analysis',\n",
       "   'Multiple correspondence analysis',\n",
       "   'Dimensionality reduction',\n",
       "   'Population',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138621811',\n",
       "   '2145962650',\n",
       "   '2139047213',\n",
       "   '78159342',\n",
       "   '2140095548',\n",
       "   '1479807131',\n",
       "   '2106053110',\n",
       "   '2145072179',\n",
       "   '2121601095']},\n",
       " {'id': '23758216',\n",
       "  'title': 'Self-organization and associative memory: 3rd edition',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,003',\n",
       "  'abstract': '',\n",
       "  'date': 1989,\n",
       "  'authors': ['T. Kohonen'],\n",
       "  'related_topics': ['Content-addressable memory',\n",
       "   'Computer science',\n",
       "   'Cognitive science',\n",
       "   'Self-organization'],\n",
       "  'references': ['2076063813',\n",
       "   '1992419399',\n",
       "   '2121601095',\n",
       "   '2161160262',\n",
       "   '2144499799',\n",
       "   '2186428165',\n",
       "   '2002016471',\n",
       "   '2171277043',\n",
       "   '2137570937',\n",
       "   '2095757522']},\n",
       " {'id': '2100659887',\n",
       "  'title': 'A database for handwritten text recognition research',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '1,784',\n",
       "  'abstract': 'An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >',\n",
       "  'date': 1994,\n",
       "  'authors': ['J.J. Hull'],\n",
       "  'related_topics': ['Alphanumeric',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Digital image',\n",
       "   'Grayscale',\n",
       "   'Pixel',\n",
       "   'Database',\n",
       "   'Computer science',\n",
       "   'State (computer science)',\n",
       "   'Text recognition'],\n",
       "  'references': ['2092642599',\n",
       "   '2164371886',\n",
       "   '2093439427',\n",
       "   '1579840964',\n",
       "   '2156113848']},\n",
       " {'id': '2159174312',\n",
       "  'title': 'Mapping a Manifold of Perceptual Observations',\n",
       "  'reference_count': '11',\n",
       "  'citation_count': '373',\n",
       "  'abstract': 'Nonlinear dimensionality reduction is formulated here as the problem of trying to find a Euclidean feature-space embedding of a set of observations that preserves as closely as possible their intrinsic metric structure - the distances between points on the observation manifold as measured along geodesic paths. Our isometric feature mapping procedure, or isomap, is able to reliably recover low-dimensional nonlinear structure in realistic perceptual data sets, such as a manifold of face images, where conventional global mapping methods find only local minima. The recovered map provides a canonical set of globally meaningful features, which allows perceptual transformations such as interpolation, extrapolation, and analogy - highly nonlinear transformations in the original observation space - to be computed with simple linear operations in feature space.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Joshua B. Tenenbaum'],\n",
       "  'related_topics': ['Isomap',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Diffusion map',\n",
       "   'Dimensionality reduction',\n",
       "   'Intrinsic metric',\n",
       "   'Geodesic',\n",
       "   'Manifold',\n",
       "   'Feature vector',\n",
       "   'Algorithm',\n",
       "   'Topology',\n",
       "   'Mathematics'],\n",
       "  'references': ['2124776405',\n",
       "   '1991848143',\n",
       "   '2107636931',\n",
       "   '2143956139',\n",
       "   '2047870719',\n",
       "   '23758216',\n",
       "   '1580684925',\n",
       "   '2114309103',\n",
       "   '2123421115',\n",
       "   '2151391352']},\n",
       " {'id': '2106346128',\n",
       "  'title': 'Learning distributed representations of concepts using linear relational embedding',\n",
       "  'reference_count': '16',\n",
       "  'citation_count': '117',\n",
       "  'abstract': 'We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.',\n",
       "  'date': 2001,\n",
       "  'authors': ['A. Paccanaro', 'G.E. Hinton'],\n",
       "  'related_topics': ['Relational algebra',\n",
       "   'Feature learning',\n",
       "   'Concept learning',\n",
       "   'Binary relation',\n",
       "   'Representation (mathematics)',\n",
       "   'Generalization',\n",
       "   'Matrix multiplication',\n",
       "   'Relation (database)',\n",
       "   'Embedding',\n",
       "   'Discriminative model',\n",
       "   'Theoretical computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '2147152072',\n",
       "   '2110485445',\n",
       "   '1983578042',\n",
       "   '2051812123',\n",
       "   '183625566',\n",
       "   '145476170',\n",
       "   '1971844566',\n",
       "   '2121553911',\n",
       "   '1982370770']},\n",
       " {'id': '2109574129',\n",
       "  'title': 'Data-intensive applications, challenges, techniques and technologies: A survey on Big Data',\n",
       "  'reference_count': '187',\n",
       "  'citation_count': '3,034',\n",
       "  'abstract': \"It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. © 2014 Elsevier Inc. All rights reserved.\",\n",
       "  'date': 2014,\n",
       "  'authors': ['C. L. Philip Chen', 'Chun-Yang Zhang'],\n",
       "  'related_topics': ['Big data',\n",
       "   'e-Science',\n",
       "   'Data visualization',\n",
       "   'Data-intensive computing',\n",
       "   'Cloud computing',\n",
       "   'Data science',\n",
       "   'Information science',\n",
       "   'National security',\n",
       "   'Computer science',\n",
       "   'Business activities'],\n",
       "  'references': ['1631356911',\n",
       "   '2173213060',\n",
       "   '2136922672',\n",
       "   '2140190241',\n",
       "   '2100495367',\n",
       "   '1554944419',\n",
       "   '2163922914',\n",
       "   '2072128103',\n",
       "   '1981420413',\n",
       "   '1494137514']},\n",
       " {'id': '2963460103',\n",
       "  'title': 'Representation Learning on Graphs: Methods and Applications',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '1,017',\n",
       "  'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.',\n",
       "  'date': 2017,\n",
       "  'authors': ['William L. Hamilton', 'Rex Ying', 'Jure Leskovec'],\n",
       "  'related_topics': ['Feature learning',\n",
       "   'Deep learning',\n",
       "   'Degree (graph theory)',\n",
       "   'Heuristics',\n",
       "   'Nonlinear dimensionality reduction',\n",
       "   'Structure (mathematical logic)',\n",
       "   'Theoretical computer science',\n",
       "   'Encoding (memory)',\n",
       "   'Domain (software engineering)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2157331557',\n",
       "   '2064675550',\n",
       "   '2187089797',\n",
       "   '2962756421',\n",
       "   '3104097132',\n",
       "   '3102641634',\n",
       "   '2001141328',\n",
       "   '1888005072',\n",
       "   '1673310716',\n",
       "   '2148847267']},\n",
       " {'id': '255556494',\n",
       "  'title': 'Data Mining and Analysis: Fundamental Concepts and Algorithms',\n",
       "  'reference_count': '136',\n",
       "  'citation_count': '1,177',\n",
       "  'abstract': 'The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more',\n",
       "  'date': 2014,\n",
       "  'authors': ['Mohammed J. Zaki 1, Wagner Meira 2'],\n",
       "  'related_topics': ['Concept mining',\n",
       "   'Analytics',\n",
       "   'Business intelligence',\n",
       "   'Exploratory data analysis',\n",
       "   'Cluster analysis',\n",
       "   'Data science',\n",
       "   'Implementation',\n",
       "   'Kernel method',\n",
       "   'Computer science',\n",
       "   'Data mining',\n",
       "   'As is',\n",
       "   'Algorithm'],\n",
       "  'references': ['2911964244',\n",
       "   '1663973292',\n",
       "   '2140190241',\n",
       "   '1995945562',\n",
       "   '2112090702',\n",
       "   '2008620264',\n",
       "   '1570448133',\n",
       "   '1565377632',\n",
       "   '3013264884',\n",
       "   '1480376833']},\n",
       " {'id': '1591018827',\n",
       "  'title': 'A Data-Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition',\n",
       "  'reference_count': '57',\n",
       "  'citation_count': '727',\n",
       "  'abstract': 'The Koopman operator is a linear but infinite-dimensional operator that governs the evolution of scalar observables defined on the state space of an autonomous dynamical system and is a powerful tool for the analysis and decomposition of nonlinear dynamical systems. In this manuscript, we present a data-driven method for approximating the leading eigenvalues, eigenfunctions, and modes of the Koopman operator. The method requires a data set of snapshot pairs and a dictionary of scalar observables, but does not require explicit governing equations or interaction with a “black box” integrator. We will show that this approach is, in effect, an extension of dynamic mode decomposition (DMD), which has been used to approximate the Koopman eigenvalues and modes. Furthermore, if the data provided to the method are generated by a Markov process instead of a deterministic dynamical system, the algorithm approximates the eigenfunctions of the Kolmogorov backward equation, which could be considered as the “stochastic Koopman operator” (Mezic in Nonlinear Dynamics 41(1–3): 309–325, 2005). Finally, four illustrative examples are presented: two that highlight the quantitative performance of the method when presented with either deterministic or stochastic data and two that show potential applications of the Koopman eigenfunctions.',\n",
       "  'date': 2015,\n",
       "  'authors': ['Matthew O. Williams',\n",
       "   'Ioannis G. Kevrekidis',\n",
       "   'Clarence W. Rowley'],\n",
       "  'related_topics': ['Composition operator',\n",
       "   'Dynamic mode decomposition',\n",
       "   'Eigenvalues and eigenvectors'],\n",
       "  'references': ['2173213060',\n",
       "   '2014356541',\n",
       "   '1805381866',\n",
       "   '1575104486',\n",
       "   '1490180844',\n",
       "   '2609271878',\n",
       "   '2044602433',\n",
       "   '3099803807',\n",
       "   '1585274809',\n",
       "   '2101121254']},\n",
       " {'id': '2162584119',\n",
       "  'title': 'TRY - a global database of plant traits',\n",
       "  'reference_count': '243',\n",
       "  'citation_count': '2,085',\n",
       "  'abstract': \"Plant traits – the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs – determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy-in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log-normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation – but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait-based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.\",\n",
       "  'date': 2011,\n",
       "  'authors': ['J. Kattge 1, S. Díaz 2, S. Lavorel 3, I. C. Prentice 4, P. Leadley 5, G. Bönisch 1, E. Garnier 3, M. Westoby 4, Peter B Reich 6, 7, I. J. Wright 4, J. H C Cornelissen 8, C. Violle 3, S. P. Harrison 4, P. M. Van Bodegom 8, M. Reichstein 1, B. J. Enquist 9, N. A. Soudzilovskaia 8, D. D. Ackerly 10, M. Anand 11, O. Atkin 12, M. Bahn 13, T. R. Baker 14, D. Baldocchi 10, R. Bekker 15, C. C. Blanco 16, B. Blonder 9, W. J. Bond 17, R. Bradstock 18, D. E. Bunker 19, F. Casanoves',\n",
       "   'Jeannine M Cavender-Bares 7, J. Q. Chambers 20, F. S. Chapin 21, J. Chave 3, D. Coomes 22, W. K. Cornwell 8, J. M. Craine 23, B. H. Dobrin 9, L. Duarte 16, W. Durka',\n",
       "   'J. Elser 24, G. Esser 25, M. Estiarte 26, W. F. Fagan 27, J. Fang',\n",
       "   'F. Fernández-Méndez 28, A. Fidelis 29, B. Finegan',\n",
       "   'O. Flores 30, H. Ford 31 +85'],\n",
       "  'related_topics': ['Functional ecology', 'Trait', 'Plant functional type'],\n",
       "  'references': ['2100235918',\n",
       "   '2118295263',\n",
       "   '2107625277',\n",
       "   '2158155342',\n",
       "   '1742512077',\n",
       "   '2134289299',\n",
       "   '2052648234',\n",
       "   '2144042033',\n",
       "   '2142097792',\n",
       "   '1979723077']},\n",
       " {'id': '1746680969',\n",
       "  'title': 'Learning in graphical models',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,680',\n",
       "  'abstract': 'Part 1 Inference: introduction to inference for Bayesian networks, Robert Cowell advanced inference in Bayesian networks, Robert Cowell inference in Bayesian networks using nested junction trees, Uffe Kjoerulff bucket elimination - a unifying framework for probabilistic inference, R. Dechter an introduction to variational methods for graphical models, Michael I. Jordan et al improving the mean field approximation via the use of mixture distributions, Tommi S. Jaakkola and Michael I. Jordan introduction to Monte Carlo methods, D.J.C. MacKay suppressing random walls in Markov chain Monte Carlo using ordered overrelaxation, Radford M. Neal. Part 2 Independence: chain graphs and symmetric associations, Thomas S. Richardson the multiinformation function as a tool for measuring stochastic dependence, M. Studeny and J. Vejnarova. Part 3 Foundations for learning: a tutorial on learning with Bayesian networks, David Heckerman a view of the EM algorithm that justifies incremental, sparse and other variants, Radford M. Neal and Geoffrey E. Hinton. Part 4 Learning from data: latent variable models, Christopher M. Bishop stochastic algorithms for exploratory data analysis - data clustering and data visualization, Joachim M. Buhmann learning Bayesian networks with local structure, Nir Friedman and Moises Goldszmidt asymptotic model selection for directed networks with hidden variables, Dan Geiger et al a hierarchical community of experts, Geoffrey E. Hinton et al an information-theoretic analysis of hard and soft assignment methods for clustering, Michael J. Kearns et al learning hybrid Bayesian networks from data, Stefano Monti and Gregory F. Cooper a mean field learning algorithm for unsupervised neural networks, Lawrence Saul and Michael Jordan edge exclusion tests for graphical Gaussian models, Peter W.F. Smith and Joe Whittaker hepatitis B - a case study in MCMC, D.J. Spiegelhalter et al prediction with Gaussian processes - from linear regression to linear prediction and beyond, C.K.I. Williams.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Michael I. Jordan'],\n",
       "  'related_topics': ['Graphical model',\n",
       "   'Bayesian network',\n",
       "   'Markov chain Monte Carlo',\n",
       "   'Inference',\n",
       "   'Artificial neural network',\n",
       "   'Cluster analysis',\n",
       "   'Latent variable',\n",
       "   'Gaussian process',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['1880262756',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2166049352',\n",
       "   '2166851633',\n",
       "   '2097089247',\n",
       "   '1755360231',\n",
       "   '1873332500']},\n",
       " {'id': '2083380015',\n",
       "  'title': 'Connectionist learning of belief networks',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '688',\n",
       "  'abstract': 'Abstract Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Radford M. Neal'],\n",
       "  'related_topics': ['Restricted Boltzmann machine',\n",
       "   'Boltzmann machine',\n",
       "   'Computational learning theory',\n",
       "   'Feature learning',\n",
       "   'Unsupervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Deep belief network',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2159080219',\n",
       "   '1652505363',\n",
       "   '1498436455',\n",
       "   '2049633694',\n",
       "   '2083875149',\n",
       "   '1593793857',\n",
       "   '1507849272',\n",
       "   '2166698530',\n",
       "   '1992880122',\n",
       "   '1547224907']},\n",
       " {'id': '2114153178',\n",
       "  'title': 'Rate-coded Restricted Boltzmann Machines for Face Recognition',\n",
       "  'reference_count': '12',\n",
       "  'citation_count': '183',\n",
       "  'abstract': 'We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Yee Whye Teh 1, Geoffrey E. Hinton 2'],\n",
       "  'related_topics': ['Feature (computer vision)',\n",
       "   'Generative model',\n",
       "   'Posterior probability',\n",
       "   'Standard test image',\n",
       "   'Boltzmann machine',\n",
       "   'Facial recognition system',\n",
       "   'Face (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Data mining',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2116064496',\n",
       "   '2138451337',\n",
       "   '1902027874',\n",
       "   '2121647436',\n",
       "   '2159080219',\n",
       "   '2125027820',\n",
       "   '2113341759',\n",
       "   '2128716185',\n",
       "   '1547224907',\n",
       "   '1813659000']},\n",
       " {'id': '1547224907',\n",
       "  'title': 'Learning and relearning in Boltzmann machines',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,094',\n",
       "  'abstract': 'This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References',\n",
       "  'date': 1986,\n",
       "  'authors': ['G. E. Hinton', 'T. J. Sejnowski'],\n",
       "  'related_topics': ['Boltzmann machine',\n",
       "   'Computation',\n",
       "   'Relaxation (approximation)',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Boltzmann constant'],\n",
       "  'references': ['2310919327',\n",
       "   '2076063813',\n",
       "   '2072128103',\n",
       "   '2116064496',\n",
       "   '2137813581',\n",
       "   '2133671888',\n",
       "   '1562911371',\n",
       "   '1516111018',\n",
       "   '2321533354']},\n",
       " {'id': '2101706260',\n",
       "  'title': 'Recognizing handwritten digits using hierarchical products of experts',\n",
       "  'reference_count': '15',\n",
       "  'citation_count': '83',\n",
       "  'abstract': 'The product of experts learning procedure can discover a set of stochastic binary features that constitute a nonlinear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, a hierarchy of separate models can be learned, for each digit class. Each model in the hierarchy learns a layer of binary feature detectors that model the probability distribution of vectors of activity of feature detectors in the layer below. The models in the hierarchy are trained sequentially and each model uses a layer of binary feature detectors to learn a generative model of the patterns of feature activities in the preceding layer. After training, each layer of feature detectors produces a separate, unnormalized log probability score. With three layers of feature detectors for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data.',\n",
       "  'date': 2002,\n",
       "  'authors': ['G. Mayraz', 'G.E. Hinton'],\n",
       "  'related_topics': ['Generative model',\n",
       "   'Feature (computer vision)',\n",
       "   'Discriminative model',\n",
       "   'Feature extraction',\n",
       "   'Product of experts',\n",
       "   'Standard test image',\n",
       "   'Artificial neural network',\n",
       "   'Handwriting recognition',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2116064496',\n",
       "   '2912934387',\n",
       "   '2159080219',\n",
       "   '2150884987',\n",
       "   '28412257',\n",
       "   '1547224907',\n",
       "   '2104867159',\n",
       "   '1667072054',\n",
       "   '1813659000',\n",
       "   '2100559472']},\n",
       " {'id': '2124351082',\n",
       "  'title': 'Training support vector machines: an application to face detection',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '3,894',\n",
       "  'abstract': \"We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.\",\n",
       "  'date': 1997,\n",
       "  'authors': ['E. Osuna', 'R. Freund', 'F. Girosit'],\n",
       "  'related_topics': ['Sequential minimal optimization',\n",
       "   'Least squares support vector machine',\n",
       "   'Support vector machine',\n",
       "   'Quadratic programming',\n",
       "   'Optimization problem',\n",
       "   'Artificial neural network',\n",
       "   'Face detection',\n",
       "   'Statistical classification',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2156909104',\n",
       "   '2119821739',\n",
       "   '2087347434',\n",
       "   '2159686933',\n",
       "   '26816478',\n",
       "   '2159173611',\n",
       "   '2137346077',\n",
       "   '2084844503',\n",
       "   '2056695679',\n",
       "   '2125713050']},\n",
       " {'id': '2155511848',\n",
       "  'title': 'A statistical method for 3D object detection applied to faces and cars',\n",
       "  'reference_count': '10',\n",
       "  'citation_count': '1,891',\n",
       "  'abstract': 'In this paper, we describe a statistical method for 3D object detection. We represent the statistics of both object appearance and \"non-object\" appearance using a product of histograms. Each histogram represents the joint statistics of a subset of wavelet coefficients and their position on the object. Our approach is to use many such histograms representing a wide variety of visual attributes. Using this method, we have developed the first algorithm that can reliably detect human faces with out-of-plane rotation and the first algorithm that can reliably detect passenger cars over a wide range of viewpoints.',\n",
       "  'date': 2000,\n",
       "  'authors': ['H. Schneiderman', 'T. Kanade'],\n",
       "  'related_topics': ['Viola–Jones object detection framework',\n",
       "   'Object detection',\n",
       "   'Face detection',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Histogram',\n",
       "   'Object (computer science)',\n",
       "   'Wavelet',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Rotation (mathematics)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '3124955340',\n",
       "   '2117812871',\n",
       "   '2217896605',\n",
       "   '1658679052',\n",
       "   '2159686933',\n",
       "   '2140785063',\n",
       "   '2166713160',\n",
       "   '2138560582',\n",
       "   '2151777012']},\n",
       " {'id': '2160225842',\n",
       "  'title': 'Learning a Sparse Representation for Object Detection',\n",
       "  'reference_count': '21',\n",
       "  'citation_count': '767',\n",
       "  'abstract': 'We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation.In addition, we discuss and offer solutions to several methodological issues that are significant for the research community to be able to evaluate object detection approaches.',\n",
       "  'date': 2002,\n",
       "  'authors': ['Shivani Agarwal', 'Dan Roth'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Object detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Object model',\n",
       "   'Spatial relation',\n",
       "   'Implicit Shape Model',\n",
       "   'Sparse approximation',\n",
       "   'Test set',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2217896605',\n",
       "   '2152473410',\n",
       "   '2124087378',\n",
       "   '2124351082',\n",
       "   '2155511848',\n",
       "   '1949116567',\n",
       "   '1564419782',\n",
       "   '2156406284',\n",
       "   '2124722975']},\n",
       " {'id': '2295106276',\n",
       "  'title': 'Matching shapes',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '584',\n",
       "  'abstract': 'We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solving for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. Dis-similarity between two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset.',\n",
       "  'date': 2000,\n",
       "  'authors': ['S. Belongie', 'J. Malik', 'J. Puzicha'],\n",
       "  'related_topics': ['Shape analysis (digital geometry)',\n",
       "   'Shape context',\n",
       "   'Similarity (geometry)'],\n",
       "  'references': ['2310919327',\n",
       "   '2123977795',\n",
       "   '2101522199',\n",
       "   '2095757522',\n",
       "   '2089181482',\n",
       "   '2062104878',\n",
       "   '2106404777',\n",
       "   '2108444897',\n",
       "   '2100318434',\n",
       "   '2096840836']},\n",
       " {'id': '2141376824',\n",
       "  'title': 'Contour and Texture Analysis for Image Segmentation',\n",
       "  'reference_count': '38',\n",
       "  'citation_count': '1,600',\n",
       "  'abstract': 'This paper provides an algorithm for partitioning grayscale images into disjoint regions of coherent brightness and texture. Natural images contain both textured and untextured regions, so the cues of contour and texture differences are exploited simultaneously. Contours are treated in the intervening contour framework, while texture is analyzed using textons. Each of these cues has a domain of applicability, so to facilitate cue combination we introduce a gating operator based on the texturedness of the neighborhood at a pixel. Having obtained a local measure of how likely two nearby pixels are to belong to the same region, we use the spectral graph theoretic framework of normalized cuts to find partitions of the image into regions of coherent texture and brightness. Experimental results on a wide range of images are shown.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Jitendra Malik',\n",
       "   'Serge Belongie',\n",
       "   'Thomas Leung',\n",
       "   'Jianbo Shi'],\n",
       "  'related_topics': ['Image texture',\n",
       "   'Texture filtering',\n",
       "   'Texture compression',\n",
       "   'Image segmentation',\n",
       "   'Grayscale',\n",
       "   'Pixel',\n",
       "   'Texton',\n",
       "   'Texture (geology)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2121947440',\n",
       "   '2145023731',\n",
       "   '1578099820',\n",
       "   '1997063559',\n",
       "   '2121927366',\n",
       "   '1634005169',\n",
       "   '3017143921',\n",
       "   '2114487471',\n",
       "   '2160167256',\n",
       "   '1490632837']},\n",
       " {'id': '1612003148',\n",
       "  'title': 'Probabilistic latent semantic analysis',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '3,044',\n",
       "  'abstract': 'Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent semantic analysis',\n",
       "   'Document-term matrix',\n",
       "   'Latent class model',\n",
       "   'Latent Dirichlet allocation',\n",
       "   'Explicit semantic analysis',\n",
       "   'Latent variable',\n",
       "   'Non-negative matrix factorization',\n",
       "   'Machine learning',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2147152072',\n",
       "   '2107743791',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '1983578042',\n",
       "   '2134731454',\n",
       "   '2127314673',\n",
       "   '2056029990',\n",
       "   '2143144851',\n",
       "   '2140842551']},\n",
       " {'id': '2122090912',\n",
       "  'title': 'Maximum-Margin Matrix Factorization',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '1,252',\n",
       "  'abstract': 'We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Nathan Srebro 1, Jason Rennie 2, Tommi S. Jaakkola 2'],\n",
       "  'related_topics': ['Matrix decomposition',\n",
       "   'Margin (machine learning)',\n",
       "   'Algebra',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics',\n",
       "   'Generalization error'],\n",
       "  'references': ['1902027874',\n",
       "   '2145295623',\n",
       "   '2134731454',\n",
       "   '2049455633',\n",
       "   '2165395308',\n",
       "   '1966096622',\n",
       "   '2151052953',\n",
       "   '2118079529',\n",
       "   '2135001774',\n",
       "   '1999613943']},\n",
       " {'id': '205159212',\n",
       "  'title': 'Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure',\n",
       "  'reference_count': '18',\n",
       "  'citation_count': '507',\n",
       "  'abstract': '',\n",
       "  'date': 2007,\n",
       "  'authors': ['Ruslan Salakhutdinov', 'Geoffrey E. Hinton'],\n",
       "  'related_topics': ['k-nearest neighbors algorithm',\n",
       "   'Neighbourhood (mathematics)',\n",
       "   'Theoretical computer science',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear embedding'],\n",
       "  'references': ['2136922672',\n",
       "   '2100495367',\n",
       "   '2116064496',\n",
       "   '2117154949',\n",
       "   '2130556178',\n",
       "   '2157364932',\n",
       "   '2144935315',\n",
       "   '2159737176',\n",
       "   '2124914669',\n",
       "   '2157444450']},\n",
       " {'id': '2165395308',\n",
       "  'title': 'Weighted low-rank approximations',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '919',\n",
       "  'abstract': 'We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task.',\n",
       "  'date': 2003,\n",
       "  'authors': ['Nathan Srebro', 'Tommi Jaakkola'],\n",
       "  'related_topics': ['Rank (linear algebra)',\n",
       "   'Matrix (mathematics)',\n",
       "   'Context (language use)',\n",
       "   'Representation (mathematics)',\n",
       "   'Collaborative filtering',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical optimization',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Noise',\n",
       "   'Mathematics',\n",
       "   'Task (computing)'],\n",
       "  'references': ['2117354486',\n",
       "   '1673941785',\n",
       "   '2170653751',\n",
       "   '2021680564',\n",
       "   '2135001774',\n",
       "   '1496451467',\n",
       "   '1516172206',\n",
       "   '1568698519',\n",
       "   '2139451327',\n",
       "   '2252194958']},\n",
       " {'id': '1989702938',\n",
       "  'title': 'Face recognition: A literature survey',\n",
       "  'reference_count': '162',\n",
       "  'citation_count': '9,023',\n",
       "  'abstract': 'As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.',\n",
       "  'date': 2003,\n",
       "  'authors': ['W. Zhao 1, R. Chellappa 2, P. J. Phillips 3, A. Rosenfeld 2'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   'Face Recognition Grand Challenge',\n",
       "   'Literature survey',\n",
       "   'Eigenface',\n",
       "   'Facial recognition system',\n",
       "   'Face hallucination',\n",
       "   'FERET database',\n",
       "   'Perception',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Computer vision',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2164598857',\n",
       "   '2138451337',\n",
       "   '2217896605',\n",
       "   '2121647436',\n",
       "   '2152826865',\n",
       "   '2108384452',\n",
       "   '2033419168',\n",
       "   '2121601095',\n",
       "   '2038952578']},\n",
       " {'id': '2098947662',\n",
       "  'title': 'View-based and modular eigenspaces for face recognition',\n",
       "  'reference_count': '13',\n",
       "  'citation_count': '2,984',\n",
       "  'abstract': 'We describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of O(10/sup 3/) faces. The problem of recognition under general viewing orientation is also examined. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demonstrated. >',\n",
       "  'date': 1994,\n",
       "  'authors': ['Pentland', 'Moghaddam', 'Starner'],\n",
       "  'related_topics': ['Three-dimensional face recognition',\n",
       "   '3D single-object recognition',\n",
       "   'Feature (machine learning)',\n",
       "   'Eigenface',\n",
       "   'Facial recognition system',\n",
       "   'Feature extraction',\n",
       "   'Face (geometry)',\n",
       "   'Orientation (computer vision)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Salient',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2138451337',\n",
       "   '2113341759',\n",
       "   '2135463994',\n",
       "   '2138313032',\n",
       "   '2130506643',\n",
       "   '2157418942',\n",
       "   '1993867646',\n",
       "   '2112684592',\n",
       "   '2121863133',\n",
       "   '2030234875']},\n",
       " {'id': '2905573712',\n",
       "  'title': 'Face recognition: A Literature Survey',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,621',\n",
       "  'abstract': '',\n",
       "  'date': 2007,\n",
       "  'authors': ['W. Zhao', 'R. Rosenfeld', 'R. Chellappa'],\n",
       "  'related_topics': ['Literature survey',\n",
       "   'Business intelligence',\n",
       "   'Facial recognition system',\n",
       "   'Knowledge management',\n",
       "   'Computer science',\n",
       "   'Process improvement'],\n",
       "  'references': ['2129812935',\n",
       "   '2111993661',\n",
       "   '2536626143',\n",
       "   '2131081720',\n",
       "   '2149382413',\n",
       "   '2027805700',\n",
       "   '2078088780',\n",
       "   '2097777575',\n",
       "   '2106488920',\n",
       "   '2096027770']},\n",
       " {'id': '2155759509',\n",
       "  'title': 'The CMU Pose, Illumination, and Expression (PIE) database',\n",
       "  'reference_count': '9',\n",
       "  'citation_count': '3,895',\n",
       "  'abstract': 'Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.',\n",
       "  'date': 2002,\n",
       "  'authors': ['T. Sim', 'S. Baker', 'M. Bsat'],\n",
       "  'related_topics': ['Facial recognition system',\n",
       "   'Computer vision',\n",
       "   'Computer graphics (images)',\n",
       "   'Expression (mathematics)',\n",
       "   'Database',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Image storage'],\n",
       "  'references': ['2125127226',\n",
       "   '2118774738',\n",
       "   '2102760078',\n",
       "   '2120420721',\n",
       "   '2110822444',\n",
       "   '2121114545',\n",
       "   '2106143125',\n",
       "   '2141503314',\n",
       "   '2144855601']},\n",
       " {'id': '2140190241',\n",
       "  'title': 'Data Mining: Concepts and Techniques',\n",
       "  'reference_count': '485',\n",
       "  'citation_count': '50,570',\n",
       "  'abstract': \"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data\",\n",
       "  'date': 2000,\n",
       "  'authors': ['Jiawei Han 1, Micheline Kamber 2, Jian Pei 2'],\n",
       "  'related_topics': ['Web mining',\n",
       "   'Data stream mining',\n",
       "   'Concept mining',\n",
       "   'Data warehouse',\n",
       "   'Predictive Model Markup Language',\n",
       "   'K-optimal pattern discovery',\n",
       "   'Field (computer science)',\n",
       "   'Association rule learning',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Data mining'],\n",
       "  'references': ['2156909104',\n",
       "   '2911964244',\n",
       "   '2912565176',\n",
       "   '2148603752',\n",
       "   '1554944419',\n",
       "   '1995945562',\n",
       "   '2008620264',\n",
       "   '1639032689',\n",
       "   '1554663460',\n",
       "   '1570448133']},\n",
       " {'id': '2148603752',\n",
       "  'title': 'Statistical learning theory',\n",
       "  'reference_count': '1',\n",
       "  'citation_count': '67,324',\n",
       "  'abstract': 'A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Vladimir Naumovich Vapnik'],\n",
       "  'related_topics': ['Algorithmic learning theory',\n",
       "   'Statistical learning theory',\n",
       "   'Stability (learning theory)',\n",
       "   'Computational learning theory',\n",
       "   'Empirical risk minimization',\n",
       "   'Statistical theory',\n",
       "   'Generalization',\n",
       "   'VC dimension',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104']},\n",
       " {'id': '2164278908',\n",
       "  'title': 'Distributed Optimization and Statistical Learning Via the Alternating Direction Method of Multipliers',\n",
       "  'reference_count': '175',\n",
       "  'citation_count': '15,215',\n",
       "  'abstract': \"Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.\",\n",
       "  'date': 2011,\n",
       "  'authors': ['Stephen Boyd 1, Neal Parikh 1, Eric Chu 1, Borja Peleato 1, Jonathan Eckstein 2'],\n",
       "  'related_topics': ['Online machine learning',\n",
       "   'Statistical learning theory',\n",
       "   'Convex optimization'],\n",
       "  'references': ['2296319761',\n",
       "   '2173213060',\n",
       "   '2156909104',\n",
       "   '2296616510',\n",
       "   '2145096794',\n",
       "   '3029645440',\n",
       "   '1554944419',\n",
       "   '2100556411',\n",
       "   '2129638195',\n",
       "   '2135046866']},\n",
       " {'id': '2129812935',\n",
       "  'title': 'Robust Face Recognition via Sparse Representation',\n",
       "  'reference_count': '65',\n",
       "  'citation_count': '10,849',\n",
       "  'abstract': 'We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.',\n",
       "  'date': 2009,\n",
       "  'authors': ['J. Wright 1, A.Y. Yang 2, A. Ganesh 1, S.S. Sastry 2, Yi Ma 1'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'K-SVD',\n",
       "   'Feature vector',\n",
       "   'Eigenface',\n",
       "   'Feature extraction',\n",
       "   'Facial recognition system',\n",
       "   'Statistical classification',\n",
       "   'Robustness (computer science)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2129638195',\n",
       "   '2135046866',\n",
       "   '2164452299',\n",
       "   '2078204800',\n",
       "   '2138451337',\n",
       "   '1989702938',\n",
       "   '2121647436',\n",
       "   '2163808566']},\n",
       " {'id': '1746819321',\n",
       "  'title': 'Gaussian Processes for Machine Learning',\n",
       "  'reference_count': '180',\n",
       "  'citation_count': '23,786',\n",
       "  'abstract': 'A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines. Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Carl Edward Rasmussen 1, Christopher K I Williams 2'],\n",
       "  'related_topics': ['Active learning (machine learning)',\n",
       "   'Semi-supervised learning',\n",
       "   'Instance-based learning',\n",
       "   'Online machine learning',\n",
       "   'Relevance vector machine',\n",
       "   'Computational learning theory',\n",
       "   'Kernel method',\n",
       "   'Unsupervised learning',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2296319761',\n",
       "   '2156909104',\n",
       "   '2148603752',\n",
       "   '2170120409',\n",
       "   '1554663460',\n",
       "   '2117812871',\n",
       "   '3140968660',\n",
       "   '3023786531',\n",
       "   '2798909945',\n",
       "   '2078206416']},\n",
       " {'id': '1570448133',\n",
       "  'title': 'Data Mining: Practical Machine Learning Tools and Techniques',\n",
       "  'reference_count': '202',\n",
       "  'citation_count': '37,112',\n",
       "  'abstract': 'Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization',\n",
       "  'date': 1999,\n",
       "  'authors': ['Ian H. Witten', 'Eibe Frank', 'Mark A. Hall'],\n",
       "  'related_topics': ['Active learning (machine learning)',\n",
       "   'Instance-based learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Data stream mining',\n",
       "   'Ensemble learning',\n",
       "   'Hyper-heuristic',\n",
       "   'Cluster analysis',\n",
       "   'Association rule learning',\n",
       "   'Data science',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Data mining',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2140190241',\n",
       "   '1995945562',\n",
       "   '1639032689',\n",
       "   '1554663460',\n",
       "   '3013264884',\n",
       "   '2119821739',\n",
       "   '2139212933',\n",
       "   '2912934387',\n",
       "   '2138621811']},\n",
       " {'id': '2139212933',\n",
       "  'title': 'A Tutorial on Support Vector Machines for Pattern Recognition',\n",
       "  'reference_count': '54',\n",
       "  'citation_count': '30,394',\n",
       "  'abstract': 'The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Christopher J. C. Burges'],\n",
       "  'related_topics': ['Margin classifier',\n",
       "   'Least squares support vector machine',\n",
       "   'Relevance vector machine',\n",
       "   'VC dimension',\n",
       "   'Sequential minimal optimization',\n",
       "   'Support vector machine',\n",
       "   'Structural risk minimization',\n",
       "   'Structured support vector machine',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '1554663460',\n",
       "   '2119821739',\n",
       "   '2610857016',\n",
       "   '2140095548',\n",
       "   '2981264952',\n",
       "   '2087347434',\n",
       "   '2432517183',\n",
       "   '740415']},\n",
       " {'id': '2145096794',\n",
       "  'title': 'Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '17,504',\n",
       "  'abstract': 'This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f/spl isin/C/sup N/ and a randomly chosen set of frequencies /spl Omega/. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set /spl Omega/? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=/spl sigma//sub /spl tau//spl isin/T/f(/spl tau/)/spl delta/(t-/spl tau/) obeying |T|/spl les/C/sub M//spl middot/(log N)/sup -1/ /spl middot/ |/spl Omega/| for some constant C/sub M/>0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N/sup -M/), f can be reconstructed exactly as the solution to the /spl lscr//sub 1/ minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C/sub M/ which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|/spl middot/logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N/sup -M/) would in general require a number of frequency samples at least proportional to |T|/spl middot/logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.',\n",
       "  'date': 2006,\n",
       "  'authors': ['E.J. Candes 1, J. Romberg 1, T. Tao 2'],\n",
       "  'related_topics': ['Convex optimization',\n",
       "   'Free probability',\n",
       "   'Trigonometric polynomial',\n",
       "   'Fourier series',\n",
       "   'Binary logarithm',\n",
       "   'Omega',\n",
       "   'Sigma',\n",
       "   'Piecewise',\n",
       "   'Combinatorics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['3029645440',\n",
       "   '2078204800',\n",
       "   '2798909945',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2103559027',\n",
       "   '2154332973',\n",
       "   '2136235822',\n",
       "   '2012365979',\n",
       "   '2158537680']},\n",
       " {'id': '2078204800',\n",
       "  'title': 'Atomic Decomposition by Basis Pursuit',\n",
       "  'reference_count': '43',\n",
       "  'citation_count': '24,846',\n",
       "  'abstract': 'The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries---stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis pursuit (BP) is a principle for decomposing a signal into an \"optimal\"\\' superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear and quadratic programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Scott Shaobing Chen 1, David L. Donoho 2, Michael A. Saunders 2'],\n",
       "  'related_topics': ['Basis pursuit',\n",
       "   'Basis pursuit denoising',\n",
       "   'Wavelet packet decomposition',\n",
       "   'Total variation denoising',\n",
       "   'Wavelet',\n",
       "   'Matching pursuit',\n",
       "   'Orthogonal basis',\n",
       "   'Quadratic programming',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2062024414',\n",
       "   '2798909945',\n",
       "   '2146842127',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2103559027',\n",
       "   '2152328854',\n",
       "   '2156447271',\n",
       "   '2611147814',\n",
       "   '2128659236']},\n",
       " {'id': '2116148865',\n",
       "  'title': 'Greed is good: algorithmic results for sparse approximation',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '4,030',\n",
       "  'abstract': \"This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho's basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms.\",\n",
       "  'date': 2004,\n",
       "  'authors': ['J.A. Tropp'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Matching pursuit',\n",
       "   'Basis pursuit',\n",
       "   'Approximation algorithm',\n",
       "   'Sparse matrix',\n",
       "   'Restricted isometry property',\n",
       "   'Greedy algorithm',\n",
       "   'Approximation theory',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematics'],\n",
       "  'references': ['2078204800',\n",
       "   '2610857016',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2154332973',\n",
       "   '2136235822',\n",
       "   '2156447271',\n",
       "   '391578156',\n",
       "   '1605417594',\n",
       "   '2167839759']},\n",
       " {'id': '2099641086',\n",
       "  'title': 'Uncertainty principles and ideal atomic decomposition',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '2,365',\n",
       "  'abstract': 'Suppose a discrete-time signal S(t), 0/spl les/t<N, is a superposition of atoms taken from a combined time-frequency dictionary made of spike sequences 1/sub {t=/spl tau/}/ and sinusoids exp{2/spl pi/iwt/N}//spl radic/N. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l/sup 1/ norm of the coefficients among all decompositions. Here \"highly sparse\" means that N/sub t/+N/sub w/</spl radic/N/2 where N/sub t/ is the number of time atoms, N/sub w/ is the number of frequency atoms, and N is the length of the discrete-time signal. Underlying this result is a general l/sup 1/ uncertainty principle which says that if two bases are mutually incoherent, no nonzero signal can have a sparse representation in both bases simultaneously. For the above setting, the bases are sinusoids and spikes, and mutual incoherence is measured in terms of the largest inner product between different basis elements. The uncertainty principle holds for a variety of interesting basis pairs, not just sinusoids and spikes. The results have idealized applications to band-limited approximation with gross errors, to error-correcting encryption, and to separation of uncoordinated sources. Related phenomena hold for functions of a real variable, with basis pairs such as sinusoids and wavelets, and for functions of two variables, with basis pairs such as wavelets and ridgelets. In these settings, if a function f is representable by a sufficiently sparse superposition of terms taken from both bases, then there is only one such sparse representation; it may be obtained by minimum l/sup 1/ norm atomic decomposition. The condition \"sufficiently sparse\" becomes a multiscale condition; for example, that the number of wavelets at level j plus the number of sinusoids in the jth dyadic frequency band are together less than a constant times 2/sup j/2/.',\n",
       "  'date': 2001,\n",
       "  'authors': ['D.L. Donoho 1, X. Huo 2'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Superposition principle',\n",
       "   'Norm (mathematics)',\n",
       "   'Wavelet',\n",
       "   'Basis pursuit',\n",
       "   'Uncertainty principle',\n",
       "   'Convex optimization',\n",
       "   'Time–frequency analysis',\n",
       "   'Combinatorics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2115755118',\n",
       "   '2062024414',\n",
       "   '2078204800',\n",
       "   '2151693816',\n",
       "   '1916685473',\n",
       "   '1604810369',\n",
       "   '2066462711',\n",
       "   '2125455772',\n",
       "   '1997149618',\n",
       "   '2033367330']},\n",
       " {'id': '2097323375',\n",
       "  'title': 'Stable recovery of sparse overcomplete representations in the presence of noise',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '2,618',\n",
       "  'abstract': 'Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.',\n",
       "  'date': 2003,\n",
       "  'authors': ['D.L. Donoho 1, M. Elad 2, V.N. Temlyakov 3'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'K-SVD',\n",
       "   'Matching pursuit',\n",
       "   'Basis pursuit',\n",
       "   'Approximation algorithm',\n",
       "   'Noise (signal processing)',\n",
       "   'Signal processing',\n",
       "   'Stability (learning theory)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2135046866',\n",
       "   '2078204800',\n",
       "   '2610857016',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2154332973',\n",
       "   '2132680427',\n",
       "   '2136235822',\n",
       "   '2069912449']},\n",
       " {'id': '2136235822',\n",
       "  'title': 'Sparse representations in unions of bases',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '1,028',\n",
       "  'abstract': 'The purpose of this correspondence is to generalize a result by Donoho and Huo and Elad and Bruckstein on sparse representations of signals in a union of two orthonormal bases for R/sup N/. We consider general (redundant) dictionaries for R/sup N/, and derive sufficient conditions for having unique sparse representations of signals in such dictionaries. The special case where the dictionary is given by the union of L/spl ges/2 orthonormal bases for R/sup N/ is studied in more detail. In particular, it is proved that the result of Donoho and Huo, concerning the replacement of the /spl lscr//sup 0/ optimization problem with a linear programming problem when searching for sparse representations, has an analog for dictionaries that may be highly redundant.',\n",
       "  'date': 2003,\n",
       "  'authors': ['R. Gribonval 1, M. Nielsen 2'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Orthonormal basis',\n",
       "   'Optimization problem',\n",
       "   'Sparse matrix',\n",
       "   'Linear programming',\n",
       "   'Discrete mathematics',\n",
       "   'Special case',\n",
       "   'Mathematics',\n",
       "   'Nonlinear approximation'],\n",
       "  'references': ['2099641086',\n",
       "   '2154332973',\n",
       "   '2167839759',\n",
       "   '2086869478',\n",
       "   '2133866430',\n",
       "   '2115090644']},\n",
       " {'id': '2147656689',\n",
       "  'title': 'Just relax: convex programming methods for identifying sparse signals in noise',\n",
       "  'reference_count': '64',\n",
       "  'citation_count': '1,273',\n",
       "  'abstract': 'This paper studies a difficult and fundamental problem that arises throughout electrical engineering, applied mathematics, and statistics. Suppose that one forms a short linear combination of elementary signals drawn from a large, fixed collection. Given an observation of the linear combination that has been contaminated with additive noise, the goal is to identify which elementary signals participated and to approximate their coefficients. Although many algorithms have been proposed, there is little theory which guarantees that these algorithms can accurately and efficiently solve the problem. This paper studies a method called convex relaxation, which attempts to recover the ideal sparse signal by solving a convex program. This approach is powerful because the optimization can be completed in polynomial time with standard scientific software. The paper provides general conditions which ensure that convex relaxation succeeds. As evidence of the broad impact of these results, the paper describes how convex relaxation can be used for several concrete signal recovery problems. It also describes applications to channel coding, linear regression, and numerical analysis',\n",
       "  'date': 2006,\n",
       "  'authors': ['J.A. Tropp'],\n",
       "  'related_topics': ['Convex optimization',\n",
       "   'Proper convex function',\n",
       "   'Relaxation (iterative method)',\n",
       "   'Linear combination',\n",
       "   'Sparse approximation',\n",
       "   'Iterative method',\n",
       "   'Time complexity',\n",
       "   'Noise (signal processing)',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296319761',\n",
       "   '2145096794',\n",
       "   '2099111195',\n",
       "   '2115755118',\n",
       "   '2129638195',\n",
       "   '2135046866',\n",
       "   '2122825543',\n",
       "   '2078204800',\n",
       "   '2610857016',\n",
       "   '2116148865']},\n",
       " {'id': '2012365979',\n",
       "  'title': 'Near-optimal sparse fourier representations via sampling',\n",
       "  'reference_count': '17',\n",
       "  'citation_count': '355',\n",
       "  'abstract': '(MATH) We give an algorithm for finding a Fourier representation R of B terms for a given discrete signal signal A of length N, such that $\\\\|\\\\signal-\\\\repn\\\\|_2^2$ is within the factor (1 +e) of best possible $\\\\|\\\\signal-\\\\repn_\\\\opt\\\\|_2^2$. Our algorithm can access A by reading its values on a sample set T ⊆[0,N), chosen randomly from a (non-product) distribution of our choice, independent of A. That is, we sample non-adaptively. The total time cost of the algorithm is polynomial in B log(N)log(M)e (where M is the ratio of largest to smallest numerical quantity encountered), which implies a similar bound for the number of samples.',\n",
       "  'date': 2002,\n",
       "  'authors': ['A. C. Gilbert 1, S. Guha 2, P. Indyk 3, S. Muthukrishnan 1, M. Strauss 1'],\n",
       "  'related_topics': ['Polynomial',\n",
       "   'Discrete-time signal',\n",
       "   'Fourier transform',\n",
       "   'Distribution (mathematics)',\n",
       "   'Combinatorics',\n",
       "   'Sampling (statistics)',\n",
       "   'Set (abstract data type)',\n",
       "   'Mathematics',\n",
       "   'Fourier representation',\n",
       "   'Time cost'],\n",
       "  'references': ['2151693816',\n",
       "   '2080745194',\n",
       "   '2125455772',\n",
       "   '1979750072',\n",
       "   '2047424291',\n",
       "   '1970950689',\n",
       "   '2095546965',\n",
       "   '2042194938',\n",
       "   '2101610102',\n",
       "   '1899006432']},\n",
       " {'id': '2096613063',\n",
       "  'title': 'Data compression and harmonic analysis',\n",
       "  'reference_count': '84',\n",
       "  'citation_count': '588',\n",
       "  'abstract': 'In this paper we review some recent interactions between harmonic analysis and data compression. The story goes back of course to Shannon\\'s R(D) theory in the case of Gaussian stationary processes, which says that transforming into a Fourier basis followed by block coding gives an optimal lossy compression technique; practical developments like transform-based image compression have been inspired by this result. In this paper we also discuss connections perhaps less familiar to the information theory community, growing out of the field of harmonic analysis. Recent harmonic analysis constructions, such as wavelet transforms and Gabor transforms, are essentially optimal transforms for transform coding in certain settings. Some of these transforms are under consideration for future compression standards. We discuss some of the lessons of harmonic analysis in this century. Typically, the problems and achievements of this field have involved goals that were not obviously related to practical data compression, and have used a language not immediately accessible to outsiders. Nevertheless, through an extensive generalization of what Shannon called the \"sampling theorem\", harmonic analysis has succeeded in developing new forms of functional representation which turn out to have significant data compression interpretations. We explain why harmonic analysis has interacted with data compression, and we describe some interesting recent ideas in the field that may affect data compression in the future.',\n",
       "  'date': 1998,\n",
       "  'authors': ['D.L. Donoho 1, M. Vetterli 2, R.A. DeVore 3, I. Daubechies 1'],\n",
       "  'related_topics': ['Data compression',\n",
       "   'Image compression',\n",
       "   'Lossy compression',\n",
       "   'Information theory',\n",
       "   'Transform coding',\n",
       "   'Harmonic analysis',\n",
       "   'Wavelet transform',\n",
       "   'Entropy (information theory)',\n",
       "   'Algorithm',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2099111195',\n",
       "   '2062024414',\n",
       "   '2132984323',\n",
       "   '2142276208',\n",
       "   '2098914003',\n",
       "   '2053691921',\n",
       "   '1634005169',\n",
       "   '2037612300',\n",
       "   '1996021349',\n",
       "   '1584610719']},\n",
       " {'id': '2050880896',\n",
       "  'title': 'Unconditional Bases Are Optimal Bases for Data Compression and for Statistical Estimation',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '576',\n",
       "  'abstract': 'Abstract An orthogonal basis of L2 which is also an unconditional basis of a functional space F is an optimal basis for compressing, estimating, and recovering functions in F . Simple thresholding operations, applied in the unconditional basis, work essentially better for compressing, estimating, and recovering than they do in any other orthogonal basis. In fact, simple thresholding in an unconditional basis works essentially better for recovery and estimation than other methods, period. (Performance is measured in an asymptotic minimax sense.) As an application, we formalize and prove Mallat′s Heuristic, which says that wavelet bases are optimal for representing functions containing singularities, when there may be an arbitrary number of singularities, arbitrarily distributed.',\n",
       "  'date': 1993,\n",
       "  'authors': ['David L. Donoho'],\n",
       "  'related_topics': ['Orthogonal basis',\n",
       "   'Basis (linear algebra)',\n",
       "   'Thresholding',\n",
       "   'Minimax',\n",
       "   'Wavelet',\n",
       "   'Heuristic',\n",
       "   'Simple (abstract algebra)',\n",
       "   'Data compression',\n",
       "   'Applied mathematics',\n",
       "   'Discrete mathematics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296616510',\n",
       "   '2146842127',\n",
       "   '2101491865',\n",
       "   '2018332268',\n",
       "   '2069912449',\n",
       "   '2110505738',\n",
       "   '2020919250',\n",
       "   '59771946',\n",
       "   '2109504624',\n",
       "   '67772112']},\n",
       " {'id': '2129638195',\n",
       "  'title': 'Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '7,858',\n",
       "  'abstract': 'Suppose we are given a vector f in a class FsubeRopfN , e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr2) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|(n)lesRmiddotn-1p/, where R>0 and p>0. Suppose that we take measurements yk=langf# ,Xkrang,k=1,...,K, where the Xk are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0<p<1 and with overwhelming probability, our reconstruction ft, defined as the solution to the constraints yk=langf# ,Xkrang with minimal lscr1 norm, obeys parf-f#parlscr2lesCp middotRmiddot(K/logN)-r, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed',\n",
       "  'date': 2006,\n",
       "  'authors': ['E.J. Candes 1, T. Tao 2'],\n",
       "  'related_topics': ['Norm (mathematics)',\n",
       "   'Restricted isometry property',\n",
       "   'Random projection',\n",
       "   'Random matrix',\n",
       "   'Concentration of measure',\n",
       "   'Gaussian',\n",
       "   'Fourier series',\n",
       "   'Basis pursuit',\n",
       "   'Discrete mathematics',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2296616510',\n",
       "   '2145096794',\n",
       "   '2115755118',\n",
       "   '2129131372',\n",
       "   '2078204800',\n",
       "   '2099641086',\n",
       "   '2103559027',\n",
       "   '2050834445',\n",
       "   '2154332973',\n",
       "   '2136235822']},\n",
       " {'id': '2050834445',\n",
       "  'title': 'For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '3,709',\n",
       "  'abstract': \"We consider linear equations y = Φx where y is a given vector in ℝn and Φ is a given n × m matrix with n 0 so that for large n and for all Φ's except a negligible fraction, the following property holds: For every y having a representation y = Φx0by a coefficient vector x0 ∈ ℝmwith fewer than ρ · n nonzeros, the solution x1of the 1-minimization problem is unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems—greedy algorithms and thresholding—perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost-spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. © 2006 Wiley Periodicals, Inc.\",\n",
       "  'date': 2006,\n",
       "  'authors': ['David L. Donoho'],\n",
       "  'related_topics': ['Underdetermined system',\n",
       "   'Eigenvalues and eigenvectors',\n",
       "   'Norm (mathematics)',\n",
       "   'System of linear equations',\n",
       "   'M-matrix',\n",
       "   'Banach space',\n",
       "   'Wishart distribution',\n",
       "   'Linear equation',\n",
       "   'Combinatorics',\n",
       "   'Mathematics'],\n",
       "  'references': ['2145096794',\n",
       "   '2078204800',\n",
       "   '2116148865',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2097323375',\n",
       "   '2154332973',\n",
       "   '2136235822',\n",
       "   '2156447271',\n",
       "   '1573820523']},\n",
       " {'id': '2154332973',\n",
       "  'title': 'Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '3,409',\n",
       "  'abstract': 'Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = ∑k γ(k)dk, with scalar coefficients γ(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the l1 norm of the coefficients γ. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.',\n",
       "  'date': 2003,\n",
       "  'authors': ['David L. Donoho', 'Michael Elad'],\n",
       "  'related_topics': ['Sparse approximation',\n",
       "   'Convex optimization',\n",
       "   'Linear combination',\n",
       "   'Basis pursuit',\n",
       "   'Combinatorial optimization',\n",
       "   'Minification',\n",
       "   'Discrete mathematics',\n",
       "   'Scalar (mathematics)',\n",
       "   'Special case',\n",
       "   'Computer science'],\n",
       "  'references': ['2115755118',\n",
       "   '2078204800',\n",
       "   '2610857016',\n",
       "   '2099641086',\n",
       "   '2151693816',\n",
       "   '2136235822',\n",
       "   '2167839759',\n",
       "   '1604810369',\n",
       "   '2125455772',\n",
       "   '1995963238']},\n",
       " {'id': '2087347434',\n",
       "  'title': 'A training algorithm for optimal margin classifiers',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '13,591',\n",
       "  'abstract': 'A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Bernhard E. Boser 1, Isabelle M. Guyon 2, Vladimir N. Vapnik 2'],\n",
       "  'related_topics': ['Margin (machine learning)',\n",
       "   'Decision boundary',\n",
       "   'Stability (learning theory)',\n",
       "   'Perceptron',\n",
       "   'Generalization',\n",
       "   'Linear combination',\n",
       "   'Radial basis function',\n",
       "   'Optical character recognition',\n",
       "   'Algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3017143921',\n",
       "   '2171277043',\n",
       "   '2165758113',\n",
       "   '2154579312',\n",
       "   '2266946488',\n",
       "   '1530699444',\n",
       "   '2076118331',\n",
       "   '2086472796',\n",
       "   '2111494971',\n",
       "   '1965770722']},\n",
       " {'id': '1530699444',\n",
       "  'title': 'Estimation of Dependences Based on Empirical Data',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '5,091',\n",
       "  'abstract': 'Realism and Instrumentalism: Classical Statistics and VC Theory (1960-1980).- Falsifiability and Parsimony: VC Dimension and the Number of Entities (1980-2000).- Noninductive Methods of Inference: Direct Inference Instead of Generalization (2000-...).- The Big Picture.',\n",
       "  'date': 2010,\n",
       "  'authors': ['Vladimir Naumovich Vapnik'],\n",
       "  'related_topics': ['Inference',\n",
       "   'VC dimension',\n",
       "   'Generalization',\n",
       "   'Falsifiability',\n",
       "   'Mathematical economics',\n",
       "   'Instrumentalism',\n",
       "   'Algorithm',\n",
       "   'Mathematics',\n",
       "   'Realism',\n",
       "   'Estimation',\n",
       "   'Empirical data'],\n",
       "  'references': ['2137775453',\n",
       "   '2119821739',\n",
       "   '2139212933',\n",
       "   '3124955340',\n",
       "   '2119479037',\n",
       "   '1964357740',\n",
       "   '607505555',\n",
       "   '2132549764',\n",
       "   '2087347434',\n",
       "   '1975846642']},\n",
       " {'id': '2168228682',\n",
       "  'title': 'Comparison of classifier methods: a case study in handwritten digit recognition',\n",
       "  'reference_count': '8',\n",
       "  'citation_count': '936',\n",
       "  'abstract': 'This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.',\n",
       "  'date': 1994,\n",
       "  'authors': ['L. Bottou 1, C. Cortes 2, 3, J.S. Denker 2, 4, H. Drucker 4, 5, I. Guyon 4, L.D. Jackel',\n",
       "   'Y. LeCun',\n",
       "   'U.A. Muller',\n",
       "   'E. Sackinger 4, P. Simard 2, 6, V. Vapnik'],\n",
       "  'related_topics': ['Handwriting recognition',\n",
       "   'Classifier (UML)',\n",
       "   'Pattern recognition',\n",
       "   'Speech recognition',\n",
       "   'NIST',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Digit recognition',\n",
       "   'Training set'],\n",
       "  'references': ['2087347434',\n",
       "   '2093717447',\n",
       "   '2137291015',\n",
       "   '2166501286',\n",
       "   '2056763477',\n",
       "   '2162363099',\n",
       "   '2093465006',\n",
       "   '2151328054']},\n",
       " {'id': '1568787085',\n",
       "  'title': 'Neural-Network and k-Nearest-neighbor Classifiers',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '24',\n",
       "  'abstract': 'The performance of a state-of-the-art neural network classifier for hand-written digits is compared to that of a k-nearest-neighbor classifier and to human performance. The neural network has a clear advantage over the k-nearest-neighbor method, but at the same time does not yet reach human performance. Two methods for combining neural-network ideas and the k-nearest-neighbor algorithm are proposed. Numerical experiments for these methods show an improvement in performance.',\n",
       "  'date': 1991,\n",
       "  'authors': ['J. Bromley', 'E. Sackinger'],\n",
       "  'related_topics': ['Time delay neural network',\n",
       "   'Probabilistic neural network',\n",
       "   'Artificial neural network',\n",
       "   'Classifier (UML)',\n",
       "   'k-nearest neighbors algorithm',\n",
       "   'Pattern recognition',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence',\n",
       "   'Neural network classifier'],\n",
       "  'references': ['2119821739',\n",
       "   '2159737176',\n",
       "   '1548139318',\n",
       "   '2138882494',\n",
       "   '2277406607',\n",
       "   '1595098149',\n",
       "   '2010332406',\n",
       "   '2125145210',\n",
       "   '2415856871',\n",
       "   '1973224984']},\n",
       " {'id': '5594912',\n",
       "  'title': 'Estimation of Dependences Based on Empirical Data: Springer Series in Statistics (Springer Series in Statistics)',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '451',\n",
       "  'abstract': '',\n",
       "  'date': 1982,\n",
       "  'authors': ['Vladimir Vapnik'],\n",
       "  'related_topics': ['Series (mathematics)',\n",
       "   'Computer science',\n",
       "   'Estimation',\n",
       "   'Statistics',\n",
       "   'Empirical data'],\n",
       "  'references': ['2119821739',\n",
       "   '2119479037',\n",
       "   '1479807131',\n",
       "   '3104887532',\n",
       "   '2113651538',\n",
       "   '2110652811',\n",
       "   '2122124659',\n",
       "   '2154952480',\n",
       "   '1574877594']},\n",
       " {'id': '2152473410',\n",
       "  'title': 'Example-based object detection in images by components',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '1,428',\n",
       "  'abstract': 'We present a general example-based framework for detecting objects in static images by components. The technique is demonstrated by developing a system that locates people in cluttered scenes. The system is structured with four distinct example-based detectors that are trained to separately find the four components of the human body: the head, legs, left arm, and right arm. After ensuring that these components are present in the proper geometric configuration, a second example-based classifier combines the results of the component detectors to classify a pattern as either a \"person\" or a \"nonperson.\" We call this type of hierarchical architecture, in which learning occurs at multiple stages, an adaptive combination of classifiers (ACC). We present results that show that this system performs significantly better than a similar full-body person detector. This suggests that the improvement in performance is due to the component-based approach and the ACC data classification architecture. The algorithm is also more robust than the full-body person detection method in that it is capable of locating partially occluded views of people and people whose body parts have little contrast with the background.',\n",
       "  'date': 2001,\n",
       "  'authors': ['A. Mohan 1, C. Papageorgiou 2, T. Poggio 3'],\n",
       "  'related_topics': ['One-class classification',\n",
       "   'Object detection',\n",
       "   'Classifier (UML)',\n",
       "   'Data classification',\n",
       "   'Computer vision',\n",
       "   'Pattern recognition',\n",
       "   'Detector',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2139212933',\n",
       "   '2912934387',\n",
       "   '2132984323',\n",
       "   '2217896605',\n",
       "   '2149684865',\n",
       "   '2112076978',\n",
       "   '2115763357',\n",
       "   '2152761983',\n",
       "   '2159686933']},\n",
       " {'id': '1992825118',\n",
       "  'title': 'Detecting Pedestrians Using Patterns of Motion and Appearance',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '3,208',\n",
       "  'abstract': 'This paper describes a pedestrian detection system that integratesimage intensity information with motion information.We use a detection style algorithm that scans a detectorover two consecutive frames of a video sequence. Thedetector is trained (using AdaBoost) to take advantage ofboth motion and appearance information to detect a walkingperson. Past approaches have built detectors based onmotion information or detectors based on appearance information,but ours is the first to combine both sources ofinformation in a single detector. The implementation describedruns at about 4 frames/second, detects pedestriansat very small scales (as small as 20x15 pixels), and has avery low false positive rate.Our approach builds on the detection work of Viola andJones. Novel contributions of this paper include: i) developmentof a representation of image motion which is extremelyefficient, and ii) implementation of a state of theart pedestrian detection system which operates on low resolutionimages under difficult conditions (such as rain andsnow).',\n",
       "  'date': 2003,\n",
       "  'authors': ['Paul Viola 1, Michael J. Jones 2, Daniel Snow 2'],\n",
       "  'related_topics': ['Pedestrian detection',\n",
       "   'AdaBoost',\n",
       "   'Pixel',\n",
       "   'Boosting (machine learning)',\n",
       "   'Computer vision',\n",
       "   'Implicit Shape Model',\n",
       "   'Detector',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '3124955340',\n",
       "   '2217896605',\n",
       "   '2115763357',\n",
       "   '2032210760',\n",
       "   '2155511848',\n",
       "   '2145073242',\n",
       "   '2162919312',\n",
       "   '2089181482',\n",
       "   '2143023146']},\n",
       " {'id': '1608462934',\n",
       "  'title': 'A Trainable System for Object Detection',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '1,779',\n",
       "  'abstract': 'This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Constantine Papageorgiou', 'Tomaso Poggio'],\n",
       "  'related_topics': ['Object-class detection',\n",
       "   'Object detection',\n",
       "   'Viola–Jones object detection framework',\n",
       "   'Face detection',\n",
       "   'Representation (systemics)',\n",
       "   'Haar wavelet',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Face (geometry)',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '2148603752',\n",
       "   '2139212933',\n",
       "   '2132984323',\n",
       "   '2128272608',\n",
       "   '2217896605',\n",
       "   '2140235142',\n",
       "   '2124351082',\n",
       "   '2159686933',\n",
       "   '26816478']},\n",
       " {'id': '2156539399',\n",
       "  'title': 'Human Detection Based on a Probabilistic Assembly of Robust Part Detectors',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '962',\n",
       "  'abstract': 'We describe a novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion. Humans are modeled as flexible assemblies of parts, and robust part detection is the key to the approach. The parts are represented by co-occurrences of local features which captures the spatial layout of the partrsquos appearance. Feature selection and the part detectors are learnt from training images using AdaBoost. The detection algorithm is very efficient as (i) all part detectors use the same initial features, (ii) a coarse-to-fine cascade approach is used for part detection, (iii) a part assembly strategy reduces the number of spurious detections and the search space. The results outperform existing human detectors.',\n",
       "  'date': 2004,\n",
       "  'authors': ['Krystian Mikolajczyk 1, Cordelia Schmid 2, Andrew Zisserman 1'],\n",
       "  'related_topics': ['Face detection',\n",
       "   'AdaBoost',\n",
       "   'Feature selection',\n",
       "   'Clutter',\n",
       "   'Probabilistic logic',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Detector',\n",
       "   'Key (cryptography)',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2164598857',\n",
       "   '2124386111',\n",
       "   '2177274842',\n",
       "   '2154422044',\n",
       "   '2152473410',\n",
       "   '1608462934',\n",
       "   '2155511848',\n",
       "   '2502277634',\n",
       "   '1555563476',\n",
       "   '2097041931']},\n",
       " {'id': '2914885528',\n",
       "  'title': 'Color indexing',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '8,631',\n",
       "  'abstract': \"Computer vision is moving into a new era in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, unconstrained environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the identity of an object with a known location, and determining the location of a known object. Color can be successfully used for both tasks. This dissertation demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection which allows real-time indexing into a large database of stored models. It demonstrates techniques for dealing with crowded scenes and with models with similar color signatures. For solving the location problem it introduces an algorithm called Histogram Backprojection which performs this task efficiently in crowded scenes.\",\n",
       "  'date': 1991,\n",
       "  'authors': ['Michael James Swain', 'Dana H. Ballard'],\n",
       "  'related_topics': ['Color normalization',\n",
       "   'Content-based image retrieval',\n",
       "   'Histogram',\n",
       "   'Intersection',\n",
       "   'Object (computer science)',\n",
       "   'Search engine indexing',\n",
       "   'Computer vision',\n",
       "   'Identity (object-oriented programming)',\n",
       "   'Robot',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2115738369',\n",
       "   '2913703059',\n",
       "   '3021212382',\n",
       "   '2415527960',\n",
       "   '2125756925',\n",
       "   '2119204143',\n",
       "   '2172373809',\n",
       "   '2489504689',\n",
       "   '2069266228',\n",
       "   '2136654525']},\n",
       " {'id': '2134731454',\n",
       "  'title': 'Unsupervised Learning by Probabilistic Latent Semantic Analysis',\n",
       "  'reference_count': '23',\n",
       "  'citation_count': '3,185',\n",
       "  'abstract': 'This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Thomas Hofmann'],\n",
       "  'related_topics': ['Probabilistic latent semantic analysis',\n",
       "   'Latent semantic analysis',\n",
       "   'Latent Dirichlet allocation'],\n",
       "  'references': ['1902027874',\n",
       "   '2798909945',\n",
       "   '2147152072',\n",
       "   '2049633694',\n",
       "   '1956559956',\n",
       "   '1983578042',\n",
       "   '2072773380',\n",
       "   '1524704912',\n",
       "   '2127314673',\n",
       "   '2064580901']},\n",
       " {'id': '2165828254',\n",
       "  'title': 'SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '1,532',\n",
       "  'abstract': 'We consider visual category recognition in the framework of measuring similarities, or equivalently perceptual distances, to prototype examples of categories. This approach is quite flexible, and permits recognition based on color, texture, and particularly shape, in a homogeneous framework. While nearest neighbor classifiers are natural in this setting, they suffer from the problem of high variance (in bias-variance decomposition) in the case of limited sampling. Alternatively, one could use support vector machines but they involve time-consuming optimization and computation of pairwise distances. We propose a hybrid of these two methods which deals naturally with the multiclass setting, has reasonable computational complexity both in training and at run time, and yields excellent results in practice. The basic idea is to find close neighbors to a query sample and train a local support vector machine that preserves the distance function on the collection of neighbors. Our method can be applied to large, multiclass data sets for which it outperforms nearest neighbor and support vector machines, and remains efficient when the problem becomes intractable for support vector machines. A wide variety of distance functions can be used and our experiments show state-of-the-art performance on a number of benchmark data sets for shape and texture classification (MNIST, USPS, CUReT) and object recognition (Caltech- 101). On Caltech-101 we achieved a correct classification rate of 59.05%(±0.56%) at 15 training images per class, and 66.23%(±0.48%) at 30 training images.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Hao Zhang', 'A.C. Berg', 'M. Maire', 'J. Malik'],\n",
       "  'related_topics': ['k-nearest neighbors algorithm',\n",
       "   'Support vector machine',\n",
       "   'Caltech 101',\n",
       "   'MNIST database',\n",
       "   'Computational complexity theory',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Discriminative model',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Machine learning',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2151103935',\n",
       "   '2310919327',\n",
       "   '2162915993',\n",
       "   '2057175746',\n",
       "   '2166049352',\n",
       "   '2104978738',\n",
       "   '1624854622',\n",
       "   '2147800946',\n",
       "   '2168002178',\n",
       "   '1484228140']},\n",
       " {'id': '2113606819',\n",
       "  'title': 'Efficient sparse coding algorithms',\n",
       "  'reference_count': '14',\n",
       "  'citation_count': '3,225',\n",
       "  'abstract': 'Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.',\n",
       "  'date': 2006,\n",
       "  'authors': ['Honglak Lee', 'Alexis Battle', 'Rajat Raina', 'Andrew Y. Ng'],\n",
       "  'related_topics': ['Sparse approximation', 'K-SVD', 'Neural coding'],\n",
       "  'references': ['2063978378',\n",
       "   '2078204800',\n",
       "   '2145889472',\n",
       "   '2105464873',\n",
       "   '2140499889',\n",
       "   '2004915807',\n",
       "   '2101933716',\n",
       "   '16591383',\n",
       "   '2074376560',\n",
       "   '2146672645']},\n",
       " {'id': '2161516371',\n",
       "  'title': 'Image super-resolution as sparse representation of raw image patches',\n",
       "  'reference_count': '26',\n",
       "  'citation_count': '1,803',\n",
       "  'abstract': 'This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse and the recovered high-resolution image is competitive or even superior in quality to images produced by other SR methods.',\n",
       "  'date': 2008,\n",
       "  'authors': ['Jianchao Yang', 'J. Wright', 'T. Huang', 'Yi Ma'],\n",
       "  'related_topics': ['K-SVD', 'Sparse approximation', 'Image processing'],\n",
       "  'references': ['2296616510',\n",
       "   '2053186076',\n",
       "   '2160547390',\n",
       "   '2153663612',\n",
       "   '2050834445',\n",
       "   '2118963448',\n",
       "   '2165939075',\n",
       "   '2097074225',\n",
       "   '2149760002',\n",
       "   '2105464873']},\n",
       " {'id': '1624854622',\n",
       "  'title': 'Object recognition with features inspired by visual cortex',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '1,203',\n",
       "  'abstract': \"We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['T. Serre', 'L. Wolf', 'T. Poggio'],\n",
       "  'related_topics': ['Cognitive neuroscience of visual object recognition',\n",
       "   '3D single-object recognition',\n",
       "   'Feature (machine learning)',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Object detection',\n",
       "   'Feature extraction',\n",
       "   'Caltech 101',\n",
       "   'Object (computer science)',\n",
       "   'Face detection',\n",
       "   'Visual cortex',\n",
       "   'Edge detection',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['3097096317',\n",
       "   '2124386111',\n",
       "   '2057175746',\n",
       "   '2154422044',\n",
       "   '2134557905',\n",
       "   '2152473410',\n",
       "   '2155511848',\n",
       "   '1949116567',\n",
       "   '2149194912',\n",
       "   '2171188998']},\n",
       " {'id': '1516111018',\n",
       "  'title': 'An introduction to variational methods for graphical models',\n",
       "  'reference_count': '59',\n",
       "  'citation_count': '3,111',\n",
       "  'abstract': 'This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Michael I. Jordan 1, Zoubin Ghahramani 2, Tommi S. Jaakkola 3, Lawrence K. Saul 4'],\n",
       "  'related_topics': ['Graphical model',\n",
       "   'Variational Bayesian methods',\n",
       "   'Variational message passing',\n",
       "   'Approximate inference',\n",
       "   'Bayesian network',\n",
       "   'Inference',\n",
       "   'Boltzmann machine',\n",
       "   'Markov chain',\n",
       "   'Theoretical computer science',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2099111195',\n",
       "   '2159080219',\n",
       "   '1573186872',\n",
       "   '2049633694',\n",
       "   '2171265988',\n",
       "   '2982720039',\n",
       "   '1746680969',\n",
       "   '2567948266',\n",
       "   '2397866408',\n",
       "   '1993845689']},\n",
       " {'id': '1699734612',\n",
       "  'title': 'Saliency, Scale and Image Description',\n",
       "  'reference_count': '39',\n",
       "  'citation_count': '1,191',\n",
       "  'abstract': 'Many computer vision problems can be considered to consist of two main tasks: the extraction of image content descriptions and their subsequent matching. The appropriate choice of type and level of description is of course task dependent, yet it is generally accepted that the low-level or so called early vision layers in the Human Visual System are context independent. This paper concentrates on the use of low-level approaches for solving computer vision problems and discusses three inter-related aspects of this: saliencys scale selection and content description. In contrast to many previous approaches which separate these tasks, we argue that these three aspects are intrinsically related. Based on this observation, a multiscale algorithm for the selection of salient regions of an image is introduced and its application to matching type problems such as tracking, object recognition and image retrieval is demonstrated.',\n",
       "  'date': 2001,\n",
       "  'authors': ['Timor Kadir', 'Michael Brady'],\n",
       "  'related_topics': ['Automatic image annotation',\n",
       "   'Human visual system model',\n",
       "   'Visual Word',\n",
       "   'Scale space',\n",
       "   'Image retrieval',\n",
       "   'Scale-space axioms',\n",
       "   'Feature extraction',\n",
       "   'Cognitive neuroscience of visual object recognition',\n",
       "   'Pattern recognition',\n",
       "   'Computer vision',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2115755118',\n",
       "   '2150134853',\n",
       "   '2914885528',\n",
       "   '2124087378',\n",
       "   '2111308925',\n",
       "   '2156447271',\n",
       "   '2103504761',\n",
       "   '2109863423',\n",
       "   '2098152234',\n",
       "   '2003370853']},\n",
       " {'id': '2165299997',\n",
       "  'title': 'A modified particle swarm optimizer',\n",
       "  'reference_count': '6',\n",
       "  'citation_count': '14,398',\n",
       "  'abstract': 'Evolutionary computation techniques, genetic algorithms, evolutionary strategies and genetic programming are motivated by the evolution of nature. A population of individuals, which encode the problem solutions are manipulated according to the rule of survival of the fittest through \"genetic\" operations, such as mutation, crossover and reproduction. A best solution is evolved through the generations. In contrast to evolutionary computation techniques, Eberhart and Kennedy developed a different algorithm through simulating social behavior (R.C. Eberhart et al., 1996; R.C. Eberhart and J. Kennedy, 1996; J. Kennedy and R.C. Eberhart, 1995; J. Kennedy, 1997). As in other algorithms, a population of individuals exists. This algorithm is called particle swarm optimization (PSO) since it resembles a school of flying birds. In a particle swarm optimizer, instead of using genetic operators, these individuals are \"evolved\" by cooperation and competition among the individuals themselves through generations. Each particle adjusts its flying according to its own flying experience and its companions\\' flying experience. We introduce a new parameter, called inertia weight, into the original particle swarm optimizer. Simulations have been done to illustrate the significant and effective impact of this new parameter on the particle swarm optimizer.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Y. Shi', 'R. Eberhart'],\n",
       "  'related_topics': ['Particle swarm optimization',\n",
       "   'Multi-swarm optimization',\n",
       "   'Evolutionary computation',\n",
       "   'Imperialist competitive algorithm',\n",
       "   'Genetic programming',\n",
       "   'Population',\n",
       "   'Crossover',\n",
       "   'Mutation (genetic algorithm)',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science'],\n",
       "  'references': ['1639032689',\n",
       "   '2109364787',\n",
       "   '1576818901',\n",
       "   '2166843422',\n",
       "   '1580876177',\n",
       "   '2135646341']},\n",
       " {'id': '2097571405',\n",
       "  'title': 'An Introduction to Genetic Algorithms',\n",
       "  'reference_count': '72',\n",
       "  'citation_count': '17,058',\n",
       "  'abstract': 'From the Publisher: \"This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what\\'s being done with them, and where they are going -- this is the book. -- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting \"general purpose\" nature of genetic algorithms as search methods that can be employed across disciplines. An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader\\'s understanding of the text. The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Melanie Mitchell'],\n",
       "  'related_topics': ['Genetic programming',\n",
       "   'Evolutionary computation',\n",
       "   'Scientific modelling',\n",
       "   'Artificial life',\n",
       "   'Terminology',\n",
       "   'Game theory',\n",
       "   'Field (computer science)',\n",
       "   'Data science',\n",
       "   'Computational model',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1639032689',\n",
       "   '2581275558',\n",
       "   '2133671888',\n",
       "   '2062663664',\n",
       "   '2028569720',\n",
       "   '2156728410',\n",
       "   '2326587081',\n",
       "   '1606791384',\n",
       "   '123765585',\n",
       "   '1587100796']},\n",
       " {'id': '2106334424',\n",
       "  'title': 'Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '8,566',\n",
       "  'abstract': \"Evolutionary algorithms (EAs) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individual's fitness dependent on the number of external nondominated points that dominate it, (c) preserving population diversity using the Pareto dominance relationship, and (d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EAs on the 0/1 knapsack problem.\",\n",
       "  'date': 1999,\n",
       "  'authors': ['E. Zitzler', 'L. Thiele'],\n",
       "  'related_topics': ['Evolutionary algorithm',\n",
       "   'Multi-objective optimization',\n",
       "   'Evolutionary computation',\n",
       "   'Knapsack problem',\n",
       "   'Optimization problem',\n",
       "   'Pareto principle',\n",
       "   'Population',\n",
       "   'Mating pool',\n",
       "   'Mathematical optimization',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['1639032689',\n",
       "   '1497256448',\n",
       "   '2125899728',\n",
       "   '2151554678',\n",
       "   '2116661285',\n",
       "   '1504943474',\n",
       "   '1905847227',\n",
       "   '2121365620',\n",
       "   '2261054240',\n",
       "   '1558919105']},\n",
       " {'id': '2168081761',\n",
       "  'title': 'Biogeography-Based Optimization',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '3,203',\n",
       "  'abstract': 'Biogeography is the study of the geographical distribution of biological organisms. Mathematical equations that govern the distribution of organisms were first discovered and developed during the 1960s. The mindset of the engineer is that we can learn from nature. This motivates the application of biogeography to optimization problems. Just as the mathematics of biological genetics inspired the development of genetic algorithms (GAs), and the mathematics of biological neurons inspired the development of artificial neural networks, this paper considers the mathematics of biogeography as the basis for the development of a new field: biogeography-based optimization (BBO). We discuss natural biogeography and its mathematics, and then discuss how it can be used to solve optimization problems. We see that BBO has features in common with other biology-based optimization methods, such as GAs and particle swarm optimization (PSO). This makes BBO applicable to many of the same types of problems that GAs and PSO are used for, namely, high-dimension problems with multiple local optima. However, BBO also has some features that are unique among biology-based optimization methods. We demonstrate the performance of BBO on a set of 14 standard benchmarks and compare it with seven other biology-based optimization algorithms. We also demonstrate BBO on a real-world sensor selection problem for aircraft engine health estimation.',\n",
       "  'date': 2008,\n",
       "  'authors': ['D. Simon'],\n",
       "  'related_topics': ['Multi-swarm optimization',\n",
       "   'Optimization problem',\n",
       "   'Evolutionary algorithm',\n",
       "   'Genetic algorithm',\n",
       "   'Local search (optimization)',\n",
       "   'Swarm intelligence',\n",
       "   'Local optimum',\n",
       "   'Particle swarm optimization',\n",
       "   'Artificial intelligence',\n",
       "   'Mathematical optimization'],\n",
       "  'references': ['1639032689',\n",
       "   '2142183404',\n",
       "   '1997600725',\n",
       "   '2102248717',\n",
       "   '2790374560',\n",
       "   '315572163',\n",
       "   '2013205100',\n",
       "   '1498178627',\n",
       "   '2027945080',\n",
       "   '2117640392']},\n",
       " {'id': '2017337590',\n",
       "  'title': 'Wrappers for feature subset selection',\n",
       "  'reference_count': '122',\n",
       "  'citation_count': '10,177',\n",
       "  'abstract': 'Abstract In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.',\n",
       "  'date': 1997,\n",
       "  'authors': ['Ron Kohavi 1, George H. John 2'],\n",
       "  'related_topics': ['Feature selection',\n",
       "   'Minimum redundancy feature selection',\n",
       "   'Selection (relational algebra)',\n",
       "   'Feature (computer vision)',\n",
       "   'Decision tree',\n",
       "   'Markov blanket',\n",
       "   'Relation (database)',\n",
       "   'Filter (signal processing)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1639032689',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '2122410182',\n",
       "   '2084812512',\n",
       "   '2125055259',\n",
       "   '1594031697',\n",
       "   '2149706766',\n",
       "   '2340020088',\n",
       "   '1680392829']},\n",
       " {'id': '1999284878',\n",
       "  'title': 'Teaching-learning-based optimization: A novel method for constrained mechanical design optimization problems',\n",
       "  'reference_count': '30',\n",
       "  'citation_count': '2,684',\n",
       "  'abstract': \"A new efficient optimization method, called 'Teaching-Learning-Based Optimization (TLBO)', is proposed in this paper for the optimization of mechanical design problems. This method works on the effect of influence of a teacher on learners. Like other nature-inspired algorithms, TLBO is also a population-based method and uses a population of solutions to proceed to the global solution. The population is considered as a group of learners or a class of learners. The process of TLBO is divided into two parts: the first part consists of the 'Teacher Phase' and the second part consists of the 'Learner Phase'. 'Teacher Phase' means learning from the teacher and 'Learner Phase' means learning by the interaction between learners. The basic philosophy of the TLBO method is explained in detail. To check the effectiveness of the method it is tested on five different constrained benchmark test functions with different characteristics, four different benchmark mechanical design problems and six mechanical design optimization problems which have real world applications. The effectiveness of the TLBO method is compared with the other population-based optimization algorithms based on the best solution, average solution, convergence rate and computational effort. Results show that TLBO is more effective and efficient than the other optimization methods for the mechanical design optimization problems considered. This novel optimization method can be easily extended to other engineering design optimization problems.\",\n",
       "  'date': 2011,\n",
       "  'authors': ['R. V. Rao', 'V. J. Savsani', 'D. P. Vakharia'],\n",
       "  'related_topics': ['Engineering optimization',\n",
       "   'Continuous optimization',\n",
       "   'Probabilistic-based design optimization',\n",
       "   'Multi-objective optimization',\n",
       "   'Multi-swarm optimization',\n",
       "   'Test functions for optimization',\n",
       "   'Optimization problem',\n",
       "   'Population',\n",
       "   'Mathematical optimization',\n",
       "   'Computer science'],\n",
       "  'references': ['2152195021',\n",
       "   '1639032689',\n",
       "   '2613176274',\n",
       "   '1573676079',\n",
       "   '2143560894',\n",
       "   '2287814884',\n",
       "   '1519405745',\n",
       "   '2144317842',\n",
       "   '3147414854',\n",
       "   '2145479420']},\n",
       " {'id': '2167101736',\n",
       "  'title': 'A survey on feature selection methods',\n",
       "  'reference_count': '80',\n",
       "  'citation_count': '2,694',\n",
       "  'abstract': 'Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Girish Chandrashekar', 'Ferat Sahin'],\n",
       "  'related_topics': ['Feature selection',\n",
       "   'Feature (machine learning)',\n",
       "   'Feature extraction',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Variable elimination',\n",
       "   'Machine learning',\n",
       "   'Data mining',\n",
       "   'Filter (signal processing)',\n",
       "   'Dimension (data warehouse)',\n",
       "   'Computer science',\n",
       "   'Computation',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153635508',\n",
       "   '2124776405',\n",
       "   '2152195021',\n",
       "   '1995945562',\n",
       "   '2904250082',\n",
       "   '1639032689',\n",
       "   '2119479037',\n",
       "   '2143426320',\n",
       "   '2154053567',\n",
       "   '2099741732']},\n",
       " {'id': '2165171393',\n",
       "  'title': 'Handling multiple objectives with particle swarm optimization',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '3,825',\n",
       "  'abstract': 'This paper presents an approach in which Pareto dominance is incorporated into particle swarm optimization (PSO) in order to allow this heuristic to handle problems with several objective functions. Unlike other current proposals to extend PSO to solve multiobjective optimization problems, our algorithm uses a secondary (i.e., external) repository of particles that is later used by other particles to guide their own flight. We also incorporate a special mutation operator that enriches the exploratory capabilities of our algorithm. The proposed approach is validated using several test functions and metrics taken from the standard literature on evolutionary multiobjective optimization. Results indicate that the approach is highly competitive and that can be considered a viable alternative to solve multiobjective optimization problems.',\n",
       "  'date': 2004,\n",
       "  'authors': ['C.A.C. Coello 1, G.T. Pulido 1, M.S. Lechuga 2'],\n",
       "  'related_topics': ['Multi-swarm optimization',\n",
       "   'Metaheuristic',\n",
       "   'Multi-objective optimization',\n",
       "   'Imperialist competitive algorithm',\n",
       "   'Particle swarm optimization',\n",
       "   'Test functions for optimization',\n",
       "   'Evolutionary algorithm',\n",
       "   'Swarm intelligence',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2126105956',\n",
       "   '1639032689',\n",
       "   '2106334424',\n",
       "   '1553373771',\n",
       "   '2125899728',\n",
       "   '2167159964',\n",
       "   '1585939719',\n",
       "   '2116661285',\n",
       "   '1504943474',\n",
       "   '2152551290']},\n",
       " {'id': '1501500081',\n",
       "  'title': 'A Survey of Clustering Data Mining Techniques',\n",
       "  'reference_count': '210',\n",
       "  'citation_count': '3,528',\n",
       "  'abstract': 'Clustering is the division of data into groups of similar objects. In clustering, some details are disregarded in exchange for data simplification. Clustering can be viewed as a data modeling technique that provides for concise summaries of the data. Clustering is therefore related to many disciplines and plays an important role in a broad range of applications. The applications of clustering usually deal with large datasets and data with many attributes. Exploration of such data is a subject of data mining. This survey concentrates on clustering algorithms from a data mining perspective.',\n",
       "  'date': 2005,\n",
       "  'authors': ['Pavel Berkhin'],\n",
       "  'related_topics': ['Cluster analysis',\n",
       "   'Consensus clustering',\n",
       "   'Hierarchical clustering',\n",
       "   'Clustering high-dimensional data',\n",
       "   'Brown clustering',\n",
       "   'Conceptual clustering',\n",
       "   'Biclustering',\n",
       "   'Data modeling',\n",
       "   'Data mining',\n",
       "   'Computer science'],\n",
       "  'references': ['2140190241',\n",
       "   '1639032689',\n",
       "   '2099111195',\n",
       "   '1679913846',\n",
       "   '1992419399',\n",
       "   '2165874743',\n",
       "   '1673310716',\n",
       "   '2156718197',\n",
       "   '2148694408',\n",
       "   '2049633694']},\n",
       " {'id': '2073257493',\n",
       "  'title': 'An interactive activation model of context effects in letter perception: I. An account of basic findings.',\n",
       "  'reference_count': '45',\n",
       "  'citation_count': '6,750',\n",
       "  'abstract': '',\n",
       "  'date': 1981,\n",
       "  'authors': ['James L. McClelland', 'David E. Rumelhart'],\n",
       "  'related_topics': ['Context effect',\n",
       "   'Visual perception',\n",
       "   'Perception',\n",
       "   'Missing letter effect',\n",
       "   'Word superiority effect',\n",
       "   'Transposed letter effect',\n",
       "   'Contextual Associations',\n",
       "   'Cognitive psychology',\n",
       "   'Cognition',\n",
       "   'Natural language processing',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['1509703770',\n",
       "   '2007780422',\n",
       "   '2045597501',\n",
       "   '2068868410',\n",
       "   '2053127376',\n",
       "   '2147311265',\n",
       "   '2098683904',\n",
       "   '2040187703',\n",
       "   '2006769754',\n",
       "   '2154634575']},\n",
       " {'id': '2021878536',\n",
       "  'title': 'User Centered System Design: New Perspectives on Human-Computer Interaction',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,667',\n",
       "  'abstract': \"Contents: S.W. Draper, D.A. Norman, C. Lewis, Introduction. Part I:User Centered System Design. K. Hooper, Architectural Design: An Analogy. L.J. Bannon, Issues in Design: Some Notes. D.A. Norman, Cognitive Engineering. Part II:The Interface Experience. B.K. Laurel, Interface as Mimesis. E.L. Hutchins, J.D. Hollan, D.A. NormanDirect Manipulation Interfaces. A.A. diSessa, Notes on the Future of Programming: Breaking the Utility Barrier. Part III:Users' Understandings. M.S. Riley, User Understanding. C. Lewis, Understanding What's Happening in System Interactions. D. Owen, Naive Theories of Computation. A.A. diSessa, Models of Computation. W. Mark, Knowledge-Based Interface Design. Part IV:User Activities. A. Cypher, The Structure of Users' Activities. Y. Miyata, D.A. Norman, Psychological Issues in Support of Multiple Activities. R. Reichman, Communication Paradigms for a Window System. Part V:Toward a Pragmatics of Human-Machine Communication. W. Buxton, There's More to Interaction Than Meets the Eye: Some Issues in Manual Input. S.W. Draper, Display Managers as the Basis for User-Machine Communication. Part VI:Information Flow. D. Owen, Answers First, Then Questions. C.E. O'Malley, Helping Users Help Themselves. L.J. Bannon, Helping Users Help Each Other. C. Lewis, D.A. Norman, Designing for Error. L.J. Bannon, Computer-Mediated Communication. Part VII:The Context of Computing. J.S. Brown, From Cognitive to Social Ergonomics and Beyond.\",\n",
       "  'date': 1985,\n",
       "  'authors': ['Donald A. Norman', 'Stephen W. Draper'],\n",
       "  'related_topics': ['User experience design',\n",
       "   'User interface',\n",
       "   'Cognitive ergonomics',\n",
       "   'Context (language use)',\n",
       "   'Analogy',\n",
       "   'Pragmatics',\n",
       "   'Interface (Java)',\n",
       "   'Human–computer interaction',\n",
       "   'Information flow',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '3121257585',\n",
       "   '2080957047',\n",
       "   '74870555',\n",
       "   '2082897112',\n",
       "   '1535418185',\n",
       "   '2160271672',\n",
       "   '2132920211']},\n",
       " {'id': '1490454746',\n",
       "  'title': 'Feature discovery by competitive learning',\n",
       "  'reference_count': '5',\n",
       "  'citation_count': '1,782',\n",
       "  'abstract': 'This paper reporis the results of our studies with an unsupervised learning paradigm which we have called “Competitive Learning.” We have examined competitive learning using both computer simulation and formal analysis and hove found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide o way to discover the salient, general features which can be used to classify o set of patterns. We show how o very simply competitive mechanism con discover a set of feature detectors which capture important aspects of the set of stimulus input patterns. We 0150 show how these feature detectors con form the basis of o multilayer system that con serve to learn categorizations of stimulus sets which ore not linearly separable. We show how the use of correlated stimuli con serve IX o kind of “teaching” input to the system to allow the development of feature detectors which would not develop otherwise. Although we find the competitive learning mechanism o very interesting and powerful learning principle, we do not, of course, imagine thot it is the only learning principle. Competitive learning is cm essentially nonassociative stotisticol learning scheme. We certainly imagine that other kinds of learning mechanisms will be involved in the building of associations among patterns of activation in o more complete neural network. We offer this analysis of these competitive learning mechanisms to further our understanding of how simple adaptive networks can discover features importont in the description of the stimulus environment in which the system finds itself.',\n",
       "  'date': 1988,\n",
       "  'authors': ['David E. Rumelhart', 'David Zipser'],\n",
       "  'related_topics': ['Competitive learning',\n",
       "   'Feature learning',\n",
       "   'Instance-based learning',\n",
       "   'Unsupervised learning',\n",
       "   'Semi-supervised learning',\n",
       "   'Self-organizing map',\n",
       "   'Artificial neural network',\n",
       "   'Linear separability',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['2073257493',\n",
       "   '1514711945',\n",
       "   '2113653296',\n",
       "   '2010315761',\n",
       "   '2103170504']},\n",
       " {'id': '2115647291',\n",
       "  'title': 'Direct manipulation interfaces',\n",
       "  'reference_count': '20',\n",
       "  'citation_count': '2,231',\n",
       "  'abstract': \"Direct manipulation has been lauded as a good form of interface design, and some interfaces that have this property have been well received by users. In this article we seek a cognitive account of both the advantages and disadvantages of direct manipulation interfaces. We identify two underlying phenomena that give rise to the feeling of directness. One deals with the information processing distance between the user's intentions and the facilities provided by the machine. Reduction of this distance makes the interface feel direct by reducing the effort required of the user to accomplish goals. The second phenomenon concerns the relation between the input and output vocabularies of the interface language. In particular, direct manipulation requires that the system provide representations of objects that behave as if they are the objects themselves. This provides the feeling of directness of manipulation.\",\n",
       "  'date': 1985,\n",
       "  'authors': ['Edwin L. Hutchins', 'James D. Hollan', 'Donald A. Norman'],\n",
       "  'related_topics': ['Direct manipulation interface',\n",
       "   'Interface (Java)',\n",
       "   'Information processing',\n",
       "   'Property (programming)',\n",
       "   'Human–computer interaction',\n",
       "   'Vocabulary',\n",
       "   'Relation (database)',\n",
       "   'Computer science',\n",
       "   'Data processing',\n",
       "   'Feeling'],\n",
       "  'references': ['2099305423',\n",
       "   '2021878536',\n",
       "   '2175030280',\n",
       "   '2005639687',\n",
       "   '1539777654',\n",
       "   '1979887415',\n",
       "   '2954951251',\n",
       "   '2037893691',\n",
       "   '121934918',\n",
       "   '2064302241']},\n",
       " {'id': '1505136099',\n",
       "  'title': 'Learning by statistical cooperation of self-interested neuron-like computing elements.',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '272',\n",
       "  'abstract': 'Since the usual approaches to cooperative computation in networks of neuron-like computating elements do not assume that network components have any \"preferences\", they do not make substantive contact with game theoretic concepts, despite their use of some of the same terminology. In the approach presented here, however, each network component, or adaptive element, is a self-interested agent that prefers some inputs over others and \"works\" toward obtaining the most highly preferred inputs. Here we describe an adaptive element that is robust enough to learn to cooperate with other elements like itself in order to further its self-interests. It is argued that some of the longstanding problems concerning adaptation and learning by networks might be solvable by this form of cooperativity, and computer simulation experiments are described that show how networks of self-interested components that are sufficiently robust can solve rather difficult learning problems. We then place the approach in its proper historical and theoretical perspective through comparison with a number of related algorithms. A secondary aim of this article is to suggest that beyond what is explicitly illustrated here, there is a wealth of ideas from game theory and allied disciplines such as mathematical economics that can be of use in thinking about cooperative computation in both nervous systems and man-made systems.',\n",
       "  'date': 1984,\n",
       "  'authors': ['Barto Ag'],\n",
       "  'related_topics': ['Game theory',\n",
       "   'Terminology',\n",
       "   'Adaptation (computer science)',\n",
       "   'Element (category theory)',\n",
       "   'Computer science',\n",
       "   'Theoretical computer science',\n",
       "   'Computation',\n",
       "   'Order (exchange)',\n",
       "   'Perspective (graphical)',\n",
       "   'Developmental psychology',\n",
       "   'Game theoretic'],\n",
       "  'references': ['2154642048',\n",
       "   '2100677568',\n",
       "   '2150884987',\n",
       "   '2119717200',\n",
       "   '2144323544',\n",
       "   '2016534914',\n",
       "   '2112841646',\n",
       "   '2126404188',\n",
       "   '2080759927',\n",
       "   '1686514609']},\n",
       " {'id': '2163352848',\n",
       "  'title': 'Multiresolution gray-scale and rotation invariant texture classification with local binary patterns',\n",
       "  'reference_count': '46',\n",
       "  'citation_count': '16,231',\n",
       "  'abstract': 'Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed \"uniform,\" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the \"uniform\" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns.',\n",
       "  'date': 2002,\n",
       "  'authors': ['T. Ojala', 'M. Pietikainen', 'T. Maenpaa'],\n",
       "  'related_topics': ['Local binary patterns',\n",
       "   'Binary pattern',\n",
       "   'Image texture',\n",
       "   'Invariant (physics)',\n",
       "   'Texture Descriptor',\n",
       "   'Multiresolution analysis',\n",
       "   'Contextual image classification',\n",
       "   'Operator (computer programming)',\n",
       "   'Pattern recognition',\n",
       "   'Mathematics',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2039051707',\n",
       "   '3017143921',\n",
       "   '2098347925',\n",
       "   '2106798282',\n",
       "   '2132047332',\n",
       "   '2159988601',\n",
       "   '2021751319',\n",
       "   '1993655741',\n",
       "   '2136343973',\n",
       "   '2124353687']},\n",
       " {'id': '2117812871',\n",
       "  'title': 'Pattern recognition and neural networks',\n",
       "  'reference_count': '102',\n",
       "  'citation_count': '9,263',\n",
       "  'abstract': 'From the Publisher: Pattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.',\n",
       "  'date': 1995,\n",
       "  'authors': ['Brian D. Ripley', 'N. L. Hjort'],\n",
       "  'related_topics': ['Feature (machine learning)',\n",
       "   'Intelligent character recognition',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Neocognitron',\n",
       "   'Cellular neural network',\n",
       "   'Artificial neural network',\n",
       "   'Relation (database)',\n",
       "   'Computer science',\n",
       "   'Pattern recognition',\n",
       "   'Subject (documents)',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2156909104',\n",
       "   '1554663460',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '1679913846',\n",
       "   '2112076978',\n",
       "   '2046079134',\n",
       "   '2147800946',\n",
       "   '1536929369']},\n",
       " {'id': '1596324102',\n",
       "  'title': 'Machine Learning: An Artificial Intelligence Approach',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '2,546',\n",
       "  'abstract': 'This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective. Research directions covered include: learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis.',\n",
       "  'date': 2013,\n",
       "  'authors': ['R. S. Michalski', 'J. G. Carbonell', 'T. M. Mitchell'],\n",
       "  'related_topics': ['Robot learning',\n",
       "   'Explanation-based learning',\n",
       "   'Algorithmic learning theory',\n",
       "   'Learning sciences',\n",
       "   'Hyper-heuristic',\n",
       "   'Knowledge acquisition',\n",
       "   'Expert system',\n",
       "   'Knowledge base',\n",
       "   'Data science',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Computer science'],\n",
       "  'references': ['1501500081',\n",
       "   '2126385963',\n",
       "   '2963305465',\n",
       "   '2137130182',\n",
       "   '2008906462',\n",
       "   '2100677568',\n",
       "   '2040884411',\n",
       "   '2147169507',\n",
       "   '2019363670',\n",
       "   '2133462743']},\n",
       " {'id': '1569296262',\n",
       "  'title': 'Temporal credit assignment in reinforcement learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '966',\n",
       "  'abstract': '',\n",
       "  'date': 1983,\n",
       "  'authors': ['Richard Stuart Sutton'],\n",
       "  'related_topics': ['Reinforcement learning',\n",
       "   'Computer science',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Credit assignment',\n",
       "   'Error-driven learning'],\n",
       "  'references': ['2121863487',\n",
       "   '2107726111',\n",
       "   '2155027007',\n",
       "   '1944672',\n",
       "   '2100677568',\n",
       "   '2119717200',\n",
       "   '2604636228',\n",
       "   '1557517019',\n",
       "   '2964352247']},\n",
       " {'id': '2075379212',\n",
       "  'title': 'Finite Markov chains',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '4,972',\n",
       "  'abstract': '',\n",
       "  'date': 1976,\n",
       "  'authors': ['John G. Kemeny', 'J. Laurie Snell'],\n",
       "  'related_topics': ['Examples of Markov chains',\n",
       "   'Markov chain',\n",
       "   'Markov chain mixing time',\n",
       "   'Markov kernel',\n",
       "   'Markov property',\n",
       "   'Markov renewal process',\n",
       "   'Lumpability',\n",
       "   'Markov process',\n",
       "   'Computer science',\n",
       "   'Statistical physics'],\n",
       "  'references': ['2136796925',\n",
       "   '2100677568',\n",
       "   '2161984370',\n",
       "   '1586184796',\n",
       "   '1480546554',\n",
       "   '2117740169',\n",
       "   '2142645441',\n",
       "   '2086645750',\n",
       "   '2014478203',\n",
       "   '2144979178']},\n",
       " {'id': '2042264548',\n",
       "  'title': 'An introduction to computing with neural nets',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '12,289',\n",
       "  'abstract': 'Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets.',\n",
       "  'date': 1988,\n",
       "  'authors': ['Richard P. Lippmann'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Cluster analysis',\n",
       "   'Field (computer science)',\n",
       "   'Massively parallel',\n",
       "   'Very-large-scale integration',\n",
       "   'Artificial intelligence',\n",
       "   'Network topology',\n",
       "   'Noise (video)',\n",
       "   'Computer science',\n",
       "   'Gaussian'],\n",
       "  'references': ['2156562940',\n",
       "   '2017224880',\n",
       "   '3001909141',\n",
       "   '2015634532',\n",
       "   '2016812413',\n",
       "   '2132832266',\n",
       "   '2113229797',\n",
       "   '1973070179',\n",
       "   '2536517113',\n",
       "   '35336130']},\n",
       " {'id': '114517082',\n",
       "  'title': 'Large-Scale Machine Learning with Stochastic Gradient Descent',\n",
       "  'reference_count': '24',\n",
       "  'citation_count': '4,731',\n",
       "  'abstract': 'During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.',\n",
       "  'date': 2009,\n",
       "  'authors': ['Léon Bottou'],\n",
       "  'related_topics': ['Stochastic gradient descent',\n",
       "   'Gradient method',\n",
       "   'Gradient descent',\n",
       "   'Backpropagation',\n",
       "   'Restricted Boltzmann machine',\n",
       "   'Wake-sleep algorithm',\n",
       "   'Computational complexity theory',\n",
       "   'Context (language use)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2147880316',\n",
       "   '2135046866',\n",
       "   '2119821739',\n",
       "   '2154642048',\n",
       "   '1652505363',\n",
       "   '2035720976',\n",
       "   '2150102617',\n",
       "   '2113651538',\n",
       "   '1535810436',\n",
       "   '2068484625']},\n",
       " {'id': '2604319603',\n",
       "  'title': 'Efficient Processing of Deep Neural Networks: A Tutorial and Survey',\n",
       "  'reference_count': '136',\n",
       "  'citation_count': '1,662',\n",
       "  'abstract': 'Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Vivienne Sze', 'Yu-Hsin Chen', 'Tien-Ju Yang', 'Joel S. Emer'],\n",
       "  'related_topics': ['Machine learning',\n",
       "   'Computer science',\n",
       "   'Efficient energy use',\n",
       "   'Field (computer science)',\n",
       "   'Benchmarking',\n",
       "   'Throughput (business)',\n",
       "   'Key (cryptography)',\n",
       "   'Robotics',\n",
       "   'Computational complexity theory',\n",
       "   'Software deployment',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2194775991',\n",
       "   '2618530766',\n",
       "   '2962835968',\n",
       "   '2097117768',\n",
       "   '2919115771',\n",
       "   '1836465849',\n",
       "   '2102605133',\n",
       "   '2117539524',\n",
       "   '1903029394',\n",
       "   '2155893237']},\n",
       " {'id': '2025605741',\n",
       "  'title': 'Recommender systems survey',\n",
       "  'reference_count': '246',\n",
       "  'citation_count': '2,786',\n",
       "  'abstract': 'Recommender systems have developed in parallel with the web. They were initially based on demographic, content-based and collaborative filtering. Currently, these systems are incorporating social information. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative filtering methods and algorithms; it also explains their evolution, provides an original classification for these systems, identifies areas of future implementation and develops certain areas selected for past, present or future importance.',\n",
       "  'date': 2013,\n",
       "  'authors': ['J. Bobadilla', 'F. Ortega', 'A. Hernando', 'A. GutiéRrez'],\n",
       "  'related_topics': ['Recommender system',\n",
       "   'Collaborative filtering',\n",
       "   'Cold start',\n",
       "   'Personally identifiable information',\n",
       "   'World Wide Web',\n",
       "   'Computer science',\n",
       "   'Internet of Things'],\n",
       "  'references': ['2171960770',\n",
       "   '2054141820',\n",
       "   '1660390307',\n",
       "   '2042281163',\n",
       "   '1971040550',\n",
       "   '2149684865',\n",
       "   '2110325612',\n",
       "   '2101409192',\n",
       "   '2097726984',\n",
       "   '2100235918']},\n",
       " {'id': '1570963478',\n",
       "  'title': 'Prediction, learning, and games',\n",
       "  'reference_count': '266',\n",
       "  'citation_count': '3,645',\n",
       "  'abstract': \"This important text and reference for researchers and students in machine learning, game theory, statistics and information theory offers a comprehensive treatment of the problem of predicting individual sequences. Unlike standard statistical approaches to forecasting, prediction of individual sequences does not impose any probabilistic assumption on the data-generating mechanism. Yet, prediction algorithms can be constructed that work well for all possible sequences, in the sense that their performance is always nearly as good as the best forecasting strategy in a given reference class. The central theme is the model of prediction using expert advice, a general framework within which many related problems can be cast and discussed. Repeated game playing, adaptive data compression, sequential investment in the stock market, sequential pattern analysis, and several other problems are viewed as instances of the experts' framework and analyzed from a common nonstochastic standpoint that often reveals new and intriguing connections.\",\n",
       "  'date': 2005,\n",
       "  'authors': ['Nicolo Cesa-Bianchi 1, Gabor Lugosi 2'],\n",
       "  'related_topics': ['Repeated game',\n",
       "   'Game theory',\n",
       "   'Probabilistic logic',\n",
       "   'Information theory',\n",
       "   'Computational statistics',\n",
       "   'Machine learning',\n",
       "   'Data compression',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science',\n",
       "   'Stock market',\n",
       "   'Theme (narrative)'],\n",
       "  'references': ['2148603752',\n",
       "   '2099111195',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '3023786531',\n",
       "   '2137813581',\n",
       "   '1510073064',\n",
       "   '2168405694',\n",
       "   '1601740268',\n",
       "   '2087347434']},\n",
       " {'id': '2098257210',\n",
       "  'title': 'Fading channels: information-theoretic and communications aspects',\n",
       "  'reference_count': '440',\n",
       "  'citation_count': '2,445',\n",
       "  'abstract': 'In this paper we review the most peculiar and interesting information-theoretic and communications features of fading channels. We first describe the statistical models of fading channels which are frequently used in the analysis and design of communication systems. Next, we focus on the information theory of fading channels, by emphasizing capacity as the most important performance measure. Both single-user and multiuser transmission are examined. Further, we describe how the structure of fading channels impacts code design, and finally overview equalization of fading multipath channels.',\n",
       "  'date': 1998,\n",
       "  'authors': ['E. Biglieri 1, J. Proakis 2, S. Shamai'],\n",
       "  'related_topics': ['Fading',\n",
       "   'Channel state information',\n",
       "   'Diversity scheme',\n",
       "   'Communications system',\n",
       "   'Channel capacity',\n",
       "   'Wireless',\n",
       "   'Transmission (telecommunications)',\n",
       "   'Equalization (audio)',\n",
       "   'Communication channel',\n",
       "   'Electronic engineering',\n",
       "   'Telecommunications',\n",
       "   'Computer science',\n",
       "   'Channel code'],\n",
       "  'references': ['2130509920',\n",
       "   '2145417574',\n",
       "   '2099111195',\n",
       "   '1667950888',\n",
       "   '2118040894',\n",
       "   '2798333393',\n",
       "   '2133475491',\n",
       "   '1549664537',\n",
       "   '2132818688',\n",
       "   '2613173048']},\n",
       " {'id': '2096544401',\n",
       "  'title': 'Distributed GraphLab: a framework for machine learning and data mining in the cloud',\n",
       "  'reference_count': '37',\n",
       "  'citation_count': '1,384',\n",
       "  'abstract': 'While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees.We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Yucheng Low 1, Danny Bickson 1, Joseph Gonzalez 1, Carlos Guestrin 1, Aapo Kyrola 1, Joseph M. Hellerstein 2'],\n",
       "  'related_topics': ['Snapshot algorithm',\n",
       "   'Asynchronous communication',\n",
       "   'Cloud computing',\n",
       "   'Fault tolerance',\n",
       "   'Data processing system',\n",
       "   'Data consistency',\n",
       "   'Distributed computing',\n",
       "   'Network congestion',\n",
       "   'Software versioning',\n",
       "   'Computer science',\n",
       "   'Data mining',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2173213060',\n",
       "   '2063978378',\n",
       "   '1854214752',\n",
       "   '2189465200',\n",
       "   '2170616854',\n",
       "   '2100830825',\n",
       "   '1512387364',\n",
       "   '2109722477',\n",
       "   '2114507260',\n",
       "   '2567948266']},\n",
       " {'id': '2010630450',\n",
       "  'title': 'User Association for Load Balancing in Heterogeneous Cellular Networks',\n",
       "  'reference_count': '33',\n",
       "  'citation_count': '1,181',\n",
       "  'abstract': 'For small cell technology to significantly increase the capacity of tower-based cellular networks, mobile users will need to be actively pushed onto the more lightly loaded tiers (corresponding to, e.g., pico and femtocells), even if they offer a lower instantaneous SINR than the macrocell base station (BS). Optimizing a function of the long-term rate for each user requires (in general) a massive utility maximization problem over all the SINRs and BS loads. On the other hand, an actual implementation will likely resort to a simple biasing approach where a BS in tier j is treated as having its SINR multiplied by a factor Aj ≥ 1, which makes it appear more attractive than the heavily-loaded macrocell. This paper bridges the gap between these approaches through several physical relaxations of the network-wide association problem, whose solution is NP hard. We provide a low-complexity distributed algorithm that converges to a near-optimal solution with a theoretical performance guarantee, and we observe that simple per-tier biasing loses surprisingly little, if the bias values Aj are chosen carefully. Numerical results show a large (3.5x) throughput gain for cell-edge users and a 2x rate gain for median users relative to a maximizing received power association.',\n",
       "  'date': 2013,\n",
       "  'authors': ['Qiaoyang Ye',\n",
       "   'Beiyu Rong',\n",
       "   'Yudong Chen',\n",
       "   'M. Al-Shalash',\n",
       "   'C. Caramanis',\n",
       "   'J. G. Andrews'],\n",
       "  'related_topics': ['Macrocell',\n",
       "   'Small cell',\n",
       "   'Utility maximization problem',\n",
       "   'Femtocell',\n",
       "   'Cellular network',\n",
       "   'Load balancing (computing)',\n",
       "   'Throughput',\n",
       "   'Base station',\n",
       "   'Distributed algorithm',\n",
       "   'Computer network',\n",
       "   'Computer science'],\n",
       "  'references': ['2296319761',\n",
       "   '2136530738',\n",
       "   '2149170915',\n",
       "   '2109159967',\n",
       "   '2034420299',\n",
       "   '2037710455',\n",
       "   '1603765807',\n",
       "   '2058254491',\n",
       "   '2143690081',\n",
       "   '2065092668']},\n",
       " {'id': '2963433607',\n",
       "  'title': 'Optimization Methods for Large-Scale Machine Learning',\n",
       "  'reference_count': '0',\n",
       "  'citation_count': '1,605',\n",
       "  'abstract': 'This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques th...',\n",
       "  'date': 2018,\n",
       "  'authors': ['Léon Bottou 1, Frank E. Curtis 2, Jorge Nocedal 3'],\n",
       "  'related_topics': ['Optimization problem',\n",
       "   'Nonlinear programming',\n",
       "   'Context (language use)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Scale (chemistry)',\n",
       "   'Artificial intelligence',\n",
       "   'Deep neural networks',\n",
       "   'Improved performance',\n",
       "   'Optimization algorithm',\n",
       "   'Optimization methods'],\n",
       "  'references': ['2963959597',\n",
       "   '2952204734',\n",
       "   '3034942609',\n",
       "   '3114728946',\n",
       "   '2963563140',\n",
       "   '2963616027',\n",
       "   '2969215180',\n",
       "   '3129410129']},\n",
       " {'id': '78077100',\n",
       "  'title': 'PowerGraph: distributed graph-parallel computation on natural graphs',\n",
       "  'reference_count': '40',\n",
       "  'citation_count': '1,921',\n",
       "  'abstract': 'Large-scale graph-structured computation is central to tasks ranging from targeted advertising to natural language processing and has led to the development of several graph-parallel abstractions including Pregel and GraphLab. However, the natural graphs commonly found in the real-world have highly skewed power-law degree distributions, which challenge the assumptions made by these abstractions, limiting performance and scalability.In this paper, we characterize the challenges of computation on natural graphs in the context of existing graph-parallel abstractions. We then introduce the PowerGraph abstraction which exploits the internal structure of graph programs to address these challenges. Leveraging the PowerGraph abstraction we introduce a new approach to distributed graph placement and representation that exploits the structure of power-law graphs. We provide a detailed analysis and experimental evaluation comparing PowerGraph to two popular graph-parallel systems. Finally, we describe three different implementation strategies for PowerGraph and discuss their relative merits with empirical evaluations on large-scale real-world problems demonstrating order of magnitude gains.',\n",
       "  'date': 2012,\n",
       "  'authors': ['Joseph E. Gonzalez 1, Yucheng Low 1, Haijie Gu 1, Danny Bickson 1, Carlos Guestrin 2'],\n",
       "  'related_topics': ['Theoretical computer science',\n",
       "   'Parallel computing',\n",
       "   'Computer science',\n",
       "   'Computation',\n",
       "   'Targeted advertising',\n",
       "   'Exploit',\n",
       "   'Abstraction',\n",
       "   'Big graph',\n",
       "   'Graph',\n",
       "   'Graph analytics',\n",
       "   'Limiting'],\n",
       "  'references': ['2101196063',\n",
       "   '2189465200',\n",
       "   '2170616854',\n",
       "   '2065769502',\n",
       "   '1976969221',\n",
       "   '2107743791',\n",
       "   '2096544401',\n",
       "   '1788180225',\n",
       "   '2997134027',\n",
       "   '2146591355']},\n",
       " {'id': '2044212084',\n",
       "  'title': 'Distributed Subgradient Methods for Multi-Agent Optimization',\n",
       "  'reference_count': '31',\n",
       "  'citation_count': '2,664',\n",
       "  'abstract': 'We study a distributed computation model for optimizing a sum of convex objective functions corresponding to multiple agents. For solving this (not necessarily smooth) optimization problem, we consider a subgradient method that is distributed among the agents. The method involves every agent minimizing his/her own objective function while exchanging information locally with other agents in the network over a time-varying topology. We provide convergence results and convergence rate estimates for the subgradient method. Our convergence rate results explicitly characterize the tradeoff between a desired accuracy of the generated approximate optimal solutions and the number of iterations needed to achieve the accuracy.',\n",
       "  'date': 2009,\n",
       "  'authors': ['A. Nedic 1, A. Ozdaglar 2'],\n",
       "  'related_topics': ['Subgradient method',\n",
       "   'Convex optimization',\n",
       "   'Rate of convergence',\n",
       "   'Optimization problem',\n",
       "   'Multi-agent system',\n",
       "   'Mathematical optimization',\n",
       "   'Computation',\n",
       "   'Stochastic process',\n",
       "   'Regular polygon',\n",
       "   'Computer science'],\n",
       "  'references': ['2107396783',\n",
       "   '2165744313',\n",
       "   '2159715570',\n",
       "   '2114791779',\n",
       "   '2015410655',\n",
       "   '1583497301',\n",
       "   '2158893758',\n",
       "   '2037710455',\n",
       "   '1603765807',\n",
       "   '2101517602']},\n",
       " {'id': '2114791779',\n",
       "  'title': 'Constrained Consensus and Optimization in Multi-Agent Networks',\n",
       "  'reference_count': '28',\n",
       "  'citation_count': '1,596',\n",
       "  'abstract': 'We present distributed algorithms that can be used by multiple agents to align their estimates with a particular value over a network with time-varying connectivity. Our framework is general in that this value can represent a consensus value among multiple agents or an optimal solution of an optimization problem, where the global objective function is a combination of local agent objective functions. Our main focus is on constrained problems where the estimates of each agent are restricted to lie in different convex sets. To highlight the effects of constraints, we first consider a constrained consensus problem and present a distributed \"projected consensus algorithm\" in which agents combine their local averaging operation with projection on their individual constraint sets. This algorithm can be viewed as a version of an alternating projection method with weights that are varying over time and across agents. We establish convergence and convergence rate results for the projected consensus algorithm. We next study a constrained optimization problem for optimizing the sum of local objective functions of the agents subject to the intersection of their local constraint sets. We present a distributed \"projected subgradient algorithm\" which involves each agent performing a local averaging operation, taking a subgradient step to minimize its own objective function, and projecting on its constraint set. We show that, with an appropriately selected stepsize rule, the agent estimates generated by this algorithm converge to the same optimal solution for the cases when the weights are constant and equal, and when the weights are time-varying but all agents have the same constraint set.',\n",
       "  'date': 2010,\n",
       "  'authors': ['A. Nedic 1, A. Ozdaglar 2, P.A. Parrilo 2'],\n",
       "  'related_topics': ['Subgradient method',\n",
       "   'Constrained optimization',\n",
       "   'Distributed algorithm',\n",
       "   'Optimization problem',\n",
       "   'Consensus',\n",
       "   'Multi-agent system',\n",
       "   'Constraint (information theory)',\n",
       "   'Autonomous agent',\n",
       "   'Mathematical optimization',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2107396783',\n",
       "   '2165744313',\n",
       "   '2044212084',\n",
       "   '1543439990',\n",
       "   '2015410655',\n",
       "   '1583497301',\n",
       "   '1603765807',\n",
       "   '2101517602',\n",
       "   '2145574455',\n",
       "   '2106221286']},\n",
       " {'id': '2951781666',\n",
       "  'title': 'HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '1,924',\n",
       "  'abstract': \"Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.\",\n",
       "  'date': 2011,\n",
       "  'authors': ['Feng Niu',\n",
       "   'Benjamin Recht',\n",
       "   'Christopher Re',\n",
       "   'Stephen J. Wright'],\n",
       "  'related_topics': ['Stochastic gradient descent',\n",
       "   'Optimization problem',\n",
       "   'Synchronization (computer science)',\n",
       "   'Non-blocking algorithm',\n",
       "   'Rate of convergence',\n",
       "   'Parallel computing',\n",
       "   'Computer science',\n",
       "   'Scheme (programming language)',\n",
       "   'Work (physics)'],\n",
       "  'references': ['2173213060',\n",
       "   '2124608575',\n",
       "   '2798766386',\n",
       "   '2121082877',\n",
       "   '2035720976',\n",
       "   '2202343345',\n",
       "   '2150102617',\n",
       "   '2113137767',\n",
       "   '1992208280',\n",
       "   '2113651538']},\n",
       " {'id': '2019207321',\n",
       "  'title': 'ANFIS: adaptive-network-based fuzzy inference system',\n",
       "  'reference_count': '59',\n",
       "  'citation_count': '18,394',\n",
       "  'abstract': 'The architecture and learning procedure underlying ANFIS (adaptive-network-based fuzzy inference system) is presented, which is a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In the simulation, the ANFIS architecture is employed to model nonlinear functions, identify nonlinear components on-line in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificial neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested. >',\n",
       "  'date': 1993,\n",
       "  'authors': ['J.-S.R. Jang'],\n",
       "  'related_topics': ['Adaptive neuro fuzzy inference system',\n",
       "   'Neuro-fuzzy',\n",
       "   'Fuzzy control system',\n",
       "   'Fuzzy set operations',\n",
       "   'Fuzzy logic',\n",
       "   'Artificial neural network',\n",
       "   'Knowledge-based systems',\n",
       "   'Inference',\n",
       "   'Automatic control',\n",
       "   'Control system',\n",
       "   'Artificial intelligence',\n",
       "   'Nonlinear system',\n",
       "   'Computer science'],\n",
       "  'references': ['2154642048',\n",
       "   '1965324089',\n",
       "   '1492221128',\n",
       "   '2330022088',\n",
       "   '2079325629',\n",
       "   '2138484437',\n",
       "   '2062706881',\n",
       "   '2895674046',\n",
       "   '2171277043',\n",
       "   '2155399784']},\n",
       " {'id': '2161406034',\n",
       "  'title': 'C ONDENSATION —Conditional Density Propagation forVisual Tracking',\n",
       "  'reference_count': '50',\n",
       "  'citation_count': '8,650',\n",
       "  'abstract': 'The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses “factored sampling”, previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.',\n",
       "  'date': 1998,\n",
       "  'authors': ['Michael Isard', 'Andrew Blake'],\n",
       "  'related_topics': ['Condensation algorithm',\n",
       "   'Conditional probability distribution',\n",
       "   'Probability distribution',\n",
       "   'Kalman filter',\n",
       "   'Motion estimation',\n",
       "   'Gaussian',\n",
       "   'Sampling (statistics)',\n",
       "   'Tracking (particle physics)',\n",
       "   'Algorithm',\n",
       "   'Mathematics'],\n",
       "  'references': ['2170120409',\n",
       "   '2104095591',\n",
       "   '1560013842',\n",
       "   '1997063559',\n",
       "   '2098613108',\n",
       "   '2085261163',\n",
       "   '2571050459',\n",
       "   '2077611006',\n",
       "   '1481420047',\n",
       "   '1569320505']},\n",
       " {'id': '2749680651',\n",
       "  'title': 'Predictive Control for Linear and Hybrid Systems',\n",
       "  'reference_count': '247',\n",
       "  'citation_count': '857',\n",
       "  'abstract': 'Model Predictive Control (MPC), the dominant advanced control approach in industry over the past twenty-five years, is presented comprehensively in this unique book. With a simple, unified approach, and with attention to real-time implementation, it covers predictive control theory including the stability, feasibility, and robustness of MPC controllers. The theory of explicit MPC, where the nonlinear optimal feedback controller can be calculated efficiently, is presented in the context of linear systems with linear constraints, switched linear systems, and, more generally, linear hybrid systems. Drawing upon years of practical experience and using numerous examples and illustrative applications, the authors discuss the techniques required to design predictive control laws, including algorithms for polyhedral manipulations, mathematical and multiparametric programming and how to validate the theoretical properties and to implement predictive control policies. The most important algorithms feature in an accompanying free online MATLAB toolbox, which allows easy access to sample solutions. Predictive Control for Linear and Hybrid Systems is an ideal reference for graduate, postgraduate and advanced control practitioners interested in theory and/or implementation aspects of predictive control.',\n",
       "  'date': 2017,\n",
       "  'authors': ['Francesco Borrelli 1, Alberto Bemporad 2, Manfred Morari 3'],\n",
       "  'related_topics': ['Model predictive control',\n",
       "   'Linear system',\n",
       "   'Hybrid system',\n",
       "   'Robustness (computer science)',\n",
       "   'Nonlinear system',\n",
       "   'Control engineering',\n",
       "   'Computer science',\n",
       "   'Feedback controller',\n",
       "   'Matlab toolbox'],\n",
       "  'references': ['2296319761',\n",
       "   '2164278908',\n",
       "   '3029645440',\n",
       "   '2798766386',\n",
       "   '3141595720',\n",
       "   '1561941139',\n",
       "   '1978956894',\n",
       "   '2134673975',\n",
       "   '2098432798',\n",
       "   '1993170675']},\n",
       " {'id': '2148885430',\n",
       "  'title': 'A Survey on Analysis and Design of Model-Based Fuzzy Control Systems',\n",
       "  'reference_count': '344',\n",
       "  'citation_count': '1,764',\n",
       "  'abstract': 'Fuzzy logic control was originally introduced and developed as a model free control design approach. However, it unfortunately suffers from criticism of lacking of systematic stability analysis and controller design though it has a great success in industry applications. In the past ten years or so, prevailing research efforts on fuzzy logic control have been devoted to model-based fuzzy control systems that guarantee not only stability but also performance of closed-loop fuzzy control systems. This paper presents a survey on recent developments (or state of the art) of analysis and design of model based fuzzy control systems. Attention will be focused on stability analysis and controller design based on the so-called Takagi-Sugeno fuzzy models or fuzzy dynamic models. Perspectives of model based fuzzy control in future are also discussed',\n",
       "  'date': 2006,\n",
       "  'authors': ['Gang Feng'],\n",
       "  'related_topics': ['Fuzzy electronics',\n",
       "   'Fuzzy set operations',\n",
       "   'Fuzzy logic',\n",
       "   'Fuzzy control system',\n",
       "   'Neuro-fuzzy',\n",
       "   'Adaptive neuro fuzzy inference system',\n",
       "   'Adaptive control',\n",
       "   'Lyapunov function',\n",
       "   'Industrial engineering',\n",
       "   'Artificial intelligence',\n",
       "   'Computer science'],\n",
       "  'references': ['2912565176',\n",
       "   '2019207321',\n",
       "   '2090167557',\n",
       "   '2079325629',\n",
       "   '1992176519',\n",
       "   '2062706881',\n",
       "   '2914825457',\n",
       "   '1516289514',\n",
       "   '2032173185',\n",
       "   '2068052921']},\n",
       " {'id': '2095227410',\n",
       "  'title': 'Manufacturing Automation: Metal Cutting Mechanics, Machine Tool Vibrations, and CNC Design',\n",
       "  'reference_count': '66',\n",
       "  'citation_count': '3,674',\n",
       "  'abstract': 'Metal cutting is a widely used method of producing manufactured products. The technology of metal cutting has advanced considerably along with new materials, computers, and sensors. This new edition treats the scientific principles of metal cutting and their practical application to manufacturing problems. It begins with metal cutting mechanics, principles of vibration, and experimental modal analysis applied to solving shop floor problems. Notable is the in-depth coverage of chatter vibrations, a problem experienced daily by manufacturing engineers. The essential topics of programming, design, and automation of CNC (computer numerical control) machine tools, NC (numerical control) programming, and CAD/CAM technology are discussed. The text also covers the selection of drive actuators, feedback sensors, modeling and control of feed drives, the design of real time trajectory generation and interpolation algorithms, and CNC-oriented error analysis in detail. Each chapter includes examples drawn from industry, design projects, and homework problems. This book is ideal for advanced undergraduate and graduate students, as well as practicing engineers.',\n",
       "  'date': 2000,\n",
       "  'authors': ['Yusuf Altintas'],\n",
       "  'related_topics': ['Computer-aided manufacturing',\n",
       "   'Numerical control',\n",
       "   'Machine tool',\n",
       "   'Machining',\n",
       "   'Automation',\n",
       "   'Manufacturing engineering',\n",
       "   'Engineering drawing',\n",
       "   'Modal analysis',\n",
       "   'Engineering',\n",
       "   'CAD',\n",
       "   'Actuator',\n",
       "   'Mechanics'],\n",
       "  'references': ['2170120409',\n",
       "   '2491188924',\n",
       "   '3022377941',\n",
       "   '1986922155',\n",
       "   '1569320505',\n",
       "   '1554878117',\n",
       "   '2031391169',\n",
       "   '2003449296',\n",
       "   '1481834144',\n",
       "   '2002500823']},\n",
       " {'id': '2119717200',\n",
       "  'title': 'Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning',\n",
       "  'reference_count': '27',\n",
       "  'citation_count': '9,097',\n",
       "  'abstract': 'This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.',\n",
       "  'date': 1992,\n",
       "  'authors': ['Ronald J. Williams'],\n",
       "  'related_topics': ['Learning classifier system',\n",
       "   'Reinforcement learning',\n",
       "   'Backpropagation',\n",
       "   'Gradient descent',\n",
       "   'Connectionism',\n",
       "   'Artificial intelligence',\n",
       "   'Machine learning',\n",
       "   'Associative property',\n",
       "   'Algorithm',\n",
       "   'Computer science',\n",
       "   'Gradient theorem',\n",
       "   'Class (computer programming)'],\n",
       "  'references': ['2154642048',\n",
       "   '1652505363',\n",
       "   '2100677568',\n",
       "   '3011120880',\n",
       "   '1569320505',\n",
       "   '2286699414',\n",
       "   '1583833196',\n",
       "   '2152475379',\n",
       "   '1547224907',\n",
       "   '1538558539']},\n",
       " {'id': '1481420047',\n",
       "  'title': 'Contour Tracking by Stochastic Propagation of Conditional Density',\n",
       "  'reference_count': '29',\n",
       "  'citation_count': '2,966',\n",
       "  'abstract': 'The problem of tracking curves in dense visual clutter is a challenging one. Trackers based on Kalman filters are of limited use; because they are based on Gaussian densities which are unimodal, they cannot represent simultaneous alternative hypotheses. Extensions to the Kalman filter to handle multiple data associations work satisfactorily in the simple case of point targets, but do not extend naturally to continuous curves. A new, stochastic algorithm is proposed here, the Condensation algorithm — Conditional Density Propagation over time. It uses ‘factored sampling’, a method previously applied to interpretation of static images, in which the distribution of possible interpretations is represented by a randomly generated set of representatives. The Condensation algorithm combines factored sampling with learned dynamical models to propagate an entire probability distribution for object position and shape, over time. The result is highly robust tracking of agile motion in clutter, markedly superior to what has previously been attainable from Kalman filtering. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.',\n",
       "  'date': 1996,\n",
       "  'authors': ['Michael Isard', 'Andrew Blake'],\n",
       "  'related_topics': ['Condensation algorithm',\n",
       "   'Conditional probability distribution',\n",
       "   'Kalman filter',\n",
       "   'Continuous-time stochastic process',\n",
       "   'Stochastic approximation',\n",
       "   'Probability distribution',\n",
       "   'Stochastic optimization',\n",
       "   'Stochastic control',\n",
       "   'Stable process',\n",
       "   'Clutter',\n",
       "   'Gaussian',\n",
       "   'Sampling (statistics)',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Affine space',\n",
       "   'Computer science'],\n",
       "  'references': ['2170120409',\n",
       "   '2104095591',\n",
       "   '1997063559',\n",
       "   '2085261163',\n",
       "   '2571050459',\n",
       "   '1569320505',\n",
       "   '2053197265',\n",
       "   '1963565426',\n",
       "   '1635989058',\n",
       "   '2160225702']},\n",
       " {'id': '2044535354',\n",
       "  'title': 'Detection of abrupt changes: theory and application',\n",
       "  'reference_count': '219',\n",
       "  'citation_count': '6,051',\n",
       "  'abstract': 'This book is downloadable from http://www.irisa.fr/sisthem/kniga/. Many monitoring problems can be stated as the problem of detecting a change in the parameters of a static or dynamic stochastic system. The main goal of this book is to describe a unified framework for the design and the performance analysis of the algorithms for solving these change detection problems. Also the book contains the key mathematical background necessary for this purpose. Finally links with the analytical redundancy approach to fault detection in linear systems are established. We call abrupt change any change in the parameters of the system that occurs either instantaneously or at least very fast with respect to the sampling period of the measurements. Abrupt changes by no means refer to changes with large magnitude; on the contrary, in most applications the main problem is to detect small changes. Moreover, in some applications, the early warning of small - and not necessarily fast - changes is of crucial interest in order to avoid the economic or even catastrophic consequences that can result from an accumulation of such small changes. For example, small faults arising in the sensors of a navigation system can result, through the underlying integration, in serious errors in the estimated position of the plane. Another example is the early warning of small deviations from the normal operating conditions of an industrial process. The early detection of slight changes in the state of the process allows to plan in a more adequate manner the periods during which the process should be inspected and possibly repaired, and thus to reduce the exploitation costs.',\n",
       "  'date': 1993,\n",
       "  'authors': ['Michèle Basseville 1, Igor V. Nikiforov 2'],\n",
       "  'related_topics': ['Change detection',\n",
       "   'Warning system',\n",
       "   'Fault detection and isolation',\n",
       "   'Redundancy (engineering)',\n",
       "   'Linear system',\n",
       "   'Navigation system',\n",
       "   'Dynamical systems theory',\n",
       "   'Multidimensional signal processing',\n",
       "   'Control theory',\n",
       "   'Computer science',\n",
       "   'Statistics'],\n",
       "  'references': ['2157202423',\n",
       "   '2913465992',\n",
       "   '2114001875',\n",
       "   '3133603318',\n",
       "   '1569320505',\n",
       "   '2109246257',\n",
       "   '2802739963',\n",
       "   '1973240854',\n",
       "   '1490746760',\n",
       "   '1568229137']},\n",
       " {'id': '2131215403',\n",
       "  'title': 'Noise in the nervous system.',\n",
       "  'reference_count': '193',\n",
       "  'citation_count': '2,294',\n",
       "  'abstract': \"Noise — random disturbances of signals — poses a fundamental problem for information processing and affects all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-to-trial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.\",\n",
       "  'date': 2008,\n",
       "  'authors': ['A. Aldo Faisal', 'Luc P. J. Selen', 'Daniel M. Wolpert'],\n",
       "  'related_topics': ['Noise',\n",
       "   'Neuronal noise',\n",
       "   'Information processing',\n",
       "   'Nervous system',\n",
       "   'Information theory',\n",
       "   'Function (engineering)',\n",
       "   'Electronic engineering',\n",
       "   'Neuroscience',\n",
       "   'Psychology',\n",
       "   'Neural variability'],\n",
       "  'references': ['2099111195',\n",
       "   '2581275558',\n",
       "   '1509562192',\n",
       "   '2179427518',\n",
       "   '1969090956',\n",
       "   '1644749979',\n",
       "   '2112927743',\n",
       "   '1980440818',\n",
       "   '1569320505',\n",
       "   '2097560155']},\n",
       " {'id': '1993717606',\n",
       "  'title': 'Extreme learning machines: a survey',\n",
       "  'reference_count': '124',\n",
       "  'citation_count': '1,842',\n",
       "  'abstract': 'Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.',\n",
       "  'date': 2011,\n",
       "  'authors': ['Guang-Bin Huang 1, Dian Hui Wang 2, Yuan Lan 1'],\n",
       "  'related_topics': ['Extreme learning machine',\n",
       "   'Computational intelligence',\n",
       "   'Artificial neural network',\n",
       "   'Support vector machine',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence',\n",
       "   'Scalability',\n",
       "   'Pattern recognition (psychology)',\n",
       "   'Computer science',\n",
       "   'Generalization',\n",
       "   'Feed forward'],\n",
       "  'references': ['2148603752',\n",
       "   '2119821739',\n",
       "   '3124955340',\n",
       "   '2912934387',\n",
       "   '2111072639',\n",
       "   '1964357740',\n",
       "   '2172000360',\n",
       "   '1498436455',\n",
       "   '1596717185',\n",
       "   '2137983211']},\n",
       " {'id': '2093229042',\n",
       "  'title': 'Recent advances in surrogate-based optimization',\n",
       "  'reference_count': '80',\n",
       "  'citation_count': '1,799',\n",
       "  'abstract': \"Abstract The evaluation of aerospace designs is synonymous with the use of long running and computationally intensive simulations. This fuels the desire to harness the efficiency of surrogate-based methods in aerospace design optimization. Recent advances in surrogate-based design methodology bring the promise of efficient global optimization closer to reality. We review the present state of the art of constructing surrogate models and their use in optimization strategies. We make extensive use of pictorial examples and, since no method is truly universal, give guidance as to each method's strengths and weaknesses.\",\n",
       "  'date': 2008,\n",
       "  'authors': ['Alexander I.J. Forrester', 'Andy J. Keane'],\n",
       "  'related_topics': ['Engineering optimization',\n",
       "   'Probabilistic-based design optimization',\n",
       "   'Global optimization',\n",
       "   'Design methods',\n",
       "   'Strengths and weaknesses',\n",
       "   'Aerospace',\n",
       "   'Systems engineering',\n",
       "   'Management science',\n",
       "   'State (computer science)',\n",
       "   'Engineering',\n",
       "   'Surrogate based optimization'],\n",
       "  'references': ['2156909104',\n",
       "   '2126105956',\n",
       "   '2148603752',\n",
       "   '1480376833',\n",
       "   '1964357740',\n",
       "   '3023786531',\n",
       "   '2044771513',\n",
       "   '2018044188',\n",
       "   '2038669746',\n",
       "   '740415']},\n",
       " {'id': '2143956139',\n",
       "  'title': 'Networks for approximation and learning',\n",
       "  'reference_count': '65',\n",
       "  'citation_count': '4,598',\n",
       "  'abstract': \"The problem of the approximation of nonlinear mapping, (especially continuous mappings) is considered. Regularization theory and a theoretical framework for approximation (based on regularization techniques) that leads to a class of three-layer networks called regularization networks are discussed. Regularization networks are mathematically related to the radial basis functions, mainly used for strict interpolation tasks. Learning as approximation and learning as hypersurface reconstruction are discussed. Two extensions of the regularization approach are presented, along with the approach's corrections to splines, regularization, Bayes formulation, and clustering. The theory of regularization networks is generalized to a formulation that includes task-dependent clustering and dimensionality reduction. Applications of regularization networks are discussed. >\",\n",
       "  'date': 1990,\n",
       "  'authors': ['T. Poggio', 'F. Girosi'],\n",
       "  'related_topics': ['Regularization perspectives on support vector machines',\n",
       "   'Backus–Gilbert method',\n",
       "   'Tikhonov regularization',\n",
       "   'Proximal gradient methods for learning',\n",
       "   'Regularization (mathematics)',\n",
       "   'Approximation theory',\n",
       "   'Artificial neural network',\n",
       "   'Cluster analysis',\n",
       "   'Applied mathematics',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2581275558',\n",
       "   '2154642048',\n",
       "   '2103496339',\n",
       "   '3017143921',\n",
       "   '2171277043',\n",
       "   '1971735090',\n",
       "   '65738273',\n",
       "   '2165758113',\n",
       "   '2019363670',\n",
       "   '94523489']},\n",
       " {'id': '2155399784',\n",
       "  'title': 'Orthogonal least squares learning algorithm for radial basis function networks',\n",
       "  'reference_count': '22',\n",
       "  'citation_count': '4,687',\n",
       "  'abstract': 'The radial basis function network offers a viable alternative to the two-layer neural network in many applications of signal processing. A common learning algorithm for radial basis function networks is based on first choosing randomly some data points as radial basis function centers and then using singular-value decomposition to solve for the weights of the network. Such a procedure has several drawbacks, and, in particular, an arbitrary selection of centers is clearly unsatisfactory. The authors propose an alternative learning procedure based on the orthogonal least-squares method. The procedure chooses radial basis function centers one by one in a rational way until an adequate network has been constructed. In the algorithm, each selected center maximizes the increment to the explained variance or energy of the desired output and does not suffer numerical ill-conditioning problems. The orthogonal least-squares learning strategy provides a simple and efficient means for fitting radial basis function networks. This is illustrated using examples taken from two different signal processing applications. >',\n",
       "  'date': 1991,\n",
       "  'authors': ['S. Chen', 'C.F.N. Cowan', 'P.M. Grant'],\n",
       "  'related_topics': ['Radial basis function network',\n",
       "   'Basis function',\n",
       "   'Radial basis function',\n",
       "   'Feedforward neural network',\n",
       "   'Artificial neural network',\n",
       "   'Multidimensional signal processing',\n",
       "   'Singular value decomposition',\n",
       "   'Hierarchical RBF',\n",
       "   'Algorithm',\n",
       "   'Mathematical optimization',\n",
       "   'Mathematics'],\n",
       "  'references': ['2581275558',\n",
       "   '2103496339',\n",
       "   '2171277043',\n",
       "   '1971735090',\n",
       "   '94523489',\n",
       "   '1524100745',\n",
       "   '2102380305',\n",
       "   '2039402388',\n",
       "   '2296618885',\n",
       "   '2078841894']},\n",
       " {'id': '2149723649',\n",
       "  'title': 'A general regression neural network',\n",
       "  'reference_count': '19',\n",
       "  'citation_count': '5,443',\n",
       "  'abstract': 'A memory-based network that provides estimates of continuous variables and converges to the underlying (linear or nonlinear) regression surface is described. The general regression neural network (GRNN) is a one-pass learning algorithm with a highly parallel structure. It is shown that, even with sparse data in a multidimensional measurement space, the algorithm provides smooth transitions from one observed value to another. The algorithmic form can be used for any regression problem in which an assumption of linearity is not justified. >',\n",
       "  'date': 1991,\n",
       "  'authors': ['D.F. Specht'],\n",
       "  'related_topics': ['Polynomial regression',\n",
       "   'Proper linear model',\n",
       "   'Regression analysis',\n",
       "   'Bayesian multivariate linear regression',\n",
       "   'Artificial neural network',\n",
       "   'Multidimensional systems',\n",
       "   'Parallel algorithm',\n",
       "   'Regression',\n",
       "   'Sparse matrix',\n",
       "   'Nonlinear system',\n",
       "   'Algorithm',\n",
       "   'Theoretical computer science',\n",
       "   'Computer science'],\n",
       "  'references': ['1965324089',\n",
       "   '2138484437',\n",
       "   '2171277043',\n",
       "   '1964168965',\n",
       "   '94523489',\n",
       "   '1548502347',\n",
       "   '2146200922',\n",
       "   '2087197472',\n",
       "   '2118020555',\n",
       "   '2112032710']},\n",
       " {'id': '2809684781',\n",
       "  'title': 'A comprehensive survey on machine learning for networking: evolution, applications and research opportunities',\n",
       "  'reference_count': '412',\n",
       "  'citation_count': '417',\n",
       "  'abstract': 'Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.',\n",
       "  'date': 2018,\n",
       "  'authors': ['Raouf Boutaba 1, Mohammad A. Salahuddin 1, Noura Limam 1, Sara Ayoubi 1, Nashid Shahriar 1, Felipe Estrada-Solano 1, 2, Oscar M. Caicedo 2'],\n",
       "  'related_topics': ['Network security',\n",
       "   'Fault management',\n",
       "   'Network congestion',\n",
       "   'Quality of service',\n",
       "   'Resource (project management)',\n",
       "   'Computer Applications',\n",
       "   'Automation',\n",
       "   'Computer science',\n",
       "   'Key (cryptography)',\n",
       "   'Machine learning',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2153635508',\n",
       "   '2145339207',\n",
       "   '2136922672',\n",
       "   '2133665775',\n",
       "   '2295598076',\n",
       "   '2064675550',\n",
       "   '2135046866',\n",
       "   '2116064496',\n",
       "   '2912934387',\n",
       "   '2122410182']},\n",
       " {'id': '2133321814',\n",
       "  'title': 'Neuro-fuzzy modeling and control',\n",
       "  'reference_count': '66',\n",
       "  'citation_count': '3,143',\n",
       "  'abstract': 'Fundamental and advanced developments in neuro-fuzzy synergisms for modeling and control are reviewed. The essential part of neuro-fuzzy synergisms comes from a common framework called adaptive networks, which unifies both neural networks and fuzzy models. The fuzzy models under the framework of adaptive networks is called adaptive-network-based fuzzy inference system (ANFIS), which possess certain advantages over neural networks. We introduce the design methods for ANFIS in both modeling and control applications. Current problems and future directions for neuro-fuzzy approaches are also addressed. >',\n",
       "  'date': 1995,\n",
       "  'authors': ['J.-S.R. Jang 1, Chuen-Tsai Sun 2'],\n",
       "  'related_topics': ['Adaptive neuro fuzzy inference system',\n",
       "   'Neuro-fuzzy',\n",
       "   'Fuzzy control system'],\n",
       "  'references': ['2912565176',\n",
       "   '1497256448',\n",
       "   '2581275558',\n",
       "   '1965324089',\n",
       "   '2079325629',\n",
       "   '1992176519',\n",
       "   '2062706881',\n",
       "   '1989555277',\n",
       "   '2016589492',\n",
       "   '1570834090']},\n",
       " {'id': '1998442441',\n",
       "  'title': 'Neural networks for the prediction and forecasting of water resources variables: a review of modelling issues and applications',\n",
       "  'reference_count': '184',\n",
       "  'citation_count': '2,497',\n",
       "  'abstract': 'Abstract Artificial Neural Networks (ANNs) are being used increasingly to predict and forecast water resources variables. In this paper, the steps that should be followed in the development of such models are outlined. These include the choice of performance criteria, the division and pre-processing of the available data, the determination of appropriate model inputs and network architecture, optimisation of the connection weights (training) and model validation. The options available to modellers at each of these steps are discussed and the issues that should be considered are highlighted. A review of 43 papers dealing with the use of neural network models for the prediction and forecasting of water resources variables is undertaken in terms of the modelling process adopted. In all but two of the papers reviewed, feedforward networks are used. The vast majority of these networks are trained using the backpropagation algorithm. Issues in relation to the optimal division of the available data, data pre-processing and the choice of appropriate model inputs are seldom considered. In addition, the process of choosing appropriate stopping criteria and optimising network geometry and internal network parameters is generally described poorly or carried out inadequately. All of the above factors can result in non-optimal model performance and an inability to draw meaningful comparisons between different models. Future research efforts should be directed towards the development of guidelines which assist with the development of ANN models and the choice of when ANNs should be used in preference to alternative approaches, the assessment of methods for extracting the knowledge that is contained in the connection weights of trained ANNs and the incorporation of uncertainty into ANN models.',\n",
       "  'date': 1999,\n",
       "  'authors': ['Holger R. Maier', 'Graeme C. Dandy'],\n",
       "  'related_topics': ['Artificial neural network',\n",
       "   'Backpropagation',\n",
       "   'Process (engineering)',\n",
       "   'Network architecture',\n",
       "   'Relation (database)',\n",
       "   'Machine learning',\n",
       "   'Computer science',\n",
       "   'Feed forward',\n",
       "   'Preference',\n",
       "   'Water resources',\n",
       "   'Artificial intelligence'],\n",
       "  'references': ['2064675550',\n",
       "   '2581275558',\n",
       "   '2154642048',\n",
       "   '1498436455',\n",
       "   '2042264548',\n",
       "   '2137983211',\n",
       "   '2138484437',\n",
       "   '3146803896',\n",
       "   '2110485445',\n",
       "   '1554576613']},\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "peripheral-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Greg Yang': 0,\n",
       " 'Linda Wang': 1,\n",
       " 'Zhong Qiu Lin': 2,\n",
       " 'Alexander Wong': 3,\n",
       " 'Han Zhang 1, Ian Goodfellow 1, Dimitris Metaxas 2, Augustus Odena 1': 4,\n",
       " 'Alexey Dosovitskiy 1, Lucas Beyer 1, Alexander Kolesnikov 1, Dirk Weissenborn 2, Xiaohua Zhai 1, Thomas Unterthiner 1, Mostafa Dehghani 1, Matthias Minderer 1, Georg Heigold 2, Sylvain Gelly 1, Jakob Uszkoreit 1, Neil Houlsby 3': 5,\n",
       " 'Volodymyr Mnih': 6,\n",
       " 'Koray Kavukcuoglu': 7,\n",
       " 'David Silver': 8,\n",
       " 'Andrei A. Rusu': 9,\n",
       " 'Joel Veness': 10,\n",
       " 'Marc G. Bellemare': 11,\n",
       " 'Alex Graves': 12,\n",
       " 'Martin Riedmiller': 13,\n",
       " 'Andreas K. Fidjeland': 14,\n",
       " 'Georg Ostrovski': 15,\n",
       " 'Stig Petersen': 16,\n",
       " 'Charles Beattie': 17,\n",
       " 'Amir Sadik': 18,\n",
       " 'Ioannis Antonoglou': 19,\n",
       " 'Helen King': 20,\n",
       " 'Dharshan Kumaran': 21,\n",
       " 'Daan Wierstra': 22,\n",
       " 'Shane Legg': 23,\n",
       " 'Demis Hassabis': 24,\n",
       " 'Tomas Mikolov': 25,\n",
       " 'Ilya Sutskever': 26,\n",
       " 'Kai Chen': 27,\n",
       " 'Greg S Corrado': 28,\n",
       " 'Jeff Dean': 29,\n",
       " 'Kaiming He': 30,\n",
       " 'Xiangyu Zhang': 31,\n",
       " 'Shaoqing Ren': 32,\n",
       " 'Jian Sun': 33,\n",
       " 'Ashish Vaswani 1, Noam Shazeer 1, Niki Parmar 2, Jakob Uszkoreit 1, Llion Jones 1, Aidan N. Gomez 1, Lukasz Kaiser 1, Illia Polosukhin 1': 34,\n",
       " 'Sergey Ioffe': 35,\n",
       " 'Christian Szegedy': 36,\n",
       " 'Dzmitry Bahdanau 1, Kyunghyun Cho 2, Yoshua Bengio 2': 37,\n",
       " 'Gao Huang 1, Zhuang Liu 2, Laurens van der Maaten 3, Kilian Q. Weinberger 1': 38,\n",
       " 'Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 1, Jian Sun 1': 39,\n",
       " 'Kyunghyun Cho 1, Bart van Merrienboer 2, Caglar Gulcehre 2, Dzmitry Bahdanau 3, Fethi Bougares 3, Holger Schwenk 3, Yoshua Bengio 4, 5, 6': 40,\n",
       " 'Yann Lecun 1, Leon Bottou 2, 3, Yoshua Bengio 3, 4, 5, Patrick Haffner 3, 6': 41,\n",
       " 'Sepp Hochreiter 1, Jürgen Schmidhuber 2': 42,\n",
       " 'Xavier Glorot': 43,\n",
       " 'Yoshua Bengio': 44,\n",
       " 'Chaolin Huang 1, Yeming Wang 2, Xingwang Li 3, Lili Ren 4, Jianping Zhao 5, Yi Hu 5, Li Zhang 1, Guohui Fan 2, Jiuyang Xu 6, Xiaoying Gu 2, Zhenshun Cheng 7, Ting Yu 1, Jiaan Xia 1, Yuan Wei 1, Wenjuan Wu 1, Xuelei Xie 1, Wen Yin 5, Hui Li 2, Min Liu 2, Yan Xiao 4, Hong Gao 4, Li Guo 4, Jungang Xie 5, Guangfa Wang 8, Rongmeng Jiang 3, Zhancheng Gao 8, Qi Jin 4, Jianwei Wang 4, Bin Cao 2': 45,\n",
       " 'Karen Simonyan': 46,\n",
       " 'Andrew Zisserman': 47,\n",
       " 'Wei-jie Guan': 48,\n",
       " 'Zheng-yi Ni': 49,\n",
       " 'Yu Hu': 50,\n",
       " 'Wenhua Liang': 51,\n",
       " 'Chun-quan Ou': 52,\n",
       " 'Jianxing He': 53,\n",
       " 'Lei Liu': 54,\n",
       " 'Hong Shan': 55,\n",
       " 'Chunliang Lei': 56,\n",
       " 'David S.C. Hui': 57,\n",
       " 'Bin Du': 58,\n",
       " 'Lan-juan Li': 59,\n",
       " 'Guang Zeng': 60,\n",
       " 'Kwok-Yung Yuen': 61,\n",
       " 'Ruchong Chen': 62,\n",
       " 'Chun-Li Tang': 63,\n",
       " 'Tao Wang': 64,\n",
       " 'Ping-yan Chen': 65,\n",
       " 'Jie Xiang': 66,\n",
       " 'Shiyue Li': 67,\n",
       " 'Jinlin Wang': 68,\n",
       " 'Zi Jing Liang': 69,\n",
       " 'Yi-xiang Peng': 70,\n",
       " 'Li Wei': 71,\n",
       " 'Yong Liu': 72,\n",
       " 'Ya-hua Hu': 73,\n",
       " 'Peng Peng': 74,\n",
       " 'Jian-ming Wang': 75,\n",
       " 'Ji-yang Liu': 76,\n",
       " 'Zhong Chen': 77,\n",
       " 'Gang Li': 78,\n",
       " 'Zhi-jian Zheng': 79,\n",
       " 'Shao-qin Qiu': 80,\n",
       " 'Jie Luo': 81,\n",
       " 'Chang-jiang Ye': 82,\n",
       " 'Shao-yong Zhu': 83,\n",
       " 'Nanshan Zhong': 84,\n",
       " 'Yann LeCun 1, 2, Yoshua Bengio 3, Geoffrey Hinton 4, 5': 85,\n",
       " 'Jia Deng': 86,\n",
       " 'Wei Dong': 87,\n",
       " 'Richard Socher': 88,\n",
       " 'Li-Jia Li': 89,\n",
       " 'Kai Li': 90,\n",
       " 'Li Fei-Fei': 91,\n",
       " 'Tao Ai 1, Zhenlu Yang 2, Hongyan Hou 3, Chenao Zhan 1, Chong Chen 1, Wenzhi Lv 1, Qian Tao 1, Ziyong Sun 1, Liming Xia 1': 92,\n",
       " 'Wenling Wang 1, Yanli Xu 2, Ruqin Gao 3, Roujian Lu 1, Kai Han 2, Guizhen Wu 1, Wenjie Tan 1': 93,\n",
       " 'Yicheng Fang': 94,\n",
       " 'Huangqi Zhang': 95,\n",
       " 'Jicheng Xie': 96,\n",
       " 'Minjie Lin': 97,\n",
       " 'Lingjun Ying': 98,\n",
       " 'Peipei Pang': 99,\n",
       " 'Wenbin Ji': 100,\n",
       " 'Diederik P. Kingma 1, Jimmy Lei Ba 2': 101,\n",
       " 'Olga Russakovsky 1, Jia Deng 2, Hao Su 1, Jonathan Krause 1, Sanjeev Satheesh 1, Sean Ma 1, Zhiheng Huang 1, Andrej Karpathy 1, Aditya Khosla 3, Michael Bernstein 1, Alexander C. Berg 4, Li Fei-Fei 1': 102,\n",
       " 'Ian Goodfellow 1, Jean Pouget-Abadie 1, Mehdi Mirza 1, Bing Xu 1, David Warde-Farley 1, Sherjil Ozair 2, Aaron Courville 1, Yoshua Bengio 1': 103,\n",
       " 'Phillip Isola': 104,\n",
       " 'Jun-Yan Zhu': 105,\n",
       " 'Tinghui Zhou': 106,\n",
       " 'Alexei A. Efros': 107,\n",
       " 'Taesung Park': 108,\n",
       " 'Alec Radford 1, Luke Metz 1, Soumith Chintala 2': 109,\n",
       " 'Tim Salimans 1, Ian Goodfellow 2, Wojciech Zaremba 3, Vicki Cheung': 110,\n",
       " 'Alec Radford 1, Xi Chen 4': 111,\n",
       " 'Christian Ledig 1, Lucas Theis 1, Ferenc Huszar 2, Jose Caballero 3, Andrew Cunningham': 112,\n",
       " 'Alejandro Acosta 4, Andrew Aitken 2, Alykhan Tejani 2, Johannes Totz 2, Zehan Wang 2, Wenzhe Shi 2': 113,\n",
       " 'Alex Krizhevsky 1, Ilya Sutskever 1, Geoffrey E. Hinton 2': 114,\n",
       " 'Jacob Devlin': 115,\n",
       " 'Ming-Wei Chang': 116,\n",
       " 'Kenton Lee': 117,\n",
       " 'Kristina N. Toutanova': 118,\n",
       " 'Alex Krizhevsky': 119,\n",
       " 'Xiaolong Wang 1, Ross Girshick 1, Abhinav Gupta 2, Kaiming He 1': 120,\n",
       " 'Ting Chen': 121,\n",
       " 'Simon Kornblith': 122,\n",
       " 'Mohammad Norouzi': 123,\n",
       " 'Geoffrey Hinton': 124,\n",
       " 'G. E. Hinton': 125,\n",
       " 'R. R. Salakhutdinov': 126,\n",
       " 'Laurens van der Maaten': 127,\n",
       " 'Vinod Nair': 128,\n",
       " 'Geoffrey E. Hinton': 129,\n",
       " 'Kevin Jarrett': 130,\n",
       " \"Marc'Aurelio Ranzato\": 131,\n",
       " 'Yann LeCun': 132,\n",
       " 'R.S. Sutton': 133,\n",
       " 'A.G. Barto': 134,\n",
       " 'Marc G. Bellemare 1, Yavar Naddaf 2, Joel Veness 1, Michael Bowling 1': 135,\n",
       " 'David E. Rumelhart 1, James L. McClelland 2': 136,\n",
       " 'Tomas Mikolov 1, Kai Chen 2, Greg S. Corrado 2, Jeffrey Dean 2': 137,\n",
       " 'Ronan Collobert': 138,\n",
       " 'Jason Weston': 139,\n",
       " 'Tomas Mikolov 1, Wen-tau Yih 2, Geoffrey Zweig 2': 140,\n",
       " 'Réjean Ducharme': 141,\n",
       " 'Pascal Vincent': 142,\n",
       " 'Christian Janvin': 143,\n",
       " 'Joseph Turian 1, Lev-Arie Ratinov 2, Yoshua Bengio 1': 144,\n",
       " 'Cliff C. Lin': 145,\n",
       " 'Chris Manning': 146,\n",
       " 'Andrew Y. Ng': 147,\n",
       " 'David E. Rumelhart 1, Geoffrey E. Hinton 2, Ronald J. Williams 1': 148,\n",
       " 'Peter D. Turney 1, Patrick Pantel 2': 149,\n",
       " 'Brody Huval': 150,\n",
       " 'Christopher D. Manning': 151,\n",
       " 'Andriy Mnih': 152,\n",
       " 'Christian Szegedy 1, Wei Liu 2, Yangqing Jia 1, Pierre Sermanet 1, Scott Reed 3, Dragomir Anguelov 1, Dumitru Erhan 1, Vincent Vanhoucke 1, Andrew Rabinovich 4': 153,\n",
       " 'Shaoqing Ren 1, Kaiming He 2, Ross Girshick 3, Jian Sun 2': 154,\n",
       " 'Ross Girshick': 155,\n",
       " 'Jeff Donahue': 156,\n",
       " 'Trevor Darrell': 157,\n",
       " 'Jitendra Malik': 158,\n",
       " 'Jonathan Long': 159,\n",
       " 'Evan Shelhamer': 160,\n",
       " 'Yangqing Jia 1, Evan Shelhamer 2, Jeff Donahue 2, Sergey Karayev 2, Jonathan Long 2, Ross Girshick 2, Sergio Guadarrama 2, Trevor Darrell 2': 161,\n",
       " 'Jie Hu 1, Li Shen 2, Samuel Albanie 2, Gang Sun 1, Enhua Wu 1': 162,\n",
       " 'Yinhan Liu': 163,\n",
       " 'Myle Ott': 164,\n",
       " 'Naman Goyal': 165,\n",
       " 'Jingfei Du': 166,\n",
       " 'Mandar Joshi': 167,\n",
       " 'Danqi Chen': 168,\n",
       " 'Omer Levy': 169,\n",
       " 'Mike Lewis': 170,\n",
       " 'Luke Zettlemoyer': 171,\n",
       " 'Veselin Stoyanov': 172,\n",
       " 'Zhilin Yang 1, Zihang Dai 1, Yiming Yang 1, Jaime G. Carbonell 1, Ruslan Salakhutdinov 1, Quoc V. Le 2': 173,\n",
       " 'Alex Wang 1, Amanpreet Singh 1, Julian Michael 2, Felix Hill 3, Omer Levy 4, Samuel R. Bowman 1': 174,\n",
       " 'Jinhyuk Lee 1, Wonjin Yoon 1, Sungdong Kim 2, Donghyeon Kim 1, Sunkyu Kim 1, Chan Ho So 1, Jaewoo Kang 1': 175,\n",
       " 'Nitish Srivastava': 176,\n",
       " 'Ruslan Salakhutdinov': 177,\n",
       " 'John Duchi 1, Elad Hazan 2, Yoram Singer 3': 178,\n",
       " 'Jeffrey Dean': 179,\n",
       " 'Greg Corrado': 180,\n",
       " 'Rajat Monga': 181,\n",
       " 'Matthieu Devin': 182,\n",
       " 'Mark Mao': 183,\n",
       " \"Marc'aurelio Ranzato\": 184,\n",
       " 'Andrew Senior': 185,\n",
       " 'Paul Tucker': 186,\n",
       " 'Ke Yang': 187,\n",
       " 'Quoc V. Le': 188,\n",
       " 'Ilya Sutskever 1, James Martens 2, George Dahl 2, Geoffrey Hinton 2': 189,\n",
       " 'Matthew D. Zeiler': 190,\n",
       " 'Nal Kalchbrenner': 191,\n",
       " 'Phil Blunsom': 192,\n",
       " 'Ian Goodfellow': 193,\n",
       " 'David Warde-Farley': 194,\n",
       " 'Mehdi Mirza': 195,\n",
       " 'Aaron Courville': 196,\n",
       " 'Kyunghyun Cho 1, Bart van Merrienboer 1, Dzmitry Bahdanau 2, Yoshua Bengio 3, 4, 5': 197,\n",
       " 'Razvan Pascanu 1, Tomas Mikolov 2, Yoshua Bengio 1': 198,\n",
       " 'Philipp Koehn': 199,\n",
       " 'Franz Josef Och': 200,\n",
       " 'Daniel Marcu': 201,\n",
       " 'Christian Szegedy 1, Vincent Vanhoucke 1, Sergey Ioffe 1, Jon Shlens 1, Zbigniew Wojna 2': 202,\n",
       " 'G. E. Dahl 1, Dong Yu 2, Li Deng 2, A. Acero 2': 203,\n",
       " 'Xavier Glorot 1, Antoine Bordes 2, Yoshua Bengio 1': 204,\n",
       " 'Andrew M. Saxe': 205,\n",
       " 'James L. McClelland': 206,\n",
       " 'Surya Ganguli': 207,\n",
       " 'John D. Lafferty': 208,\n",
       " 'Andrew McCallum': 209,\n",
       " 'Fernando C. N. Pereira': 210,\n",
       " 'P.Y. Simard': 211,\n",
       " 'D. Steinkraus': 212,\n",
       " 'J.C. Platt': 213,\n",
       " 'Hongyi Zhang 1, Moustapha Cisse 2, Yann N. Dauphin 2, David Lopez-Paz 2': 214,\n",
       " 'Joan Bruna 1, Wojciech Zaremba 1, Arthur Szlam 2, Yann LeCun 1': 215,\n",
       " 'S. Chopra': 216,\n",
       " 'R. Hadsell': 217,\n",
       " 'Y. LeCun': 218,\n",
       " 'Michael C Mozer': 219,\n",
       " 'Fernando J. Pineda': 220,\n",
       " 'G.V. Puskorius': 221,\n",
       " 'L.A. Feldkamp': 222,\n",
       " 'Barak A. Pearlmutter': 223,\n",
       " 'B.A. Pearlmutter': 224,\n",
       " 'Tsungnan Lin 1, B.G. Horne 2, P. Tino 3, C.L. Giles 4': 225,\n",
       " 'Kevin J. Lang 1, Alex H. Waibel 1, Geoffrey E. Hinton 2': 226,\n",
       " 'Ronald J. Williams': 227,\n",
       " 'David Zipser': 228,\n",
       " 'Geoffrey E. Hinton 1, Simon Osindero 1, Yee-Whye Teh 2': 229,\n",
       " 'Hugo Larochelle': 230,\n",
       " 'Pierre-Antoine Manzagol': 231,\n",
       " 'Pascal Lamblin': 232,\n",
       " 'Dan Popovici': 233,\n",
       " 'Dumitru Erhan': 234,\n",
       " 'James Bergstra': 235,\n",
       " 'Christopher Poultney': 236,\n",
       " 'Sumit Chopra': 237,\n",
       " 'Yann L. Cun': 238,\n",
       " 'Jie Cui 1, Fang Li 2, Zheng Li Shi 1': 239,\n",
       " 'Ali Moh Zaki 1, Sander Van Boheemen 2, Theo M. Bestebroer 2, Albert D.M.E. Osterhaus': 240,\n",
       " 'Ron A.M. Fouchier': 241,\n",
       " 'Timothy P. Sheahan 1, Amy C. Sims 1, Sarah R. Leist 1, Alexandra Schäfer 1, John Won 1, Ariane J. Brown 1, Stephanie A. Montgomery 1, Alison Hogg 2, Darius Babusis 2, Michael O. Clarke 2, Jamie E. Spahn 2, Laura Bauer 2, Scott Sellers 2, Danielle Porter 2, Joy Y. Feng 2, Tomas Cihlar 2, Robert Jordan 2, Mark R. Denison 3, Ralph S. Baric 1': 242,\n",
       " 'Christian Drosten 1, Stephan Günther 1, Wolfgang Preiser 2, Sylvie van der Werf 3, Hans-Reinhard Brodt 4, Stephan Becker 5, Holger Rabenau 2, Marcus Panning 1, Larissa Kolesnikova 5, Ron A.M. Fouchier 6, Annemarie Berger 2, Ana-Maria Burguière 3, Jindrich Cinatl 2, Markus Eickmann 5, Nicolas Escriou 3, Klaus Grywna 1, Stefanie Kramme 1, Jean-Claude Manuguerra 3, Stefanie Müller 1, Volker Rickerts 4, Martin Stürmer 2, Simon Vieth 1, Hans-Dieter Klenk 5, Albert D.M.E. Osterhaus 6, Herbert Schmitz 1, Hans Wilhelm Doerr 2': 243,\n",
       " 'Arif Khwaja': 244,\n",
       " 'Ksiazek Tg 1, Erdman D 1, Goldsmith Cs 1, Zaki 1, Peret T 1, Emery S 1, Tong S 1, Urbani C 2, Comer Ja 1, Lim W 3, Rollin Pe 1, Dowell Sf 4, Ling Ae 5, Humphrey Cd 1, Shieh Wj 1, Guarner J 1, Paddock Cd 1, Rota P 1, Fields B 1, DeRisi J 6, Yang Jy 1, Cox N 1, Hughes Jm 1, LeDuc Jw 1, Bellini Wj 1, Anderson Lj 1': 245,\n",
       " 'Nelson Lee 1, David Hui 1, Alan Wu 1, Paul Chan 1, Peter Cameron 2, Gavin M Joynt 1, Anil Ahuja 1, Man Yee Yung 1, C B Leung 1, K F To 1, S F Lui 1, C C Szeto 1, Sydney Chung 1, Joseph J Y Sung 1': 246,\n",
       " 'Abdullah Assiri 1, Jaffar A Al-Tawfiq 2, Abdullah A Al-Rabeeah 1, Fahad A Al-Rabiah 3, Sami Al-Hajjar 3, Ali Al-Barrak 4, Hesham Flemban 5, Wafa N Al-Nassir 6, Hanan H Balkhy 7, Rafat F Al-Hakeem 1, Hatem Q Makhdoom 8, Alimuddin I Zumla 9, 10, Ziad A Memish 11': 247,\n",
       " 'Wenjie Tan': 248,\n",
       " 'Xiang Zhao': 249,\n",
       " 'Xuejun Ma': 250,\n",
       " 'Wenling Wang': 251,\n",
       " 'Peihua Niu': 252,\n",
       " 'Wenbo Xu': 253,\n",
       " 'George F. Gao': 254,\n",
       " 'Guizhen Wu': 255,\n",
       " 'Timothy P. Sheahan 1, Amy C. Sims 1, Rachel L. Graham 1, Vineet D. Menachery 1, Lisa E. Gralinski 1, James B. Case 2, Sarah R. Leist 1, Krzysztof Pyrc 3, Joy Y. Feng 4, Iva Trantcheva 4, Roy Bannister 4, Yeojin Park 4, Darius Babusis 4, Michael O. Clarke 4, Richard L. Mackman 4, Jamie E. Spahn 4, Christopher A. Palmiotti 4, Dustin Siegel 4, Adrian S. Ray 4, Tomas Cihlar 4, Robert Jordan 4, Mark R. Denison 5, Ralph S. Baric 1': 256,\n",
       " 'Olaf Ronneberger': 257,\n",
       " 'Philipp Fischer': 258,\n",
       " 'Thomas Brox': 259,\n",
       " 'Wei Liu 1, Dragomir Anguelov 2, Dumitru Erhan 3, Christian Szegedy 3, Scott E. Reed 4, Cheng-Yang Fu 1, Alexander C. Berg 1': 260,\n",
       " 'Na Zhu 1, Dingyu Zhang 2, Wenling Wang 1, Xingwang Li 3, Bo Yang 1, Jingdong Song 1, Xiang Zhao 1, Baoying Huang 1, Weifeng Shi 4, Roujian Lu 1, Peihua Niu 1, Faxian Zhan 1, Xuejun Ma 1, 5, Dayan Wang 1, Wenbo Xu 6, Guizhen Wu 1, George F. Gao 7, Wenjie Tan 1': 261,\n",
       " \"Nanshan Chen 1, Min Zhou 2, Xuan Dong 1, Jieming Qu 2, Fengyun Gong 1, Yang Han 1, Yang Qiu 3, Jingli Wang 1, Ying Liu 1, Yuan Wei 1, Jia'an Xia 1, Ting Yu 1, Xinxin Zhang 2, Li Zhang 1\": 262,\n",
       " 'Dawei Wang': 263,\n",
       " 'Bo Hu': 264,\n",
       " 'Chang Hu': 265,\n",
       " 'Fangfang Zhu': 266,\n",
       " 'Xing Liu': 267,\n",
       " 'Jing Zhang': 268,\n",
       " 'Binbin Wang': 269,\n",
       " 'Hui Xiang': 270,\n",
       " 'Zhenshun Cheng': 271,\n",
       " 'Yong Xiong': 272,\n",
       " 'Yan Zhao': 273,\n",
       " 'Yirong Li': 274,\n",
       " 'Xinghuan Wang': 275,\n",
       " 'Zhiyong Peng': 276,\n",
       " 'Qun Li 1, Xuhua Guan 1, Peng Wu 2, Xiaoye Wang 1, Lei Zhou 1, Yeqing Tong 1, Ruiqi Ren 1, Kathy S.M. Leung 2, Eric H.Y. Lau 2, Jessica Y. Wong 2, Xuesen Xing 1, Nijuan Xiang 1, Yang Wu 1, Chao Li 1, Qi Chen 1, Dan Li 1, Tian Liu 1, Jing Zhao 1, Man Liu 1, Wenxiao Tu 1, Chuding Chen 1, Lianmei Jin 1, Rui Yang 1, Qi Wang 1, Suhua Zhou 1, Rui Wang 1, Hui Liu 1, Yingbo Luo 1, Yuan Liu 1, Ge Shao 1, Huan Li 1, Zhongfa Tao 1, Yang Yang 3, Zhiqiang Deng 3, Boxi Liu 3, Zhitao Ma 3, Yanping Zhang 1, Guoqing Shi 1, Tommy T.Y. Lam 2, Joseph T. Wu 2, George F. Gao 1, Benjamin J. Cowling 2, Bo Yang 3, Gabriel M. Leung 2, Zijian Feng 1': 277,\n",
       " 'Jasper Fuk Woo Chan 1, Shuofeng Yuan 1, Kin Hang Kok 1, Kelvin Kai Wang To 1, 2, Hin Chu 1, Jin Yang 2, Fanfan Xing 2, Jieling Liu 2, Cyril Chik Yan Yip 1, Rosana Wing Shan Poon 1, Hoi Wah Tsoi 1, Simon Kam Fai Lo 2, Kwok Hung Chan 1, Vincent Kwok Man Poon 1, Wan Mui Chan 1, Jonathan Daniel Ip 1, Jian Piao Cai 1, Vincent Chi Chung Cheng 1, Honglin Chen 1, 2, Christopher Kim Ming Hui 2, Kwok Yung Yuen 2': 278,\n",
       " 'Roujian Lu 1, Xiang Zhao 1, Juan Li 2, Peihua Niu 1, Bo Yang 3, Honglong Wu 4, Wenling Wang 1, Hao Song 5, Baoying Huang 1, Na Zhu 1, Yuhai Bi 5, Xuejun Ma 1, Faxian Zhan 3, Liang Wang 5, Tao Hu 2, Hong Zhou 2, Zhenhong Hu 6, Weimin Zhou 1, Li Zhao 1, Jing Chen 7, Yao Meng 1, Ji Wang 1, Yang Lin 4, Jianying Yuan 4, Zhihao Xie 4, Jinmin Ma 4, William J Liu 1, Dayan Wang 1, Wenbo Xu 1, Edward C Holmes 8, George F Gao 1, 5, Guizhen Wu 1, Weijun Chen 4, Weifeng Shi 2, Wenjie Tan 1, 5': 279,\n",
       " 'Michelle L Holshue 1, Chas DeBolt 2, Scott Lindquist': 280,\n",
       " 'Kathy H Lofy': 281,\n",
       " 'John Wiesman': 282,\n",
       " 'Hollianne Bruce': 283,\n",
       " 'Christopher Spitters': 284,\n",
       " 'Keith Ericson': 285,\n",
       " 'Sara Wilkerson': 286,\n",
       " 'Ahmet Tural': 287,\n",
       " 'George Diaz': 288,\n",
       " 'Amanda Cohn': 289,\n",
       " 'LeAnne Fox': 290,\n",
       " 'Anita Patel': 291,\n",
       " 'Susan I Gerber': 292,\n",
       " 'Lindsay Kim': 293,\n",
       " 'Suxiang Tong': 294,\n",
       " 'Xiaoyan Lu': 295,\n",
       " 'Steve Lindstrom': 296,\n",
       " 'Mark A Pallansch': 297,\n",
       " 'William C Weldon': 298,\n",
       " 'Holly M Biggs': 299,\n",
       " 'Timothy M Uyeki': 300,\n",
       " 'Satish K Pillai': 301,\n",
       " 'Camilla Rothe 1, Mirjam Schunk 1, Peter Sothmann 1, Gisela Bretzel 1, Guenter Froeschl 1, Claudia Wallrauch 1, Thorbjörn Zimmer 1, Verena Thiel 1, Christian Janke 1, Wolfgang Guggemos 2, Michael Seilmaier 2, Christian Drosten 3, Patrick Vollmar 4, Katrin Zwirglmaier 4, Sabine Zange 4, Roman Wölfel 4, Michael Hoelscher 1': 302,\n",
       " 'Joseph T Wu': 303,\n",
       " 'Kathy Leung': 304,\n",
       " 'Gabriel M Leung': 305,\n",
       " 'Y. Bengio': 306,\n",
       " 'A. Courville': 307,\n",
       " 'P. Vincent': 308,\n",
       " 'G. Hinton 1, Li Deng 2, Dong Yu 2, G. E. Dahl 1, A. Mohamed 1, N. Jaitly 1, Andrew Senior 3, V. Vanhoucke 3, P. Nguyen 3, T. N. Sainath 4, B. Kingsbury 4': 309,\n",
       " 'C. Farabet 1, C. Couprie 1, L. Najman 2, Y. LeCun 1': 310,\n",
       " 'A. Mohamed': 311,\n",
       " 'G. E. Dahl': 312,\n",
       " 'G. Hinton': 313,\n",
       " 'David G. Lowe': 314,\n",
       " 'Christiane Fellbaum': 315,\n",
       " 'D. Nister': 316,\n",
       " 'H. Stewenius': 317,\n",
       " 'Bryan C. Russell 1, Antonio Torralba 1, Kevin P. Murphy 2, William T. Freeman 1': 318,\n",
       " 'Gary B. Huang 1, Marwan Mattar 1, Tamara Berg 2, Eric Learned-Miller 1': 319,\n",
       " 'Gregory Griffin': 320,\n",
       " 'Alex Holub': 321,\n",
       " 'Pietro Perona': 322,\n",
       " 'A. Torralba 1, R. Fergus 2, W.T. Freeman 1': 323,\n",
       " 'Luis von Ahn': 324,\n",
       " 'Laura Dabbish': 325,\n",
       " 'Li Fei-Fei 1, R. Fergus 2, P. Perona 2': 326,\n",
       " 'Jamie Shotton 1, John Winn 2, Carsten Rother 2, Antonio Criminisi 2': 327,\n",
       " 'Novel Coronavirus Pneumonia Emergency Response Epidemiology Team': 328,\n",
       " 'Michael Chung 1, Adam Bernheim 1, Xueyan Mei 1, Ning Zhang 2, Mingqian Huang 1, Xianjun Zeng 2, Jiufa Cui 3, Wenjian Xu 3, Yang Yang 1, Zahi A. Fayad 1, Adam Jacobi 1, Kunwei Li 4, Shaolin Li 4, Hong Shan 4': 329,\n",
       " 'Feng Pan': 330,\n",
       " 'Tianhe Ye': 331,\n",
       " 'Peng Sun': 332,\n",
       " 'Shan Gui': 333,\n",
       " 'Bo Liang': 334,\n",
       " 'Lingli Li': 335,\n",
       " 'Dandan Zheng': 336,\n",
       " 'Jiazheng Wang': 337,\n",
       " 'Richard L Hesketh': 338,\n",
       " 'Lian Yang': 339,\n",
       " 'Chuansheng Zheng': 340,\n",
       " 'Xingzhi Xie': 341,\n",
       " 'Zheng Zhong': 342,\n",
       " 'Wei Zhao': 343,\n",
       " 'Chao Zheng': 344,\n",
       " 'Fei Wang': 345,\n",
       " 'Jun Liu': 346,\n",
       " 'Yueying Pan': 347,\n",
       " 'Hanxiong Guan': 348,\n",
       " 'Shuchang Zhou': 349,\n",
       " 'Yujin Wang': 350,\n",
       " 'Qian Li': 351,\n",
       " 'Tingting Zhu': 352,\n",
       " 'Qiongjie Hu': 353,\n",
       " 'Liming Xia': 354,\n",
       " 'Junqiang Lei': 355,\n",
       " 'Junfeng Li': 356,\n",
       " 'Xun Li': 357,\n",
       " 'Xiaolong Qi': 358,\n",
       " 'Peikai Huang': 359,\n",
       " 'Tianzhu Liu': 360,\n",
       " 'Lesheng Huang': 361,\n",
       " 'Hailong Liu': 362,\n",
       " 'Ming Lei': 363,\n",
       " 'Wangdong Xu': 364,\n",
       " 'Xiaolu Hu': 365,\n",
       " 'Jun Chen': 366,\n",
       " 'Bo Liu': 367,\n",
       " 'Heshui Shi 1, Xiaoyu Han 2, Chuansheng Zheng': 368,\n",
       " 'Wei Zhang 1, Rong-Hui Du 2, Bei Li 1, Xiao-Shuang Zheng 1, Xing-Lou Yang 1, Ben Hu 1, Yan-Yi Wang 1, Geng-Fu Xiao 1, Bing Yan 1, Zheng-Li Shi 1, Peng Zhou 1': 369,\n",
       " 'Yang Pan 1, 2, Daitao Zhang 2, Peng Yang 1, 2, Leo L M Poon 3, Quanyi Wang 2': 370,\n",
       " 'Jie Yu': 371,\n",
       " 'Peiwei Chai': 372,\n",
       " 'Shengfang Ge': 373,\n",
       " 'Xianqun Fan': 374,\n",
       " 'Axel Kramer 1, Rüdiger Külpmann 2, Arnold Brunner 3, Michael Müller 4, Georgi Wassilew 1': 375,\n",
       " 'Niamh Cahill': 376,\n",
       " 'Dearbháile Morris': 377,\n",
       " 'Lionel Roques 1, Etienne K. Klein 1, Julien Papaïx 1, Antoine Sar 2, Samuel Soubeyrand 1': 378,\n",
       " 'Yee Ling Lau 1, Ilyiana Ismail 2, Nur Izati Mustapa 2, Meng Yee Lai 1, Tuan Suhaila Tuan Soh 2, Afifah Hassan 2, Kalaiarasu M Peariasamy 3, Yee Leng Lee 3, Yoong Min Chong 1, I-Ching Sam 1, Pik Pin Goh 4': 379,\n",
       " 'Song Su 1, Jun Shen 2, Liangru Zhu 3, Yun Qiu 4, Jin-Shen He 4, Jin-Yu Tan 4, Marietta Iacucci 5, Siew C Ng 6, Subrata Ghosh 5, Ren Mao 4, Jie Liang 1': 380,\n",
       " 'Ali Reza Rahmani': 381,\n",
       " 'Mostafa Leili': 382,\n",
       " 'Ghasem Azarian': 383,\n",
       " 'Ali Poormohammadi': 384,\n",
       " 'Raymund R. Razonable': 385,\n",
       " 'Kelly M. Pennington': 386,\n",
       " 'Anne M. Meehan': 387,\n",
       " 'John W. Wilson': 388,\n",
       " 'Adam T. Froemming': 389,\n",
       " 'Courtney E. Bennett': 390,\n",
       " 'Ariela L. Marshall': 391,\n",
       " 'Abinash Virk': 392,\n",
       " 'Eva M. Carmona': 393,\n",
       " 'Kamal Kant Sahu 1, Ajay Kumar Mishra 1, Amos Lal 2': 394,\n",
       " 'Daniel Shyu': 395,\n",
       " 'James Dorroh': 396,\n",
       " 'Caleb Holtmeyer': 397,\n",
       " 'Detlef Ritter': 398,\n",
       " 'Anandhi Upendran': 399,\n",
       " 'Raghuraman Kannan': 400,\n",
       " 'Dima Dandachi': 401,\n",
       " 'Christian Rojas-Moreno': 402,\n",
       " 'Stevan P Whitt': 403,\n",
       " 'Hariharan Regunath': 404,\n",
       " 'Ioana M Ciuca': 405,\n",
       " 'Matthew E. Peters 1, Mark Neumann 1, Mohit Iyyer 2, Matt Gardner 1, Christopher Clark 1, Kenton Lee 3, Luke Zettlemoyer 4': 406,\n",
       " 'Justin Johnson': 407,\n",
       " 'Alexandre Alahi': 408,\n",
       " 'Thomas N. Kipf': 409,\n",
       " 'Max Welling': 410,\n",
       " 'Kelvin Xu 1, Jimmy Ba 2, Ryan Kiros 2, Kyunghyun Cho 1, Aaron Courville 1, Ruslan Salakhudinov 2, 3, Rich Zemel 2, 3, Yoshua Bengio 1, 3': 411,\n",
       " 'P F Felzenszwalb 1, R B Girshick 1, D McAllester 2, D Ramanan 3': 412,\n",
       " 'Rob Fergus': 413,\n",
       " 'Diederik P Kingma': 414,\n",
       " 'Ruslan R. Salakhutdinov': 415,\n",
       " 'Christian Szegedy 1, Wojciech Zaremba 2, Ilya Sutskever 1, Joan Bruna 2, Dumitru Erhan 1, Ian Goodfellow 3, Rob Fergus 2, 4': 416,\n",
       " 'Zhou Wang 1, A.C. Bovik 2, H.R. Sheikh 2, E.P. Simoncelli 3': 417,\n",
       " 'Marius Cordts 1, Mohamed Omran 2, Sebastian Ramos 3, Timo Rehfeld 1, Markus Enzweiler 3, Rodrigo Benenson 2, Uwe Franke 3, Stefan Roth 1, Bernt Schiele 2': 418,\n",
       " 'Deepak Pathak': 419,\n",
       " 'Philipp Krahenbuhl': 420,\n",
       " 'Scott Reed 1, Zeynep Akata 2, Xinchen Yan 1, Lajanugen Logeswaran 1, Bernt Schiele 2, Honglak Lee 1': 421,\n",
       " 'Ting-Chun Wang 1, Ming-Yu Liu 1, Jun-Yan Zhu 2, Andrew Tao 1, Jan Kautz 1, Bryan Catanzaro 1': 422,\n",
       " 'Takeru Miyato 1, Toshiki Kataoka': 423,\n",
       " 'Masanori Koyama 2, Yuichi Yoshida 3': 424,\n",
       " 'Andrew Brock 1, Jeff Donahue 2, Karen Simonyan 2': 425,\n",
       " 'Satoshi Iizuka': 426,\n",
       " 'Edgar Simo-Serra': 427,\n",
       " 'Hiroshi Ishikawa': 428,\n",
       " 'Martín Abadi': 429,\n",
       " 'Ashish Agarwal': 430,\n",
       " 'Paul Barham': 431,\n",
       " 'Eugene Brevdo': 432,\n",
       " 'Zhifeng Chen': 433,\n",
       " 'Craig Citro': 434,\n",
       " 'Gregory S. Corrado': 435,\n",
       " 'Andy Davis': 436,\n",
       " 'Sanjay Ghemawat': 437,\n",
       " 'Ian J. Goodfellow': 438,\n",
       " 'Andrew Harp': 439,\n",
       " 'Geoffrey Irving': 440,\n",
       " 'Michael Isard': 441,\n",
       " 'Yangqing Jia': 442,\n",
       " 'Rafal Józefowicz': 443,\n",
       " 'Lukasz Kaiser': 444,\n",
       " 'Manjunath Kudlur': 445,\n",
       " 'Josh Levenberg': 446,\n",
       " 'Dan Mané': 447,\n",
       " 'Sherry Moore': 448,\n",
       " 'Derek Gordon Murray': 449,\n",
       " 'Chris Olah': 450,\n",
       " 'Mike Schuster': 451,\n",
       " 'Jonathon Shlens': 452,\n",
       " 'Benoit Steiner': 453,\n",
       " 'Kunal Talwar': 454,\n",
       " 'Paul A. Tucker': 455,\n",
       " 'Vincent Vanhoucke': 456,\n",
       " 'Vijay Vasudevan': 457,\n",
       " 'Fernanda B. Viégas': 458,\n",
       " 'Oriol Vinyals': 459,\n",
       " 'Pete Warden': 460,\n",
       " 'Martin Wattenberg': 461,\n",
       " 'Martin Wicke': 462,\n",
       " 'Yuan Yu': 463,\n",
       " 'Xiaoqiang Zheng': 464,\n",
       " 'Emily Denton 1, Soumith Chintala 2, Arthur Szlam 2, Rob Fergus 2': 465,\n",
       " 'Diederik P. Kingma': 466,\n",
       " 'Danilo J. Rezende': 467,\n",
       " 'Shakir Mohamed': 468,\n",
       " 'Tim Salimans': 469,\n",
       " 'Antti Rasmus 1, Harri Valpola 1, Mikko Honkala 2, Mathias Berglund 3, Tapani Raiko 3': 470,\n",
       " 'Yujia Li 1, Kevin Swersky 1, Rich Zemel 1, 2': 471,\n",
       " 'Leo Breiman': 472,\n",
       " 'Honglak Lee': 473,\n",
       " 'Roger Grosse': 474,\n",
       " 'Rajesh Ranganath': 475,\n",
       " 'Jeffrey Pennington 1, Richard Socher 2, Christopher Manning 1': 476,\n",
       " 'Quoc Le': 477,\n",
       " 'Richard Socher 1, Alex Perelygin': 478,\n",
       " 'Jean Wu 1, Jason Chuang 2, Christopher D. Manning 1, Andrew Ng 1, Christopher Potts 1': 479,\n",
       " 'Pranav Rajpurkar': 480,\n",
       " 'Jian Zhang': 481,\n",
       " 'Konstantin Lopyrev': 482,\n",
       " 'Percy Liang': 483,\n",
       " 'Iain Murray': 484,\n",
       " 'George A. Miller': 485,\n",
       " 'Yoav Freund': 486,\n",
       " 'David Haussler': 487,\n",
       " 'Georgia Gkioxari': 488,\n",
       " 'Piotr Dollar': 489,\n",
       " 'Tsung-Yi Lin 1, Michael Maire 2, Serge J. Belongie 1, James Hays 3, Pietro Perona 2, Deva Ramanan 4, Piotr Dollár 5, C. Lawrence Zitnick 5': 490,\n",
       " 'Tsung-Yi Lin 1, Piotr Dollar 2, Ross Girshick 2, Kaiming He 2, Bharath Hariharan 2, Serge Belongie 1': 491,\n",
       " 'Triantafyllos Afouras 1, Andrew Owens 2, Joon Son Chung 3, Andrew Zisserman 1': 492,\n",
       " 'Carl Doersch 1, Ankush Gupta 1, Andrew Zisserman 2': 493,\n",
       " 'Taesung Park 1, Alexei A. Efros 1, Richard Zhang 2, Jun-Yan Zhu 2': 494,\n",
       " 'Michael Laskin 1, Kimin Lee 1, Adam Stooke 1, Lerrel Pinto 2, Pieter Abbeel 1, Aravind Srinivas 1': 495,\n",
       " 'Amy Zhang 1, Rowan Thomas McAllister 2, Roberto Calandra 1, Yarin Gal 3, Sergey Levine 2': 496,\n",
       " 'Rui Qian 1, Tianjian Meng 2, Boqing Gong 2, Ming-Hsuan Yang': 497,\n",
       " 'Huisheng Wang 2, Serge J. Belongie 1, Yin Cui 2': 498,\n",
       " 'Yanqiao Zhu 1, Yichen Xu 2, Feng Yu 3, Qiang Liu 1, Shu Wu 1, Liang Wang 1': 499,\n",
       " 'Beliz Gunel 1, Jingfei Du 2, Alexis Conneau 2, Veselin Stoyanov 3': 500,\n",
       " 'Sam T. Roweis 1, Lawrence K. Saul 2': 501,\n",
       " 'J. B. Tenenbaum 1, V. de Silva 1, J. C. Langford 2': 502,\n",
       " 'John J. Hopfield': 503,\n",
       " 'Nandakishore Kambhatla': 504,\n",
       " 'Todd K. Leen': 505,\n",
       " 'Robert Hecht-Nielsen': 506,\n",
       " 'David C. Plaut': 507,\n",
       " 'Mikhail Belkin': 508,\n",
       " 'Partha Niyogi': 509,\n",
       " 'L. Grady': 510,\n",
       " 'Xiaojin Zhu': 511,\n",
       " 'Zoubin Ghahramani': 512,\n",
       " 'John Lafferty': 513,\n",
       " 'Eric Postma 1, Jaap van den Herik 2': 514,\n",
       " 'Sam T. Roweis': 515,\n",
       " 'John A. Lee': 516,\n",
       " 'Michel Verleysen': 517,\n",
       " 'Y. LeCun 1, Fu Jie Huang 1, L. Bottou 2': 518,\n",
       " 'Neeraj Kumar': 519,\n",
       " 'Alexander C. Berg': 520,\n",
       " 'Peter N. Belhumeur': 521,\n",
       " 'Shree K. Nayar': 522,\n",
       " 'Vladimir N. Vapnik': 523,\n",
       " 'D.L. Donoho': 524,\n",
       " 'E.J. Candes 1, T. Tao 2': 525,\n",
       " 'Corinna Cortes': 526,\n",
       " 'Vladimir Vapnik': 527,\n",
       " 'N. Dalal': 528,\n",
       " 'B. Triggs': 529,\n",
       " 'S. Lazebnik 1, C. Schmid 2, J. Ponce 3': 530,\n",
       " 'Jianchao Yang 1, Kai Yu 2, Yihong Gong 2, Thomas Huang 1': 531,\n",
       " 'Li Fei-Fei 1, Rob Fergus 2, Pietro Perona 3': 532,\n",
       " 'David E. Goldberg': 533,\n",
       " 'D. E. Rumelhart': 534,\n",
       " 'R. J. Williams': 535,\n",
       " 'Richard O. Duda': 536,\n",
       " 'Peter E. Hart': 537,\n",
       " 'Richard S. Sutton': 538,\n",
       " 'Bernard Widrow': 539,\n",
       " 'Marcian E. Hoff': 540,\n",
       " 'Dimitri P. Bertsekas': 541,\n",
       " 'John N. Tsitsiklis': 542,\n",
       " 'Graham C. Goodwin': 543,\n",
       " 'Kwai Sang Sin': 544,\n",
       " 'David S. Broomhead': 545,\n",
       " 'David Lowe': 546,\n",
       " 'Arthur L. Samuel': 547,\n",
       " 'C. B. Browne 1, E. Powley 2, D. Whitehouse 2, S. M. Lucas 3, P. I. Cowling 2, P. Rohlfshagen 3, S. Tavener 1, D. Perez 3, S. Samothrakis 3, S. Colton 1': 548,\n",
       " 'Andrew G. Barto': 549,\n",
       " 'Levente Kocsis': 550,\n",
       " 'Csaba Szepesvári': 551,\n",
       " 'Aristides Gionis': 552,\n",
       " 'Piotr Indyk': 553,\n",
       " 'Rajeev Motwani': 554,\n",
       " 'Michael R. Genesereth': 555,\n",
       " 'Nathaniel Love': 556,\n",
       " 'Barney Pell': 557,\n",
       " 'Marcus Hutter': 558,\n",
       " 'Carlos Diuk': 559,\n",
       " 'Andre Cohen': 560,\n",
       " 'Michael L. Littman': 561,\n",
       " 'Richard S. Sutton 1, Joseph Modayil 1, Michael Delp 1, Thomas Degris 1, Patrick M. Pilarski 1, Adam White 1, Doina Precup 2': 562,\n",
       " 'Jürgen Schmidhuber': 563,\n",
       " 'Isabelle Lajoie': 564,\n",
       " 'M. Dorigo 1, V. Maniezzo 2, A. Colorni 3': 565,\n",
       " 'Bryan Perozzi': 566,\n",
       " 'Rami Al-Rfou': 567,\n",
       " 'Steven Skiena': 568,\n",
       " 'Alexander Toshev': 569,\n",
       " 'Samy Bengio': 570,\n",
       " 'Jian Tang 1, Meng Qu 2, Mingzhe Wang 2, Ming Zhang 2, Jun Yan 1, Qiaozhu Mei 3': 571,\n",
       " 'Ryan Kiros 1, Yukun Zhu 1, Ruslan Salakhutdinov 2, Richard S. Zemel 2, Antonio Torralba 3, Raquel Urtasun 1, Sanja Fidler 1': 572,\n",
       " 'Michaël Defferrard': 573,\n",
       " 'Xavier Bresson': 574,\n",
       " 'Pierre Vandergheynst': 575,\n",
       " 'Sébastien Jean': 576,\n",
       " 'Kyunghyun Cho': 577,\n",
       " 'Roland Memisevic': 578,\n",
       " 'Andrea Frome': 579,\n",
       " 'Jon Shlens': 580,\n",
       " 'Rie Kubota Ando': 581,\n",
       " 'Tong Zhang': 582,\n",
       " 'Martha Palmer 1, Daniel Gildea 2, Paul Kingsbury 1': 583,\n",
       " 'Thorsten Joachims': 584,\n",
       " 'Rich Caruana': 585,\n",
       " 'Alexander Waibel': 586,\n",
       " 'Toshiyuki Hanazawa': 587,\n",
       " 'Kiyohiro Shikano': 588,\n",
       " 'Kevin J. Lang': 589,\n",
       " 'Sameer S. Pradhan': 590,\n",
       " 'Wayne H. Ward': 591,\n",
       " 'Kadri Hacioglu': 592,\n",
       " 'James H. Martin': 593,\n",
       " 'Daniel Jurafsky': 594,\n",
       " 'Charles Sutton': 595,\n",
       " 'Khashayar Rohanimanesh': 596,\n",
       " 'David McClosky': 597,\n",
       " 'Eugene Charniak': 598,\n",
       " 'Mark Johnson': 599,\n",
       " 'Tomas Mikolov 1, Martin Karafiát 1, Lukás Burget 1, Jan Cernocký': 600,\n",
       " 'Sanjeev Khudanpur 2': 601,\n",
       " 'Scott Deerwester 1, Susan T. Dumais 2, George W. Furnas 2, Thomas K. Landauer 2, Richard Harshman 3': 602,\n",
       " 'Mitchell P. Marcus 1, Mary Ann Marcinkiewicz 1, Beatrice Santorini 2': 603,\n",
       " 'Holger Schwenk': 604,\n",
       " 'Andreas Stolcke': 605,\n",
       " 'Adam L. Berger 1, Vincent J. Della Pietra 2, Stephen A. Della Pietra 2': 606,\n",
       " 'Jeffrey L. Elman': 607,\n",
       " 'Message P Forum': 608,\n",
       " 'Stanley F. Chen 1, Joshua Goodman 2': 609,\n",
       " 'Peter F. Brown': 610,\n",
       " 'Peter V. deSouza': 611,\n",
       " 'Robert L. Mercer': 612,\n",
       " 'Vincent J. Della Pietra': 613,\n",
       " 'Jenifer C. Lai': 614,\n",
       " 'David M. Blei 1, Andrew Y. Ng 2, Michael I. Jordan 1': 615,\n",
       " 'Radim Řehůřek': 616,\n",
       " 'Petr Sojka': 617,\n",
       " 'Yoshua Bengio 1, Jérôme Louradour 2, Ronan Collobert 3, Jason Weston 3': 618,\n",
       " 'Thomas K Landauer 1, Peter W. Foltz 2, Darrell Laham 1': 619,\n",
       " 'Lev Ratinov': 620,\n",
       " 'Dan Roth': 621,\n",
       " 'Fei Sha': 622,\n",
       " 'Fernando Pereira': 623,\n",
       " 'D. Comaniciu': 624,\n",
       " 'P. Meer': 625,\n",
       " 'Christopher D. Manning 1, Hinrich Schütze 2': 626,\n",
       " 'Aude Oliva 1, Antonio Torralba 2': 627,\n",
       " 'Stephen Gould': 628,\n",
       " 'Richard Fulton': 629,\n",
       " 'Daphne Koller': 630,\n",
       " 'Christopher D. Manning 1, Prabhakar Raghavan 2, Hinrich Schütze 3': 631,\n",
       " 'Tamara G. Kolda': 632,\n",
       " 'Brett W. Bader': 633,\n",
       " 'Ricardo A. Baeza-Yates': 634,\n",
       " 'Berthier Ribeiro-Neto': 635,\n",
       " 'Bo Pang 1, Lillian Lee 1, Shivakumar Vaithyanathan 2': 636,\n",
       " 'A. K. Jain 1, M. N. Murty 2, P. J. Flynn 3': 637,\n",
       " 'Jeffrey Pennington': 638,\n",
       " 'Eric H. Huang': 639,\n",
       " 'Dan Klein': 640,\n",
       " 'Lev Ratinov 1, Dan Roth 1, Doug Downey 2, Mike Anderson 3': 641,\n",
       " 'Frederic Morin': 642,\n",
       " 'Fernando Pereira 1, Naftali Tishby 2, Lillian Lee 3': 643,\n",
       " 'Jean-Luc Gauvain': 644,\n",
       " 'Joshua T. Goodman': 645,\n",
       " 'Jean-Sébastien Senecal': 646,\n",
       " 'Pierre Sermanet': 647,\n",
       " 'David Eigen': 648,\n",
       " 'Xiang Zhang': 649,\n",
       " 'Michael Mathieu': 650,\n",
       " 'Min Lin': 651,\n",
       " 'Qiang Chen': 652,\n",
       " 'Shuicheng Yan': 653,\n",
       " 'Dragomir Anguelov': 654,\n",
       " 'Mark Everingham 1, Luc Gool 2, Christopher K. Williams 3, John Winn 4, Andrew Zisserman 5': 655,\n",
       " 'J. R. Uijlings 1, K. E. Sande 2, T. Gevers 2, A. W. Smeulders 2': 656,\n",
       " 'Judy Hoffman': 657,\n",
       " 'Ning Zhang': 658,\n",
       " 'Eric Tzeng': 659,\n",
       " 'Christopher M. Bishop': 660,\n",
       " 'Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 3, Jian Sun 1': 661,\n",
       " 'Ronan Collobert 1, Koray Kavukcuoglu 2, Clément Farabet 2': 662,\n",
       " 'Sergio Guadarrama 1, Erik Rodner 2, Kate Saenko 3, Ning Zhang 1, Ryan Farrell 1, Jeff Donahue 1, Trevor Darrell 1': 663,\n",
       " \"Ning Zhang 1, Manohar Paluri 2, Marc'Aurelio Ranzato 2, Trevor Darrell 1, Lubomir Bourdev 2\": 664,\n",
       " 'Vincent Dumoulin': 665,\n",
       " 'Razvan Pascanu': 666,\n",
       " 'Frédéric Bastien': 667,\n",
       " 'Adam Paszke': 668,\n",
       " 'Sam Gross': 669,\n",
       " 'Soumith Chintala': 670,\n",
       " 'Gregory Chanan': 671,\n",
       " 'Edward Yang': 672,\n",
       " 'Zachary DeVito': 673,\n",
       " 'Zeming Lin': 674,\n",
       " 'Alban Desmaison': 675,\n",
       " 'Luca Antiga': 676,\n",
       " 'Adam Lerer': 677,\n",
       " 'Rico Sennrich': 678,\n",
       " 'Barry Haddow': 679,\n",
       " 'Alexandra Birch': 680,\n",
       " 'Samuel R. Bowman': 681,\n",
       " 'Gabor Angeli': 682,\n",
       " 'Christopher Potts': 683,\n",
       " 'Jeremy Howard': 684,\n",
       " 'Sebastian Ruder': 685,\n",
       " 'Kevin Clark 1, Minh-Thang Luong 2, Quoc V. Le 2, Christopher D. Manning 1': 686,\n",
       " 'Alex Wang 1, Yada Pruksachatkun 1, Nikita Nangia 1, Amanpreet Singh 2, Julian Michael 3, Felix Hill 4, Omer Levy 2, Samuel R. Bowman 1': 687,\n",
       " 'Suchin Gururangan 1, Ana Marasović 1, Swabha Swayamdipta 1, Kyle Lo 1, Iz Beltagy 1, Doug Downey 2, Noah A. Smith 1': 688,\n",
       " 'Xiaoqi Jiao 1, Yichun Yin 2, Lifeng Shang 2, Xin Jiang 2, Xiao Chen 2, Linlin Li 2, Fang Wang 2, Qun Liu 2': 689,\n",
       " 'Yonatan Bisk 1, Ari Holtzman 2, Jesse Thomason 2, Jacob Andreas 3, Yoshua Bengio 4, Joyce Chai 5, Mirella Lapata 6, Angeliki Lazaridou 7, Jonathan May 8, Aleksandr Nisnevich 9, Nicolas Pinto 3, Joseph P. Turian 4': 690,\n",
       " 'Prannay Khosla': 691,\n",
       " 'Piotr Teterwak 1, Chen Wang 2, Aaron Sarna 1, Yonglong Tian 3, Phillip Isola 3, Aaron Maschinot 1, Ce Liu 1, Dilip Krishnan 1': 692,\n",
       " 'Alexis Conneau 1, Douwe Kiela 2, Holger Schwenk 3, Loïc Barrault 3, Antoine Bordes 1': 693,\n",
       " 'Adina Williams': 694,\n",
       " 'Nikita Nangia': 695,\n",
       " 'Ido Dagan': 696,\n",
       " 'Oren Glickman': 697,\n",
       " 'Bernardo Magnini': 698,\n",
       " 'Suchin Gururangan 1, Swabha Swayamdipta 2, Omer Levy 1, Roy Schwartz 1, 3, Samuel R. Bowman 4, Noah A. Smith 1': 699,\n",
       " 'Daniel M. Cer 1, Mona T. Diab 2, Eneko Agirre 3, Iñigo Lopez-Gazpio 3, Lucia Specia 4': 700,\n",
       " 'Danilo Giampiccolo 1, Bernardo Magnini 2, Ido Dagan 3, Bill Dolan 4': 701,\n",
       " 'Yonghui Wu': 702,\n",
       " 'Wolfgang Macherey': 703,\n",
       " 'Maxim Krikun': 704,\n",
       " 'Yuan Cao': 705,\n",
       " 'Qin Gao': 706,\n",
       " 'Klaus Macherey': 707,\n",
       " 'Jeff Klingner': 708,\n",
       " 'Apurva Shah': 709,\n",
       " 'Melvin Johnson': 710,\n",
       " 'Xiaobing Liu': 711,\n",
       " 'Łukasz Kaiser': 712,\n",
       " 'Stephan Gouws': 713,\n",
       " 'Yoshikiyo Kato': 714,\n",
       " 'Taku Kudo': 715,\n",
       " 'Hideto Kazawa': 716,\n",
       " 'Keith Stevens': 717,\n",
       " 'George Kurian': 718,\n",
       " 'Nishant Patil': 719,\n",
       " 'Wei Wang': 720,\n",
       " 'Cliff Young': 721,\n",
       " 'Jason Smith': 722,\n",
       " 'Jason Riesa': 723,\n",
       " 'Alex Rudnick': 724,\n",
       " 'Macduff Hughes': 725,\n",
       " 'Bryan McCann': 726,\n",
       " 'James Bradbury': 727,\n",
       " 'Caiming Xiong': 728,\n",
       " 'Robert Tibshirani': 729,\n",
       " 'Jasper Snoek 1, Hugo Larochelle 2, Ryan P Adams 3': 730,\n",
       " 'Yuval Netzer 1, Tao Wang 1, Adam Coates 1, Alessandro Bissacco 2, Bo Wu 2, Andrew Y. Ng 2': 731,\n",
       " 'Stephen Boyd 1, Lieven Vandenberghe 2': 732,\n",
       " 'A. Asuncion': 733,\n",
       " 'Dimitri Bertsekas': 734,\n",
       " 'Roger A. Horn 1, Charles R. Johnson 2': 735,\n",
       " 'David D. Lewis': 736,\n",
       " 'Yiming Yang': 737,\n",
       " 'Tony G. Rose': 738,\n",
       " 'Fan Li': 739,\n",
       " 'Yu Nesterov': 740,\n",
       " 'A. Nemirovski 1, A. Juditsky 2, G. Lan 1, A. Shapiro 1': 741,\n",
       " 'Koby Crammer 1, 2, Ofer Dekel 2, Joseph Keshet 2, Shai Shalev-Shwartz 2, Yoram Singer 2': 742,\n",
       " 'Gerard Salton': 743,\n",
       " 'Christopher Buckley': 744,\n",
       " 'Dan Cireşan': 745,\n",
       " 'Ueli Meier': 746,\n",
       " 'Juergen Schmidhuber': 747,\n",
       " 'Adam Coates 1, Andrew Y. Ng 2, Honglak Lee 1': 748,\n",
       " 'I︠u︡. E. Nesterov': 749,\n",
       " 'Tom Schaul': 750,\n",
       " 'Sixin Zhang': 751,\n",
       " 'S. Becker': 752,\n",
       " 'Yann Lecun': 753,\n",
       " 'Herbert Robbins': 754,\n",
       " 'Sutton Monro': 755,\n",
       " 'Tomas Mikolov 1, Stefan Kombrink 1, Lukas Burget 1, Jan Cernocky 1, Sanjeev Khudanpur 2': 756,\n",
       " 'Stephen A. Della Pietra': 757,\n",
       " 'James Martens': 758,\n",
       " 'Abdel-rahman Mohamed': 759,\n",
       " 'Yee W. Teh': 760,\n",
       " 'Amittai Axelrod 1, Xiaodong He 2, Jianfeng Gao 2': 761,\n",
       " 'Xingyi Song': 762,\n",
       " 'Trevor Cohn': 763,\n",
       " 'Lucia Specia': 764,\n",
       " 'Arnaud Bergeron': 765,\n",
       " 'Nicolas Bouchard': 766,\n",
       " 'Mantas Lukoševičius': 767,\n",
       " 'Herbert Jaeger': 768,\n",
       " 'A. Graves 1, M. Liwicki 2, S. Fernandez 3, R. Bertolami 4, H. Bunke 4, J. Schmidhuber 1': 769,\n",
       " 'Frederick Jelinek': 770,\n",
       " 'Hermann Ney': 771,\n",
       " 'Michael Collins': 772,\n",
       " 'Kenji Yamada': 773,\n",
       " 'Kevin Knight': 774,\n",
       " 'Christoph Tillmann': 775,\n",
       " 'Daniel Marcu 1, Daniel Wong 2': 776,\n",
       " 'Philip Clarkson': 777,\n",
       " 'Ronald Rosenfeld': 778,\n",
       " 'Dekai Wu': 779,\n",
       " 'Florian Schroff': 780,\n",
       " 'Dmitry Kalenichenko': 781,\n",
       " 'James Philbin': 782,\n",
       " 'Andrej Karpathy': 783,\n",
       " 'George Toderici': 784,\n",
       " 'Sanketh Shetty': 785,\n",
       " 'Thomas Leung': 786,\n",
       " 'Rahul Sukthankar': 787,\n",
       " 'Bo Pang 1, Lillian Lee 2': 788,\n",
       " 'Y. Freund': 789,\n",
       " 'R. Schapire': 790,\n",
       " 'Richard Durbin 1, Sean Eddy 2, Anders Stærmose Krogh 3, Graeme Mitchison 4': 791,\n",
       " 'Dayne Freitag 1, Fernando C. N. Pereira 2': 792,\n",
       " 'Adwait Ratnaparkhi': 793,\n",
       " 'S. Della Pietra 1, V. Della Pietra': 794,\n",
       " 'J. Lafferty 2': 795,\n",
       " 'Eric Brill': 796,\n",
       " 'Dennis Decoste 1, Bernhard Schölkopf 2': 797,\n",
       " 'Kurt Hornik': 798,\n",
       " 'Maxwell Stinchcombe': 799,\n",
       " 'Halbert White': 800,\n",
       " 'Yong Haur Tay': 801,\n",
       " 'P.M. Lallican': 802,\n",
       " 'M. Khalid': 803,\n",
       " 'C. Viard-Gaudin': 804,\n",
       " 'S. Kneer': 805,\n",
       " 'Anshu Sinha': 806,\n",
       " 'Larry S. Yaeger 1, Richard F. Lyon 1, Brandyn J. Webb 2': 807,\n",
       " 'Daiki Tanaka': 808,\n",
       " 'Daiki Ikami': 809,\n",
       " 'Toshihiko Yamasaki': 810,\n",
       " 'Kiyoharu Aizawa': 811,\n",
       " 'Vikas Verma 1, Alex Lamb 2, Juho Kannala 1, Yoshua Bengio 2, David Lopez-Paz 3': 812,\n",
       " 'Tong He': 813,\n",
       " 'Zhi Zhang': 814,\n",
       " 'Hang Zhang': 815,\n",
       " 'Zhongyue Zhang': 816,\n",
       " 'Junyuan Xie': 817,\n",
       " 'Mu Li': 818,\n",
       " 'Sylvain Gugger': 819,\n",
       " 'Aniruddh Raghu 1, Maithra Raghu 2, Simon Kornblith 3, David Duvenaud 4, Geoffrey Hinton 4': 820,\n",
       " 'Christopher Beckham 1, Sina Honari 2, Vikas Verma 3, Alex M. Lamb 2, Farnoosh Ghadiri 4, R Devon Hjelm 5, Yoshua Bengio 2, Chris Pal 2': 821,\n",
       " 'Devansh Arpit 1, Víctor Campos 2, Yoshua Bengio 3': 822,\n",
       " 'Cihang Xie 1, Mingxing Tan 2, Boqing Gong 2, Jiang Wang 2, Alan L. Yuille 1, Quoc V. Le 2': 823,\n",
       " 'Trevor Hastie': 824,\n",
       " 'Robert J. Tibshirani': 825,\n",
       " 'Jerome Friedman': 826,\n",
       " 'Ulrike Luxburg': 827,\n",
       " 'Fan R K Chung': 828,\n",
       " 'Will Y. Zou': 829,\n",
       " 'Serena Y. Yeung': 830,\n",
       " 'Jiquan Ngiam': 831,\n",
       " 'Zhenghao Chen': 832,\n",
       " 'Daniel Chia': 833,\n",
       " 'Pang W. Koh': 834,\n",
       " 'Ronald J. Williams 1, David Zipser 2': 835,\n",
       " 'Michael I. Jordan': 836,\n",
       " 'Pearlmutter': 837,\n",
       " 'Richard Rohwer': 838,\n",
       " 'J. J. Hopfield': 839,\n",
       " 'Shun-Ichi Amarimber': 840,\n",
       " 'K.S. Narendra': 841,\n",
       " 'K. Parthasarathy': 842,\n",
       " 'P.J. Werbos': 843,\n",
       " 'Charles W. Anderson': 844,\n",
       " 'Jing Peng': 845,\n",
       " 'Sharad Singhal': 846,\n",
       " 'Lance Wu': 847,\n",
       " 'Jordan': 848,\n",
       " 'S. Kirkpatrick 1, C. D. Gelatt 1, M. P. Vecchi 2': 849,\n",
       " 'J. J. Hopfield 1, D. W. Tank 2': 850,\n",
       " 'David H. Ackley 1, Geoffrey E. Hinton 1, Terrence J. Sejnowski 2': 851,\n",
       " 'Paul J. Werbos': 852,\n",
       " 'P. Werbos': 853,\n",
       " 'B. Boser': 854,\n",
       " 'J. S. Denker': 855,\n",
       " 'D. Henderson': 856,\n",
       " 'R. E. Howard': 857,\n",
       " 'W. Hubbard': 858,\n",
       " 'L. D. Jackel': 859,\n",
       " 'Bernard Widrow 1, Samuel D. Stearns 2': 860,\n",
       " 'Y. Bengio 1, P. Simard 2, P. Frasconi 3': 861,\n",
       " 'Thomas Kailath': 862,\n",
       " 'S. Chen 1, S. A. Billings 2, Peter Grant 1': 863,\n",
       " 'Lalit R. Bahl': 864,\n",
       " 'Kunihiko Fukushima': 865,\n",
       " 'A. Viterbi': 866,\n",
       " 'D. Marr 1, T. Poggio 2': 867,\n",
       " 'Alexander Graves': 868,\n",
       " 'Felix A. Gers': 869,\n",
       " 'Jürgen A. Schmidhuber': 870,\n",
       " 'Fred A. Cummins': 871,\n",
       " 'Katerina Fragkiadaki': 872,\n",
       " 'Sergey Levine': 873,\n",
       " 'Panna Felsen': 874,\n",
       " 'Alex Graves 1, Jürgen Schmidhuber 2': 875,\n",
       " 'Jack Kelly': 876,\n",
       " 'William Knottenbelt': 877,\n",
       " 'Felix A. Gers 1, Nicol N. Schraudolph 2, Jürgen Schmidhuber 1': 878,\n",
       " 'S. Belongie 1, J. Malik 2, J. Puzicha 3': 879,\n",
       " 'Judea Pearl': 880,\n",
       " 'S. Roth': 881,\n",
       " 'M.J. Black': 882,\n",
       " 'Feng Ning 1, D. Delhomme 2, Y. LeCun 3, F. Piano 1, L. Bottou 4, P.E. Barbano 3': 883,\n",
       " 'Radford M. Neal': 884,\n",
       " 'Max Welling 1, Michal Rosen-zvi 1, Geoffrey E. Hinton 2': 885,\n",
       " 'M. Elad': 886,\n",
       " 'M. Aharon': 887,\n",
       " 'Yoshua Bengio 1, 2, 3, Yann Lecun': 888,\n",
       " 'Peter Dayan': 889,\n",
       " 'Brendan J. Frey': 890,\n",
       " 'Scott E. Fahlman': 891,\n",
       " 'Christian Lebiere': 892,\n",
       " 'Gerald Tesauro': 893,\n",
       " 'Olivier Delalleau': 894,\n",
       " 'Nicolas L. Roux': 895,\n",
       " 'Patrice Marcotte': 896,\n",
       " 'Chih-Chung Chang': 897,\n",
       " 'Chih-Jen Lin': 898,\n",
       " 'Daniel D. Lee 1, H. Sebastian Seung 1, 2': 899,\n",
       " 'Bruno A. Olshausen': 900,\n",
       " 'David J. Field': 901,\n",
       " 'Yee Whye Teh 1, Max Welling 1, Simon Osindero 2, Geoffrey E. Hinton 1': 902,\n",
       " 'Peter Lennie': 903,\n",
       " 'Geoffrey E. Hinton 1, Richard S. Zemel 2': 904,\n",
       " 'Eero P. Simoncelli': 905,\n",
       " 'Paul Emsley': 906,\n",
       " 'Kevin Cowtan': 907,\n",
       " 'Stéphane Guindon 1, Jean-François Dufayard 2, Vincent Lefort 1, Maria Anisimova 2, Wim Hordijk 2, Olivier Gascuel 1': 908,\n",
       " 'Emmie de Wit 1, Neeltje van Doremalen 1, Darryl Falzarano 2, Vincent J. Munster 1': 909,\n",
       " 'Shuo Su 1, Gary Wong 2, Weifeng Shi 3, Jun Liu 2, 4, Alexander C.K. Lai 5, Jiyong Zhou 1, Wenjun Liu 2, Yuhai Bi 2, George F. Gao 6': 910,\n",
       " 'Xing Yi Ge 1, Jia Lu Li 1, Xing Lou Yang 1, Aleksei A. Chmura 2, Guangjian Zhu 2, Jonathan H. Epstein 2, Jonna A Mazet 3, Ben Hu 1, Wei Zhang 1, Cheng Peng 1, Yu Ji Zhang 1, Chu Ming Luo 1, Bing Tan 1, Ning Wang 1, Yan Zhu 1, Gary Crameri 4, Shu Yi Zhang 5, Lin Fa Wang 4, 6, Peter Daszak 2, Zheng Li Shi 1': 911,\n",
       " 'Ben Hu 1, Lei Ping Zeng 1, Xing Lou Yang 1, Xing Yi Ge 1, Wei Zhang 1, Bei Li 1, Jia Zheng Xie 1, Xu Rui Shen 1, Yun Zhi Zhang 2, Ning Wang 1, Dong Sheng Luo 1, Xiao Shuang Zheng 1, Mei Niang Wang 1, Peter Daszak 3, Lin Fa Wang 4, Jie Cui 1, Zheng Li Shi 1': 912,\n",
       " 'V. Stalin Raj 1, Huihui Mou 2, Saskia L. Smits 1, Dick H. W. Dekkers 1, Marcel A. Müller 3, Ronald Dijkman 4, Doreen Muth 3, Jeroen A. A. Demmers 1, Ali Zaki 5, Ron A. M. Fouchier 1, Volker Thiel 4, 6, Christian Drosten 3, Peter J. M. Rottier 2, Albert D. M. E. Osterhaus 1, Berend Jan Bosch 2, Bart L. Haagmans 1': 913,\n",
       " 'Jsm Peiris 1, ST Lai 2, Llm Poon 1, Y Guan 1, Lyc Yam 3, W Lim 4, J Nicholls 1, Wks Yee 5, WW Yan 2, MT Cheung 3, Vcc Cheng 1, KH Chan 1, Dnc Tsang 6, Rwh Yung 3, TK Ng 2, KY Yuen 1': 914,\n",
       " 'J S M Peiris 1, C M Chu 2, V C C Cheng 1, K S Chan 2, I F N Hung 1, L L M Poon 1, K I Law 2, B S F Tang 1, T Y W Hon 2, C S Chan 2, K H Chan 1, J S C Ng 2, B J Zheng 1, W L Ng 2, R W M Lai 2, Y Guan 1, Kwok-Yung Yuen 1': 915,\n",
       " 'V M Corman 1, I Eckerle 1, T Bleicker 1, A Zaki 2, O Landt 3, M Eschbach-Bludau 1, S van Boheemen 4, R Gopal 5, M Ballhause 3, T M Bestebroer 4, D Muth 1, M A Müller 1, J F Drexler 1, M Zambon 5, A D Osterhaus 4, R M Fouchier 4, C Drosten 1': 916,\n",
       " 'Paul A. Rota 1, M. Steven Oberste 1, Stephan S. Monroe 1, W. Allan Nix 1, Ray Campagnoli 1, Joseph P. Icenogle 1, Silvia Peñaranda 1, Bettina Bankamp 1, Kaija Maher 1, Min hsin Chen 1, Suxiong Tong 1, Azaibi Tamin 1, Luis Lowe 1, Michael Frace 1, Joseph L. DeRisi 2, Qi Chen 1, David Wang 2, Dean D. Erdman 1, Teresa C.T. Peret 1, Cara Burns 1, Thomas G. Ksiazek 1, Pierre E. Rollin 1, Anthony Sanchez 1, Stephanie Liffick 1, Brian Holloway 1, Josef Limor 1, Karen McCaustland 1, Mellissa Olsen-Rasmussen 1, Ron Fouchier 3, Stephan Günther 4, Albert D.H.E. Osterhaus 3, Christian Drosten 4, Mark A. Pallansch 1, Larry J. Anderson 1, William J. Bellini 1': 917,\n",
       " 'Andrew Davies 1, Daryl Jones 1, Michael Bailey 1, John Beca 2, Rinaldo Bellomo 1, Nikki Blackwell 3, Paul Forrest 4, David Gattas 4, Emily Granger 5, Robert Herkes 4, Andrew Jackson 5, Shay McGuinness 2, Priya Nair 5, Vincent Pellegrino 1, Ville Yrjo Olavi Pettila 1, Brian Plunkett 4, Roger Pye 5, Paul Torzillo 4, Steven Webb 6, Michael Wilson 4, Marc Ziegenfuss 3': 918,\n",
       " 'Lia van der Hoek 1, Krzysztof Pyrc 1, Maarten F Jebbink 1, Wilma Vermeulen-Oost 2, Ron J M Berkhout 2, Katja C Wolthers 1, Pauline M E Wertheim-van Dillen 1, Jos Kaandorp 3, Joke Spaargaren 2, Ben Berkhout 1': 919,\n",
       " 'John Welsh': 920,\n",
       " 'Michael McClelland': 921,\n",
       " 'Patrick C. Y. Woo 1, Susanna K. P. Lau 1, Chung-ming Chu 2, Kwok-hung Chan 3, Hoi-wah Tsoi 3, Yi Huang 3, Beatrice H. L. Wong 3, Rosana W. S. Poon 3, James J. Cai 3, Wei-kwang Luk 4, Leo L. M. Poon 1, Samson S. Y. Wong 1, Yi Guan 1, J. S. Malik Peiris 1, Kwok-yung Yuen 1': 922,\n",
       " 'J S M Peiris': 923,\n",
       " 'Y Guan': 924,\n",
       " 'K Y Yuen': 925,\n",
       " 'Kenneth J. Livak 1, Thomas D. Schmittgen 2': 926,\n",
       " 'Yaseen M. Arabi 1, 2, 3, Adel Alothman 2, 3, Hanan H. Balkhy 2, 3, Abdulaziz Al-Dawood 2, 3, Sameera AlJohani 2, 3, Shmeylan Al Harbi 2, 3, Suleiman Kojan 2, 3, Majed Al Jeraisy 2, 3, Ahmad M. Deeb 2, 3, Abdullah M. Assiri 4, Fahad Al-Hameed 2, 3, Asim AlSaedi 2, 3, Yasser Mandourah 5, Ghaleb A. Almekhlafi 5, Nisreen Murad Sherbeeni 6, Fatehi Elnour Elzein 6, Javed Memon 7, Yusri Taha 8, Abdullah Almotairi 9, Khalid A. Maghrabi 10, Ismael Qushmaq 11, Ali Al Bshabshe 12, Ayman Kharaba 13, Sarah Shalhoub 14, Jesna Jose 2, Robert A. Fowler 15, 16, Frederick G. Hayden 17, Mohamed A. Hussein 2': 927,\n",
       " 'Alimuddin Zumla 1, Jasper F. W. Chan 2, Esam I. Azhar 3, David S. C. Hui 4, Kwok-Yung Yuen 2': 928,\n",
       " 'Travis K. Warren 1, Robert Jordan': 929,\n",
       " 'Michael K. Lo 2, Adrian S. Ray': 930,\n",
       " 'Richard L. Mackman': 931,\n",
       " 'Veronica Soloveva 1, Dustin Siegel': 932,\n",
       " 'Michel Perron': 933,\n",
       " 'Roy Bannister': 934,\n",
       " 'Hon C. Hui': 935,\n",
       " 'Nate Larson': 936,\n",
       " 'Robert Strickley': 937,\n",
       " 'Jay Wells 1, Kelly S. Stuthman 1, Sean A. Van Tongeren 1, Nicole L. Garza 1, Ginger Donnelly 1, Amy C. Shurtleff 1, Cary J. Retterer 1, Dima Gharaibeh 1, Rouzbeh Zamani 1, Tara Kenny 1, Brett P. Eaton 1, Elizabeth Grimes 1, Lisa S. Welch 1, Laura Gomba 1, Catherine L. Wilhelmsen 1, Donald K. Nichols 1, Jonathan E. Nuss 1, Elyse R. Nagle 1, Jeffrey R. Kugelman 1, Gustavo Palacios 1, Edward Doerffler': 938,\n",
       " 'Sean Neville': 939,\n",
       " 'Ernest Carra': 940,\n",
       " 'Michael O. Clarke': 941,\n",
       " 'Lijun Zhang': 942,\n",
       " 'Willard Lew': 943,\n",
       " 'Bruce Ross': 944,\n",
       " 'Queenie Wang': 945,\n",
       " 'Kwon Chun': 946,\n",
       " 'Lydia Wolfe': 947,\n",
       " 'Darius Babusis': 948,\n",
       " 'Yeojin Park': 949,\n",
       " 'Kirsten M. Stray': 950,\n",
       " 'Iva Trancheva': 951,\n",
       " 'Joy Y. Feng': 952,\n",
       " 'Ona Barauskas': 953,\n",
       " 'Yili Xu': 954,\n",
       " 'Pamela Wong +12': 955,\n",
       " 'Anroop B Nair 1, Shery Jacob 2': 956,\n",
       " 'Maria L. Agostini 1, Erica L. Andres 1, Amy C Sims 2, Rachel Lauren Graham 2, Timothy Patrick Sheahan 2, Xiaotao Lu 1, Everett Clinton Smith 1, 3, James Brett Case 1, Joy Y. Feng 4, Robert Jordan 5, Adrian S. Ray 5, Tomas Cihlar 5, Dustin Siegel 5, Richard L. Mackman 5, Michael O. Clarke 5, Ralph S Baric 2, Mark R. Denison 1': 957,\n",
       " 'Susan M. Poutanen 1, 2, Donald E. Low 1, Bonnie Henry 3, Sandy Finkelstein 4, David Rose 4, Karen Green 1, Raymond Tellier 5, 6, Ryan Draker 1, Dena Adachi 1, Melissa Ayers 1, Adrienne K. Chan 1, Danuta M. Skowronski 7, Irving Salit 1, Andrew E. Simor 1, Arthur S. Slutsky 1, Patrick W. Doyle 8, Mel Krajden 7, Martin Petric 7, Robert C. Brunham 8, Allison J. McGeer 1': 958,\n",
       " 'Kenneth W. Tsang': 959,\n",
       " 'Pak L. Ho': 960,\n",
       " 'Gaik C. Ooi': 961,\n",
       " 'Wilson K. Yee': 962,\n",
       " 'Teresa Wang': 963,\n",
       " 'Moira Chan-Yeung': 964,\n",
       " 'Wah K. Lam': 965,\n",
       " 'Wing H. Seto': 966,\n",
       " 'Loretta Y. Yam': 967,\n",
       " 'Thomas M. Cheung': 968,\n",
       " 'Poon C. Wong': 969,\n",
       " 'Bing Lam': 970,\n",
       " 'Mary S. Ip': 971,\n",
       " 'Jane Chan': 972,\n",
       " 'Kwok Y. Yuen': 973,\n",
       " 'Kar N. Lai': 974,\n",
       " 'Christian Drosten': 975,\n",
       " 'Stephan Göttig': 976,\n",
       " 'Stefan Schilling': 977,\n",
       " 'Marcel Asper': 978,\n",
       " 'Marcus Panning': 979,\n",
       " 'Herbert Schmitz': 980,\n",
       " 'Stephan Günther': 981,\n",
       " 'Mika J. Mäkelä 1, Tuomo Puhakka 1, Olli Ruuskanen 1, Maija Leinonen 2, Pekka Saikku 2, Marko Kimpimäki 3, Soile Blomqvist 3, Timo Hyypiä 4, Pertti Arstila 4': 982,\n",
       " 'Charles B. Stephensen': 983,\n",
       " 'Donald B. Casebolt': 984,\n",
       " 'Nupur N. Gangopadhyay': 985,\n",
       " 'S Günther': 986,\n",
       " 'P Emmerich': 987,\n",
       " 'T Laue': 988,\n",
       " 'O Kühle': 989,\n",
       " 'M Asper': 990,\n",
       " 'A Jung': 991,\n",
       " 'T Grewing': 992,\n",
       " 'J ter Meulen': 993,\n",
       " 'H Schmitz': 994,\n",
       " 'Kyoung Oh Cho 1, Armando E. Hoet 2, Steven C. Loerch 3, Thomas E. Wittum 2, Linda J. Saif 2': 995,\n",
       " 'Michael C Kew': 996,\n",
       " 'Chris Kassianides': 997,\n",
       " 'Ravindra L Mehta 1, John A Kellum 2, Sudhir V Shah 3, Bruce A Molitoris 4, Claudio Ronco 5, David G Warnock 6, Adeera Levin 7': 998,\n",
       " 'Glenn M. Chertow': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_to_index = {}\n",
    "for paper in data:\n",
    "    for author in paper[\"authors\"]:\n",
    "        if author not in authors_to_index:\n",
    "            authors_to_index[author] = len(authors_to_index)\n",
    "authors_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bacterial-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_ref_matrix = np.zeros((len(authors_to_index), len(authors_to_index)))\n",
    "for i in range(len(data)):\n",
    "    authors = data[i][\"authors\"]\n",
    "    refs = data[i][\"references\"]\n",
    "    for j in range(len(data)):\n",
    "        if i != j:\n",
    "            if data[j][\"id\"] in refs:\n",
    "                target_authors = data[j][\"authors\"]\n",
    "                for a in authors:\n",
    "                    for b in target_authors:\n",
    "                        authors_ref_matrix[authors_to_index[a], authors_to_index[b]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "local-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.ones(len(authors_ref_matrix))\n",
    "a = np.ones(len(authors_ref_matrix))\n",
    "for rep in range(5):\n",
    "    for i in range(len(authors_ref_matrix)):\n",
    "        h[i] += a.reshape((1, len(a))).dot(authors_ref_matrix[i].reshape((len(authors_ref_matrix), 1)))\n",
    "        \n",
    "    for i in range(len(authors_ref_matrix)):\n",
    "        a[i] += h.reshape((1, len(h))).dot(authors_ref_matrix[:, i])\n",
    "    a = a / sum(a)\n",
    "    h = h / sum(h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "creative-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Suxiang Tong', 0.008042227831284632),\n",
       " ('Michelle L Holshue 1, Chas DeBolt 2, Scott Lindquist',\n",
       "  0.007436545497314237),\n",
       " ('Kathy H Lofy', 0.007436545497314237),\n",
       " ('John Wiesman', 0.007436545497314237),\n",
       " ('Hollianne Bruce', 0.007436545497314237),\n",
       " ('Christopher Spitters', 0.007436545497314237),\n",
       " ('Keith Ericson', 0.007436545497314237),\n",
       " ('Sara Wilkerson', 0.007436545497314237),\n",
       " ('Ahmet Tural', 0.007436545497314237),\n",
       " ('George Diaz', 0.007436545497314237),\n",
       " ('Amanda Cohn', 0.007436545497314237),\n",
       " ('LeAnne Fox', 0.007436545497314237),\n",
       " ('Anita Patel', 0.007436545497314237),\n",
       " ('Susan I Gerber', 0.007436545497314237),\n",
       " ('Lindsay Kim', 0.007436545497314237),\n",
       " ('Xiaoyan Lu', 0.007436545497314237),\n",
       " ('Steve Lindstrom', 0.007436545497314237),\n",
       " ('Mark A Pallansch', 0.007436545497314237),\n",
       " ('William C Weldon', 0.007436545497314237),\n",
       " ('Holly M Biggs', 0.007436545497314237),\n",
       " ('Timothy M Uyeki', 0.007436545497314237),\n",
       " ('Satish K Pillai', 0.007436545497314237),\n",
       " ('Chaolin Huang 1, Yeming Wang 2, Xingwang Li 3, Lili Ren 4, Jianping Zhao 5, Yi Hu 5, Li Zhang 1, Guohui Fan 2, Jiuyang Xu 6, Xiaoying Gu 2, Zhenshun Cheng 7, Ting Yu 1, Jiaan Xia 1, Yuan Wei 1, Wenjuan Wu 1, Xuelei Xie 1, Wen Yin 5, Hui Li 2, Min Liu 2, Yan Xiao 4, Hong Gao 4, Li Guo 4, Jungang Xie 5, Guangfa Wang 8, Rongmeng Jiang 3, Zhancheng Gao 8, Qi Jin 4, Jianwei Wang 4, Bin Cao 2',\n",
       "  0.007068051690965784),\n",
       " ('Ilya Sutskever', 0.006843881068773035),\n",
       " ('Li Fei-Fei', 0.006544393405108983),\n",
       " ('Qun Li 1, Xuhua Guan 1, Peng Wu 2, Xiaoye Wang 1, Lei Zhou 1, Yeqing Tong 1, Ruiqi Ren 1, Kathy S.M. Leung 2, Eric H.Y. Lau 2, Jessica Y. Wong 2, Xuesen Xing 1, Nijuan Xiang 1, Yang Wu 1, Chao Li 1, Qi Chen 1, Dan Li 1, Tian Liu 1, Jing Zhao 1, Man Liu 1, Wenxiao Tu 1, Chuding Chen 1, Lianmei Jin 1, Rui Yang 1, Qi Wang 1, Suhua Zhou 1, Rui Wang 1, Hui Liu 1, Yingbo Luo 1, Yuan Liu 1, Ge Shao 1, Huan Li 1, Zhongfa Tao 1, Yang Yang 3, Zhiqiang Deng 3, Boxi Liu 3, Zhitao Ma 3, Yanping Zhang 1, Guoqing Shi 1, Tommy T.Y. Lam 2, Joseph T. Wu 2, George F. Gao 1, Benjamin J. Cowling 2, Bo Yang 3, Gabriel M. Leung 2, Zijian Feng 1',\n",
       "  0.006051525070586503),\n",
       " ('Dawei Wang', 0.0060102030197505246),\n",
       " ('Bo Hu', 0.0060102030197505246),\n",
       " ('Chang Hu', 0.0060102030197505246),\n",
       " ('Fangfang Zhu', 0.0060102030197505246),\n",
       " ('Xing Liu', 0.0060102030197505246),\n",
       " ('Jing Zhang', 0.0060102030197505246),\n",
       " ('Binbin Wang', 0.0060102030197505246),\n",
       " ('Hui Xiang', 0.0060102030197505246),\n",
       " ('Zhenshun Cheng', 0.0060102030197505246),\n",
       " ('Yong Xiong', 0.0060102030197505246),\n",
       " ('Yan Zhao', 0.0060102030197505246),\n",
       " ('Yirong Li', 0.0060102030197505246),\n",
       " ('Xinghuan Wang', 0.0060102030197505246),\n",
       " ('Zhiyong Peng', 0.0060102030197505246),\n",
       " ('Richard Socher', 0.0054975649243287844),\n",
       " ('Na Zhu 1, Dingyu Zhang 2, Wenling Wang 1, Xingwang Li 3, Bo Yang 1, Jingdong Song 1, Xiang Zhao 1, Baoying Huang 1, Weifeng Shi 4, Roujian Lu 1, Peihua Niu 1, Faxian Zhan 1, Xuejun Ma 1, 5, Dayan Wang 1, Wenbo Xu 6, Guizhen Wu 1, George F. Gao 7, Wenjie Tan 1',\n",
       "  0.005413997215747181),\n",
       " ('Jasper Fuk Woo Chan 1, Shuofeng Yuan 1, Kin Hang Kok 1, Kelvin Kai Wang To 1, 2, Hin Chu 1, Jin Yang 2, Fanfan Xing 2, Jieling Liu 2, Cyril Chik Yan Yip 1, Rosana Wing Shan Poon 1, Hoi Wah Tsoi 1, Simon Kam Fai Lo 2, Kwok Hung Chan 1, Vincent Kwok Man Poon 1, Wan Mui Chan 1, Jonathan Daniel Ip 1, Jian Piao Cai 1, Vincent Chi Chung Cheng 1, Honglin Chen 1, 2, Christopher Kim Ming Hui 2, Kwok Yung Yuen 2',\n",
       "  0.005277605160761454),\n",
       " ('Alex Krizhevsky 1, Ilya Sutskever 1, Geoffrey E. Hinton 2',\n",
       "  0.005232873198965312),\n",
       " ('Yoshua Bengio', 0.005226283016388376),\n",
       " (\"Nanshan Chen 1, Min Zhou 2, Xuan Dong 1, Jieming Qu 2, Fengyun Gong 1, Yang Han 1, Yang Qiu 3, Jingli Wang 1, Ying Liu 1, Yuan Wei 1, Jia'an Xia 1, Ting Yu 1, Xinxin Zhang 2, Li Zhang 1\",\n",
       "  0.005198576437739951),\n",
       " ('Alex Krizhevsky', 0.004983455178909473),\n",
       " ('Jia Deng', 0.004907322558210296),\n",
       " ('Wei Dong', 0.004907322558210296),\n",
       " ('Li-Jia Li', 0.004907322558210296),\n",
       " ('Kai Li', 0.004907322558210296),\n",
       " ('Oriol Vinyals', 0.0046975510933168815),\n",
       " ('Christian Szegedy', 0.0046102649068035595),\n",
       " ('Geoffrey Hinton', 0.004335210317637859),\n",
       " ('Quoc V. Le', 0.004293363252222383),\n",
       " ('Geoffrey E. Hinton', 0.004259864615626998),\n",
       " ('Sergey Ioffe', 0.004120068724014886),\n",
       " ('Roujian Lu 1, Xiang Zhao 1, Juan Li 2, Peihua Niu 1, Bo Yang 3, Honglong Wu 4, Wenling Wang 1, Hao Song 5, Baoying Huang 1, Na Zhu 1, Yuhai Bi 5, Xuejun Ma 1, Faxian Zhan 3, Liang Wang 5, Tao Hu 2, Hong Zhou 2, Zhenhong Hu 6, Weimin Zhou 1, Li Zhao 1, Jing Chen 7, Yao Meng 1, Ji Wang 1, Yang Lin 4, Jianying Yuan 4, Zhihao Xie 4, Jinmin Ma 4, William J Liu 1, Dayan Wang 1, Wenbo Xu 1, Edward C Holmes 8, George F Gao 1, 5, Guizhen Wu 1, Weijun Chen 4, Weifeng Shi 2, Wenjie Tan 1, 5',\n",
       "  0.00405558842404215),\n",
       " ('Jeffrey Dean', 0.004039242057291057),\n",
       " ('Trevor Darrell', 0.004025976458024666),\n",
       " ('Kai Chen', 0.00399928240837124),\n",
       " ('Camilla Rothe 1, Mirjam Schunk 1, Peter Sothmann 1, Gisela Bretzel 1, Guenter Froeschl 1, Claudia Wallrauch 1, Thorbjörn Zimmer 1, Verena Thiel 1, Christian Janke 1, Wolfgang Guggemos 2, Michael Seilmaier 2, Christian Drosten 3, Patrick Vollmar 4, Katrin Zwirglmaier 4, Sabine Zange 4, Roman Wölfel 4, Michael Hoelscher 1',\n",
       "  0.003965963734773269),\n",
       " ('Christian Szegedy 1, Wei Liu 2, Yangqing Jia 1, Pierre Sermanet 1, Scott Reed 3, Dragomir Anguelov 1, Dumitru Erhan 1, Vincent Vanhoucke 1, Andrew Rabinovich 4',\n",
       "  0.003929679237215916),\n",
       " ('Andrew Y. Ng', 0.0038673212438931934),\n",
       " ('Yann LeCun', 0.003773578763002427),\n",
       " ('Joseph T Wu', 0.0036368411353731845),\n",
       " ('Gabriel M Leung', 0.0036368411353731845),\n",
       " ('Kathy Leung', 0.0036088380277422027),\n",
       " ('Ross Girshick', 0.0035581852782869917),\n",
       " ('Kaiming He', 0.003527065115402023),\n",
       " ('Yann Lecun 1, Leon Bottou 2, 3, Yoshua Bengio 3, 4, 5, Patrick Haffner 3, 6',\n",
       "  0.0034015618239191248),\n",
       " ('Xiangyu Zhang', 0.003370818441673553),\n",
       " ('Shaoqing Ren', 0.003370818441673553),\n",
       " ('Jian Sun', 0.003370818441673553),\n",
       " (\"Marc'aurelio Ranzato\", 0.003362421425347252),\n",
       " ('G. E. Hinton', 0.003361212096864803),\n",
       " ('Jeff Donahue', 0.0033603122822162663),\n",
       " ('Andrew Senior', 0.003350318657422813),\n",
       " ('Sepp Hochreiter 1, Jürgen Schmidhuber 2', 0.0033097629259598083),\n",
       " ('Greg Corrado', 0.003285151328119192),\n",
       " ('Tomas Mikolov 1, Kai Chen 2, Greg S. Corrado 2, Jeffrey Dean 2',\n",
       "  0.0031892256528329973),\n",
       " ('Rajat Monga', 0.0031106849283381727),\n",
       " ('Matthieu Devin', 0.0031106849283381727),\n",
       " ('Nitish Srivastava', 0.003057451154072488),\n",
       " ('R. R. Salakhutdinov', 0.0030135623391325307),\n",
       " ('Jitendra Malik', 0.00297388550700877),\n",
       " ('Mark Mao', 0.002970636562175862),\n",
       " ('Paul Tucker', 0.002970636562175862),\n",
       " ('Ke Yang', 0.002970636562175862),\n",
       " ('Yangqing Jia 1, Evan Shelhamer 2, Jeff Donahue 2, Sergey Karayev 2, Jonathan Long 2, Ross Girshick 2, Sergio Guadarrama 2, Trevor Darrell 2',\n",
       "  0.0029012829784694338),\n",
       " ('Pascal Vincent', 0.002888323529403705),\n",
       " ('Andrew Zisserman', 0.0026488103018697537),\n",
       " ('Rob Fergus', 0.002645072988015552),\n",
       " ('Karen Simonyan', 0.0026390216369270316),\n",
       " ('Olga Russakovsky 1, Jia Deng 2, Hao Su 1, Jonathan Krause 1, Sanjeev Satheesh 1, Sean Ma 1, Zhiheng Huang 1, Andrej Karpathy 1, Aditya Khosla 3, Michael Bernstein 1, Alexander C. Berg 4, Li Fei-Fei 1',\n",
       "  0.002614391790870884),\n",
       " ('Hugo Larochelle', 0.0022837537835436617),\n",
       " ('Ronan Collobert', 0.0021797645217703308),\n",
       " ('Thomas Leung', 0.0021352295199078162),\n",
       " ('Andrej Karpathy', 0.0020890197745606436),\n",
       " ('Ksiazek Tg 1, Erdman D 1, Goldsmith Cs 1, Zaki 1, Peret T 1, Emery S 1, Tong S 1, Urbani C 2, Comer Ja 1, Lim W 3, Rollin Pe 1, Dowell Sf 4, Ling Ae 5, Humphrey Cd 1, Shieh Wj 1, Guarner J 1, Paddock Cd 1, Rota P 1, Fields B 1, DeRisi J 6, Yang Jy 1, Cox N 1, Hughes Jm 1, LeDuc Jw 1, Bellini Wj 1, Anderson Lj 1',\n",
       "  0.0020839992753735187),\n",
       " ('G. Hinton 1, Li Deng 2, Dong Yu 2, G. E. Dahl 1, A. Mohamed 1, N. Jaitly 1, Andrew Senior 3, V. Vanhoucke 3, P. Nguyen 3, T. N. Sainath 4, B. Kingsbury 4',\n",
       "  0.002075800957190514),\n",
       " ('George Toderici', 0.002058975648484108),\n",
       " ('Sanketh Shetty', 0.002058975648484108),\n",
       " ('Rahul Sukthankar', 0.002058975648484108),\n",
       " ('Koray Kavukcuoglu', 0.0020557330611986786),\n",
       " ('N. Dalal', 0.0020006838037515805),\n",
       " ('B. Triggs', 0.0020006838037515805),\n",
       " ('Paul A. Rota 1, M. Steven Oberste 1, Stephan S. Monroe 1, W. Allan Nix 1, Ray Campagnoli 1, Joseph P. Icenogle 1, Silvia Peñaranda 1, Bettina Bankamp 1, Kaija Maher 1, Min hsin Chen 1, Suxiong Tong 1, Azaibi Tamin 1, Luis Lowe 1, Michael Frace 1, Joseph L. DeRisi 2, Qi Chen 1, David Wang 2, Dean D. Erdman 1, Teresa C.T. Peret 1, Cara Burns 1, Thomas G. Ksiazek 1, Pierre E. Rollin 1, Anthony Sanchez 1, Stephanie Liffick 1, Brian Holloway 1, Josef Limor 1, Karen McCaustland 1, Mellissa Olsen-Rasmussen 1, Ron Fouchier 3, Stephan Günther 4, Albert D.H.E. Osterhaus 3, Christian Drosten 4, Mark A. Pallansch 1, Larry J. Anderson 1, William J. Bellini 1',\n",
       "  0.00198323575214076),\n",
       " ('Diederik P. Kingma 1, Jimmy Lei Ba 2', 0.00198068871937201),\n",
       " ('Jonathan Long', 0.001964253693403057),\n",
       " ('Evan Shelhamer', 0.001964253693403057),\n",
       " ('Ruslan R. Salakhutdinov', 0.0019332977969591125),\n",
       " ('Y. Guan', 0.001932283913898376),\n",
       " ('B. J. Zheng', 0.001932283913898376),\n",
       " ('Y. Q. He', 0.001932283913898376),\n",
       " ('X. L. Liu', 0.001932283913898376),\n",
       " ('Z. X. Zhuang', 0.001932283913898376),\n",
       " ('C. L. Cheung', 0.001932283913898376),\n",
       " ('S. W. Luo', 0.001932283913898376),\n",
       " ('P. H. Li', 0.001932283913898376),\n",
       " ('L. J. Zhang', 0.001932283913898376),\n",
       " ('Y. J. Guan', 0.001932283913898376),\n",
       " ('K. M. Butt', 0.001932283913898376),\n",
       " ('K. L. Wong', 0.001932283913898376),\n",
       " ('K. W. Chan', 0.001932283913898376),\n",
       " ('W. Lim', 0.001932283913898376),\n",
       " ('K. F. Shortridge', 0.001932283913898376),\n",
       " ('K. Y. Yuen', 0.001932283913898376),\n",
       " ('J. S. M. Peiris', 0.001932283913898376),\n",
       " ('L. L. M. Poon', 0.001932283913898376),\n",
       " ('Marco A. Marra 1, Steven J. M. Jones 2, Caroline R. Astell 2, Robert A. Holt 2, Angela Brooks-Wilson 2, Yaron S. N. Butterfield 2, Jaswinder Khattra 2, Jennifer K. Asano 2, Sarah A. Barber 2, Susanna Y. Chan 2, Alison Cloutier 2, Shaun M. Coughlin 2, Doug Freeman 2, Noreen Girn 2, Obi L. Griffith',\n",
       "  0.0019311194132028212),\n",
       " ('Stephen R. Leach 2, Michael Mayo 2, Helen McDonald 2, Stephen B. Montgomery 2, Pawan K. Pandoh 2, Anca S. Petrescu 2, A. Gordon Robertson 2, Jacqueline E. Schein 2, Asim Siddiqui 2, Duane E. Smailus 2, Jeff M. Stott 2, George S. Yang 2, Francis Plummer 3, Anton Andonov 3, Harvey Artsob 3, Nathalie Bastien 3, Kathy Bernard 3, Timothy F. Booth 3, Donnie Bowness 3, Martin Czub 3, Michael Drebot 3, Lisa Fernando 3, Ramon Flick 3, Michael Garbutt 3, Michael Gray 3, Allen Grolla 3, Steven Jones 3, Heinz Feldmann 3, Adrienne Meyers 3, Amin Kabani 3, Yan Li 3, Susan Normand 3, Ute Stroher 3, Graham A. Tipples 3, Shaun Tyler 3 +9',\n",
       "  0.0019311194132028212),\n",
       " ('Laurens van der Maaten', 0.001928432585775571),\n",
       " ('Pierre Sermanet', 0.0019029698840466532),\n",
       " ('Geoffrey E. Hinton 1, Simon Osindero 1, Yee-Whye Teh 2',\n",
       "  0.0018837500922775808),\n",
       " ('Jason Weston', 0.0018752494417524793),\n",
       " ('Ruslan Salakhutdinov', 0.0018675096822551903),\n",
       " ('Xiang Zhang', 0.001855765745792916),\n",
       " ('Réjean Ducharme', 0.001826976940866649),\n",
       " ('Christian Janvin', 0.001826976940866649),\n",
       " ('Matei Zaharia', 0.0018232657088420971),\n",
       " ('Mosharaf Chowdhury', 0.0018232657088420971),\n",
       " ('Tathagata Das', 0.0018232657088420971),\n",
       " ('Ankur Dave', 0.0018232657088420971),\n",
       " ('Justin Ma', 0.0018232657088420971),\n",
       " ('Murphy McCauley', 0.0018232657088420971),\n",
       " ('Michael J. Franklin', 0.0018232657088420971),\n",
       " ('Scott Shenker', 0.0018232657088420971),\n",
       " ('Ion Stoica', 0.0018232657088420971),\n",
       " ('Tomas Mikolov', 0.001815828937976565),\n",
       " (\"Marc'Aurelio Ranzato\", 0.0017996669787682053),\n",
       " ('Matthew D. Zeiler', 0.0017452768349243578),\n",
       " ('Kevin Jarrett', 0.0017443300469934344),\n",
       " ('Greg S Corrado', 0.001739322698764537),\n",
       " ('Jeff Dean', 0.001739322698764537),\n",
       " ('Peng Zhou 1, Xing Lou Yang 1, Xian Guang Wang 2, Ben Hu 1, Lei Zhang 1, Wei Zhang 1, Hao Rui Si 1, Yan Zhu 1, Bei Li 1, Chao Lin Huang 2, Hui Dong Chen 2, Jing Chen 1, Yun Luo 1, Hua Guo 1, Ren Di Jiang 1, Mei Qin Liu 1, Ying Chen 1, Xu Rui Shen 1, Xi Wang 1, Xiao Shuang Zheng 1, Kai Zhao 1, Quan Jiao Chen 1, Fei Deng 1, Lin Lin Liu 3, Bing Yan 1, Fa Xian Zhan 3, Yan Yi Wang 1, Geng Fu Xiao 1, Zheng Li Shi 1',\n",
       "  0.0017321286326867622),\n",
       " ('David Eigen', 0.0017022728262679877),\n",
       " ('Michael Mathieu', 0.0017022728262679877),\n",
       " ('Dzmitry Bahdanau 1, Kyunghyun Cho 2, Yoshua Bengio 2',\n",
       "  0.001686209977850999),\n",
       " ('Shaoqing Ren 1, Kaiming He 2, Ross Girshick 3, Jian Sun 2',\n",
       "  0.001533247389441249),\n",
       " ('Jacob Devlin', 0.0015223724602359723),\n",
       " ('Ming-Wei Chang', 0.0015223724602359723),\n",
       " ('Kenton Lee', 0.0015223724602359723),\n",
       " ('Kristina N. Toutanova', 0.0015223724602359723),\n",
       " ('John Duchi 1, Elad Hazan 2, Yoram Singer 3', 0.0015178415329556738),\n",
       " ('Christian Drosten 1, Stephan Günther 1, Wolfgang Preiser 2, Sylvie van der Werf 3, Hans-Reinhard Brodt 4, Stephan Becker 5, Holger Rabenau 2, Marcus Panning 1, Larissa Kolesnikova 5, Ron A.M. Fouchier 6, Annemarie Berger 2, Ana-Maria Burguière 3, Jindrich Cinatl 2, Markus Eickmann 5, Nicolas Escriou 3, Klaus Grywna 1, Stefanie Kramme 1, Jean-Claude Manuguerra 3, Stefanie Müller 1, Volker Rickerts 4, Martin Stürmer 2, Simon Vieth 1, Hans-Dieter Klenk 5, Albert D.M.E. Osterhaus 6, Herbert Schmitz 1, Hans Wilhelm Doerr 2',\n",
       "  0.0015014213372603476),\n",
       " ('Kwok-Yung Yuen', 0.0014741896687132802),\n",
       " ('Ashish Vaswani 1, Noam Shazeer 1, Niki Parmar 2, Jakob Uszkoreit 1, Llion Jones 1, Aidan N. Gomez 1, Lukasz Kaiser 1, Illia Polosukhin 1',\n",
       "  0.0014675461478292151),\n",
       " ('Yangqing Jia', 0.001451230403936571),\n",
       " ('G. E. Dahl 1, Dong Yu 2, Li Deng 2, A. Acero 2', 0.0013791057442936662),\n",
       " ('Judy Hoffman', 0.0013788336754958763),\n",
       " ('Ning Zhang', 0.0013788336754958763),\n",
       " ('Eric Tzeng', 0.0013788336754958763),\n",
       " ('Pascal Lamblin', 0.001355300996833089),\n",
       " ('Hong Shan', 0.0013523071175651417),\n",
       " ('Christopher D. Manning', 0.0013171056049409641),\n",
       " ('Wenhui Li 1, Michael J. Moore 1, Natalya Vasilieva 2, Jianhua Sui 3, Swee Kee Wong 1, Michael A. Berne 4, Mohan Somasundaran 5, John L. Sullivan 5, Katherine Luzuriaga 5, Thomas C. Greenough 5, Hyeryun Choe 2, Michael Farzan 1',\n",
       "  0.001292197876501666),\n",
       " ('P F Felzenszwalb 1, R B Girshick 1, D McAllester 2, D Ramanan 3',\n",
       "  0.0012770008093610328),\n",
       " ('Pierre-Antoine Manzagol', 0.0012734830955911072),\n",
       " ('Kyunghyun Cho 1, Bart van Merrienboer 2, Caglar Gulcehre 2, Dzmitry Bahdanau 3, Fethi Bougares 3, Holger Schwenk 3, Yoshua Bengio 4, 5, 6',\n",
       "  0.001271323699979972),\n",
       " ('Alex Graves', 0.0012694197276648424),\n",
       " ('Wei-jie Guan', 0.0012537805603677395),\n",
       " ('Zheng-yi Ni', 0.0012537805603677395),\n",
       " ('Yu Hu', 0.0012537805603677395),\n",
       " ('Wenhua Liang', 0.0012537805603677395),\n",
       " ('Chun-quan Ou', 0.0012537805603677395),\n",
       " ('Jianxing He', 0.0012537805603677395),\n",
       " ('Lei Liu', 0.0012537805603677395),\n",
       " ('Chunliang Lei', 0.0012537805603677395),\n",
       " ('David S.C. Hui', 0.0012537805603677395),\n",
       " ('Bin Du', 0.0012537805603677395),\n",
       " ('Lan-juan Li', 0.0012537805603677395),\n",
       " ('Guang Zeng', 0.0012537805603677395),\n",
       " ('Ruchong Chen', 0.0012537805603677395),\n",
       " ('Chun-Li Tang', 0.0012537805603677395),\n",
       " ('Tao Wang', 0.0012537805603677395),\n",
       " ('Ping-yan Chen', 0.0012537805603677395),\n",
       " ('Jie Xiang', 0.0012537805603677395),\n",
       " ('Shiyue Li', 0.0012537805603677395),\n",
       " ('Jinlin Wang', 0.0012537805603677395),\n",
       " ('Zi Jing Liang', 0.0012537805603677395),\n",
       " ('Yi-xiang Peng', 0.0012537805603677395),\n",
       " ('Li Wei', 0.0012537805603677395),\n",
       " ('Yong Liu', 0.0012537805603677395),\n",
       " ('Ya-hua Hu', 0.0012537805603677395),\n",
       " ('Peng Peng', 0.0012537805603677395),\n",
       " ('Jian-ming Wang', 0.0012537805603677395),\n",
       " ('Ji-yang Liu', 0.0012537805603677395),\n",
       " ('Zhong Chen', 0.0012537805603677395),\n",
       " ('Gang Li', 0.0012537805603677395),\n",
       " ('Zhi-jian Zheng', 0.0012537805603677395),\n",
       " ('Shao-qin Qiu', 0.0012537805603677395),\n",
       " ('Jie Luo', 0.0012537805603677395),\n",
       " ('Chang-jiang Ye', 0.0012537805603677395),\n",
       " ('Shao-yong Zhu', 0.0012537805603677395),\n",
       " ('Nanshan Zhong', 0.0012537805603677395),\n",
       " ('Mohammad Norouzi', 0.0012368788159771956),\n",
       " ('Dumitru Erhan', 0.001221519993103297),\n",
       " ('Dan Popovici', 0.0011748898163346647),\n",
       " ('Victor M. Corman 1, Olfert Landt 2, Marco Kaiser 3, Richard Molenkamp 4, Adam Meijer 5, Daniel K.W. Chu 6, Tobias Bleicker 1, Sebastian Brünink 1, Julia Schneider 1, Marie Luisa Schmidt 1, Daphne G.J.C. Mulders 4, Bart L. Haagmans 4, Bas Van Der Veer 5, Sharon Van Den Brink 5, Lisa Wijsman 5, Gabriel Goderski 5, Jean Louis Romette 7, Joanna Ellis 8, Maria Zambon 8, Malik Peiris 6, Herman Goossens 9, Chantal Reusken 5, Marion P.G. Koopmans 4, Christian Drosten 1',\n",
       "  0.001158916724572792),\n",
       " ('Jsm Peiris 1, ST Lai 2, Llm Poon 1, Y Guan 1, Lyc Yam 3, W Lim 4, J Nicholls 1, Wks Yee 5, WW Yan 2, MT Cheung 3, Vcc Cheng 1, KH Chan 1, Dnc Tsang 6, Rwh Yung 3, TK Ng 2, KY Yuen 1',\n",
       "  0.0011490831883607558),\n",
       " ('Ian Goodfellow 1, Jean Pouget-Abadie 1, Mehdi Mirza 1, Bing Xu 1, David Warde-Farley 1, Sherjil Ozair 2, Aaron Courville 1, Yoshua Bengio 1',\n",
       "  0.0011343045138123462),\n",
       " ('David E. Rumelhart 1, Geoffrey E. Hinton 2, Ronald J. Williams 1',\n",
       "  0.0011283525146362206),\n",
       " ('Nal Kalchbrenner', 0.001124277968571842),\n",
       " ('Phil Blunsom', 0.001124277968571842),\n",
       " ('Alexander Toshev', 0.0011002840921648651),\n",
       " ('Mark Everingham 1, Luc Gool 2, Christopher K. Williams 3, John Winn 4, Andrew Zisserman 5',\n",
       "  0.0010720095117185639),\n",
       " ('Mike Schuster', 0.0010098212726202354),\n",
       " ('Tsung-Yi Lin 1, Michael Maire 2, Serge J. Belongie 1, James Hays 3, Pietro Perona 2, Deva Ramanan 4, Piotr Dollár 5, C. Lawrence Zitnick 5',\n",
       "  0.001003978794519756),\n",
       " ('Sébastien Jean', 0.0009693794297206675),\n",
       " ('Kyunghyun Cho', 0.0009693794297206675),\n",
       " ('Roland Memisevic', 0.0009693794297206675),\n",
       " ('Vinod Nair', 0.0009403072074864001),\n",
       " ('Ueli Meier', 0.0009381085763593071),\n",
       " ('Yuval Netzer 1, Tao Wang 1, Adam Coates 1, Alessandro Bissacco 2, Bo Wu 2, Andrew Y. Ng 2',\n",
       "  0.0009075728652363875),\n",
       " ('Eric J. Snijder 1, Peter J. Bredenbeek 1, Jessika C. Dobbe 1, Volker Thiel 2, John Ziebuhr 2, Leo L.M. Poon 3, Yi Guan 3, Mikhail Rozanov 4, Willy J.M. Spaan 1, Alexander E. Gorbalenya 1',\n",
       "  0.0009010156162509623),\n",
       " ('Sanjay Ghemawat', 0.0008990645906841443),\n",
       " ('David G. Lowe', 0.0008778121944610746),\n",
       " ('Zhifeng Chen', 0.0008731482171035314),\n",
       " ('Max Welling', 0.0008703054237816176),\n",
       " ('Jeffrey Pennington 1, Richard Socher 2, Christopher Manning 1',\n",
       "  0.0008686191850752513),\n",
       " ('Adam Coates 1, Andrew Y. Ng 2, Honglak Lee 1', 0.000857667851211994),\n",
       " ('Vincent Vanhoucke', 0.000842619146525242),\n",
       " ('Juergen Schmidhuber', 0.0008357707809699218),\n",
       " ('Abdel-rahman Mohamed', 0.0008311682960284618),\n",
       " ('Dan Cireşan', 0.0008307796923289549),\n",
       " ('Michael Chung 1, Adam Bernheim 1, Xueyan Mei 1, Ning Zhang 2, Mingqian Huang 1, Xianjun Zeng 2, Jiufa Cui 3, Wenjian Xu 3, Yang Yang 1, Zahi A. Fayad 1, Adam Jacobi 1, Kunwei Li 4, Shaolin Li 4, Hong Shan 4',\n",
       "  0.0008212154366972967),\n",
       " ('Fei Zhou 1, Ting Yu 2, Ronghui Du 3, Guohui Fan 4, Ying Liu 2, Zhibo Liu 1, Jie Xiang 5, Yeming Wang 6, Bin Song 2, Xiaoying Gu 1, 4, Lulu Guan 3, Yuan Wei 2, Hui Li 1, Xudong Wu 7, Jiuyang Xu 8, Shengjin Tu 2, Yi Zhang 1, Hua Chen 2, Bin Cao',\n",
       "  0.0008059472987956718),\n",
       " ('S. Lazebnik 1, C. Schmid 2, J. Ponce 3', 0.0008001765910200853),\n",
       " ('Jürgen Schmidhuber', 0.0007761853412052988),\n",
       " ('Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 1, Jian Sun 1',\n",
       "  0.0007728991668305961),\n",
       " ('Andriy Mnih', 0.0007587318657544323),\n",
       " ('Cliff C. Lin', 0.0007568486262726998),\n",
       " ('Chris Manning', 0.0007568486262726998),\n",
       " ('Rico Sennrich', 0.0007526838332009807),\n",
       " ('Barry Haddow', 0.0007526838332009807),\n",
       " ('Alexandra Birch', 0.0007526838332009807),\n",
       " ('Xavier Glorot', 0.0007312675333328879),\n",
       " ('Wenling Wang 1, Yanli Xu 2, Ruqin Gao 3, Roujian Lu 1, Kai Han 2, Guizhen Wu 1, Wenjie Tan 1',\n",
       "  0.0007105788645541918),\n",
       " ('George Dahl', 0.0007093955025561263),\n",
       " ('David E. Rumelhart 1, James L. McClelland 2', 0.0007049036687990397),\n",
       " ('Yicheng Fang', 0.0007040221044971587),\n",
       " ('Huangqi Zhang', 0.0007040221044971587),\n",
       " ('Jicheng Xie', 0.0007040221044971587),\n",
       " ('Peipei Pang', 0.0007040221044971587),\n",
       " ('Wenbin Ji', 0.0007040221044971587),\n",
       " ('Y. LeCun 1, Fu Jie Huang 1, L. Bottou 2', 0.0007021758242877348),\n",
       " ('Yonghui Wu', 0.000693063238241639),\n",
       " ('Maxim Krikun', 0.000693063238241639),\n",
       " ('Melvin Johnson', 0.000693063238241639),\n",
       " ('Macduff Hughes', 0.000693063238241639),\n",
       " ('Taku Kudo', 0.0006855615219922626),\n",
       " ('Wolfgang Macherey', 0.0006837169934209808),\n",
       " ('Yuan Cao', 0.0006837169934209808),\n",
       " ('Qin Gao', 0.0006837169934209808),\n",
       " ('Klaus Macherey', 0.0006837169934209808),\n",
       " ('Jeff Klingner', 0.0006837169934209808),\n",
       " ('Apurva Shah', 0.0006837169934209808),\n",
       " ('Xiaobing Liu', 0.0006837169934209808),\n",
       " ('Łukasz Kaiser', 0.0006837169934209808),\n",
       " ('Stephan Gouws', 0.0006837169934209808),\n",
       " ('Yoshikiyo Kato', 0.0006837169934209808),\n",
       " ('Hideto Kazawa', 0.0006837169934209808),\n",
       " ('Keith Stevens', 0.0006837169934209808),\n",
       " ('George Kurian', 0.0006837169934209808),\n",
       " ('Nishant Patil', 0.0006837169934209808),\n",
       " ('Wei Wang', 0.0006837169934209808),\n",
       " ('Cliff Young', 0.0006837169934209808),\n",
       " ('Jason Smith', 0.0006837169934209808),\n",
       " ('Jason Riesa', 0.0006837169934209808),\n",
       " ('Alex Rudnick', 0.0006837169934209808),\n",
       " ('Lan T. Phan 1, Thuong V. Nguyen 1, Quang C. Luong 1, Thinh V. Nguyen 1, Hieu T. Nguyen 1, Hung Q. Le 2, Thuc T. Nguyen 2, Thang M. Cao 3, Quang D. Pham 3',\n",
       "  0.0006783134830301905),\n",
       " ('Li Deng', 0.0006718804124840081),\n",
       " ('Dong Yu', 0.0006718804124840081),\n",
       " ('Navdeep Jaitly', 0.0006625341676633497),\n",
       " ('Patrick Nguyen', 0.0006625341676633497),\n",
       " ('Tara Sainath', 0.0006625341676633497),\n",
       " ('Brian Kingsbury', 0.0006625341676633497),\n",
       " ('Isabelle Lajoie', 0.0006602942099735617),\n",
       " ('Christiane Fellbaum', 0.0006602676089838627),\n",
       " ('Zunyou Wu', 0.0006552938371196562),\n",
       " ('Jennifer M. McGoogan', 0.0006552938371196562),\n",
       " ('Kaiming He 1, Xiangyu Zhang 2, Shaoqing Ren 3, Jian Sun 1',\n",
       "  0.0006520684266857847),\n",
       " ('Robert Tibshirani', 0.0006496600887325468),\n",
       " ('Aaron Courville', 0.0006479533751380915),\n",
       " ('Gaik C. Ooi', 0.0006381602705590511),\n",
       " ('Bing Lam', 0.0006381602705590511),\n",
       " ('Y. LeCun', 0.000630554703507932),\n",
       " ('Honglak Lee', 0.0006304655826572865),\n",
       " ('A. Mohamed', 0.0006271235553120167),\n",
       " ('G. E. Dahl', 0.0006271235553120167),\n",
       " ('G. Hinton', 0.0006271235553120167),\n",
       " ('Tomas Mikolov 1, Martin Karafiát 1, Lukás Burget 1, Jan Cernocký',\n",
       "  0.0006216394906245387),\n",
       " ('Sanjeev Khudanpur 2', 0.0006216394906245387),\n",
       " ('Jason Weston 1, Léon Bottou', 0.0006091554280386059),\n",
       " ('Michael Karlen', 0.0006091554280386059),\n",
       " ('Koray Kavukcuoglu 2, Pavel Kuksa 3', 0.0006091554280386059),\n",
       " ('Samy Bengio', 0.0006086223463175974),\n",
       " ('Rajesh Ranganath', 0.0006081102068443786),\n",
       " ('Joseph Turian 1, Lev-Arie Ratinov 2, Yoshua Bengio 1',\n",
       "  0.0006077035064760642),\n",
       " ('Huai-Dong Song', 0.0006025912186824403),\n",
       " ('Chang-Chun Tu', 0.0006025912186824403),\n",
       " ('Guo-Wei Zhang', 0.0006025912186824403),\n",
       " ('Sheng-Yue Wang', 0.0006025912186824403),\n",
       " ('Kui Zheng', 0.0006025912186824403),\n",
       " ('Lian-Cheng Lei', 0.0006025912186824403),\n",
       " ('Qiu-Xia Chen', 0.0006025912186824403),\n",
       " ('Yu-Wei Gao', 0.0006025912186824403),\n",
       " ('Hui-Qiong Zhou', 0.0006025912186824403),\n",
       " ('Hua Xiang', 0.0006025912186824403),\n",
       " ('Hua-Jun Zheng', 0.0006025912186824403),\n",
       " ('Shur-Wern Wang Chern', 0.0006025912186824403),\n",
       " ('Feng Cheng', 0.0006025912186824403),\n",
       " ('Chun-Ming Pan', 0.0006025912186824403),\n",
       " ('Hua Xuan', 0.0006025912186824403),\n",
       " ('Sai-Juan Chen', 0.0006025912186824403),\n",
       " ('Hui-Ming Luo', 0.0006025912186824403),\n",
       " ('Duan-Hua Zhou', 0.0006025912186824403),\n",
       " ('Yu-Fei Liu', 0.0006025912186824403),\n",
       " ('Jian-Feng He', 0.0006025912186824403),\n",
       " ('Peng-Zhe Qin', 0.0006025912186824403),\n",
       " ('Ling-Hui Li', 0.0006025912186824403),\n",
       " ('Yu-Qi Ren', 0.0006025912186824403),\n",
       " ('Wen-Jia Liang', 0.0006025912186824403),\n",
       " ('Ye-Dong Yu', 0.0006025912186824403),\n",
       " ('Larry Anderson', 0.0006025912186824403),\n",
       " ('Ming Wang', 0.0006025912186824403),\n",
       " ('Rui-Heng Xu', 0.0006025912186824403),\n",
       " ('Xin-Wei Wu', 0.0006025912186824403),\n",
       " ('Huan-Ying Zheng', 0.0006025912186824403),\n",
       " ('Jin-Ding Chen', 0.0006025912186824403),\n",
       " ('Guodong Liang', 0.0006025912186824403),\n",
       " ('Yang Gao', 0.0006025912186824403),\n",
       " ('Ming Liao', 0.0006025912186824403),\n",
       " ('Ling Fang', 0.0006025912186824403),\n",
       " ('Li-Yun Jiang', 0.0006025912186824403),\n",
       " ('Hui Li', 0.0006025912186824403),\n",
       " ('Fang Chen', 0.0006025912186824403),\n",
       " ('Biao Di', 0.0006025912186824403),\n",
       " ('Li-Juan He', 0.0006025912186824403),\n",
       " ('Jin-Yan Lin', 0.0006025912186824403),\n",
       " ('Xiangang Kong', 0.0006025912186824403),\n",
       " ('Lin Du', 0.0006025912186824403),\n",
       " ('Pei Hao', 0.0006025912186824403),\n",
       " ('Hua Tang', 0.0006025912186824403),\n",
       " ('Andrea Bernini', 0.0006025912186824403),\n",
       " ('Xiao-Jing Yu', 0.0006025912186824403),\n",
       " ('Ottavia Spiga', 0.0006025912186824403),\n",
       " ('Zong-Ming Guo +9', 0.0006025912186824403),\n",
       " ('Minh-Thang Luong', 0.0005978115920162257),\n",
       " ('Hieu Pham', 0.0005920169635200182),\n",
       " ('Yann LeCun 1, 2, Yoshua Bengio 3, Geoffrey Hinton 4, 5',\n",
       "  0.0005906190259574759),\n",
       " ('Min Lin', 0.0005786244951537914),\n",
       " ('Qiang Chen', 0.0005786244951537914),\n",
       " ('Shuicheng Yan', 0.0005786244951537914),\n",
       " ('Diederik P Kingma', 0.0005629051887823594),\n",
       " ('Christian Szegedy 1, Vincent Vanhoucke 1, Sergey Ioffe 1, Jon Shlens 1, Zbigniew Wojna 2',\n",
       "  0.000561034788580679),\n",
       " ('Richard Socher 1, Alex Perelygin', 0.0005601737638866604),\n",
       " ('Jean Wu 1, Jason Chuang 2, Christopher D. Manning 1, Andrew Ng 1, Christopher Potts 1',\n",
       "  0.0005601737638866604),\n",
       " ('K. B. Chua 1, W. J. Bellini 2, P. A. Rota 2, B. H. Harcourt 2, A. Tamin 2, S. K. Lam 1, T. G. Ksiazek 2, P. E. Rollin 2, S. R. Zaki 2, W.-J. Shieh 2, C. S. Goldsmith 2, D. J. Gubler 3, J. T. Roehrig 3, B. Eaton 4, A. R. Gould 4, J. Olson 2, P. Daniels 4, A. E. Ling 5, C. J. Peters 2, L. J. Anderson 2, B. W. J. Mahy 2',\n",
       "  0.0005593817043100502),\n",
       " ('K Murray', 0.0005593817043100502),\n",
       " ('P Selleck', 0.0005593817043100502),\n",
       " ('P Hooper', 0.0005593817043100502),\n",
       " ('A Hyatt', 0.0005593817043100502),\n",
       " ('A Gould', 0.0005593817043100502),\n",
       " ('L Gleeson', 0.0005593817043100502),\n",
       " ('H Westbury', 0.0005593817043100502),\n",
       " ('L Hiley', 0.0005593817043100502),\n",
       " ('L Selvey', 0.0005593817043100502),\n",
       " ('B Rodwell', 0.0005593817043100502),\n",
       " ('Kaisuke Nakajima', 0.0005570168057563149),\n",
       " ('Daxiang Dong', 0.0005570168057563149),\n",
       " ('Hua Wu', 0.0005570168057563149),\n",
       " ('Wei He', 0.0005570168057563149),\n",
       " ('Dianhai Yu', 0.0005570168057563149),\n",
       " ('Haifeng Wang', 0.0005570168057563149),\n",
       " ('Lonnie Chrisman', 0.0005570168057563149),\n",
       " ('Ian J. Goodfellow', 0.0005551312177274137),\n",
       " ('Ting Chen', 0.0005531618225562146),\n",
       " ('Simon Kornblith', 0.0005531618225562146),\n",
       " ('Matthew E. Peters 1, Mark Neumann 1, Mohit Iyyer 2, Matt Gardner 1, Christopher Clark 1, Kenton Lee 3, Luke Zettlemoyer 4',\n",
       "  0.0005513229795394965),\n",
       " ('Roger Grosse', 0.0005510983678698027),\n",
       " ('Gao Huang 1, Zhuang Liu 2, Laurens van der Maaten 3, Kilian Q. Weinberger 1',\n",
       "  0.0005479818104100839),\n",
       " ('Josephine Sullivan', 0.000536045643007453),\n",
       " ('Herbert Bay 1, Tinne Tuytelaars 2, Luc Van Gool 1', 0.0005283869836825069),\n",
       " ('David Warde-Farley', 0.0005261546158461474),\n",
       " ('James Bergstra', 0.0005201505483804108),\n",
       " ('Ali Sharif Razavian', 0.0005171317863970218),\n",
       " ('Hossein Azizpour', 0.0005171317863970218),\n",
       " ('Stefan Carlsson', 0.0005171317863970218),\n",
       " ('Peter D. Turney 1, Patrick Pantel 2', 0.000504735454312068),\n",
       " ('Christian Szegedy 1, Wojciech Zaremba 2, Ilya Sutskever 1, Joan Bruna 2, Dumitru Erhan 1, Ian Goodfellow 3, Rob Fergus 2, 4',\n",
       "  0.0005038822252522528),\n",
       " ('Tao Ai 1, Zhenlu Yang 2, Hongyan Hou 3, Chenao Zhan 1, Chong Chen 1, Wenzhi Lv 1, Qian Tao 1, Ziyong Sun 1, Liming Xia 1',\n",
       "  0.0004961794580972688),\n",
       " ('D. E. Rumelhart', 0.0004943863465376559),\n",
       " ('R. J. Williams', 0.0004943863465376559),\n",
       " ('Christopher M. Bishop', 0.0004935398481524891),\n",
       " ('Ilya Sutskever 1, James Martens 2, George Dahl 2, Geoffrey Hinton 2',\n",
       "  0.0004926837482363402),\n",
       " ('Robert L. Mercer', 0.0004922265867783563),\n",
       " ('Jonathon Shlens', 0.0004919290643956061),\n",
       " ('R. E. Howard', 0.0004883726802388918),\n",
       " ('W. Hubbard', 0.0004883726802388918),\n",
       " ('L. D. Jackel', 0.0004883726802388918),\n",
       " ('C. Farabet 1, C. Couprie 1, L. Najman 2, Y. LeCun 1',\n",
       "  0.0004882424506679562),\n",
       " ('Y. Bengio', 0.00048710381660439176),\n",
       " ('Mitchell P. Marcus 1, Mary Ann Marcinkiewicz 1, Beatrice Santorini 2',\n",
       "  0.00048305928455524215),\n",
       " ('Thomas Brox', 0.0004752890351004929),\n",
       " ('Jeffrey P Kanne', 0.0004722992461298052),\n",
       " ('B. Boser', 0.00046931422069748733),\n",
       " ('J. S. Denker', 0.00046931422069748733),\n",
       " ('D. Henderson', 0.00046931422069748733),\n",
       " ('Sivic', 0.00045682957407742363),\n",
       " ('Zisserman', 0.00045682957407742363),\n",
       " ('Sumit Chopra', 0.0004547584693214781),\n",
       " ('Tim Salimans 1, Ian Goodfellow 2, Wojciech Zaremba 3, Vicki Cheung',\n",
       "  0.0004536643999246751),\n",
       " ('Alec Radford 1, Xi Chen 4', 0.0004536643999246751),\n",
       " ('Peter F. Brown', 0.0004523627287031109),\n",
       " ('Vincent J. Della Pietra', 0.0004523627287031109),\n",
       " ('Scott Deerwester 1, Susan T. Dumais 2, George W. Furnas 2, Thomas K. Landauer 2, Richard Harshman 3',\n",
       "  0.00044655437258206697),\n",
       " ('Yunyu Xu', 0.0004425822761861709),\n",
       " ('Tomas Mikolov 1, Wen-tau Yih 2, Geoffrey Zweig 2', 0.0004371003776788034),\n",
       " ('Jasper Snoek 1, Hugo Larochelle 2, Ryan P Adams 3', 0.00043673275826760187),\n",
       " ('Chen Wang 1, Peter W Horby 2, Frederick G Hayden 3, George F Gao 4',\n",
       "  0.0004352090307454953),\n",
       " ('Olivier Chapelle', 0.0004337135753095371),\n",
       " ('Marc G. Bellemare 1, Yavar Naddaf 2, Joel Veness 1, Michael Bowling 1',\n",
       "  0.00042858253387732993),\n",
       " ('R.S. Sutton', 0.0004278107288445121),\n",
       " ('A.G. Barto', 0.0004278107288445121),\n",
       " ('Abdullah Assiri 1, Jaffar A Al-Tawfiq 2, Abdullah A Al-Rabeeah 1, Fahad A Al-Rabiah 3, Sami Al-Hajjar 3, Ali Al-Barrak 4, Hesham Flemban 5, Wafa N Al-Nassir 6, Hanan H Balkhy 7, Rafat F Al-Hakeem 1, Hatem Q Makhdoom 8, Alimuddin I Zumla 9, 10, Ziad A Memish 11',\n",
       "  0.0004237753674002229),\n",
       " ('Saining Xie', 0.0004187459007547469),\n",
       " ('Bernhard Schlkopf', 0.00041397113186638127),\n",
       " ('Alexander Zien', 0.00041397113186638127),\n",
       " ('Roman Wölfel 1, Victor M. Corman 2, Wolfgang Guggemos 3, Michael Seilmaier 3, Sabine Zange 1, Marcel A. Müller 2, Daniela Niemeyer 2, Terry C. Jones 2, 4, Patrick Vollmar 1, Camilla Rothe 5, Michael Hoelscher 5, Tobias Bleicker 2, Sebastian Brünink 2, Julia Schneider 2, Rosina Ehmann 1, Katrin Zwirglmaier 1, Christian Drosten 2, Clemens Wendtner 3',\n",
       "  0.00041099022179139094),\n",
       " ('M.A. Ranzato', 0.00040833022289514547),\n",
       " ('Fu Jie Huang', 0.00040833022289514547),\n",
       " ('Y.-L. Boureau', 0.00040833022289514547),\n",
       " ('Olaf Ronneberger', 0.000401021634032077),\n",
       " ('Philipp Fischer', 0.000401021634032077),\n",
       " ('Ron A.M. Fouchier', 0.0004010133753324574),\n",
       " ('Alec Radford 1, Luke Metz 1, Soumith Chintala 2', 0.00039950111671783556),\n",
       " ('Gary B. Huang 1, Marwan Mattar 1, Tamara Berg 2, Eric Learned-Miller 1',\n",
       "  0.000399101887520081),\n",
       " ('Mehdi Mirza', 0.00039598843120317934),\n",
       " ('Pek L. Khong', 0.00039488252843699614),\n",
       " ('Nestor L. Müller', 0.00039488252843699614),\n",
       " ('Wai C. Yiu', 0.00039488252843699614),\n",
       " ('Lin J. Zhou', 0.00039488252843699614),\n",
       " ('James C. M. Ho', 0.00039488252843699614),\n",
       " ('Savvas Nicolaou', 0.00039488252843699614),\n",
       " ('Kenneth W. T. Tsang', 0.00039488252843699614),\n",
       " ('Hyun Jung Koo', 0.00039404324300575203),\n",
       " ('Soyeoun Lim', 0.00039404324300575203),\n",
       " ('Jooae Choe', 0.00039404324300575203),\n",
       " ('Sang Ho Choi', 0.00039404324300575203),\n",
       " ('Heungsup Sung', 0.00039404324300575203),\n",
       " ('Kyung Hyun Do', 0.00039404324300575203),\n",
       " ('Soumith Chintala', 0.00039256911745702436),\n",
       " ('Stanley F. Chen 1, Joshua Goodman 2', 0.00039179941624162035),\n",
       " ('Brody Huval', 0.0003915514988249626),\n",
       " ('James Martens', 0.00039077490938976324),\n",
       " ('J. B. Tenenbaum 1, V. de Silva 1, J. C. Langford 2',\n",
       "  0.00038936986154528995),\n",
       " ('Judea Pearl', 0.00038884308788913547),\n",
       " ('J S M Peiris 1, C M Chu 2, V C C Cheng 1, K S Chan 2, I F N Hung 1, L L M Poon 1, K I Law 2, B S F Tang 1, T Y W Hon 2, C S Chan 2, K H Chan 1, J S C Ng 2, B J Zheng 1, W L Ng 2, R W M Lai 2, Y Guan 1, Kwok-Yung Yuen 1',\n",
       "  0.0003864689410047196),\n",
       " ('Max Welling 1, Michal Rosen-zvi 1, Geoffrey E. Hinton 2',\n",
       "  0.00038611861222273065),\n",
       " ('Hongyi Zhang 1, Moustapha Cisse 2, Yann N. Dauphin 2, David Lopez-Paz 2',\n",
       "  0.0003855137038572717),\n",
       " ('J. R. Uijlings 1, K. E. Sande 2, T. Gevers 2, A. W. Smeulders 2',\n",
       "  0.0003807745239023082),\n",
       " ('Michael Isard', 0.0003803745392630998),\n",
       " ('Ali Moh Zaki 1, Sander Van Boheemen 2, Theo M. Bestebroer 2, Albert D.M.E. Osterhaus',\n",
       "  0.0003767195535332216),\n",
       " ('P Arbeláez 1, M Maire 2, C Fowlkes 3, J Malik 1', 0.000373210488586046),\n",
       " ('David M Hansell', 0.00037092170502779203),\n",
       " ('Alexander A Bankier', 0.00037092170502779203),\n",
       " ('Heber MacMahon', 0.00037092170502779203),\n",
       " ('Theresa C McLoud', 0.00037092170502779203),\n",
       " ('Nestor L Müller', 0.00037092170502779203),\n",
       " ('Jacques Remy', 0.00037092170502779203),\n",
       " ('Rachel L. Graham', 0.00036917809631603794),\n",
       " ('Eric F. Donaldson', 0.00036917809631603794),\n",
       " ('Ralph S. Baric', 0.00036917809631603794),\n",
       " ('David M. Blei 1, Andrew Y. Ng 2, Michael I. Jordan 1',\n",
       "  0.0003686022763650257),\n",
       " ('Leo Breiman', 0.0003673734185517243),\n",
       " ('Li Fei-Fei 1, Rob Fergus 2, Pietro Perona 3', 0.000352678316991106),\n",
       " ('D.G. Lowe', 0.0003461896224223953),\n",
       " ('Sam T. Roweis 1, Lawrence K. Saul 2', 0.0003459703438368397),\n",
       " ('Luca M. Gambardella', 0.00034314650997597856),\n",
       " ('Ian Goodfellow', 0.00034108846143050803),\n",
       " (\"Xiaobo Yang 1, Yuan Yu 1, Jiqian Xu 1, Huaqing Shu 1, Jia'an Xia 2, Hong Liu 1, 2, Yongran Wu 1, Lu Zhang 3, Zhui Yu 4, Minghao Fang 1, Ting Yu 2, Yaxin Wang 1, Shangwen Pan 1, Xiaojing Zou 1, Shiying Yuan 1, You Shang 1, 2\",\n",
       "  0.00033832522832684717),\n",
       " ('John J. Hopfield', 0.00033762415174797635),\n",
       " ('P.Y. Simard', 0.0003363099472343979),\n",
       " ('D. Steinkraus', 0.0003363099472343979),\n",
       " ('J.C. Platt', 0.0003363099472343979),\n",
       " ('Lia van der Hoek 1, Krzysztof Pyrc 1, Maarten F Jebbink 1, Wilma Vermeulen-Oost 2, Ron J M Berkhout 2, Katja C Wolthers 1, Pauline M E Wertheim-van Dillen 1, Jos Kaandorp 3, Joke Spaargaren 2, Ben Berkhout 1',\n",
       "  0.00033560655124267934),\n",
       " ('Sean R. Eddy', 0.00033486901215553933),\n",
       " ('Marco Punta', 0.0003342153487201875),\n",
       " ('Penny C. Coggill', 0.0003342153487201875),\n",
       " ('Ruth Y. Eberhardt', 0.0003342153487201875),\n",
       " ('Jaina Mistry', 0.0003342153487201875),\n",
       " ('John G. Tate', 0.0003342153487201875),\n",
       " ('Chris Boursnell', 0.0003342153487201875),\n",
       " ('Ningze Pang', 0.0003342153487201875),\n",
       " ('Kristoffer Forslund', 0.0003342153487201875),\n",
       " ('Goran Ceric', 0.0003342153487201875),\n",
       " ('Jody Clements', 0.0003342153487201875),\n",
       " ('Andreas Heger', 0.0003342153487201875),\n",
       " ('Liisa Holm', 0.0003342153487201875),\n",
       " ('Erik L. L. Sonnhammer', 0.0003342153487201875),\n",
       " ('Alex Bateman', 0.0003342153487201875),\n",
       " ('Robert D. Finn', 0.0003342153487201875),\n",
       " ('Pietro Perona', 0.00033050490658047586),\n",
       " ('Radford M. Neal', 0.0003279967620775631),\n",
       " ('A. Courville', 0.0003260073071963055),\n",
       " ('P. Vincent', 0.0003260073071963055),\n",
       " ('Percy Liang', 0.0003219198482850556),\n",
       " ('Christopher Poultney', 0.00031694198155960145),\n",
       " ('Yann L. Cun', 0.00031694198155960145),\n",
       " ('David J. Field', 0.00031386764079178715),\n",
       " ('Jasper Fuk Woo Chan 1, Kin Hang Kok 1, 2, Zheng Zhu 2, Hin Chu 1, 2, Kelvin Kai Wang To 3, Shuofeng Yuan 1, 2, Kwok Yung Yuen 1',\n",
       "  0.00031215285706902133),\n",
       " ('Yann Lecun', 0.0003080314140753583),\n",
       " ('H Nielsen 1, J Engelbrecht 2, S Brunak', 0.00030789542767176956),\n",
       " ('G von Heijne 3', 0.00030789542767176956),\n",
       " ('Daan Wierstra', 0.0003048666573369157),\n",
       " ('Thomas N. Kipf', 0.0003045064613503587),\n",
       " ('Bruno A. Olshausen', 0.00030203557028370283),\n",
       " ('Yoshua Bengio 1, 2, 3, Yann Lecun', 0.0002950962599035269),\n",
       " ('Abdullah Assiri 1, Allison McGeer 2, Trish M. Perl 3, Connie S. Price 4, Abdullah A. Al Rabeeah 1, Derek A.T. Cummings 3, Zaki N. Alabdullatif 1, Maher Assad 1, Abdulmohsen Almulhim 1, Hatem Makhdoom 1, Hossam Madani 1, Rafat Alhakeem 1, Jaffar A. Al-Tawfiq 3, Matthew Cotten 5, Simon J. Watson 5, Paul Kellam 5, 6, Alimuddin I. Zumla 6, 7, Ziad A. Memish 8',\n",
       "  0.00029485088771253866),\n",
       " ('Peter V. deSouza', 0.00029152003903274814),\n",
       " ('Jenifer C. Lai', 0.00029152003903274814),\n",
       " ('Sixin Zhang', 0.0002910137212559888),\n",
       " ('Donald B. Rubin', 0.0002881540690631778),\n",
       " ('Dennis Decoste 1, Bernhard Schölkopf 2', 0.00028735516441806346),\n",
       " ('V M Corman 1, I Eckerle 1, T Bleicker 1, A Zaki 2, O Landt 3, M Eschbach-Bludau 1, S van Boheemen 4, R Gopal 5, M Ballhause 3, T M Bestebroer 4, D Muth 1, M A Müller 1, J F Drexler 1, M Zambon 5, A D Osterhaus 4, R M Fouchier 4, C Drosten 1',\n",
       "  0.0002865891086052195),\n",
       " ('Nelson Lee 1, David Hui 1, Alan Wu 1, Paul Chan 1, Peter Cameron 2, Gavin M Joynt 1, Anil Ahuja 1, Man Yee Yung 1, C B Leung 1, K F To 1, S F Lui 1, C C Szeto 1, Sydney Chung 1, Joseph J Y Sung 1',\n",
       "  0.00028370544623796345),\n",
       " ('Novel Coronavirus Pneumonia Emergency Response Epidemiology Team',\n",
       "  0.00028204525219001944),\n",
       " ('Martin Riedmiller', 0.00028189310610111755),\n",
       " ('Rajat Raina', 0.00028057273360876817),\n",
       " ('Alexis Battle', 0.00028057273360876817),\n",
       " ('Stuart Geman 1, Donald Geman 2', 0.0002799712540245327),\n",
       " ('Frederic Morin', 0.00027955505587343307),\n",
       " ('Pranav Rajpurkar', 0.0002776440012344911),\n",
       " ('Jian Zhang', 0.0002776440012344911),\n",
       " ('Konstantin Lopyrev', 0.0002776440012344911),\n",
       " ('Arthur P. Dempster', 0.00027752981430814104),\n",
       " ('Nan M. Laird', 0.00027752981430814104),\n",
       " ('Andrew McCallum', 0.0002760757900523923),\n",
       " ('Philipp Koehn', 0.0002752795691469904),\n",
       " ('Haoqi Fan', 0.00027443271001808514),\n",
       " ('Yuxin Wu', 0.00027443271001808514),\n",
       " ('Wei Liu 1, Dragomir Anguelov 2, Dumitru Erhan 3, Christian Szegedy 3, Scott E. Reed 4, Cheng-Yang Fu 1, Alexander C. Berg 1',\n",
       "  0.00027319162963998907),\n",
       " ('George A. Miller', 0.0002731720161670248),\n",
       " ('John D. Lafferty', 0.00027128638409082927),\n",
       " ('Adam L. Berger 1, Vincent J. Della Pietra 2, Stephen A. Della Pietra 2',\n",
       "  0.00026888900555226857),\n",
       " ('Anthony J. Bell', 0.00026449746596778827),\n",
       " ('Terrence J. Sejnowski', 0.00026449746596778827),\n",
       " ('Minjie Lin', 0.0002614398283109876),\n",
       " ('Lingjun Ying', 0.0002614398283109876),\n",
       " ('Holger Schwenk', 0.00026022149333769967),\n",
       " ('Alexei A. Efros', 0.0002584272919597533),\n",
       " ('Bradley Efron 1, Trevor Hastie 1, Iain Johnstone 1, Robert Tibshirani 1, Hemant Ishwaran 2, Keith Knight 3, Jean Michel Loubes 4, 5, Pascal Massart 4, 6, David Madigan 7, Greg Ridgeway 7, 8, Saharon Rosset 1, 9, J. I. Zhu 10, Robert A. Stine 11, Berwin A. Turlach 12, Sanford Weisberg 13',\n",
       "  0.00025746283351703043),\n",
       " ('Quoc Le', 0.0002545555483324734),\n",
       " ('A. Torralba 1, R. Fergus 2, W.T. Freeman 1', 0.00025444495096802334),\n",
       " ('S.G. Mallat', 0.0002540089119461611),\n",
       " ('Xing Yi Ge 1, Jia Lu Li 1, Xing Lou Yang 1, Aleksei A. Chmura 2, Guangjian Zhu 2, Jonathan H. Epstein 2, Jonna A Mazet 3, Ben Hu 1, Wei Zhang 1, Cheng Peng 1, Yu Ji Zhang 1, Chu Ming Luo 1, Bing Tan 1, Ning Wang 1, Yan Zhu 1, Gary Crameri 4, Shu Yi Zhang 5, Lin Fa Wang 4, 6, Peter Daszak 2, Zheng Li Shi 1',\n",
       "  0.0002537268960124795),\n",
       " ('Marius Cordts 1, Mohamed Omran 2, Sebastian Ramos 3, Timo Rehfeld 1, Markus Enzweiler 3, Rodrigo Benenson 2, Uwe Franke 3, Stefan Roth 1, Bernt Schiele 2',\n",
       "  0.00025262861972947604),\n",
       " ('Thorsten Joachims', 0.0002525043350619915),\n",
       " ('Lukasz Kaiser', 0.00025114725456864157),\n",
       " ('Jonathan H. Epstein', 0.0002496369528552835),\n",
       " ('Peter Daszak', 0.0002496369528552835),\n",
       " (\"Yaniv Taigman 1, Ming Yang 1, Marc'Aurelio Ranzato 1, Lior Wolf 2\",\n",
       "  0.00024876099510356254),\n",
       " ('Razvan Pascanu', 0.00024850856199004596),\n",
       " ('Frédéric Bastien', 0.00024850856199004596),\n",
       " ('Jianbo Shi 1, J. Malik 2', 0.0002473892585574604),\n",
       " ('Scott Shaobing Chen 1, David L. Donoho 2, Michael A. Saunders 2',\n",
       "  0.0002462521280467146),\n",
       " ('Marc Lipsitch 1, Ted Cohen 1, Ben Cooper 1, James M. Robins 1, Stefan Ma 2, Lyn James 2, Gowri Gopalakrishna 2, Suok Kai Chew 2, Chorh Chuan Tan 2, Matthew H. Samore 3, David Fisman 4, Megan Murray 1',\n",
       "  0.00024504558838467426),\n",
       " ('Junqiang Lei', 0.00024473622409373793),\n",
       " ('Junfeng Li', 0.00024473622409373793),\n",
       " ('Xun Li', 0.00024473622409373793),\n",
       " ('Xiaolong Qi', 0.00024473622409373793),\n",
       " ('Zhilin Yang 1, Zihang Dai 1, Yiming Yang 1, Jaime G. Carbonell 1, Ruslan Salakhutdinov 1, Quoc V. Le 2',\n",
       "  0.0002445243427706845),\n",
       " ('Pedro F. Felzenszwalb 1, Daniel P. Huttenlocher 2', 0.00024345159920975118),\n",
       " ('Kenneth W. Tsang', 0.00024327774212205497),\n",
       " ('Pak L. Ho', 0.00024327774212205497),\n",
       " ('Wilson K. Yee', 0.00024327774212205497),\n",
       " ('Teresa Wang', 0.00024327774212205497),\n",
       " ('Moira Chan-Yeung', 0.00024327774212205497),\n",
       " ('Wah K. Lam', 0.00024327774212205497),\n",
       " ('Wing H. Seto', 0.00024327774212205497),\n",
       " ('Loretta Y. Yam', 0.00024327774212205497),\n",
       " ('Thomas M. Cheung', 0.00024327774212205497),\n",
       " ('Poon C. Wong', 0.00024327774212205497),\n",
       " ('Mary S. Ip', 0.00024327774212205497),\n",
       " ('Jane Chan', 0.00024327774212205497),\n",
       " ('Kwok Y. Yuen', 0.00024327774212205497),\n",
       " ('Kar N. Lai', 0.00024327774212205497),\n",
       " ('Vladimir N. Vapnik', 0.00024253685144537033),\n",
       " ('Zhifeng Zhang', 0.00024196177919376437),\n",
       " ('J. Malik', 0.00024030538489956782),\n",
       " ('Wendong Li', 0.00023975559354462417),\n",
       " ('Zhengli Shi', 0.00023975559354462417),\n",
       " ('Meng Yu', 0.00023975559354462417),\n",
       " ('Wuze Ren', 0.00023975559354462417),\n",
       " ('Craig Smith', 0.00023975559354462417),\n",
       " ('Hanzhong Wang', 0.00023975559354462417),\n",
       " ('Gary Crameri', 0.00023975559354462417),\n",
       " ('Zhihong Hu', 0.00023975559354462417),\n",
       " ('Huajun Zhang', 0.00023975559354462417),\n",
       " ('Jianhong Zhang', 0.00023975559354462417),\n",
       " ('Jennifer McEachern', 0.00023975559354462417),\n",
       " ('Hume Field', 0.00023975559354462417),\n",
       " ('Bryan T. Eaton', 0.00023975559354462417),\n",
       " ('Shuyi Zhang', 0.00023975559354462417),\n",
       " ('Lin-Fa Wang', 0.00023975559354462417),\n",
       " ('Fernando C. N. Pereira', 0.00023934575433340088),\n",
       " ('Sinno Jialin Pan', 0.00023858043570943458),\n",
       " ('Qiang Yang', 0.00023858043570943458),\n",
       " ('Xiaojin Zhu', 0.00023671801152798234),\n",
       " ('Li Wan', 0.00023596082691842174),\n",
       " ('Matthew Zeiler', 0.00023596082691842174),\n",
       " ('Yann Le Cun', 0.00023596082691842174),\n",
       " ('Christopher Manning', 0.0002356146246056963),\n",
       " ('David Silver', 0.00023500791660061506),\n",
       " ('Chih-Jen Lin', 0.00023328248903891458),\n",
       " ('Chih-Chung Chang', 0.00023253905899480655),\n",
       " ('Kelvin Xu 1, Jimmy Ba 2, Ryan Kiros 2, Kyunghyun Cho 1, Aaron Courville 1, Ruslan Salakhudinov 2, 3, Rich Zemel 2, 3, Yoshua Bengio 1, 3',\n",
       "  0.00023187994605427983),\n",
       " ('Phillip Isola', 0.00023031432063465922),\n",
       " ('Jun-Yan Zhu', 0.00023031432063465922),\n",
       " ('Franz Josef Och', 0.0002279364060704519),\n",
       " ('Volodymyr Mnih', 0.00022757709194110446),\n",
       " ('Andrei A. Rusu', 0.00022757709194110446),\n",
       " ('Joel Veness', 0.00022757709194110446),\n",
       " ('Marc G. Bellemare', 0.00022757709194110446),\n",
       " ('Andreas K. Fidjeland', 0.00022757709194110446),\n",
       " ('Georg Ostrovski', 0.00022757709194110446),\n",
       " ('Stig Petersen', 0.00022757709194110446),\n",
       " ('Charles Beattie', 0.00022757709194110446),\n",
       " ('Amir Sadik', 0.00022757709194110446),\n",
       " ('Ioannis Antonoglou', 0.00022757709194110446),\n",
       " ('Helen King', 0.00022757709194110446),\n",
       " ('Dharshan Kumaran', 0.00022757709194110446),\n",
       " ('Shane Legg', 0.00022757709194110446),\n",
       " ('Demis Hassabis', 0.00022757709194110446),\n",
       " ('Richard O. Duda', 0.00022730488932961476),\n",
       " ('Peter E. Hart', 0.00022730488932961476),\n",
       " ('Florian Schroff', 0.0002269065087591345),\n",
       " ('Dmitry Kalenichenko', 0.0002269065087591345),\n",
       " ('James Philbin', 0.0002269065087591345),\n",
       " ('Koen E. A. van de Sande 1, Jasper R. R. Uijlings 2, Theo Gevers 1, Arnold W. M. Smeulders 1',\n",
       "  0.00022684241254986796),\n",
       " ('Jie Zhou 1, Cun Li 1, Guangyu Zhao 2, Hin Chu 1, Dong Wang 1, Helen Hoi-Ning Yan 1, Vincent Kwok-Man Poon 1, Lei Wen 1, Bosco Ho-Yin Wong 1, Xiaoyu Zhao 1, Man Chun Chiu 1, Dong Yang 1, Yixin Wang 1, Rex K. H. Au-Yeung 1, Ivy Hau-Yee Chan 3, Shihui Sun 2, Jasper Fuk-Woo Chan 4, Kelvin Kai-Wang To 4, Ziad Ahmed Memish 5, 6, Victor M. Corman 7, Christian Drosten 7, Ivan Fan-Ngai Hung 1, Yusen Zhou 2, Suet Yi Leung 1, Kwok-Yung Yuen 4',\n",
       "  0.00022542105275823721),\n",
       " ('Bo Pang', 0.00022530812591695548),\n",
       " ('Lillian Lee', 0.00022530812591695548),\n",
       " ('Sebastian Ruder', 0.00022474872960376175),\n",
       " ('Christopher D. Manning 1, Hinrich Schütze 2', 0.0002241282927136456),\n",
       " ('Christopher Potts', 0.00022222734095436871),\n",
       " ('Trevor Hastie', 0.000222088990678677),\n",
       " ('Zhe Xu 1, Lei Shi 1, Yijin Wang 2, Jiyuan Zhang 1, Lei Huang 1, Chao Zhang 1, Shuhong Liu 2, Peng Zhao 1, Hongxia Liu 1, Li Zhu 2, Yanhong Tai 2, Changqing Bai 3, Tingting Gao 2, Jinwen Song 1, Peng Xia 1, Jinghui Dong 4, Jingmin Zhao 2, Fu Sheng Wang 1',\n",
       "  0.00022093879969070504),\n",
       " ('S. Kirkpatrick 1, C. D. Gelatt 1, M. P. Vecchi 2', 0.0002208886767474918),\n",
       " ('Susanna K. P. Lau', 0.0002204091083455404),\n",
       " ('Patrick C. Y. Woo', 0.0002204091083455404),\n",
       " ('Jeremy Howard', 0.00021906539681465216),\n",
       " ('Jeffrey Pennington', 0.00021896418274528533),\n",
       " ('Eric H. Huang', 0.00021896418274528533),\n",
       " ('Yoon Kim', 0.00021868147164320076),\n",
       " ('Stephen A. Della Pietra', 0.00021839023418910626),\n",
       " ('Zhou Wang 1, A.C. Bovik 2, H.R. Sheikh 2, E.P. Simoncelli 3',\n",
       "  0.00021702491254048056),\n",
       " ('Andrea Vedaldi', 0.00021662230001445574),\n",
       " ('Ken Chatfield', 0.00021571270076856696),\n",
       " ('Jorma Rissanen', 0.00021565704061866467),\n",
       " ('Tinghui Zhou', 0.00021441179557687558),\n",
       " ('Bo Pang 1, Lillian Lee 2', 0.00021171731913025227),\n",
       " ('T. Poggio', 0.00021161450617743858),\n",
       " ('Brendan J. Frey', 0.000211040885006068),\n",
       " ('S. Katz', 0.00021086242107924954),\n",
       " ('Bo Pang 1, Lillian Lee 1, Shivakumar Vaithyanathan 2',\n",
       "  0.00020990816323179039),\n",
       " ('Francois Chollet', 0.00020982011582103657),\n",
       " ('Dan Ciresan', 0.00020906640286084618),\n",
       " ('Alessandro Giusti', 0.00020906640286084618),\n",
       " ('Serge Belongie', 0.00020901722199611765),\n",
       " ('Michael I. Jordan 1, Zoubin Ghahramani 2, Tommi S. Jaakkola 3, Lawrence K. Saul 4',\n",
       "  0.00020824949769278637),\n",
       " ('Joseph Redmon 1, Santosh Divvala 2, Ross Girshick 3, Ali Farhadi 2',\n",
       "  0.00020823536934894436),\n",
       " ('P. Felzenszwalb 1, D. McAllester 2, D. Ramanan 3', 0.00020792351922689783),\n",
       " ('Timothy P. Sheahan 1, Amy C. Sims 1, Sarah R. Leist 1, Alexandra Schäfer 1, John Won 1, Ariane J. Brown 1, Stephanie A. Montgomery 1, Alison Hogg 2, Darius Babusis 2, Michael O. Clarke 2, Jamie E. Spahn 2, Laura Bauer 2, Scott Sellers 2, Danielle Porter 2, Joy Y. Feng 2, Tomas Cihlar 2, Robert Jordan 2, Mark R. Denison 3, Ralph S. Baric 1',\n",
       "  0.00020770462652633056),\n",
       " ('Dan Klein', 0.00020751817021260707),\n",
       " ('Andrew Blake', 0.00020656565815411787),\n",
       " ('Yuelong Shu 1, John McCauley 2', 0.00020550614572356575),\n",
       " ('Mulangu S', 0.00020535607974601832),\n",
       " ('Dodd Le 1, Davey Rt', 0.00020535607974601832),\n",
       " ('Tshiani Mbaya O 2, Proschan M 3, Mukadi D 4, Lusakibanza Manzo M 5, Nzolo D 6, Tshomba Oloma A',\n",
       "  0.00020535607974601832),\n",
       " ('Ibanda A', 0.00020535607974601832),\n",
       " ('Ali R 7, Coulibaly S', 0.00020535607974601832),\n",
       " ('Levine Ac', 0.00020535607974601832),\n",
       " ('Grais R', 0.00020535607974601832),\n",
       " ('Diaz J', 0.00020535607974601832),\n",
       " ('Lane Hc', 0.00020535607974601832),\n",
       " ('Muyembe-Tamfum Jj 8, Sivahera B', 0.00020535607974601832),\n",
       " ('Camara M', 0.00020535607974601832),\n",
       " ('Kojan R', 0.00020535607974601832),\n",
       " ('Walker R', 0.00020535607974601832),\n",
       " ('Dighero-Kemp B', 0.00020535607974601832),\n",
       " ('Cao H', 0.00020535607974601832),\n",
       " ('Mukumbayi P', 0.00020535607974601832),\n",
       " ('Mbala-Kingebeni P', 0.00020535607974601832),\n",
       " ('Ahuka S', 0.00020535607974601832),\n",
       " ('Albert S', 0.00020535607974601832),\n",
       " ('Bonnett T', 0.00020535607974601832),\n",
       " ('Crozier I', 0.00020535607974601832),\n",
       " ('Duvenhage M', 0.00020535607974601832),\n",
       " ('Proffitt C', 0.00020535607974601832),\n",
       " ('Teitelbaum M', 0.00020535607974601832),\n",
       " ('Moench T', 0.00020535607974601832),\n",
       " ('Aboulhab J', 0.00020535607974601832),\n",
       " ('Barrett K', 0.00020535607974601832),\n",
       " ('Cahill K', 0.00020535607974601832),\n",
       " ('Cone K', 0.00020535607974601832),\n",
       " ('Eckes R', 0.00020535607974601832),\n",
       " ('Hensley L', 0.00020535607974601832),\n",
       " ('Herpin B', 0.00020535607974601832),\n",
       " ('Higgs E', 0.00020535607974601832),\n",
       " ('Ledgerwood J', 0.00020535607974601832),\n",
       " ('Pierson J', 0.00020535607974601832),\n",
       " ('Smolskis M', 0.00020535607974601832),\n",
       " ('Sow Y', 0.00020535607974601832),\n",
       " ('Tierney J', 0.00020535607974601832),\n",
       " ('Sivapalasingam S', 0.00020535607974601832),\n",
       " ('Holman W', 0.00020535607974601832),\n",
       " ('Gettinger N', 0.00020535607974601832),\n",
       " ('Vallée D +1', 0.00020535607974601832),\n",
       " ('Alexander Graves', 0.00020479420704144408),\n",
       " ('T. Serre', 0.00020280034700811657),\n",
       " ('L. Wolf', 0.00020280034700811657),\n",
       " ('Iain Murray', 0.00020265462866912487),\n",
       " ('Alex Graves 1, Santiago Fernández 1, Faustino Gomez 1, Jürgen Schmidhuber 2',\n",
       "  0.00020245550698703936),\n",
       " ('Andreas Stolcke', 0.0002022892620208314),\n",
       " ('James L. McClelland', 0.000202247299573946),\n",
       " ('Joshua T. Goodman', 0.00020223646551394196),\n",
       " ('Erkki Oja', 0.00020188535882543742),\n",
       " ('Joan Bruna 1, Wojciech Zaremba 1, Arthur Szlam 2, Yann LeCun 1',\n",
       "  0.00020170222511024465),\n",
       " ('Jeffrey L. Elman', 0.00020149206665411411),\n",
       " ('Yinhan Liu', 0.00020144567830954752),\n",
       " ('Myle Ott', 0.00020144567830954752),\n",
       " ('Naman Goyal', 0.00020144567830954752),\n",
       " ('Jingfei Du', 0.00020144567830954752),\n",
       " ('Mandar Joshi', 0.00020144567830954752),\n",
       " ('Danqi Chen', 0.00020144567830954752),\n",
       " ('Omer Levy', 0.00020144567830954752),\n",
       " ('Mike Lewis', 0.00020144567830954752),\n",
       " ('Luke Zettlemoyer', 0.00020144567830954752),\n",
       " ('Veselin Stoyanov', 0.00020144567830954752),\n",
       " ('Sara Sabour', 0.00020107244059413532),\n",
       " ('Nicholas Frosst', 0.00020107244059413532),\n",
       " ('Neeraj Kumar', 0.00020051524062386682),\n",
       " ('Alexander C. Berg', 0.00020051524062386682),\n",
       " ('Peter N. Belhumeur', 0.00020051524062386682),\n",
       " ('Shree K. Nayar', 0.00020051524062386682),\n",
       " ('Aapo Hyvarinen', 0.0002000535091391491),\n",
       " ('Juha Karhunen', 0.0002000535091391491),\n",
       " ('Y. Bengio 1, P. Simard 2, P. Frasconi 3', 0.0001979340549190155),\n",
       " ('D. Nister', 0.00019666411156281062),\n",
       " ('H. Stewenius', 0.00019666411156281062),\n",
       " ('David D. Lewis', 0.0001965207355676292),\n",
       " ('Jamie Shotton 1, John Winn 2, Carsten Rother 2, Antonio Criminisi 2',\n",
       "  0.00019651999548679167),\n",
       " ('Kenneth S. M. Li', 0.0001938271474940598),\n",
       " ('Yi Huang', 0.0001938271474940598),\n",
       " ('Hoi-Wah Tsoi', 0.0001938271474940598),\n",
       " ('Beatrice H. L. Wong', 0.0001938271474940598),\n",
       " ('Samson S. Y. Wong', 0.0001938271474940598),\n",
       " ('Suet-Yi Leung', 0.0001938271474940598),\n",
       " ('Kwok-Hung Chan', 0.0001938271474940598),\n",
       " ('Shi Zhao 1, Qianyin Lin 2, Jinjun Ran 3, Salihu S Musa 4, Guangpu Yang 1, Weiming Wang 5, Yijun Lou 3, Daozhou Gao 6, Lin Yang 4, Daihai He 4, Maggie H Wang 1',\n",
       "  0.00019370689135710467),\n",
       " ('Junbo Zhao', 0.00019240425573226137),\n",
       " ('Alex Wang 1, Amanpreet Singh 1, Julian Michael 2, Felix Hill 3, Omer Levy 4, Samuel R. Bowman 1',\n",
       "  0.00019155672304064798),\n",
       " ('Dragomir Anguelov', 0.0001910678809223284),\n",
       " ('Bryan C. Russell 1, Antonio Torralba 1, Kevin P. Murphy 2, William T. Freeman 1',\n",
       "  0.00019106136173142652),\n",
       " ('Aude Oliva 1, Antonio Torralba 2', 0.0001897120058766451),\n",
       " ('Peter Dayan', 0.00018944915381518054),\n",
       " ('Fernanda B. Viégas', 0.0001894312236825507),\n",
       " ('Martin Wattenberg', 0.0001894312236825507),\n",
       " ('Piotr Dollár', 0.00018891958779352938),\n",
       " ('Miguel Á. Carreira-Perpiñán', 0.00018854298750539612),\n",
       " ('Susan M. Poutanen 1, 2, Donald E. Low 1, Bonnie Henry 3, Sandy Finkelstein 4, David Rose 4, Karen Green 1, Raymond Tellier 5, 6, Ryan Draker 1, Dena Adachi 1, Melissa Ayers 1, Adrienne K. Chan 1, Danuta M. Skowronski 7, Irving Salit 1, Andrew E. Simor 1, Arthur S. Slutsky 1, Patrick W. Doyle 8, Mel Krajden 7, Martin Petric 7, Robert C. Brunham 8, Allison J. McGeer 1',\n",
       "  0.000188441788857618),\n",
       " ('Y. Weiss', 0.00018761481636724093),\n",
       " ('V. Stalin Raj 1, Huihui Mou 2, Saskia L. Smits 1, Dick H. W. Dekkers 1, Marcel A. Müller 3, Ronald Dijkman 4, Doreen Muth 3, Jeroen A. A. Demmers 1, Ali Zaki 5, Ron A. M. Fouchier 1, Volker Thiel 4, 6, Christian Drosten 3, Peter J. M. Rottier 2, Albert D. M. E. Osterhaus 1, Berend Jan Bosch 2, Bart L. Haagmans 1',\n",
       "  0.0001863335909368963),\n",
       " ('Jianchao Yang 1, Kai Yu 2, Yihong Gong 2, Thomas Huang 1',\n",
       "  0.0001861431827827128),\n",
       " ('Maxime Oquab 1, 2, Leon Bottou 2, Ivan Laptev 1, Josef Sivic 1',\n",
       "  0.00018590159836619202),\n",
       " ('Tsung-Yi Lin', 0.00018352490964792717),\n",
       " ('Michael Maire', 0.00018352490964792717),\n",
       " ('Lubomir Bourdev', 0.00018352490964792717),\n",
       " ('James Hays', 0.00018352490964792717),\n",
       " ('Deva Ramanan', 0.00018352490964792717),\n",
       " ('C. Lawrence Zitnick', 0.00018352490964792717),\n",
       " ('Samuel R. Bowman', 0.00018341333873618962),\n",
       " ('Daniel Marcu', 0.00018239854758235087),\n",
       " ('Gabor Angeli', 0.00018232329112171516),\n",
       " ('Jean-Luc Gauvain', 0.00018176340111228235),\n",
       " ('Tommi Jaakkola', 0.0001813544094336167),\n",
       " ('Paul Viola 1, Michael J. Jones 2', 0.0001811319061129299),\n",
       " ('Sam T. Roweis', 0.00018109915374999247),\n",
       " ('Martín Abadi', 0.00018008497886189232),\n",
       " ('Ashish Agarwal', 0.00018008497886189232),\n",
       " ('Paul Barham', 0.00018008497886189232),\n",
       " ('Eugene Brevdo', 0.00018008497886189232),\n",
       " ('Craig Citro', 0.00018008497886189232),\n",
       " ('Gregory S. Corrado', 0.00018008497886189232),\n",
       " ('Andy Davis', 0.00018008497886189232),\n",
       " ('Andrew Harp', 0.00018008497886189232),\n",
       " ('Geoffrey Irving', 0.00018008497886189232),\n",
       " ('Rafal Józefowicz', 0.00018008497886189232),\n",
       " ('Manjunath Kudlur', 0.00018008497886189232),\n",
       " ('Josh Levenberg', 0.00018008497886189232),\n",
       " ('Dan Mané', 0.00018008497886189232),\n",
       " ('Sherry Moore', 0.00018008497886189232),\n",
       " ('Derek Gordon Murray', 0.00018008497886189232),\n",
       " ('Chris Olah', 0.00018008497886189232),\n",
       " ('Benoit Steiner', 0.00018008497886189232),\n",
       " ('Kunal Talwar', 0.00018008497886189232),\n",
       " ('Paul A. Tucker', 0.00018008497886189232),\n",
       " ('Vijay Vasudevan', 0.00018008497886189232),\n",
       " ('Pete Warden', 0.00018008497886189232),\n",
       " ('Martin Wicke', 0.00018008497886189232),\n",
       " ('Yuan Yu', 0.00018008497886189232),\n",
       " ('Xiaoqiang Zheng', 0.00018008497886189232),\n",
       " ('Nicolas L. Roux', 0.0001788684050640706),\n",
       " ('Fernando Pereira 1, Naftali Tishby 2, Lillian Lee 3',\n",
       "  0.00017764039823620878),\n",
       " ('Arnaud Bergeron', 0.00017522516755180733),\n",
       " ('Nicolas Bouchard', 0.00017522516755180733),\n",
       " ('Aleksander Madry 1, Aleksandar Makelov 1, Ludwig Schmidt 1, Dimitris Tsipras 1, Adrian Vladu 2',\n",
       "  0.00017377218596508498),\n",
       " ('K. Mikolajczyk 1, C. Schmid 2', 0.00017088657723303581),\n",
       " ('Graham W. Taylor', 0.0001699523483797395),\n",
       " ('Zhenzhong Lan 1, Mingda Chen 2, Sebastian Goodman 1, Kevin Gimpel 3, Piyush Sharma 1, Radu Soricut 1',\n",
       "  0.0001685359186294689),\n",
       " ('Jerome H. Friedman', 0.00016707139876927455),\n",
       " ('Christopher Manning 1, Mihai Surdeanu 2, John Bauer 1, Jenny Finkel 1, Steven Bethard 3, David McClosky 4',\n",
       "  0.0001659946860114658),\n",
       " ('Joao Carreira', 0.00016535502174983544),\n",
       " ('Cristian Sminchisescu', 0.00016535502174983544),\n",
       " ('Bogdan Alexe', 0.00016535502174983544),\n",
       " ('Thomas Deselaers', 0.00016535502174983544),\n",
       " ('Vittorio Ferrari', 0.00016535502174983544),\n",
       " ('Thomas Dean', 0.00016535502174983544),\n",
       " ('Mark A. Ruzon', 0.00016535502174983544),\n",
       " ('Mark Segal', 0.00016535502174983544),\n",
       " ('Sudheendra Vijayanarasimhan', 0.00016535502174983544),\n",
       " ('Jay Yagnik', 0.00016535502174983544),\n",
       " ('Tim Salimans', 0.00016476879792102386),\n",
       " ('Bryan McCann', 0.00016422698879992164),\n",
       " ('James Bradbury', 0.00016422698879992164),\n",
       " ('Caiming Xiong', 0.00016422698879992164),\n",
       " ('W.R. Gilks', 0.0001636457078703494),\n",
       " ('S. Richardson', 0.0001636457078703494),\n",
       " ('David Spiegelhalter', 0.0001636457078703494),\n",
       " ('Jie Hu 1, Li Shen 2, Samuel Albanie 2, Gang Sun 1, Enhua Wu 1',\n",
       "  0.0001634853024465646),\n",
       " ('Daniel K W Chu 1, Yang Pan 2, 3, Samuel M S Cheng 1, Kenrie P Y Hui 1, Pavithra Krishnan 1, Yingzhi Liu 1, Daisy Y M Ng 1, Carrie K C Wan 1, Peng Yang 2, 3, Quanyi Wang 3, Malik Peiris 1, Leo L M Poon 1',\n",
       "  0.00016276932628965906),\n",
       " ('David L. Donoho', 0.00016267690577319814),\n",
       " ('Kevin Patrick Murphy', 0.0001625075656789496),\n",
       " ('Stuart Russell', 0.0001625075656789496),\n",
       " ('Gerard Salton', 0.0001623939013646118),\n",
       " ('P. Smolensky', 0.00016094657289539313),\n",
       " ('R. J. de Groot 1, S. C. Baker 2, R. S. Baric 3, C. S. Brown 4, C. Drosten 5, L. Enjuanes 6, R. A. M. Fouchier 7, M. Galiano 8, A. E. Gorbalenya 9, Z. A. Memish 10, S. Perlman 11, L. L. M. Poon 12, E. J. Snijder 9, G. M. Stephens 10, P. C. Y. Woo 12, A. M. Zaki 13, M. Zambon 8, J. Ziebuhr 14',\n",
       "  0.00016088710046774653),\n",
       " ('Yoshua Bengio 1, Jérôme Louradour 2, Ronan Collobert 3, Jason Weston 3',\n",
       "  0.00016062925257832997),\n",
       " ('Vladimir Naumovich Vapnik', 0.00016031683646681994),\n",
       " ('Michael Elad', 0.00015966093379696586),\n",
       " ('Catharine I. Paules 1, Hilary D. Marston 2, Anthony S. Fauci 2',\n",
       "  0.00015922557621506853),\n",
       " ('John Lafferty', 0.00015917636653785348),\n",
       " ('Michael I. Jordan', 0.00015842534082197754),\n",
       " ('Philipp Slusallek 1, Peter Shirley 2, William Mark 3, Gordon Stoll 4, Ingo Wald 5',\n",
       "  0.00015793017302344976),\n",
       " ('Ards Definition Task Force 1, V Marco Ranieri 1, Gordon D Rubenfeld 2, B Taylor Thompson 2, Niall D Ferguson 3, Ellen Caldwell 2, Eddy Fan 4, Luigi Camporota 2, 5, Arthur S Slutsky 2',\n",
       "  0.00015775242164784852),\n",
       " ('Cheng-tao Chu', 0.00015622223021888258),\n",
       " ('Sang K. Kim', 0.00015622223021888258),\n",
       " ('Yi-an Lin', 0.00015622223021888258),\n",
       " ('Yuanyuan Yu', 0.00015622223021888258),\n",
       " ('Gary Bradski', 0.00015622223021888258),\n",
       " ('Kunle Olukotun', 0.00015622223021888258),\n",
       " ('Pieter Abbeel', 0.00015620492166540682),\n",
       " ('Theresa Wilson', 0.0001558275785397603),\n",
       " ('Janyce Wiebe', 0.0001558275785397603),\n",
       " ('Paul Hoffmann', 0.0001558275785397603),\n",
       " ('S. Chopra', 0.00015508748182816573),\n",
       " ('R. Hadsell', 0.00015508748182816573),\n",
       " ('Zoubin Ghahramani 1, Michael I. Jordan 2', 0.00015507494403152358),\n",
       " ('Zoubin Ghahramani', 0.00015445678167228698),\n",
       " ('Albert Benveniste 1, Pierre Priouret 2, Michel Métivier',\n",
       "  0.00015406606192017126),\n",
       " ('Alexander Waibel', 0.00015293686790182067),\n",
       " ('Toshiyuki Hanazawa', 0.00015293686790182067),\n",
       " ('Kiyohiro Shikano', 0.00015293686790182067),\n",
       " ('Kevin J. Lang', 0.00015293686790182067),\n",
       " ('Peter', 0.0001517852776201168),\n",
       " ('Turney', 0.0001517852776201168),\n",
       " ('Fan R K Chung', 0.00015160398150553626),\n",
       " ('David H. Ackley 1, Geoffrey E. Hinton 1, Terrence J. Sejnowski 2',\n",
       "  0.0001514688254272998),\n",
       " ('Kyunghyun Cho 1, Bart van Merrienboer 1, Dzmitry Bahdanau 2, Yoshua Bengio 3, 4, 5',\n",
       "  0.00015132566447863605),\n",
       " ('S. Becker', 0.0001492474123326925),\n",
       " ('Mikhail Belkin', 0.00014903670913125628),\n",
       " ('Partha Niyogi', 0.00014903670913125628),\n",
       " ('Sergey Levine', 0.0001481538264522841),\n",
       " ('Xavier Glorot 1, Antoine Bordes 2, Yoshua Bengio 1',\n",
       "  0.00014740080398677337),\n",
       " ('David S. Hui 1, Esam Ei Azhar 2, Tariq A. Madani 2, Francine Ntoumi 3, Richard Kock 4, Osman Dar 5, Giuseppe Ippolito 6, Timothy D. Mchugh 7, Ziad A. Memish 8, Christian Drosten 9, Alimuddin Zumla 7, Eskild Petersen 10',\n",
       "  0.000147368858225242),\n",
       " ('Gregory Griffin', 0.0001469799969325487),\n",
       " ('Alex Holub', 0.0001469799969325487),\n",
       " ('Yueying Pan', 0.00014693448641346125),\n",
       " ('Hanxiong Guan', 0.00014693448641346125),\n",
       " ('Shuchang Zhou', 0.00014693448641346125),\n",
       " ('Yujin Wang', 0.00014693448641346125),\n",
       " ('Qian Li', 0.00014693448641346125),\n",
       " ('Tingting Zhu', 0.00014693448641346125),\n",
       " ('Qiongjie Hu', 0.00014693448641346125),\n",
       " ('Liming Xia', 0.00014693448641346125),\n",
       " ('Zhuowen Tu', 0.00014431319073666173),\n",
       " ('I. Laptev 1, M. Marszalek 1, C. Schmid 1, B. Rozenfeld 2',\n",
       "  0.00014421279944137235),\n",
       " ('Herbert Bay 1, Andreas Ess 1, Tinne Tuytelaars 2, Luc Van Gool 1',\n",
       "  0.00014409111604693977),\n",
       " ('Yee Whye Teh 1, Max Welling 1, Simon Osindero 2, Geoffrey E. Hinton 1',\n",
       "  0.0001436262455643405),\n",
       " ('Georgia Gkioxari', 0.00014322192472533978),\n",
       " ('Piotr Dollar', 0.00014322192472533978),\n",
       " ('A.J. Robinson', 0.00014181559009457937),\n",
       " ('Danqi Chen 1, Jason Bolton 2, Christopher D. Manning 2',\n",
       "  0.00014156465875746733),\n",
       " ('Christopher Buckley', 0.0001411568596521125),\n",
       " ('John H. Holland', 0.00014097109362866067),\n",
       " ('R. De Mori', 0.00014067517026378798),\n",
       " ('Jean-Sébastien Senecal', 0.0001404843211208236),\n",
       " ('C.C. Tappert 1, C.Y. Suen 2, T. Wakahara 3', 0.00014024862688236052),\n",
       " ('Daphne Koller', 0.0001395451099616243),\n",
       " ('William H. Press', 0.00013926526956448338),\n",
       " ('Saul A. Teukolsky', 0.00013926526956448338),\n",
       " ('William T. Vetterling', 0.00013926526956448338),\n",
       " ('Brian P. Flannery', 0.00013926526956448338),\n",
       " ('G. Flammia', 0.00013914342010064052),\n",
       " ('R. Kompe', 0.00013914342010064052),\n",
       " ('Chelsea Finn', 0.0001382265417824993),\n",
       " ('Antoine Bordes', 0.0001378164877618766),\n",
       " ('D. Comaniciu', 0.0001377135477739639),\n",
       " ('P. Meer', 0.0001377135477739639),\n",
       " ('L.R. Rabiner', 0.000137587612984775),\n",
       " ('Yoav Freund', 0.00013574030783580372),\n",
       " ('Yiming Yang', 0.00013494403853025184),\n",
       " ('Carl Doersch 1, Abhinav Gupta 1, Alexei A. Efros 2',\n",
       "  0.00013426988122305466),\n",
       " ('Jerome Friedman', 0.00013409614566501943),\n",
       " ('Dan C. Cireşan', 0.00013408010711513237),\n",
       " ('Jonathan Masci', 0.00013408010711513237),\n",
       " ('Thang Luong', 0.00013392027744209555),\n",
       " ('Lev Ratinov 1, Dan Roth 1, Doug Downey 2, Mike Anderson 3',\n",
       "  0.00013346046935440884),\n",
       " ('Howard Gobioff', 0.000131614915008268),\n",
       " ('Shun-Tak Leung', 0.000131614915008268),\n",
       " ('T. Tsang', 0.00013154072985461024),\n",
       " ('L. Pak-Yin', 0.00013154072985461024),\n",
       " ('M. Lee', 0.00013154072985461024),\n",
       " ('J.-S. Wu', 0.00013154072985461024),\n",
       " ('Y.-C. Wu', 0.00013154072985461024),\n",
       " ('I.-H. Chiang', 0.00013154072985461024),\n",
       " ('K.-T. Chen', 0.00013154072985461024),\n",
       " ('K.-H. Hsu', 0.00013154072985461024),\n",
       " ('T.-J. Chen', 0.00013154072985461024),\n",
       " ('H.-T. Lee', 0.00013154072985461024),\n",
       " ('S.-J. Twu', 0.00013154072985461024),\n",
       " ('S. Chunsuttiwat', 0.00013154072985461024),\n",
       " ('P. Sawanpanyalert', 0.00013154072985461024),\n",
       " ('K. Ungchusak', 0.00013154072985461024),\n",
       " ('A. Chaovavanich', 0.00013154072985461024),\n",
       " ('Tony G. Rose', 0.00013122229738951916),\n",
       " ('Fan Li', 0.00013122229738951916),\n",
       " ('L.A. Barroso', 0.00013064570032731476),\n",
       " ('J. Dean', 0.00013064570032731476),\n",
       " ('U. Holzle', 0.00013064570032731476),\n",
       " ('Tsung-Yi Lin 1, Piotr Dollar 2, Ross Girshick 2, Kaiming He 2, Bharath Hariharan 2, Serge Belongie 1',\n",
       "  0.00012931029607660415),\n",
       " ('Bharath Hariharan 1, Pablo Arbelaez 2, Ross Girshick 3, Jitendra Malik 1',\n",
       "  0.00012929341431132092),\n",
       " ('Victor Corman 1, Marcel Müller 1, U. Costabel 2, J. Timm 2, Tabea Binger 1, Bernhard Meyer 1, P. Kreher 3, Erik Lattwein 4, Monika Eschbach-Bludau 1, A. Nitsche 3, T. Bleicker 1, O. Landt 5, Brunhilde Schweiger 3, Jan-Felix Drexler 1, Albert Osterhaus 6, Bart Haagmans 6, U. Dittmer 2, F. Bonin 2, Thorsten Wolff 3, Christian Drosten 1',\n",
       "  0.0001280285981781258),\n",
       " ('Olivier Delalleau', 0.00012617729438417223),\n",
       " ('John S. Denker', 0.00012589885416912383),\n",
       " ('Sara A. Solla', 0.00012589885416912383),\n",
       " ('Razvan Pascanu 1, Tomas Mikolov 2, Yoshua Bengio 1',\n",
       "  0.00012576232863219282),\n",
       " ('Diederik P. Kingma', 0.00012508458440195185),\n",
       " ('Hongzhou Lu 1, Charles W. Stratton 2, Yi Wei Tang 3',\n",
       "  0.00012424732024728194),\n",
       " ('Y. Freund', 0.0001241023024498561),\n",
       " ('R. Schapire', 0.0001241023024498561),\n",
       " ('Rui Qian 1, Tianjian Meng 2, Boqing Gong 2, Ming-Hsuan Yang',\n",
       "  0.00012402306462978147),\n",
       " ('Huisheng Wang 2, Serge J. Belongie 1, Yin Cui 2', 0.00012402306462978147),\n",
       " ('Bryan Perozzi', 0.00012320003807733392),\n",
       " ('Rami Al-Rfou', 0.00012320003807733392),\n",
       " ('Steven Skiena', 0.00012320003807733392),\n",
       " ('D. Martin', 0.0001223163904147774),\n",
       " ('C. Fowlkes', 0.0001223163904147774),\n",
       " ('D. Tal', 0.0001223163904147774),\n",
       " ('Michael Collins', 0.00012178984556117678),\n",
       " ('Avrim Blum', 0.00012072789288818593),\n",
       " ('Stephen Gould', 0.00012041706139065017),\n",
       " ('Richard Fulton', 0.00012041706139065017),\n",
       " ('Miron Livny', 0.00012036687160359876),\n",
       " ('Frederick Jelinek', 0.00012023928641315575),\n",
       " ('Allen Gersho 1, Robert M. Gray 2', 0.00012016596304806931),\n",
       " ('Luis von Ahn', 0.00011992802922808114),\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorities = dict([(i, a[authors_to_index[i]]) for i in authors_to_index])\n",
    "sorted(authorities.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aboriginal-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = pd.read_csv(\"data.csv\")\n",
    "user_profiles.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "boring-gnome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['algorithms', 'AI', 'computational science', 'computer architecture',\n",
       "       'computer engineering', 'computer graphics', 'computer hardware',\n",
       "       'computer network', 'computer security', 'computer vision',\n",
       "       'data mining', 'data science', 'database', 'distributed computing',\n",
       "       'embedded systems', 'human-computer interaction',\n",
       "       'information retrieval', 'internet privacy', 'knowledge management',\n",
       "       'library science', 'machine learning', 'multimedia',\n",
       "       'natural language processing', 'operating systems',\n",
       "       'parallel computing', 'pattern recognition', 'programming languages',\n",
       "       'real-time computing', 'simulation', 'software engineering',\n",
       "       'speech recognition', 'telecommunications',\n",
       "       'theoretical computer science', 'world wide web'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_profiles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "passive-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_papers_profiles(papers_data, topics):\n",
    "#     papers_vectors = np.zeros((len(papers_data), len(topics)))\n",
    "    papers_profiles = {}\n",
    "    for paper in papers_data:\n",
    "        vec = np.zeros(len(topics))\n",
    "        for i in range(len(topics)):\n",
    "            for topic in paper[\"related_topics\"]:\n",
    "                if topic.lower() == topics[i].lower():\n",
    "                    vec[i] = 1\n",
    "        papers_profiles[paper[\"id\"]] = vec\n",
    "#         papers_profiles[paper[\"id\"]] = np.array([1 if topics[i] in paper[\"related_topics\"] else 0 for i in range(len(topics))])\n",
    "    return papers_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conventional-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2981549002': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3105081694': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2950893734': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3119786062': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145339207': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153579005': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2194775991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963403868': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1836465849': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964308564': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963446712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1677182931': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2157331557': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2310919327': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2064675550': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1533861849': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001118548': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962835968': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008827533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2919115771': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2108598243': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007497549': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010604545': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008985036': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964121744': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117539524': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099471712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2963073614': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962793481': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963684088': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963373786': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963470893': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2618530766': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963341956': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3118608800': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963091558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034978746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100495367': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2187089797': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1665214252': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2072128103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2546302380': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121863487': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2952509347': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1652505363': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1614298861': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117130368': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141599568': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2132339004': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158139315': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1423339008': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1498436455': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1662133657': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1889268436': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131462252': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097117768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '639708223': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102605133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1903029394': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155893237': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1536680647': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963420686': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2965373594': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2970597249': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2923014074': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2911489562': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2095705004': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2146502635': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168231600': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '104184427': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '6908809': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1753482797': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2294059674': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1810943226': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2964199361': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1815076433': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153653739': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2183341477': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147768505': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2156387975': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963504252': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147880316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156163116': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963399829': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964311892': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2157364932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2128499899': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2007431958': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123716044': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143503258': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154890045': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2103452139': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2048060899': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1674799117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2136922672': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2025768430': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110798204': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1994197834': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2172174689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2903899730': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166867592': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3000413850': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132260239': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2026274122': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104548316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131262274': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2006434809': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3017468735': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2725497285': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1901129140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3106250896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001897055': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002108456': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005079553': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003668884': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002539152': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004318991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003465021': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004239190': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003573988': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2163922914': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160815625': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2022508996': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993882792': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2151103935': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2038721957': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2128017662': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110764733': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1782590233': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1576445103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145607950': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141282920': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115733720': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1528789833': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008818676': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004906315': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006643024': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006110666': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006354146': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003901880': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005656138': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004511262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008962515': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008452791': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3033453353': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034408674': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035275617': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034059415': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3033952286': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3036958556': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035464429': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3028749392': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3032185657': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3042098369': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037255629': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962739339': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2331128040': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964015378': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1514535095': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168356304': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1849277567': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1959608418': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1904365287': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964153729': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2133665775': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2340897893': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963420272': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2405756170': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963800363': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963836885': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2893749619': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2738588019': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2271840356': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '648143168': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2949416428': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963685250': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '830076066': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1487641199': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2911964244': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130325614': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2250539671': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131744502': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251939518': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963748441': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096192494': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2081580037': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165225968': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2806070179': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1861492603': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2565639579': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3116298410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3099495704': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3108316907': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3098053103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3132401450': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3125947392': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3145385912': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3154503084': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3122924117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2053186076': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2001141328': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2293063825': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121122425': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2032647857': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2021774695': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156718197': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125637308': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2139823104': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2137570937': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2157444450': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1742512077': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2116064496': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2134557905': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099866409': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2536626143': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156909104': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2296616510': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129131372': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119821739': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161969291': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162915993': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097018403': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166049352': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1639032689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154642048': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3017143921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100677568': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1535810436': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1603765807': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1569320505': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '94523489': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2178806388': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2126316555': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1515851193': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1625390266': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1502916507': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099587183': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2013391942': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2101355568': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132622533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2076063813': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145094598': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107941094': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3104097132': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1895577753': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1888005072': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1486649854': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2964321699': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2100664567': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2123024445': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130903752': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158847908': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107008379': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2914746235': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2173629880': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2885050925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158823144': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2163568299': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '179875071': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2147152072': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1632114991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1970689298': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1631260214': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096175520': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110485445': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1575350781': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2158195707': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2121227244': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1880262756': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '168564468': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2296073425': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158997610': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2004763266': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156515921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2067191022': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1574901103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1566135517': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2536208356': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2173213060': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1532325895': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2024165284': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1660390307': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2166706824': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1992419399': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '71795751': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097606805': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2151048449': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '36903255': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2091812280': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2127314673': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2056590938': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2111305191': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1558797106': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963542991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963911037': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2068730032': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2031489346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2088049833': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155541015': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1663973292': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2109255472': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '753012316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1825604117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147414309': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1872489089': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2899771611': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962784628': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1840435438': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963026768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2996035354': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2990704537': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " '3127550471': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3105966348': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100307207': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100345210': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963918774': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963846996': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2525127255': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962736243': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3104033643': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130158090': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2525778437': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2963756346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2135046866': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131241448': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2335728318': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2296319761': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3120740533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2798766386': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2610857016': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150102617': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167732364': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1992208280': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160218441': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1978394996': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141125852': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2118858186': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3141595720': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120420045': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '19621276': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1994616650': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2171928131': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251222643': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2006969979': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '196214544': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '189596042': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1554663460': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143612262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '44815768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2108677974': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120861206': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1828163288': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1905522558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2341457423': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1606347560': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2171865010': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122585011': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1508165687': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1973923101': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1986543644': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2116316001': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1517947178': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2161792612': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1549285799': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158388102': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096733369': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2016053056': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097726431': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3124955340': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2009570821': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1934019294': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1773803948': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160842254': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3021452258': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117400858': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159737176': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2027197837': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147345686': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '51975515': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2166469100': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2964274690': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2966415767': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963855133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3098350627': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3129973872': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2970902013': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2971149989': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035743198': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1554944419': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132914434': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1578099820': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962820688': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1999192586': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147860648': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2016589492': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1959983357': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2028629011': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2053127376': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167607759': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2177721432': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2075510082': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2138484437': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150355110': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1583833196': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143787696': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2057653135': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132152975': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112462566': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2090248140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1529008516': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2581275558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1597286183': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1507849272': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1971129545': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3121926921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147800946': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2895674046': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107878631': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2798813531': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098398123': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2176028050': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1966812932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2101926813': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991133427': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2048330959': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2144499799': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2136848157': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1735317348': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2079735306': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '3099873379': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147568880': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2057175746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159080219': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131686571': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158778629': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2567948266': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124914669': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153663612': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2613634265': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993845689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2109779438': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103626435': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125569215': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167967601': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153635508': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1902027874': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2105464873': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1802356529': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2075187489': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102409316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '11828546': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2144081223': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " '2111211467': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2470646526': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2306794997': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993577573': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2775086803': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119111857': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2025170735': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129542667': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1703839189': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2116586125': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1987783718': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111412754': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1963953102': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2170933940': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2126707939': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107277218': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2791599184': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2255243349': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2292021561': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2290466312': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2793022939': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100820722': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125251240': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107922358': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2127062009': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2084994773': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2149579937': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2090060897': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2030133843': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1967300023': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131419242': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143432233': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117958746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1531106656': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2157775267': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2028701043': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111704803': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2135163018': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2042074736': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2106882534': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2463755683': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2398786667': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2127949919': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1576737979': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2128788856': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2076620790': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123324969': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130141864': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991467275': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1982444609': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107053896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112147913': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2045002682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1852588318': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2163627712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140143765': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119775949': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010819577': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007273493': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007814559': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002764620': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005655936': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015190630': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009739970': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2195009776': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115555188': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2298153446': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2525468044': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1945961678': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099941783': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1948751323': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167510172': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1893585201': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148349024': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1909499787': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3027518954': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2792024998': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2955025503': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2257005270': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2999409984': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2999318660': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2999364275': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2909194930': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2991899552': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3000834295': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003951199': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1803784511': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002533507': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002715510': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001971765': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147166346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2149508011': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103503670': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2807736175': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2105637133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2889758689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2769543984': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140338292': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004280078': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103441770': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2141052558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2804822363': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2991491848': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2605343262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001388158': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004397688': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002533591': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1815575713': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2069251911': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096145431': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104595316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1968393246': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158899491': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110158442': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1999478155': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143516773': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2169551590': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2031342017': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2913932916': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103359087': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2033819227': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124386111': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154422044': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2012778485': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124404372': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1676552347': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124087378': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111308925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165497495': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1949116567': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160660844': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2952122856': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2022166150': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2177274842': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131846894': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1980911747': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104978738': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2172188317': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147717514': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162006472': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2138451337': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107034620': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156598602': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3097096317': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121647436': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2033419168': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123921160': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2137659841': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098693229': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125310925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2994340921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2006793117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963173190': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2295107390': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111993661': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1666447063': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1934863104': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166770390': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1587328194': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2293605478': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2055225264': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2050457084': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '181417509': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2612148268': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2217896605': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2045656233': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130416410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2030536784': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124351162': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2024046085': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168002178': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1484228140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035018050': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037451072': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034593359': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3033301213': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3021916232': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3031029566': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037552531': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037851904': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102634410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2800783955': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112136274': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2056155046': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2279340859': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2080286891': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162899218': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004802901': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2092969802': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005272159': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001465255': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001456238': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004668429': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3012211282': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008627141': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3025334942': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009992310': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008801544': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008928918': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3014051579': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013468450': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2786098272': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2021442163': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3025232310': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3028321619': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3027541845': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003637715': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009885589': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006961006': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3130405932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3144171767': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013893137': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004824173': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003464757': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009834387': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3011863580': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010096538': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006846061': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008443627': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013967887': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015571324': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006642361': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013594674': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3012789146': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001195213': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2105275554': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3011969828': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2770752141': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2263084061': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2175815746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991420168': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2073600962': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2084576921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008090866': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007940623': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3011242477': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010449299': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3018334611': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015704123': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3030968929': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158121945': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015636815': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3018724240': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008028633': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010930696': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3014294089': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2493916176': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2962711740': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2907492528': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100848837': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2963224980': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2962883549': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963184176': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100278010': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2905224888': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2918342466': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2796426482': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120419212': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2152826865': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145072179': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1576520375': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115763357': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161381512': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166851633': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2097268041': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963173382': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2951493172': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2171490498': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119196781': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3104819538': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '4919037': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2206858481': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120480077': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150165932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159269332': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2142276208': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2118217749': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2053691921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153777140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2912116903': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107790757': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158564760': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124731682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115838129': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2481240925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1947481528': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003301247': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962974533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2804078698': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035574324': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2982763192': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2890139949': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963841322': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2985068832': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963840672': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131975293': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125389028': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962897886': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2136504847': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1676820704': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2407712691': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158049734': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " '2122457239': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963207607': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963382180': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1479807131': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112076978': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1975846642': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2152761983': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2113242816': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1605688901': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120240539': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099968818': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2067885219': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1580948147': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145889472': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122922389': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2139427956': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2118020653': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2164019165': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1544827683': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125436846': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964267515': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962809918': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2171278097': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962790689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251818205': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251349042': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2169415915': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158164339': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '66838807': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2064630666': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1513873506': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2135094946': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102381086': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103318667': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2065157922': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1483126227': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2017668967': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1518768680': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '13823885': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1997063559': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2049633694': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1964724001': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121407732': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2725061391': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2315016682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035524453': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '343636949': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '219040644': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2808631503': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '602397586': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2619697695': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2962865004': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3101577715': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3161838454': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3119997354': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3159056052': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3139613640': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3143315506': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3135385999': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3136670918': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3139264293': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3126009523': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3122301478': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3131944163': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3102762742': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3137443434': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " '3132674603': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3134085768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3094246835': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3107857059': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3134032827': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3133595589': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3131871335': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963703618': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1821462560': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963069010': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3129170303': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3139419546': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3162945626': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3159394092': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3135138557': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3134210100': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3132869322': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3154313998': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991848143': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107636931': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122538988': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2047870719': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2070320140': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1513400187': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2019020850': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099741732': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2108384452': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2587818897': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123977795': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1594524188': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2086789740': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2970228278': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2076870593': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2137983211': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140196014': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1634005169': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3146803896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1971735090': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2913399920': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096710051': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2017977879': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166116275': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '5731987': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2142228262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2063971957': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2079782346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2089419199': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162604518': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155487652': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1995169133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2591802459': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2010581677': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121947440': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '108654854': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3029645440': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104095591': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150134853': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165874743': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154579312': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2122837498': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1511160855': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1585385982': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1979711143': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2113592823': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '200434350': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1497256448': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148694408': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '23758216': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100659887': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159174312': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2106346128': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2109574129': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963460103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '255556494': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1591018827': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162584119': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1746680969': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2083380015': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2114153178': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1547224907': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2101706260': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2124351082': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155511848': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160225842': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2295106276': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141376824': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1612003148': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122090912': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '205159212': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2165395308': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1989702938': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098947662': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2905573712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155759509': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140190241': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148603752': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2164278908': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129812935': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1746819321': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1570448133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2139212933': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145096794': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2078204800': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2116148865': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2099641086': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097323375': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2136235822': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147656689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2012365979': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096613063': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2050880896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129638195': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2050834445': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154332973': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2087347434': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1530699444': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168228682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1568787085': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '5594912': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2152473410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1992825118': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1608462934': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156539399': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2914885528': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2134731454': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165828254': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2113606819': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161516371': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1624854622': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1516111018': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1699734612': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165299997': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097571405': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2106334424': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168081761': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2017337590': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1999284878': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167101736': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165171393': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1501500081': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2073257493': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2021878536': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1490454746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115647291': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1505136099': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2163352848': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117812871': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1596324102': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1569296262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2075379212': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2042264548': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '114517082': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2604319603': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2025605741': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '1570963478': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098257210': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " '2096544401': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2010630450': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963433607': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '78077100': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2044212084': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2114791779': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2951781666': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2019207321': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161406034': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2749680651': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148885430': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2095227410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119717200': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1481420047': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2044535354': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131215403': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993717606': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2093229042': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143956139': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155399784': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2149723649': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2809684781': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2133321814': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1998442441': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = list(user_profiles.columns)\n",
    "papers_profiles = cal_papers_profiles(data, topics)\n",
    "papers_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "massive-haven",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2981549002': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3105081694': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2950893734': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3119786062': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145339207': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153579005': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2194775991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963403868': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1836465849': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964308564': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963446712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1677182931': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2157331557': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2310919327': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2064675550': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1533861849': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001118548': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962835968': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008827533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2919115771': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2108598243': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007497549': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010604545': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008985036': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964121744': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117539524': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099471712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2963073614': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962793481': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963684088': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963373786': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963470893': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2618530766': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963341956': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3118608800': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963091558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034978746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100495367': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2187089797': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1665214252': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2072128103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2546302380': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121863487': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2952509347': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1652505363': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1614298861': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117130368': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141599568': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2132339004': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158139315': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1423339008': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1498436455': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1662133657': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1889268436': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131462252': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097117768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '639708223': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102605133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1903029394': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155893237': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1536680647': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963420686': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2965373594': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2970597249': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2923014074': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2911489562': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2095705004': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2146502635': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168231600': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '104184427': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '6908809': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1753482797': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2294059674': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1810943226': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2964199361': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1815076433': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153653739': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2183341477': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147768505': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2156387975': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963504252': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147880316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156163116': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963399829': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964311892': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2157364932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2128499899': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2007431958': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123716044': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143503258': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154890045': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2103452139': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2048060899': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1674799117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2136922672': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2025768430': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110798204': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1994197834': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2172174689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2903899730': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166867592': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3000413850': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132260239': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2026274122': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104548316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131262274': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2006434809': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3017468735': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2725497285': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1901129140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3106250896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001897055': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002108456': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005079553': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003668884': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002539152': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004318991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003465021': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004239190': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003573988': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2163922914': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160815625': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2022508996': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993882792': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2151103935': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2038721957': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2128017662': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110764733': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1782590233': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1576445103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145607950': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141282920': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115733720': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1528789833': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008818676': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004906315': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006643024': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006110666': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006354146': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003901880': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005656138': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004511262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008962515': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008452791': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3033453353': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034408674': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035275617': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034059415': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3033952286': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3036958556': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035464429': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3028749392': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3032185657': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3042098369': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037255629': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962739339': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2331128040': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964015378': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1514535095': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168356304': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1849277567': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1959608418': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1904365287': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964153729': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2133665775': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2340897893': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963420272': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2405756170': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963800363': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963836885': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2893749619': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2738588019': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2271840356': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '648143168': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2949416428': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963685250': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '830076066': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1487641199': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2911964244': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130325614': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2250539671': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131744502': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251939518': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963748441': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096192494': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2081580037': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165225968': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2806070179': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1861492603': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2565639579': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3116298410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3099495704': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3108316907': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3098053103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3132401450': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3125947392': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3145385912': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3154503084': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3122924117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2053186076': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2001141328': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2293063825': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121122425': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2032647857': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2021774695': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156718197': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125637308': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2139823104': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2137570937': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2157444450': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1742512077': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2116064496': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2134557905': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099866409': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2536626143': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156909104': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2296616510': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129131372': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119821739': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161969291': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162915993': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097018403': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166049352': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1639032689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154642048': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3017143921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100677568': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1535810436': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1603765807': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1569320505': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '94523489': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2178806388': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2126316555': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1515851193': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1625390266': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1502916507': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099587183': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2013391942': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2101355568': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132622533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2076063813': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145094598': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107941094': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3104097132': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1895577753': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1888005072': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1486649854': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2964321699': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2100664567': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2123024445': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130903752': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158847908': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107008379': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2914746235': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2173629880': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2885050925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158823144': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2163568299': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '179875071': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2147152072': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1632114991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1970689298': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1631260214': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096175520': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110485445': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1575350781': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2158195707': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2121227244': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1880262756': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '168564468': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2296073425': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158997610': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2004763266': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156515921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2067191022': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1574901103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1566135517': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2536208356': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2173213060': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1532325895': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2024165284': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1660390307': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2166706824': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1992419399': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '71795751': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097606805': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2151048449': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '36903255': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2091812280': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2127314673': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2056590938': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2111305191': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1558797106': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963542991': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963911037': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2068730032': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2031489346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2088049833': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155541015': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1663973292': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2109255472': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '753012316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1825604117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147414309': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1872489089': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2899771611': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962784628': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1840435438': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963026768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2996035354': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2990704537': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " '3127550471': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3105966348': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100307207': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100345210': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963918774': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963846996': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2525127255': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962736243': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3104033643': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130158090': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2525778437': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2963756346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2135046866': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131241448': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2335728318': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2296319761': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3120740533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2798766386': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2610857016': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150102617': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167732364': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1992208280': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160218441': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1978394996': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141125852': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2118858186': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3141595720': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120420045': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '19621276': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1994616650': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2171928131': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251222643': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2006969979': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '196214544': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '189596042': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1554663460': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143612262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '44815768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2108677974': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120861206': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1828163288': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1905522558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2341457423': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1606347560': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2171865010': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122585011': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1508165687': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1973923101': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1986543644': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2116316001': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1517947178': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2161792612': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1549285799': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158388102': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096733369': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2016053056': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097726431': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3124955340': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2009570821': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1934019294': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1773803948': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160842254': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3021452258': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117400858': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159737176': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2027197837': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147345686': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '51975515': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2166469100': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2964274690': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2966415767': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963855133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3098350627': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3129973872': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2970902013': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2971149989': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035743198': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1554944419': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132914434': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1578099820': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962820688': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1999192586': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147860648': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2016589492': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1959983357': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2028629011': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2053127376': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167607759': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2177721432': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2075510082': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2138484437': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150355110': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1583833196': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143787696': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2057653135': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2132152975': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112462566': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2090248140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1529008516': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2581275558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1597286183': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1507849272': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1971129545': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3121926921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147800946': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2895674046': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107878631': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2798813531': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098398123': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2176028050': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1966812932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2101926813': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991133427': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2048330959': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2144499799': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2136848157': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1735317348': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2079735306': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '3099873379': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147568880': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2057175746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159080219': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131686571': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158778629': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2567948266': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124914669': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153663612': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2613634265': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993845689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2109779438': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103626435': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125569215': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167967601': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153635508': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1902027874': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2105464873': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1802356529': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2075187489': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102409316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '11828546': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2144081223': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " '2111211467': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2470646526': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2306794997': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993577573': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2775086803': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119111857': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2025170735': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129542667': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1703839189': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2116586125': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1987783718': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111412754': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1963953102': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2170933940': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2126707939': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107277218': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2791599184': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2255243349': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2292021561': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2290466312': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2793022939': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100820722': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125251240': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107922358': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2127062009': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2084994773': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2149579937': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2090060897': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2030133843': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1967300023': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131419242': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143432233': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117958746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1531106656': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2157775267': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2028701043': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111704803': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2135163018': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2042074736': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2106882534': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2463755683': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2398786667': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2127949919': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1576737979': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2128788856': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2076620790': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123324969': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130141864': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991467275': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1982444609': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107053896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112147913': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2045002682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1852588318': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2163627712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140143765': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119775949': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010819577': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007273493': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007814559': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002764620': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005655936': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015190630': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009739970': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2195009776': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115555188': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2298153446': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2525468044': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1945961678': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099941783': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1948751323': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167510172': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1893585201': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148349024': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1909499787': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3027518954': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2792024998': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2955025503': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2257005270': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2999409984': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2999318660': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2999364275': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2909194930': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2991899552': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3000834295': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003951199': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1803784511': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002533507': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002715510': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001971765': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147166346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2149508011': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103503670': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2807736175': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2105637133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2889758689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2769543984': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140338292': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004280078': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103441770': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2141052558': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2804822363': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2991491848': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2605343262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001388158': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004397688': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3002533591': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1815575713': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2069251911': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096145431': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104595316': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1968393246': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158899491': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2110158442': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1999478155': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143516773': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2169551590': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2031342017': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2913932916': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103359087': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2033819227': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124386111': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154422044': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2012778485': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124404372': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1676552347': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124087378': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111308925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165497495': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1949116567': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160660844': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2952122856': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2022166150': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2177274842': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131846894': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1980911747': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104978738': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2172188317': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147717514': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162006472': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2138451337': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107034620': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156598602': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3097096317': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121647436': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2033419168': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123921160': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2137659841': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098693229': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125310925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2994340921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2006793117': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963173190': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2295107390': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2111993661': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1666447063': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1934863104': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166770390': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1587328194': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2293605478': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2055225264': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '2050457084': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '181417509': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2612148268': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2217896605': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2045656233': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2130416410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2030536784': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124351162': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2024046085': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168002178': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1484228140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035018050': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037451072': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3034593359': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3033301213': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3021916232': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3031029566': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037552531': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3037851904': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102634410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2800783955': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112136274': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2056155046': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2279340859': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2080286891': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162899218': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004802901': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2092969802': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3005272159': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001465255': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001456238': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004668429': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3012211282': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008627141': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3025334942': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009992310': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008801544': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008928918': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3014051579': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013468450': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2786098272': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2021442163': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3025232310': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3028321619': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3027541845': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003637715': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009885589': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006961006': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3130405932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3144171767': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013893137': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3004824173': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003464757': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3009834387': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3011863580': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010096538': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006846061': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008443627': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013967887': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015571324': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3006642361': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3013594674': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3012789146': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3001195213': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2105275554': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3011969828': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2770752141': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2263084061': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2175815746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991420168': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2073600962': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2084576921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008090866': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3007940623': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3011242477': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010449299': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3018334611': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015704123': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3030968929': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158121945': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3015636815': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3018724240': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3008028633': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3010930696': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3014294089': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2493916176': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2962711740': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2907492528': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100848837': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2963224980': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2962883549': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963184176': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3100278010': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2905224888': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2918342466': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2796426482': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120419212': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2152826865': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145072179': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1576520375': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115763357': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161381512': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166851633': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2097268041': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963173382': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2951493172': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2171490498': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119196781': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3104819538': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '4919037': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2206858481': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120480077': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150165932': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159269332': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2142276208': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2118217749': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2053691921': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2153777140': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2912116903': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107790757': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158564760': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2124731682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115838129': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2481240925': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1947481528': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3003301247': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962974533': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2804078698': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035574324': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2982763192': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2890139949': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963841322': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2985068832': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963840672': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131975293': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125389028': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962897886': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2136504847': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1676820704': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2407712691': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158049734': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " '2122457239': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963207607': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963382180': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1479807131': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2112076978': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1975846642': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2152761983': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2113242816': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1605688901': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2120240539': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099968818': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2067885219': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1580948147': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145889472': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122922389': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2139427956': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2118020653': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2164019165': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1544827683': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2125436846': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2964267515': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962809918': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2171278097': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2962790689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251818205': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2251349042': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2169415915': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2158164339': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '66838807': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2064630666': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1513873506': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2135094946': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2102381086': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2103318667': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2065157922': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1483126227': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2017668967': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1518768680': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '13823885': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1997063559': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2049633694': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1964724001': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121407732': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2725061391': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2315016682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3035524453': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '343636949': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '219040644': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2808631503': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '602397586': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2619697695': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2962865004': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3101577715': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3161838454': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3119997354': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3159056052': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3139613640': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3143315506': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3135385999': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3136670918': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3139264293': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3126009523': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3122301478': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3131944163': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3102762742': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3137443434': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " '3132674603': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3134085768': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3094246835': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3107857059': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3134032827': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3133595589': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3131871335': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963703618': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1821462560': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963069010': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3129170303': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3139419546': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3162945626': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3159394092': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3135138557': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3134210100': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3132869322': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3154313998': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1991848143': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2107636931': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122538988': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2047870719': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2070320140': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1513400187': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2019020850': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2099741732': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2108384452': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2587818897': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2123977795': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1594524188': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2086789740': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2970228278': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2076870593': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2137983211': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140196014': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1634005169': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '3146803896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1971735090': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2913399920': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096710051': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2017977879': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2166116275': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '5731987': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2142228262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2063971957': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2079782346': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2089419199': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162604518': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155487652': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1995169133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2591802459': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2010581677': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2121947440': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '108654854': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '3029645440': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2104095591': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2150134853': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165874743': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154579312': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2122837498': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1511160855': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1585385982': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1979711143': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2113592823': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '200434350': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1497256448': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148694408': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '23758216': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2100659887': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2159174312': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2106346128': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2109574129': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963460103': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '255556494': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1591018827': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2162584119': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1746680969': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2083380015': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2114153178': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1547224907': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2101706260': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '2124351082': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155511848': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2160225842': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2295106276': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2141376824': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1612003148': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2122090912': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '205159212': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2165395308': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1989702938': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098947662': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2905573712': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155759509': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2140190241': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148603752': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2164278908': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129812935': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1746819321': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1570448133': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2139212933': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2145096794': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2078204800': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2116148865': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2099641086': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097323375': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2136235822': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2147656689': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2012365979': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2096613063': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2050880896': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2129638195': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2050834445': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2154332973': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2087347434': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1530699444': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168228682': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " '1568787085': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '5594912': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2152473410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1992825118': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1608462934': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2156539399': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2914885528': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2134731454': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165828254': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2113606819': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161516371': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1624854622': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1516111018': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '1699734612': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165299997': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2097571405': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2106334424': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2168081761': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2017337590': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1999284878': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2167101736': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2165171393': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1501500081': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2073257493': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2021878536': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1490454746': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2115647291': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1505136099': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2163352848': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2117812871': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1596324102': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1569296262': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2075379212': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2042264548': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '114517082': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2604319603': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2025605741': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " '1570963478': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2098257210': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " '2096544401': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2010630450': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2963433607': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '78077100': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2044212084': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2114791779': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2951781666': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2019207321': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2161406034': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2749680651': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2148885430': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2095227410': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2119717200': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1481420047': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2044535354': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2131215403': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1993717606': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2093229042': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2143956139': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2155399784': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2149723649': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " '2809684781': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '2133321814': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " '1998442441': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tested-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_paper(user_profile, papers_profiles):\n",
    "    scores = {}\n",
    "    valid_user_profile = sum(user_profile) != 0\n",
    "    for paper_id in papers_profiles:\n",
    "        if not valid_user_profile or sum(papers_profiles[paper_id]) == 0:\n",
    "            scores[paper_id] = 0\n",
    "            continue\n",
    "        norm_factor = np.linalg.norm(user_profile, 2) * np.linalg.norm(papers_profiles[paper_id], 2)\n",
    "        scores[paper_id] = user_profile.reshape((1, -1)).dot(papers_profiles[paper_id].reshape(-1, 1))[0, 0] / norm_factor\n",
    "    return sorted(scores.items(), key = lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "civil-percentage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2141599568', 0.7519334528499158),\n",
       " ('1753482797', 0.7519334528499158),\n",
       " ('1486649854', 0.7519334528499158),\n",
       " ('2163568299', 0.7519334528499158),\n",
       " ('1970689298', 0.7519334528499158),\n",
       " ('2056590938', 0.7519334528499158),\n",
       " ('2525778437', 0.7519334528499158),\n",
       " ('2251222643', 0.7519334528499158),\n",
       " ('1905522558', 0.7519334528499158),\n",
       " ('1508165687', 0.7519334528499158)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_paper(np.array(user_profiles.loc[1, :]), papers_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
